{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: Calibrating hyperparameters with weights and biases\n",
    "date: 2025-04-10\n",
    "author: Sofia Salazar\n",
    "categories:\n",
    "    - gene46100\n",
    "    - project\n",
    "    - notebook\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibrating hyperparameters with weights and biases\n",
    "\n",
    "### Overview\n",
    "\n",
    "* **Goal:** Learn how to use weights and biases to calibrate the hyperparameters of a DL model.\n",
    "\n",
    "[Weights and biases](https://wandb.ai/site/) is a platform used for AI developers to track, visualize and manage their ML models and experiments. The coolest part is that W&B allows you to log various performance metrics during training, like training and validation loss, test set correlations, etc. Additionally, it allows you to compare between different experiments or **versions** of your models. Making it easier to identify the best performing models and see which hyperparameter configuration is the optimal.\n",
    "\n",
    "In this notebook, we will focus on using W&B as a tool to help callibrate the hyperparameters of the TF binding prediction model to find an optimal solution. However, we encourage you to explore other applications that W&B offers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Installing W&B\n",
    "\n",
    "First install w&b in your environment with the following command, it should take only a couple of seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wandb onnx -Uq\n",
    "%pip install nbformat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Login to w&b\n",
    "\n",
    "If you don't already have a w&b account, sign up [here](https://wandb.ai/site/). Then, run the following command which will prompt you to insert your API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mssalazar_02\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define your regular functions\n",
    "\n",
    "Then, define your functions, this will remain unchanged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS device.\n"
     ]
    }
   ],
   "source": [
    "def get_device():\n",
    "  \"\"\"\n",
    "  Determines the device to use for PyTorch computations.\n",
    "\n",
    "  Prioritizes Metal Performance Shaders (MPS), then CUDA, then CPU.\n",
    "\n",
    "  Returns:\n",
    "    torch.device: The selected device.\n",
    "  \"\"\"\n",
    "  if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device.\")\n",
    "  elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA device.\")\n",
    "  else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device.\")\n",
    "  return device\n",
    "\n",
    "# Example usage:\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(seq):\n",
    "    \"\"\"\n",
    "    Given a DNA sequence, return its one-hot encoding\n",
    "    \"\"\"\n",
    "    # Make sure seq has only allowed bases\n",
    "    allowed = set(\"ACTGN\")\n",
    "    if not set(seq).issubset(allowed):\n",
    "        invalid = set(seq) - allowed\n",
    "        print(seq)\n",
    "        raise ValueError(f\"Sequence contains chars not in allowed DNA alphabet (ACGTN): {invalid}\")\n",
    "\n",
    "    # Dictionary returning one-hot encoding for each nucleotide\n",
    "    nuc_d = {'A':[1.0,0.0,0.0,0.0],\n",
    "             'C':[0.0,1.0,0.0,0.0],\n",
    "             'G':[0.0,0.0,1.0,0.0],\n",
    "             'T':[0.0,0.0,0.0,1.0],\n",
    "             'N':[0.0,0.0,0.0,0.0]}\n",
    "\n",
    "    # Create array from nucleotide sequence\n",
    "    vec=np.array([nuc_d[x] for x in seq],dtype='float32')\n",
    "\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_split(df, split_frac=0.8, verbose=False):\n",
    "    '''\n",
    "    Given a df of samples, randomly split indices between\n",
    "    train and test at the desired fraction\n",
    "    '''\n",
    "    cols = df.columns # original columns, use to clean up reindexed cols\n",
    "    df = df.reset_index()\n",
    "\n",
    "    # shuffle indices\n",
    "    idxs = list(range(df.shape[0]))\n",
    "    random.shuffle(idxs)\n",
    "\n",
    "    # split shuffled index list by split_frac\n",
    "    split = int(len(idxs)*split_frac)\n",
    "    train_idxs = idxs[:split]\n",
    "    test_idxs = idxs[split:]\n",
    "\n",
    "    # split dfs and return\n",
    "    train_df = df[df.index.isin(train_idxs)]\n",
    "    test_df = df[df.index.isin(test_idxs)]\n",
    "\n",
    "    return train_df[cols], test_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(sequences_df):\n",
    "    full_train_sequences, test_sequences = quick_split(sequences_df)\n",
    "    train_sequences, val_sequences = quick_split(full_train_sequences)\n",
    "    print(\"Train:\", train_sequences.shape)\n",
    "    print(\"Val:\", val_sequences.shape)\n",
    "    print(\"Test:\", test_sequences.shape)\n",
    "    return train_sequences, val_sequences, test_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_tensors(scores_df, sequences_df):\n",
    "    # split sequences in train, validation and test sets\n",
    "    train_sequences, val_sequences, test_sequences = split_sequences(sequences_df)\n",
    "    # get scores for each set of sequences\n",
    "    train_scores = scores_df[train_sequences['window_name'].to_list()].transpose().values.astype('float32') # shape is (num_sequences, 300)\n",
    "    val_scores = scores_df[val_sequences['window_name'].to_list()].transpose().values.astype('float32')\n",
    "    test_scores = scores_df[test_sequences['window_name'].to_list()].transpose().values.astype('float32')\n",
    "\n",
    "    train_scores = torch.tensor(train_scores, dtype=torch.float32).to(device)\n",
    "    val_scores = torch.tensor(val_scores, dtype=torch.float32).to(device)\n",
    "    test_scores = torch.tensor(test_scores, dtype=torch.float32).to(device)\n",
    "\n",
    "    # get one hot encoded sequences for each set\n",
    "    train_one_hot = [one_hot_encode(seq) for seq in train_sequences['sequence'].to_list()]\n",
    "    train_sequences_tensor = torch.tensor(np.stack(train_one_hot))\n",
    "\n",
    "    val_one_hot = [one_hot_encode(seq) for seq in val_sequences['sequence'].to_list()]\n",
    "    val_sequences_tensor = torch.tensor(np.stack(val_one_hot))\n",
    "\n",
    "    test_one_hot = [one_hot_encode(seq) for seq in test_sequences['sequence'].to_list()]\n",
    "    test_sequences_tensor = torch.tensor(np.stack(test_one_hot))\n",
    "\n",
    "    return train_scores, train_sequences_tensor, val_scores, val_sequences_tensor, test_scores, test_sequences_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(predictors, targets, batch_size, is_train = True):\n",
    "    '''\n",
    "    features: one hot encoded sequences\n",
    "    targets: sequence scores\n",
    "    batch_size\n",
    "    is_train: if True, data is reshuffled at every epoch\n",
    "    '''\n",
    "\n",
    "    dataset = torch.utils.data.TensorDataset(predictors, targets)\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle = is_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNA_CNN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 seq_len,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=10,\n",
    "                 add_sigmoid=False):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.add_sigmoid = add_sigmoid\n",
    "        # Define layers individually\n",
    "        self.conv = nn.Conv1d(in_channels = 4, out_channels = num_filters, kernel_size=kernel_size)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.linear = nn.Linear(num_filters*(seq_len-kernel_size+1), 300)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, xb):\n",
    "        # reshape view to batch_size x 4channel x seq_len\n",
    "        # permute to put channel in correct order\n",
    "        xb = xb.permute(0,2,1) # (batch_size, 300, 4) to (batch_size, 4, 300)\n",
    "\n",
    "        # Apply layers step by step\n",
    "        x = self.conv(xb)\n",
    "        x = self.relu(x)\n",
    "        x = x.flatten(1)  # flatten all dimensions except batch\n",
    "        out = self.linear(x)\n",
    "        \n",
    "        if self.add_sigmoid:\n",
    "            out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(model, loss_func, x_batch, y_batch, opt=None):\n",
    "    xb_out = model(x_batch.to(torch.float32))\n",
    "\n",
    "    loss = loss_func(xb_out, y_batch)\n",
    "\n",
    "    if opt is not None: # backpropagate if train step (optimizer given)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "\n",
    "    return loss.item(), len(x_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dl, loss_func, device, opt):\n",
    "\n",
    "    model.train()\n",
    "    tl = [] # train losses\n",
    "    ns = [] # batch sizes, n\n",
    "\n",
    "    # loop through batches\n",
    "    for x_batch, y_batch in train_dl:\n",
    "\n",
    "        x_batch, y_batch = x_batch.to(device),y_batch.to(device)\n",
    "\n",
    "        t, n = process_batch(model, loss_func, x_batch, y_batch, opt=opt)\n",
    "\n",
    "        # collect train loss and batch sizes\n",
    "        tl.append(t)\n",
    "        ns.append(n)\n",
    "\n",
    "    # average the losses over all batches\n",
    "    train_loss = np.sum(np.multiply(tl, ns)) / np.sum(ns)\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_epoch(model, val_dl, loss_func, device):\n",
    "\n",
    "    # Set model to Evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        vl = [] # val losses\n",
    "        ns = [] # batch sizes, n\n",
    "\n",
    "        # loop through validation DataLoader\n",
    "        for x_batch, y_batch in val_dl:\n",
    "\n",
    "            x_batch, y_batch = x_batch.to(device),y_batch.to(device)\n",
    "\n",
    "            v, n = process_batch(model, loss_func, x_batch, y_batch)\n",
    "\n",
    "            # collect val loss and batch sizes\n",
    "            vl.append(v)\n",
    "            ns.append(n)\n",
    "\n",
    "    # average the losses over all batches\n",
    "    val_loss = np.sum(np.multiply(vl, ns)) / np.sum(ns)\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Modify the `train_loop()` function\n",
    "\n",
    "The only function that we need to change is the `train_loop` function, because here is where we are recovering the parameters that we want to track with w&b. We will use `wandb.log` and create a dictionary with the parmeters that we want to track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(epochs, model, loss_func, opt, train_dl, val_dl, device):\n",
    "\n",
    "    # keep track of losses\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    # loop through epochs\n",
    "    for epoch in range(epochs):\n",
    "        # take a training step\n",
    "        train_loss = train_epoch(model,train_dl,loss_func,device,opt)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # take a validation step\n",
    "        val_loss = val_epoch(model,val_dl,loss_func,device)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} | train loss: {train_loss:.3f} | val loss: {val_loss:.3f}\")\n",
    "        wandb.log({\"epoch\": epoch + 1,\n",
    "                   \"train_loss\": train_loss,\n",
    "                   \"val_loss\": val_loss})\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the `train_model` function. We omit the `plot_curves` function since all performance metrics will be tracked on w&b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dl,val_dl,model,device, lr=0.01, epochs=50, lossf=None,opt=None):\n",
    "\n",
    "    # define optimizer\n",
    "    if opt:\n",
    "        optimizer = opt(model.parameters(), lr=lr)\n",
    "    else: # if no opt provided, just use SGD\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "    # define loss function\n",
    "    if lossf:\n",
    "        loss_func = lossf\n",
    "    else: # if no loss function provided, just use MSE\n",
    "        loss_func = torch.nn.MSELoss()\n",
    "\n",
    "    # run the training loop\n",
    "    train_losses, val_losses = train_loop(\n",
    "                                epochs,\n",
    "                                model,\n",
    "                                loss_func,\n",
    "                                optimizer,\n",
    "                                train_dl,\n",
    "                                val_dl,\n",
    "                                device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Also track the correlation metrics of the test set\n",
    "\n",
    "As a way to evaluate each model, let's modify the `test_model()` function so that wand also keeps track of the performance metrics. In this case the metrics are `pearson_per_sample`, `test_pearson_r` and `best_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_features, test_targets):\n",
    "  model.eval()\n",
    "  predictions = model(test_features.to(torch.float32).to(device)).detach().cpu().numpy()\n",
    "  observations = test_targets.cpu().numpy()\n",
    "  pearson_per_sample = np.array([pearsonr(predictions[i], observations[i])[0] for i in range(300)])\n",
    "  test_pearsonr = pearson_per_sample.mean()\n",
    "  best_test = pearson_per_sample.max()\n",
    "  wandb.log({'test_avg_pearsonr': test_pearsonr,\n",
    "             'beast_pearsonr': best_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Define the `sweep` configuration\n",
    "\n",
    "A `sweep` is the training and testing of a single model with a given configuration of hyperparameters. with `wandb.sweep` we define the set of hyperparameters to test, which will be then combined in different configurations each sweep.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'metric': {'name': 'test_avg_pearsonr', 'goal': 'maximize'},\n",
    "    'parameters': {\n",
    "        'num_filters': {'values': [4, 16]},\n",
    "        'kernel_size': {'values': [5, 10]},\n",
    "        'add_sigmoid': {'values': [True, False],},\n",
    "        'learning_rate':{'values':[0.1, 0.05]},\n",
    "        'batch_size': {'values':[16, 32, 64]},\n",
    "        'optimizer': {'values': ['SGD','Adam']}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a project ID for your model, all your tests will be saved in this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: yfwcc62j\n",
      "Sweep URL: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"DNA_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Get the training, val and test sets ready for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = '/Users/sofiasalazar/Library/CloudStorage/Box-Box/imlab-data/Courses/AI-in-Genomics-2025/data/'\n",
    "sequences = pd.read_csv(os.path.join(DIR, 'chr22_sequences.txt.gz'), sep=\"\\t\", compression='gzip')\n",
    "scores = pd.read_csv(os.path.join(DIR, 'chr22_scores.txt.gz'), sep=\"\\t\", compression='gzip',dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (14808, 2)\n",
      "Val: (3703, 2)\n",
      "Test: (4628, 2)\n"
     ]
    }
   ],
   "source": [
    "train_scores, train_sequences_tensor, val_scores, val_sequences_tensor, test_scores, test_sequences_tensor = get_data_tensors(scores, sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Initialize the sweep\n",
    "\n",
    "With `wandb.init` we initialize one sweep and what we want to do in each. This consists on\n",
    "\n",
    "1. Loading a configuration of hyperparameters with `wandb.config`\n",
    "\n",
    "2. Loading the model\n",
    "\n",
    "3. Telling wandb to track the training with `wandb.watch`\n",
    "\n",
    "4. Create the dataloaders\n",
    "\n",
    "5. Train and test the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sweep():\n",
    "    with wandb.init(project = \"DNA_model\"):\n",
    "        config = wandb.config\n",
    "        model = DNA_CNN(seq_len=300, num_filters=config.num_filters, kernel_size=config.kernel_size, add_sigmoid=config.add_sigmoid).to(device)\n",
    "        wandb.watch(model, log=\"all\", log_freq=10) # log all: logs all gradients and parameters, every log_freq number of training steps (batches) \n",
    "        train_loader = create_dataloader(train_sequences_tensor, train_scores, batch_size=config.batch_size)\n",
    "        val_loader = create_dataloader(val_sequences_tensor, val_scores, batch_size=config.batch_size, is_train=False)\n",
    "        if config.optimizer == 'SGD':\n",
    "            opt = torch.optim.SGD\n",
    "        else: opt = torch.optim.Adam \n",
    "\n",
    "        train_model(train_loader, val_loader, model, device, epochs=30, lr = config.learning_rate, opt=opt)\n",
    "        test_model(model, test_sequences_tensor, test_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train with `wandb.agent`, the argument `count` is the number of combinations of hyperparameters I want to try. The maximum in my case is 240 combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0un6by39 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadd_sigmoid: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: SGD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'DNA_model' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151021-0un6by39</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ssalazar_02/DNA_model/runs/0un6by39' target=\"_blank\">stellar-sweep-1</a></strong> to <a href='https://wandb.ai/ssalazar_02/DNA_model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ssalazar_02/DNA_model' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ssalazar_02/DNA_model/runs/0un6by39' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/runs/0un6by39</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train loss: 2.423 | val loss: 2.452\n",
      "Epoch 2 | train loss: 2.417 | val loss: 2.447\n",
      "Epoch 3 | train loss: 2.402 | val loss: 2.417\n",
      "Epoch 4 | train loss: 2.352 | val loss: 2.363\n",
      "Epoch 5 | train loss: 2.314 | val loss: 2.344\n",
      "Epoch 6 | train loss: 2.297 | val loss: 2.334\n",
      "Epoch 7 | train loss: 2.287 | val loss: 2.328\n",
      "Epoch 8 | train loss: 2.279 | val loss: 2.324\n",
      "Epoch 9 | train loss: 2.273 | val loss: 2.321\n",
      "Epoch 10 | train loss: 2.267 | val loss: 2.318\n",
      "Epoch 11 | train loss: 2.262 | val loss: 2.317\n",
      "Epoch 12 | train loss: 2.257 | val loss: 2.313\n",
      "Epoch 13 | train loss: 2.252 | val loss: 2.311\n",
      "Epoch 14 | train loss: 2.246 | val loss: 2.311\n",
      "Epoch 15 | train loss: 2.240 | val loss: 2.308\n",
      "Epoch 16 | train loss: 2.235 | val loss: 2.305\n",
      "Epoch 17 | train loss: 2.229 | val loss: 2.303\n",
      "Epoch 18 | train loss: 2.223 | val loss: 2.301\n",
      "Epoch 19 | train loss: 2.217 | val loss: 2.301\n",
      "Epoch 20 | train loss: 2.211 | val loss: 2.299\n",
      "Epoch 21 | train loss: 2.205 | val loss: 2.298\n",
      "Epoch 22 | train loss: 2.199 | val loss: 2.296\n",
      "Epoch 23 | train loss: 2.193 | val loss: 2.295\n",
      "Epoch 24 | train loss: 2.187 | val loss: 2.295\n",
      "Epoch 25 | train loss: 2.181 | val loss: 2.294\n",
      "Epoch 26 | train loss: 2.176 | val loss: 2.294\n",
      "Epoch 27 | train loss: 2.170 | val loss: 2.293\n",
      "Epoch 28 | train loss: 2.165 | val loss: 2.293\n",
      "Epoch 29 | train loss: 2.160 | val loss: 2.292\n",
      "Epoch 30 | train loss: 2.155 | val loss: 2.292\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>beast_pearsonr</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>test_avg_pearsonr</td><td>▁</td></tr><tr><td>train_loss</td><td>██▇▆▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▆▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>beast_pearsonr</td><td>0.68177</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>test_avg_pearsonr</td><td>0.23191</td></tr><tr><td>train_loss</td><td>2.15465</td></tr><tr><td>val_loss</td><td>2.29175</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">stellar-sweep-1</strong> at: <a href='https://wandb.ai/ssalazar_02/DNA_model/runs/0un6by39' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/runs/0un6by39</a><br> View project at: <a href='https://wandb.ai/ssalazar_02/DNA_model' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250407_151021-0un6by39/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bt4sqsaj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadd_sigmoid: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: SGD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'DNA_model' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151218-bt4sqsaj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ssalazar_02/DNA_model/runs/bt4sqsaj' target=\"_blank\">vivid-sweep-2</a></strong> to <a href='https://wandb.ai/ssalazar_02/DNA_model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ssalazar_02/DNA_model' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ssalazar_02/DNA_model/runs/bt4sqsaj' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/runs/bt4sqsaj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train loss: 2.387 | val loss: 2.354\n",
      "Epoch 2 | train loss: 2.291 | val loss: 2.311\n",
      "Epoch 3 | train loss: 2.259 | val loss: 2.314\n",
      "Epoch 4 | train loss: 2.244 | val loss: 2.289\n",
      "Epoch 5 | train loss: 2.229 | val loss: 2.259\n",
      "Epoch 6 | train loss: 2.128 | val loss: 2.137\n",
      "Epoch 7 | train loss: 2.045 | val loss: 2.091\n",
      "Epoch 8 | train loss: 1.995 | val loss: 2.044\n",
      "Epoch 9 | train loss: 1.934 | val loss: 1.987\n",
      "Epoch 10 | train loss: 1.873 | val loss: 1.942\n",
      "Epoch 11 | train loss: 1.815 | val loss: 1.886\n",
      "Epoch 12 | train loss: 1.757 | val loss: 1.846\n",
      "Epoch 13 | train loss: 1.708 | val loss: 1.807\n",
      "Epoch 14 | train loss: 1.666 | val loss: 1.774\n",
      "Epoch 15 | train loss: 1.632 | val loss: 1.746\n",
      "Epoch 16 | train loss: 1.602 | val loss: 1.724\n",
      "Epoch 17 | train loss: 1.576 | val loss: 1.702\n",
      "Epoch 18 | train loss: 1.552 | val loss: 1.684\n",
      "Epoch 19 | train loss: 1.530 | val loss: 1.667\n",
      "Epoch 20 | train loss: 1.509 | val loss: 1.650\n",
      "Epoch 21 | train loss: 1.489 | val loss: 1.637\n",
      "Epoch 22 | train loss: 1.473 | val loss: 1.635\n",
      "Epoch 23 | train loss: 1.458 | val loss: 1.622\n",
      "Epoch 24 | train loss: 1.446 | val loss: 1.608\n",
      "Epoch 25 | train loss: 1.435 | val loss: 1.603\n",
      "Epoch 26 | train loss: 1.425 | val loss: 1.599\n",
      "Epoch 27 | train loss: 1.416 | val loss: 1.593\n",
      "Epoch 28 | train loss: 1.407 | val loss: 1.586\n",
      "Epoch 29 | train loss: 1.397 | val loss: 1.584\n",
      "Epoch 30 | train loss: 1.388 | val loss: 1.580\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>beast_pearsonr</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>test_avg_pearsonr</td><td>▁</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▆▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>███▇▇▆▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>beast_pearsonr</td><td>0.92833</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>test_avg_pearsonr</td><td>0.57389</td></tr><tr><td>train_loss</td><td>1.38804</td></tr><tr><td>val_loss</td><td>1.57982</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vivid-sweep-2</strong> at: <a href='https://wandb.ai/ssalazar_02/DNA_model/runs/bt4sqsaj' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/runs/bt4sqsaj</a><br> View project at: <a href='https://wandb.ai/ssalazar_02/DNA_model' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250407_151218-bt4sqsaj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7ca57ikz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadd_sigmoid: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: SGD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'DNA_model' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151414-7ca57ikz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ssalazar_02/DNA_model/runs/7ca57ikz' target=\"_blank\">unique-sweep-3</a></strong> to <a href='https://wandb.ai/ssalazar_02/DNA_model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ssalazar_02/DNA_model' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ssalazar_02/DNA_model/runs/7ca57ikz' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/runs/7ca57ikz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train loss: 2.423 | val loss: 2.451\n",
      "Epoch 2 | train loss: 2.415 | val loss: 2.447\n",
      "Epoch 3 | train loss: 2.408 | val loss: 2.436\n",
      "Epoch 4 | train loss: 2.385 | val loss: 2.404\n",
      "Epoch 5 | train loss: 2.345 | val loss: 2.370\n",
      "Epoch 6 | train loss: 2.316 | val loss: 2.350\n",
      "Epoch 7 | train loss: 2.300 | val loss: 2.340\n",
      "Epoch 8 | train loss: 2.289 | val loss: 2.336\n",
      "Epoch 9 | train loss: 2.281 | val loss: 2.328\n",
      "Epoch 10 | train loss: 2.274 | val loss: 2.325\n",
      "Epoch 11 | train loss: 2.268 | val loss: 2.323\n",
      "Epoch 12 | train loss: 2.262 | val loss: 2.321\n",
      "Epoch 13 | train loss: 2.257 | val loss: 2.318\n",
      "Epoch 14 | train loss: 2.251 | val loss: 2.318\n",
      "Epoch 15 | train loss: 2.247 | val loss: 2.316\n",
      "Epoch 16 | train loss: 2.242 | val loss: 2.316\n",
      "Epoch 17 | train loss: 2.237 | val loss: 2.315\n",
      "Epoch 18 | train loss: 2.232 | val loss: 2.315\n",
      "Epoch 19 | train loss: 2.227 | val loss: 2.313\n",
      "Epoch 20 | train loss: 2.222 | val loss: 2.314\n",
      "Epoch 21 | train loss: 2.217 | val loss: 2.314\n",
      "Epoch 22 | train loss: 2.212 | val loss: 2.314\n",
      "Epoch 23 | train loss: 2.206 | val loss: 2.313\n",
      "Epoch 24 | train loss: 2.201 | val loss: 2.314\n",
      "Epoch 25 | train loss: 2.196 | val loss: 2.315\n",
      "Epoch 26 | train loss: 2.191 | val loss: 2.315\n",
      "Epoch 27 | train loss: 2.186 | val loss: 2.315\n",
      "Epoch 28 | train loss: 2.181 | val loss: 2.315\n",
      "Epoch 29 | train loss: 2.176 | val loss: 2.315\n",
      "Epoch 30 | train loss: 2.171 | val loss: 2.316\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>beast_pearsonr</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>test_avg_pearsonr</td><td>▁</td></tr><tr><td>train_loss</td><td>███▇▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▇▆▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>beast_pearsonr</td><td>0.6957</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>test_avg_pearsonr</td><td>0.21384</td></tr><tr><td>train_loss</td><td>2.17106</td></tr><tr><td>val_loss</td><td>2.31633</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">unique-sweep-3</strong> at: <a href='https://wandb.ai/ssalazar_02/DNA_model/runs/7ca57ikz' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/runs/7ca57ikz</a><br> View project at: <a href='https://wandb.ai/ssalazar_02/DNA_model' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250407_151414-7ca57ikz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2pk9jgsh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadd_sigmoid: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'DNA_model' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151710-2pk9jgsh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ssalazar_02/DNA_model/runs/2pk9jgsh' target=\"_blank\">breezy-sweep-4</a></strong> to <a href='https://wandb.ai/ssalazar_02/DNA_model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ssalazar_02/DNA_model' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ssalazar_02/DNA_model/runs/2pk9jgsh' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/runs/2pk9jgsh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train loss: 8.430 | val loss: 2.453\n",
      "Epoch 2 | train loss: 2.422 | val loss: 2.457\n",
      "Epoch 3 | train loss: 2.423 | val loss: 2.455\n",
      "Epoch 4 | train loss: 2.424 | val loss: 2.457\n",
      "Epoch 5 | train loss: 2.425 | val loss: 2.457\n",
      "Epoch 6 | train loss: 2.425 | val loss: 2.459\n",
      "Epoch 7 | train loss: 2.427 | val loss: 2.464\n",
      "Epoch 8 | train loss: 2.428 | val loss: 2.463\n",
      "Epoch 9 | train loss: 2.429 | val loss: 2.461\n",
      "Epoch 10 | train loss: 2.430 | val loss: 2.463\n",
      "Epoch 11 | train loss: 2.429 | val loss: 2.459\n",
      "Epoch 12 | train loss: 2.430 | val loss: 2.465\n",
      "Epoch 13 | train loss: 2.430 | val loss: 2.461\n",
      "Epoch 14 | train loss: 2.431 | val loss: 2.458\n",
      "Epoch 15 | train loss: 2.430 | val loss: 2.462\n",
      "Epoch 16 | train loss: 2.431 | val loss: 2.464\n",
      "Epoch 17 | train loss: 2.431 | val loss: 2.463\n",
      "Epoch 18 | train loss: 2.430 | val loss: 2.461\n",
      "Epoch 19 | train loss: 2.431 | val loss: 2.458\n",
      "Epoch 20 | train loss: 2.430 | val loss: 2.466\n",
      "Epoch 21 | train loss: 2.431 | val loss: 2.466\n",
      "Epoch 22 | train loss: 2.431 | val loss: 2.463\n",
      "Epoch 23 | train loss: 2.431 | val loss: 2.463\n",
      "Epoch 24 | train loss: 2.430 | val loss: 2.470\n",
      "Epoch 25 | train loss: 2.431 | val loss: 2.460\n",
      "Epoch 26 | train loss: 2.431 | val loss: 2.459\n",
      "Epoch 27 | train loss: 2.431 | val loss: 2.463\n",
      "Epoch 28 | train loss: 2.431 | val loss: 2.462\n",
      "Epoch 29 | train loss: 2.431 | val loss: 2.462\n",
      "Epoch 30 | train loss: 2.431 | val loss: 2.461\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>beast_pearsonr</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>test_avg_pearsonr</td><td>▁</td></tr><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>▁▃▂▂▃▄▆▅▄▅▄▆▄▃▅▅▅▄▃▆▆▅▅█▄▃▅▅▅▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>beast_pearsonr</td><td>0.35183</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>test_avg_pearsonr</td><td>-0.00231</td></tr><tr><td>train_loss</td><td>2.43134</td></tr><tr><td>val_loss</td><td>2.46144</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">breezy-sweep-4</strong> at: <a href='https://wandb.ai/ssalazar_02/DNA_model/runs/2pk9jgsh' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/runs/2pk9jgsh</a><br> View project at: <a href='https://wandb.ai/ssalazar_02/DNA_model' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250407_151710-2pk9jgsh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ohia2jz9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadd_sigmoid: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: Adam\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'DNA_model' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151916-ohia2jz9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ssalazar_02/DNA_model/runs/ohia2jz9' target=\"_blank\">vibrant-sweep-5</a></strong> to <a href='https://wandb.ai/ssalazar_02/DNA_model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ssalazar_02/DNA_model' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ssalazar_02/DNA_model/runs/ohia2jz9' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/runs/ohia2jz9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train loss: 2.520 | val loss: 2.554\n",
      "Epoch 2 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 3 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 4 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 5 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 6 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 7 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 8 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 9 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 10 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 11 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 12 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 13 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 14 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 15 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 16 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 17 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 18 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 19 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 20 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 21 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 22 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 23 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 24 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 25 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 26 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 27 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 28 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 29 | train loss: 2.520 | val loss: 2.556\n",
      "Epoch 30 | train loss: 2.520 | val loss: 2.556\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>beast_pearsonr</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>test_avg_pearsonr</td><td>▁</td></tr><tr><td>train_loss</td><td>▁█████████████████████████████</td></tr><tr><td>val_loss</td><td>▁█████████████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>beast_pearsonr</td><td>0.20144</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>test_avg_pearsonr</td><td>-0.00629</td></tr><tr><td>train_loss</td><td>2.52015</td></tr><tr><td>val_loss</td><td>2.5559</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">vibrant-sweep-5</strong> at: <a href='https://wandb.ai/ssalazar_02/DNA_model/runs/ohia2jz9' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/runs/ohia2jz9</a><br> View project at: <a href='https://wandb.ai/ssalazar_02/DNA_model' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250407_151916-ohia2jz9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: gmg0gdbw with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tadd_sigmoid: False\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tkernel_size: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: SGD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'DNA_model' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_152128-gmg0gdbw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ssalazar_02/DNA_model/runs/gmg0gdbw' target=\"_blank\">upbeat-sweep-6</a></strong> to <a href='https://wandb.ai/ssalazar_02/DNA_model' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ssalazar_02/DNA_model' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ssalazar_02/DNA_model/runs/gmg0gdbw' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/runs/gmg0gdbw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | train loss: 2.424 | val loss: 2.440\n",
      "Epoch 2 | train loss: 2.370 | val loss: 2.371\n",
      "Epoch 3 | train loss: 2.307 | val loss: 2.336\n",
      "Epoch 4 | train loss: 2.278 | val loss: 2.320\n",
      "Epoch 5 | train loss: 2.260 | val loss: 2.312\n",
      "Epoch 6 | train loss: 2.245 | val loss: 2.304\n",
      "Epoch 7 | train loss: 2.231 | val loss: 2.297\n",
      "Epoch 8 | train loss: 2.216 | val loss: 2.294\n",
      "Epoch 9 | train loss: 2.202 | val loss: 2.290\n",
      "Epoch 10 | train loss: 2.185 | val loss: 2.285\n",
      "Epoch 11 | train loss: 2.168 | val loss: 2.281\n",
      "Epoch 12 | train loss: 2.148 | val loss: 2.271\n",
      "Epoch 13 | train loss: 2.119 | val loss: 2.255\n",
      "Epoch 14 | train loss: 2.080 | val loss: 2.228\n",
      "Epoch 15 | train loss: 2.039 | val loss: 2.208\n",
      "Epoch 16 | train loss: 2.001 | val loss: 2.185\n",
      "Epoch 17 | train loss: 1.966 | val loss: 2.169\n",
      "Epoch 18 | train loss: 1.933 | val loss: 2.157\n",
      "Epoch 19 | train loss: 1.903 | val loss: 2.140\n",
      "Epoch 20 | train loss: 1.875 | val loss: 2.126\n",
      "Epoch 21 | train loss: 1.848 | val loss: 2.113\n",
      "Epoch 22 | train loss: 1.822 | val loss: 2.107\n",
      "Epoch 23 | train loss: 1.799 | val loss: 2.098\n",
      "Epoch 24 | train loss: 1.776 | val loss: 2.092\n",
      "Epoch 25 | train loss: 1.755 | val loss: 2.086\n",
      "Epoch 26 | train loss: 1.736 | val loss: 2.081\n",
      "Epoch 27 | train loss: 1.717 | val loss: 2.075\n",
      "Epoch 28 | train loss: 1.699 | val loss: 2.075\n",
      "Epoch 29 | train loss: 1.682 | val loss: 2.069\n",
      "Epoch 30 | train loss: 1.666 | val loss: 2.068\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>beast_pearsonr</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>test_avg_pearsonr</td><td>▁</td></tr><tr><td>train_loss</td><td>██▇▇▆▆▆▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▆▆▆▅▅▅▅▅▅▅▅▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>beast_pearsonr</td><td>0.80236</td></tr><tr><td>epoch</td><td>30</td></tr><tr><td>test_avg_pearsonr</td><td>0.38552</td></tr><tr><td>train_loss</td><td>1.66555</td></tr><tr><td>val_loss</td><td>2.06831</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">upbeat-sweep-6</strong> at: <a href='https://wandb.ai/ssalazar_02/DNA_model/runs/gmg0gdbw' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model/runs/gmg0gdbw</a><br> View project at: <a href='https://wandb.ai/ssalazar_02/DNA_model' target=\"_blank\">https://wandb.ai/ssalazar_02/DNA_model</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250407_152128-gmg0gdbw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# wandb.agent(sweep_id, train_sweep, count=240)\n",
    "wandb.agent(sweep_id, train_sweep, count=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchGPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
