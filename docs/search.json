[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning in Genomics Course Material",
    "section": "",
    "text": "This page contains material for the GENE46100 Deep learning in genomics course.\nFind the 2025 syllabus here.\nEdit github source here\n\nSee all notes here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTF binding prediction challenge\n\n\n\n\n\n\ngene46100\n\n\nproject\n\n\n\nCompetition details for TF binding prediction challenge\n\n\n\n\n\nApr 8, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 2\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nApr 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nUpdated - DNA score prediction with Pytorch\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nApr 4, 2025\n\n\nErin Wilson\n\n\n\n\n\n\n\n\n\n\n\n\nTF Binding prediction project\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2025\n\n\nSofia Salazar\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 1\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMar 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nQuick introduction to deep learning\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nMar 25, 2025\n\n\nBoxiang Liu, modified by Haky Im\n\n\n\n\n\n\n\n\n\n\n\n\npreparing environment for unit 00\n\n\n\n\n\n\ngene46100\n\n\nhowto\n\n\n\nsetting up conda environement and installing packages\n\n\n\n\n\nMar 24, 2025\n\n\nHaky Im\n\n\n\n\n\n\nNo matching items\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html",
    "href": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html",
    "title": "Fit linear mode to linear data",
    "section": "",
    "text": "We generate a data Y linearly dependent on features X: \\(Y = X\\beta + \\epsilon\\)\nWe fit a linear model\n\nwith gradient descent\nwith pytorch framework\nwith traditional linear regression estimate \\(\\hat\\beta = (X^TX)^{-1}X^TY\\)\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nfrom plotnine import *\n\n# Generate synthetic data\nnp.random.seed(42)\nX = np.random.randn(100, 1) * 2  # 100 samples, 1 feature\ny = 3 * X + 2 + np.random.randn(100, 1) * 0.5  # y = 3x + 2 + noise\n\n# Convert to PyTorch tensors\nX_tensor = torch.FloatTensor(X)\ny_tensor = torch.FloatTensor(y)\n\n# define a linear model\nclass LinearModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(1, 1)  # 1 input feature, 1 output\n    def forward(self, x):\n        return self.linear(x)\n\n# instantiate the model\nmodel = LinearModel()\n\n# define a loss function\nloss_fn = nn.MSELoss()\n\n# define an optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Train: iterate over the data and update the model\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Forward pass: compute model predictions and loss\n    y_pred = model(X_tensor)\n    loss = loss_fn(y_pred, y_tensor)\n    \n    # Backward pass and parameter update\n    optimizer.zero_grad()  # Clear previous gradients\n    loss.backward()        # Compute gradients via backpropagation\n    optimizer.step()       # Update model parameters\n    \n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# check performance\n\n# compute PyTorch model predictions\nmodel.eval()\nwith torch.no_grad():\n    y_pred_pytorch = model(X_tensor).numpy()\n\n# compute scikit-learn predictions\nlr = LinearRegression()\nlr.fit(X, y)\ny_pred_sklearn = lr.predict(X)\n\n# compare coefficients\nprint(\"\\nPyTorch model coefficients:\")\nprint(f\"Slope: {model.linear.weight.item():.4f}\")\nprint(f\"Intercept: {model.linear.bias.item():.4f}\")\n\nprint(\"\\nScikit-learn coefficients:\")\nprint(f\"Slope: {lr.coef_[0][0]:.4f}\")\nprint(f\"Intercept: {lr.intercept_[0]:.4f}\")\n\n# compare MSE\nmse_pytorch = np.mean((y - y_pred_pytorch) ** 2)\nmse_sklearn = np.mean((y - y_pred_sklearn) ** 2)\nprint(f\"\\nPyTorch MSE: {mse_pytorch:.4f}\")\nprint(f\"Scikit-learn MSE: {mse_sklearn:.4f}\")\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html#fit-a-linear-model-to-linear-data",
    "href": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html#fit-a-linear-model-to-linear-data",
    "title": "Fit linear mode to linear data",
    "section": "",
    "text": "We generate a data Y linearly dependent on features X: \\(Y = X\\beta + \\epsilon\\)\nWe fit a linear model\n\nwith gradient descent\nwith pytorch framework\nwith traditional linear regression estimate \\(\\hat\\beta = (X^TX)^{-1}X^TY\\)\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nfrom plotnine import *\n\n# Generate synthetic data\nnp.random.seed(42)\nX = np.random.randn(100, 1) * 2  # 100 samples, 1 feature\ny = 3 * X + 2 + np.random.randn(100, 1) * 0.5  # y = 3x + 2 + noise\n\n# Convert to PyTorch tensors\nX_tensor = torch.FloatTensor(X)\ny_tensor = torch.FloatTensor(y)\n\n# define a linear model\nclass LinearModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(1, 1)  # 1 input feature, 1 output\n    def forward(self, x):\n        return self.linear(x)\n\n# instantiate the model\nmodel = LinearModel()\n\n# define a loss function\nloss_fn = nn.MSELoss()\n\n# define an optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Train: iterate over the data and update the model\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Forward pass: compute model predictions and loss\n    y_pred = model(X_tensor)\n    loss = loss_fn(y_pred, y_tensor)\n    \n    # Backward pass and parameter update\n    optimizer.zero_grad()  # Clear previous gradients\n    loss.backward()        # Compute gradients via backpropagation\n    optimizer.step()       # Update model parameters\n    \n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# check performance\n\n# compute PyTorch model predictions\nmodel.eval()\nwith torch.no_grad():\n    y_pred_pytorch = model(X_tensor).numpy()\n\n# compute scikit-learn predictions\nlr = LinearRegression()\nlr.fit(X, y)\ny_pred_sklearn = lr.predict(X)\n\n# compare coefficients\nprint(\"\\nPyTorch model coefficients:\")\nprint(f\"Slope: {model.linear.weight.item():.4f}\")\nprint(f\"Intercept: {model.linear.bias.item():.4f}\")\n\nprint(\"\\nScikit-learn coefficients:\")\nprint(f\"Slope: {lr.coef_[0][0]:.4f}\")\nprint(f\"Intercept: {lr.intercept_[0]:.4f}\")\n\n# compare MSE\nmse_pytorch = np.mean((y - y_pred_pytorch) ** 2)\nmse_sklearn = np.mean((y - y_pred_sklearn) ** 2)\nprint(f\"\\nPyTorch MSE: {mse_pytorch:.4f}\")\nprint(f\"Scikit-learn MSE: {mse_sklearn:.4f}\")"
  },
  {
    "objectID": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html#plot-predictions-vs-true",
    "href": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html#plot-predictions-vs-true",
    "title": "Fit linear mode to linear data",
    "section": "plot predictions vs true",
    "text": "plot predictions vs true\n\n# Create a plot comparing predictions\n# Create a DataFrame for plotting\nplot_df = pd.DataFrame({\n    'y_true': y.flatten(),\n    'y_pred_pytorch': y_pred_pytorch.flatten(),\n    'y_pred_sklearn': y_pred_sklearn.flatten()\n})\n\n# Create the plot\np = (ggplot(plot_df)\n    + geom_point(aes(x='y_true', y='y_pred_pytorch', color='\"PyTorch\"'), size=1)\n    + geom_point(aes(x='y_true', y='y_pred_sklearn', color='\"Scikit-learn\"'), size=1)\n    + geom_abline(intercept=0, slope=1, color='gray', linetype='dashed', alpha=0.5)  # Identity line\n    + scale_color_manual(values=['red', 'blue'])\n    + labs(x='True Values', y='Predicted Values', \n           title='True vs Predicted Values',\n           color='Model')\n    + theme_minimal()\n)\n\np\n\n\n\n\n\n\n\n\n\n(ggplot(plot_df)\n    + geom_point(aes(x='y_pred_sklearn', y='y_pred_pytorch'), size=1)\n    + geom_abline(intercept=0, slope=1, color='gray', linetype='dashed', alpha=0.5)  # Identity line\n    + labs(x='Y sklearn', y='Y pytorch', \n           title='ptorch vs sklearn predictions',\n           color='Model')\n    + theme_minimal()\n)"
  },
  {
    "objectID": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html#fit-linear-model-using-the-gpu",
    "href": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html#fit-linear-model-using-the-gpu",
    "title": "Fit linear mode to linear data",
    "section": "Fit linear model using the GPU",
    "text": "Fit linear model using the GPU\n\n# Train on GPU (checking for both MPS and CUDA)\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nfrom plotnine import *\n\n# Check for available GPU devices\nif torch.backends.mps.is_available():\n    device = torch.device('mps')  # Apple Silicon\n    print(\"Using MPS (Apple Silicon GPU)\")\nelif torch.cuda.is_available():\n    device = torch.device('cuda')  # NVIDIA GPU\n    print(\"Using CUDA (NVIDIA GPU)\")\nelse:\n    device = torch.device('cpu')  # Fallback to CPU\n    print(\"Using CPU (no GPU available)\")\n\n# Generate synthetic data (stays on CPU)\nnp.random.seed(42)\nX = np.random.randn(100, 1) * 2\ny = 3 * X + 2 + np.random.randn(100, 1) * 0.5\n\n# Convert to PyTorch tensors and move to GPU\nX_tensor = torch.FloatTensor(X).to(device)\ny_tensor = torch.FloatTensor(y).to(device)\n\n# Define and move model to GPU\nclass LinearModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(1, 1)\n\n    def forward(self, x):\n        return self.linear(x)\n\nmodel = LinearModel().to(device)\nloss_fn = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Training loop (same as before, but data is on GPU)\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Forward pass\n    y_pred = model(X_tensor)\n    loss = loss_fn(y_pred, y_tensor)\n    \n    # Backward pass and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Get predictions back to CPU for comparison\nmodel.eval()\nwith torch.no_grad():\n    y_pred_pytorch = model(X_tensor).cpu().numpy()  # Move back to CPU before converting to numpy\n\n# Compare with scikit-learn (on CPU)\nlr = LinearRegression()\nlr.fit(X, y)\ny_pred_sklearn = lr.predict(X)\n\n# Compare coefficients\nprint(\"\\nPyTorch model coefficients:\")\nprint(f\"Slope: {model.linear.weight.item():.4f}\")\nprint(f\"Intercept: {model.linear.bias.item():.4f}\")\n\nprint(\"\\nScikit-learn coefficients:\")\nprint(f\"Slope: {lr.coef_[0][0]:.4f}\")\nprint(f\"Intercept: {lr.intercept_[0]:.4f}\")\n\n# Compare MSE\nmse_pytorch = np.mean((y - y_pred_pytorch) ** 2)\nmse_sklearn = np.mean((y - y_pred_sklearn) ** 2)\nprint(f\"\\nPyTorch MSE: {mse_pytorch:.4f}\")\nprint(f\"Scikit-learn MSE: {mse_sklearn:.4f}\")\n\n# Create a plot comparing predictions\nplot_df = pd.DataFrame({\n    'y_true': y.flatten(),\n    'y_pred_pytorch': y_pred_pytorch.flatten(),\n    'y_pred_sklearn': y_pred_sklearn.flatten()\n})\n\n# Create the plot\np = (ggplot(plot_df)\n    + geom_point(aes(x='y_true', y='y_pred_pytorch', color='\"PyTorch\"'), size=1)\n    + geom_point(aes(x='y_true', y='y_pred_sklearn', color='\"Scikit-learn\"'), size=1)\n    + geom_abline(intercept=0, slope=1, color='gray', linetype='dashed', alpha=0.5)  # Identity line\n    + scale_color_manual(values=['red', 'blue'])\n    + labs(x='True Values', y='Predicted Values', \n           title=f'True vs Predicted Values (Training on {device})',\n           color='Model')\n    + theme_minimal()\n)\n\np"
  },
  {
    "objectID": "DRAFTS/measuring-info-in-DNA-seq.html",
    "href": "DRAFTS/measuring-info-in-DNA-seq.html",
    "title": "Measuring information in DNA sequence",
    "section": "",
    "text": "In the analysis of DNA sequences, particularly when studying motifs and patterns, two key concepts from information theory are entropy and information content. These measures help quantify the variability and conservation of nucleotides at specific positions within a set of aligned sequences.\n\nEntropy (H):\n\nEntropy measures the uncertainty or randomness at a given position. For DNA with four possible bases (A, C, G, T) and their respective probabilities at a position being p(A), p(C), p(G), and p(T) (where p(A) + p(C) + p(G) + p(T) = 1), the Shannon entropy (H) in bits is calculated using the following formula:\nH = - (p(A) * log2(p(A)) + p(C) * log2(p(C)) + p(G) * log2(p(G)) + p(T) * log2(p(T))) A higher entropy value indicates greater uncertainty or variability in the nucleotides at that position. A lower entropy value indicates less uncertainty, meaning one or a few nucleotides are more dominant. The maximum possible entropy for DNA (when all four bases are equally likely) is 2 bits. The minimum possible entropy (when only one base is present) is 0 bits. 2. Information Content (IC):\nInformation content measures the amount of information gained by knowing the nucleotide at a particular position. It reflects the conservation of that position relative to a background distribution (often assumed to be uniform). The information content (IC) in bits is calculated as:\nIC = log2(N) - H Where:\nN is the number of possible nucleotides (4 for DNA).\nlog2(N) represents the maximum possible information content (2 bits for DNA).\nH is the Shannon entropy calculated for that position.\nA higher information content value indicates greater conservation and importance of specific nucleotides at that position.\nA lower information content value indicates less conservation and more variability.\nThe maximum possible information content for DNA is 2 bits (when entropy is 0).\nThe minimum possible information content (when entropy is maximum) is 0 bits.\n\nimport math\n\ndef calculate_entropy(pA, pC, pG, pT):\n    \"\"\"\n    Calculates the Shannon entropy for DNA bases given their probabilities.\n\n    Args:\n        pA (float): Probability of Adenine (A).\n        pC (float): Probability of Cytosine (C).\n        pG (float): Probability of Guanine (G).\n        pT (float): Probability of Thymine (T).\n\n    Returns:\n        float: The Shannon entropy in bits.\n    \"\"\"\n    entropy = 0\n    if pA &gt; 0:\n        entropy -= pA * math.log2(pA)\n    if pC &gt; 0:\n        entropy -= pC * math.log2(pC)\n    if pG &gt; 0:\n        entropy -= pG * math.log2(pG)\n    if pT &gt; 0:\n        entropy -= pT * math.log2(pT)\n    return entropy\n\ndef calculate_information_content(pA, pC, pG, pT):\n    \"\"\"\n    Calculates the information content (in bits) at a given position\n    based on the nucleotide probabilities.\n\n    Args:\n        pA (float): Probability of Adenine (A).\n        pC (float): Probability of Cytosine (C).\n        pG (float): Probability of Guanine (G).\n        pT (float): Probability of Thymine (T).\n\n    Returns:\n        float: The information content in bits.\n    \"\"\"\n    num_nucleotides = 4  # A, C, G, T\n    max_entropy = math.log2(num_nucleotides)  # Maximum entropy for 4 equally likely bases\n    entropy = calculate_entropy(pA, pC, pG, pT)\n    information_content = max_entropy - entropy\n    return information_content\n\n# Example Usage:\nprobability_A = 0.3\nprobability_C = 0.2\nprobability_G = 0.25\nprobability_T = 0.25\n\nentropy_value = calculate_entropy(probability_A, probability_C, probability_G, probability_T)\ninformation_content_value = calculate_information_content(probability_A, probability_C, probability_G, probability_T)\n\nprint(f\"Probabilities: p(A)={probability_A:.2f}, p(C)={probability_C:.2f}, p(G)={probability_G:.2f}, p(T)={probability_T:.2f}\")\nprint(f\"Entropy: {entropy_value:.4f} bits\")\nprint(f\"Information Content: {information_content_value:.4f} bits\")\n\n# Example with uniform distribution:\nentropy_uniform = calculate_entropy(0.25, 0.25, 0.25, 0.25)\ninformation_content_uniform = calculate_information_content(0.25, 0.25, 0.25, 0.25)\nprint(f\"\\nProbabilities (uniform): p(A)=0.25, p(C)=0.25, p(G)=0.25, p(T)=0.25\")\nprint(f\"Entropy (uniform): {entropy_uniform:.4f} bits\")\nprint(f\"Information Content (uniform): {information_content_uniform:.4f} bits\")\n\n# Example with a fully conserved position:\nentropy_conserved = calculate_entropy(1.0, 0.0, 0.0, 0.0)\ninformation_content_conserved = calculate_information_content(1.0, 0.0, 0.0, 0.0)\nprint(f\"\\nProbabilities (fully conserved): p(A)=1.00, p(C)=0.00, p(G)=0.00, p(T)=0.00\")\nprint(f\"Entropy (fully conserved): {entropy_conserved:.4f} bits\")\nprint(f\"Information Content (fully conserved): {information_content_conserved:.4f} bits\")\n\nProbabilities: p(A)=0.30, p(C)=0.20, p(G)=0.25, p(T)=0.25\nEntropy: 1.9855 bits\nInformation Content: 0.0145 bits\n\nProbabilities (uniform): p(A)=0.25, p(C)=0.25, p(G)=0.25, p(T)=0.25\nEntropy (uniform): 2.0000 bits\nInformation Content (uniform): 0.0000 bits\n\nProbabilities (fully conserved): p(A)=1.00, p(C)=0.00, p(G)=0.00, p(T)=0.00\nEntropy (fully conserved): 0.0000 bits\nInformation Content (fully conserved): 2.0000 bits\n\n\n** text and code generated with gemini 2.0 via a series of prompting **\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "DRAFTS/measuring-info-in-DNA-seq.html#understanding-entropy-and-information-content-in-dna-sequence-analysis",
    "href": "DRAFTS/measuring-info-in-DNA-seq.html#understanding-entropy-and-information-content-in-dna-sequence-analysis",
    "title": "Measuring information in DNA sequence",
    "section": "",
    "text": "In the analysis of DNA sequences, particularly when studying motifs and patterns, two key concepts from information theory are entropy and information content. These measures help quantify the variability and conservation of nucleotides at specific positions within a set of aligned sequences.\n\nEntropy (H):\n\nEntropy measures the uncertainty or randomness at a given position. For DNA with four possible bases (A, C, G, T) and their respective probabilities at a position being p(A), p(C), p(G), and p(T) (where p(A) + p(C) + p(G) + p(T) = 1), the Shannon entropy (H) in bits is calculated using the following formula:\nH = - (p(A) * log2(p(A)) + p(C) * log2(p(C)) + p(G) * log2(p(G)) + p(T) * log2(p(T))) A higher entropy value indicates greater uncertainty or variability in the nucleotides at that position. A lower entropy value indicates less uncertainty, meaning one or a few nucleotides are more dominant. The maximum possible entropy for DNA (when all four bases are equally likely) is 2 bits. The minimum possible entropy (when only one base is present) is 0 bits. 2. Information Content (IC):\nInformation content measures the amount of information gained by knowing the nucleotide at a particular position. It reflects the conservation of that position relative to a background distribution (often assumed to be uniform). The information content (IC) in bits is calculated as:\nIC = log2(N) - H Where:\nN is the number of possible nucleotides (4 for DNA).\nlog2(N) represents the maximum possible information content (2 bits for DNA).\nH is the Shannon entropy calculated for that position.\nA higher information content value indicates greater conservation and importance of specific nucleotides at that position.\nA lower information content value indicates less conservation and more variability.\nThe maximum possible information content for DNA is 2 bits (when entropy is 0).\nThe minimum possible information content (when entropy is maximum) is 0 bits.\n\nimport math\n\ndef calculate_entropy(pA, pC, pG, pT):\n    \"\"\"\n    Calculates the Shannon entropy for DNA bases given their probabilities.\n\n    Args:\n        pA (float): Probability of Adenine (A).\n        pC (float): Probability of Cytosine (C).\n        pG (float): Probability of Guanine (G).\n        pT (float): Probability of Thymine (T).\n\n    Returns:\n        float: The Shannon entropy in bits.\n    \"\"\"\n    entropy = 0\n    if pA &gt; 0:\n        entropy -= pA * math.log2(pA)\n    if pC &gt; 0:\n        entropy -= pC * math.log2(pC)\n    if pG &gt; 0:\n        entropy -= pG * math.log2(pG)\n    if pT &gt; 0:\n        entropy -= pT * math.log2(pT)\n    return entropy\n\ndef calculate_information_content(pA, pC, pG, pT):\n    \"\"\"\n    Calculates the information content (in bits) at a given position\n    based on the nucleotide probabilities.\n\n    Args:\n        pA (float): Probability of Adenine (A).\n        pC (float): Probability of Cytosine (C).\n        pG (float): Probability of Guanine (G).\n        pT (float): Probability of Thymine (T).\n\n    Returns:\n        float: The information content in bits.\n    \"\"\"\n    num_nucleotides = 4  # A, C, G, T\n    max_entropy = math.log2(num_nucleotides)  # Maximum entropy for 4 equally likely bases\n    entropy = calculate_entropy(pA, pC, pG, pT)\n    information_content = max_entropy - entropy\n    return information_content\n\n# Example Usage:\nprobability_A = 0.3\nprobability_C = 0.2\nprobability_G = 0.25\nprobability_T = 0.25\n\nentropy_value = calculate_entropy(probability_A, probability_C, probability_G, probability_T)\ninformation_content_value = calculate_information_content(probability_A, probability_C, probability_G, probability_T)\n\nprint(f\"Probabilities: p(A)={probability_A:.2f}, p(C)={probability_C:.2f}, p(G)={probability_G:.2f}, p(T)={probability_T:.2f}\")\nprint(f\"Entropy: {entropy_value:.4f} bits\")\nprint(f\"Information Content: {information_content_value:.4f} bits\")\n\n# Example with uniform distribution:\nentropy_uniform = calculate_entropy(0.25, 0.25, 0.25, 0.25)\ninformation_content_uniform = calculate_information_content(0.25, 0.25, 0.25, 0.25)\nprint(f\"\\nProbabilities (uniform): p(A)=0.25, p(C)=0.25, p(G)=0.25, p(T)=0.25\")\nprint(f\"Entropy (uniform): {entropy_uniform:.4f} bits\")\nprint(f\"Information Content (uniform): {information_content_uniform:.4f} bits\")\n\n# Example with a fully conserved position:\nentropy_conserved = calculate_entropy(1.0, 0.0, 0.0, 0.0)\ninformation_content_conserved = calculate_information_content(1.0, 0.0, 0.0, 0.0)\nprint(f\"\\nProbabilities (fully conserved): p(A)=1.00, p(C)=0.00, p(G)=0.00, p(T)=0.00\")\nprint(f\"Entropy (fully conserved): {entropy_conserved:.4f} bits\")\nprint(f\"Information Content (fully conserved): {information_content_conserved:.4f} bits\")\n\nProbabilities: p(A)=0.30, p(C)=0.20, p(G)=0.25, p(T)=0.25\nEntropy: 1.9855 bits\nInformation Content: 0.0145 bits\n\nProbabilities (uniform): p(A)=0.25, p(C)=0.25, p(G)=0.25, p(T)=0.25\nEntropy (uniform): 2.0000 bits\nInformation Content (uniform): 0.0000 bits\n\nProbabilities (fully conserved): p(A)=1.00, p(C)=0.00, p(G)=0.00, p(T)=0.00\nEntropy (fully conserved): 0.0000 bits\nInformation Content (fully conserved): 2.0000 bits\n\n\n** text and code generated with gemini 2.0 via a series of prompting **"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "",
    "text": "created by Erin Wilson. Downloaded from here.\nSome edits by Haky Im and Ran Blekhman for the deep learning in genomics gene46100 course.\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#import-necessary-modules",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#import-necessary-modules",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "import necessary modules",
    "text": "import necessary modules\n\nfrom collections import defaultdict\nfrom itertools import product\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport random\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nif torch.backends.mps.is_available():\n    torch.set_default_dtype(torch.float32)\n    print(\"Set default to float32 for MPS compatibility\")\n\nSet default to float32 for MPS compatibility"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#set-seeds-for-reproduciblity-across-runs",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#set-seeds-for-reproduciblity-across-runs",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "set seeds for reproduciblity across runs",
    "text": "set seeds for reproduciblity across runs\n\n# Set a random seed in a bunch of different places\ndef set_seed(seed: int = 42) -&gt; None:\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    \n    if torch.backends.mps.is_available():\n        # For MacBooks with Apple Silicon\n        torch.mps.manual_seed(seed)\n    elif torch.cuda.is_available():\n        # For CUDA GPUs\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    print(f\"Random seed set as {seed}\")\n    \nset_seed(17)\n\nRandom seed set as 17"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#define-gpu-device",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#define-gpu-device",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "define GPU device",
    "text": "define GPU device\nAre you working on a GPU? If so, you can put your data/models on DEVICE (and have to do so explicity)! If not, you can probably remove all instances of foo.to(DEVICE) and it should still work fine on a CPU.\n\nDEVICE = torch.device('mps' if torch.backends.mps.is_available() \n                     else 'cuda' if torch.cuda.is_available() \n                     else 'cpu')\nDEVICE\n\ndevice(type='mps')\n\n\n ## 1. Generate synthetic DNA data\nUsually scientists might be interested in predicting something like a binding score, an expression strength, or classifying a TF binding event. But here, we are going to keep it simple: the goal in this tutorial is to observe if a deep learning model can learn to detect a very small, simple pattern in a DNA sequence and score it appropriately (again, just a practice task to convince ourselves that we have actually set up the Pytorch pieces correctly such that it can learn from input that looks like a DNA sequence).\nSo arbitrarily, let’s say that given an 8-mer DNA sequence, we will score it based on the following rules: * A = +20 points * C = +17 points * G = +14 points * T = +11 points\nFor every 8-mer, let’s sum up its total points based on the nucleotides in its sequence, then take the average. For example,\n\nAAAAAAAA would score 20.0\n\n(mean(20 + 20 + 20 + 20 + 20 + 20 + 20 + 20) = 20.0)\n\nACAAAAAA would score 19.625\n\n(mean(20 + 17 + 20 + 20 + 20 + 20 + 20 + 20) = 19.625)\n\n\nThese values for the nucleotides are arbitrary - there’s no real biology here! It’s just a way to assign sequences a score for the purposes of our Pytorch practice.\nHowever, since many recent papers use methods like CNNs to automatically detect “motifs,” or short patterns in the DNA that can activate or repress a biological response, let’s add one more piece to our scoring system. To simulate something like motifs influencing gene expression, let’s say a given sequence gets a +10 bump if TAT appears anywhere in the 8-mer, and a -10 bump if it has a GCG in it. Again, these motifs don’t mean anything in real life, they are just a mechanism for simulating a really simple activation or repression effect.\n\nSo let’s implement this basic scoring function!\n\n# define function for generating all k-mers of length k\ndef kmers(k):\n    '''Generate a list of all k-mers for a given k'''\n    \n    return [''.join(x) for x in product(['A','C','G','T'], repeat=k)]\n\n\ngenerate all 8-mers\n\n# generate all 8-mers\nseqs8 = kmers(8)\nprint('Total 8mers:',len(seqs8))\n\nTotal 8mers: 65536\n\n\n\n\ndefine scoring function for the DNA sequences\n\n# define score_dict\nscore_dict = {\n    'A':20,\n    'C':17,\n    'G':14,\n    'T':11\n}\n# define function for scoring sequences\ndef score_seqs_motif(seqs):\n    '''\n    Calculate the scores for a list of sequences based on \n    the above score_dict\n    '''\n    data = []\n    for seq in seqs:\n        # get the average score by nucleotide\n        score = np.mean([score_dict[base] for base in seq],dtype=np.float32)\n        \n        # give a + or - bump if this k-mer has a specific motif\n        if 'TAT' in seq:\n            score += 10\n        if 'GCG' in seq:\n            score -= 10\n        data.append([seq,score])\n        \n    df = pd.DataFrame(data, columns=['seq','score'])\n    return df\n\n\nmer8 = score_seqs_motif(seqs8)\nmer8.head()\n\n\n\n\n\n\n\n\nseq\nscore\n\n\n\n\n0\nAAAAAAAA\n20.000\n\n\n1\nAAAAAAAC\n19.625\n\n\n2\nAAAAAAAG\n19.250\n\n\n3\nAAAAAAAT\n18.875\n\n\n4\nAAAAAACA\n19.625\n\n\n\n\n\n\n\nSpot check scores of a couple seqs with motifs:\n\nmer8[mer8['seq'].isin(['TGCGTTTT','CCCCCTAT'])]\n\n\n\n\n\n\n\n\nseq\nscore\n\n\n\n\n21875\nCCCCCTAT\n25.875\n\n\n59135\nTGCGTTTT\n2.500"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#plot-distribution-of-motif-scores",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#plot-distribution-of-motif-scores",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "plot distribution of motif scores",
    "text": "plot distribution of motif scores\n\nplt.hist(mer8['score'].values,bins=20)\nplt.title(\"8-mer with Motifs score distribution\")\nplt.xlabel(\"seq score\",fontsize=14)\nplt.ylabel(\"count\",fontsize=14)\nplt.show()\n\n\n\n\n\n\n\n\nAs expected, the distribution of scores across all 8-mers has 3 groups: * No motif (centered around ~15) * contains TAT (~25) * contains GCG (~5)"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-1",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-1",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "Question 1",
    "text": "Question 1\nModify the scoring function to create a more complex pattern. Instead of giving fixed bonuses for “TAT” and “GCG”, implement a position-dependent scoring where a motif gets a higher bonus if it appears at the beginning of the sequence compared to the end. How does this change the distribution of scores?"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#next-we-want-to-train-a-model-to-predict-this-score-by-from-the-dna-sequence",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#next-we-want-to-train-a-model-to-predict-this-score-by-from-the-dna-sequence",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "Next, we want to train a model to predict this score by from the DNA sequence",
    "text": "Next, we want to train a model to predict this score by from the DNA sequence\n ## 2. Prepare data for Pytorch training\nFor neural networks to make predictions, you have to give it your input as a matrix of numbers. For example, to classify images by whether or not they contain a cat, a network “sees” the image as a matrix of pixel values and learns relevant patterns in the relative arrangement of pixels (e.g. patterns that correspond to cat ears, or a nose with whiskers).\nWe similarly need to turn our DNA sequences (strings of ACGTs) into a matrix of numbers. So how do we pretend our DNA is a cat?\nOne common strategy is to one-hot encode the DNA: treat each nucleotide as a vector of length 4, where 3 positions are 0 and one position is a 1, depending on the nucleotide.\n\nturn DNA sequences into numbers with one hot encoding\n\nThis one-hot encoding has the nice property that it makes your DNA appear like how a computer sees a picture of a cat!\n\ndef one_hot_encode(seq):\n    \"\"\"\n    Given a DNA sequence, return its one-hot encoding\n    \"\"\"\n    # Make sure seq has only allowed bases\n    allowed = set(\"ACTGN\")\n    if not set(seq).issubset(allowed):\n        invalid = set(seq) - allowed\n        raise ValueError(f\"Sequence contains chars not in allowed DNA alphabet (ACGTN): {invalid}\")\n        \n    # Dictionary returning one-hot encoding for each nucleotide \n    nuc_d = {'A':[1.0,0.0,0.0,0.0],\n             'C':[0.0,1.0,0.0,0.0],\n             'G':[0.0,0.0,1.0,0.0],\n             'T':[0.0,0.0,0.0,1.0],\n             'N':[0.0,0.0,0.0,0.0]}\n    \n    # Create array from nucleotide sequence\n    vec=np.array([nuc_d[x] for x in seq], dtype=np.float32)\n        \n    return vec\n\n\n# one hot encoding of 8 As\na8 = one_hot_encode(\"AAAAAAAA\")\nprint(\"AAAAAA:\\n\",a8)\n\n\nAAAAAA:\n [[1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]]\n\n\n\n# one hot encoding of another DNA\ns = one_hot_encode(\"AGGTACCT\")\nprint(\"AGGTACC:\\n\",s)\nprint(\"shape:\",s.shape)\n\nAGGTACC:\n [[1. 0. 0. 0.]\n [0. 0. 1. 0.]\n [0. 0. 1. 0.]\n [0. 0. 0. 1.]\n [1. 0. 0. 0.]\n [0. 1. 0. 0.]\n [0. 1. 0. 0.]\n [0. 0. 0. 1.]]\nshape: (8, 4)\n\n\n\n\nsplit data into train, validation, and test\nData are typically split into training, validation, and test sets. This helps avoid overfitting and achieve better generalization to new data.\n\nTraining set (e.g. 70% of the data)\n\nused to train the model\nmodel learns patterns from this data\nanalogy: studying for an exam\n\nValidation or tuning set (e.g. 15% of the data)\n\nused to tune hypterparameters\nhelps prevent overfitting\nanalogy: pratice problems while studying\n\nTest set or held out set (e.g. 15% of the data)\n\nused only to evaluate final model performance\nnever used during training or tuning\nanalogy: actual exam\n\n\nNote: in this tutorial, that uses the quick_split function defined here splits the data into 20% for the test set, 80% of the remaining 80% as the training set (i.e. 64% of the total) and 20% of the non test sets as validation (i.e. 16%).\n\ndefine quick splitting function\n\n# define function for splitting data\ndef quick_split(df, split_frac=0.8, verbose=False):\n    '''\n    Given a df of samples, randomly split indices between\n    train and test at the desired fraction\n    '''\n    cols = df.columns # original columns, use to clean up reindexed cols\n    df = df.reset_index()\n\n    # shuffle indices\n    idxs = list(range(df.shape[0]))\n    random.shuffle(idxs)\n\n    # split shuffled index list by split_frac\n    split = int(len(idxs)*split_frac)\n    train_idxs = idxs[:split]\n    test_idxs = idxs[split:]\n    \n    # split dfs and return\n    train_df = df[df.index.isin(train_idxs)]\n    test_df = df[df.index.isin(test_idxs)]\n        \n    return train_df[cols], test_df[cols]\n\n\n\nsplit data into train, validation, and test sets\n\n# split data into train, validation, and test\nfull_train_df, test_df = quick_split(mer8)\ntrain_df, val_df = quick_split(full_train_df)\n\nprint(\"Train:\", train_df.shape)\nprint(\"Val:\", val_df.shape)\nprint(\"Test:\", test_df.shape)\n\ntrain_df.head()\n\nTrain: (41942, 2)\nVal: (10486, 2)\nTest: (13108, 2)\n\n\n\n\n\n\n\n\n\nseq\nscore\n\n\n\n\n0\nAAAAAAAA\n20.000\n\n\n1\nAAAAAAAC\n19.625\n\n\n2\nAAAAAAAG\n19.250\n\n\n3\nAAAAAAAT\n18.875\n\n\n4\nAAAAAACC\n19.250\n\n\n\n\n\n\n\n\n\n\nplot distribution of train, validation, and test data and check that they are similarly distributed\n\ndef plot_train_test_hist(train_df, val_df,test_df,bins=20):\n    ''' Check distribution of train/test scores, sanity check that its not skewed'''\n    plt.hist(train_df['score'].values,bins=bins,label='train',alpha=0.5)\n    plt.hist(val_df['score'].values,bins=bins,label='val',alpha=0.75)\n    plt.hist(test_df['score'].values,bins=bins,label='test',alpha=0.4)\n    plt.legend()\n    plt.xlabel(\"seq score\",fontsize=14)\n    plt.ylabel(\"count\",fontsize=14)\n    plt.show()\n\nWith the below histogram, we can confirm that the train, test, and val sets contain example sequences from each bucket of the distribution (each set has some examples with each kind of motif)\n\nplot_train_test_hist(train_df, val_df,test_df)\n\n\n\n\n\n\n\n\n\n\ndefine dataset and dataloader classes\nDataset and DataLoader classes allow efficient data handling in deep learning.\nDataset class allows standardized way to access and preprocess data.\nDataloader handles batching, shuffling, parallel data loading to make it easier to feed the data for training.\nYou can read more about DataLoader and Dataset objects.\n\nfrom torch.utils.data import Dataset, DataLoader\n\n\ndefine one hot encoded dataset class\nThis class is essential for preparing DNA sequence data for deep learning models, converting the DNA sequences into a numerical format that neural networks can process.\n\nclass SeqDatasetOHE(Dataset):\n    '''\n    Dataset for one-hot-encoded sequences\n    '''\n    def __init__(self, df, seq_col='seq', target_col='score'):\n        # Input: DataFrame with DNA sequences and their scores\n        self.seqs = list(df[seq_col].values)  # Get DNA sequences\n        self.seq_len = len(self.seqs[0])      # Length of each sequence\n        \n        # Convert DNA sequences to one-hot encoding\n        self.ohe_seqs = torch.stack([torch.tensor(one_hot_encode(x)) for x in self.seqs])\n        \n        # Get target scores\n        self.labels = torch.tensor(list(df[target_col].values)).unsqueeze(1)\n        \n    def __len__(self): return len(self.seqs)\n    \n    def __getitem__(self,idx):\n        # Given an index, return a tuple of an X with it's associated Y\n        # This is called inside DataLoader\n        seq = self.ohe_seqs[idx]\n        label = self.labels[idx]\n        \n        return seq, label\n\n\n\nconstruct DataLoaders from Datasets.\n\ndef build_dataloaders(train_df,\n                      test_df,\n                      seq_col='seq',\n                      target_col='score',\n                      batch_size=128,\n                      shuffle=True\n                     ):\n    '''\n    Given a train and test df with some batch construction\n    details, put them into custom SeqDatasetOHE() objects. \n    Give the Datasets to the DataLoaders and return.\n    '''\n    \n    # create Datasets    \n    train_ds = SeqDatasetOHE(train_df,seq_col=seq_col,target_col=target_col)\n    test_ds = SeqDatasetOHE(test_df,seq_col=seq_col,target_col=target_col)\n\n    # Put DataSets into DataLoaders\n    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle)\n    test_dl = DataLoader(test_ds, batch_size=batch_size)\n\n    \n    return train_dl,test_dl\n\n\ntrain_dl, val_dl = build_dataloaders(train_df, val_df)\n\nThese dataloaders are now ready to be used in a training loop!\n ## 3. Define Pytorch models The primary model I was interested in trying was a Convolutional Neural Network, as these have been shown to be useful for learning motifs from genomic data. But as a point of comparison, I included a simple Linear model. Here are some model definitions:\n\n# very simple linear model\nclass DNA_Linear(nn.Module):\n    def __init__(self, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        # the 4 is for our one-hot encoded vector length 4!\n        self.lin = nn.Linear(4*seq_len, 1)\n\n    def forward(self, xb):\n        # reshape to flatten sequence dimension\n        xb = xb.view(xb.shape[0],self.seq_len*4)\n        # Linear wraps up the weights/bias dot product operations\n        out = self.lin(xb)\n        return out\n    \n## CNN model\nclass DNA_CNN(nn.Module):\n    def __init__(self, seq_len, num_filters=32, kernel_size=3):\n        super().__init__()\n        self.seq_len = seq_len\n        \n        # Define layers individually\n        self.conv = nn.Conv1d(4, num_filters, kernel_size=kernel_size)\n        self.relu = nn.ReLU(inplace=True)\n        self.linear = nn.Linear(num_filters*(seq_len-kernel_size+1), 1)\n\n    def forward(self, xb):\n        # reshape view to batch_size x 4channel x seq_len\n        xb = xb.permute(0, 2, 1)\n        \n        # Apply layers step by step\n        x = self.conv(xb)\n        x = self.relu(x)\n        x = x.flatten(1)  # flatten all dimensions except batch\n        out = self.linear(x)\n        return out \n\nThese aren’t optimized models, just something to start with (again, we’re just practicing connecting the Pytorch tubes in the context of DNA!). * The Linear model tries to predict the score by simply weighting the nucleotides that appears in each position. * The CNN model uses 32 filters of length (kernel_size) 3 to scan across the 8-mer sequences for informative 3-mer patterns.\n ## 4. Define the training loop functions\nThe model training process is structured into a series of modular functions, each responsible for a specific part of the workflow. This design improves clarity, reusability, and flexibility when working with different models, optimizers, or loss functions.\nThe overall structure is as follows:\n# Initializes optimizer and loss function if not provided, then trains the model\nrun_training()\n\n    # Iterates over multiple epochs\n    train_loop()\n\n        # Performs one complete pass over the training dataset\n        train_epoch()\n\n            # Computes loss and backprops for a single batch\n            process_batch()\n\n        # Performs one complete pass over the validation dataset\n        val_epoch()\n\n            # Computes loss for a single batch no gradient updates\n            process_batch()\n\n# +--------------------------------+\n# | Training and fitting functions |\n# +--------------------------------+\n\ndef process_batch(model, loss_func, xb, yb, opt=None,verbose=False):\n    '''\n    Apply loss function to a batch of inputs. If no optimizer\n    is provided, skip the back prop step.\n    '''\n    if verbose:\n        print('loss batch ****')\n        print(\"xb shape:\",xb.shape)\n        print(\"yb shape:\",yb.shape)\n        print(\"yb shape:\",yb.squeeze(1).shape)\n        #print(\"yb\",yb)\n\n    # get the batch output from the model given your input batch \n    # ** This is the model's prediction for the y labels! **\n    xb_out = model(xb.float())\n    \n    if verbose:\n        print(\"model out pre loss\", xb_out.shape)\n        #print('xb_out', xb_out)\n        print(\"xb_out:\",xb_out.shape)\n        print(\"yb:\",yb.shape)\n        print(\"yb.long:\",yb.long().shape)\n    \n    loss = loss_func(xb_out, yb.float()) # for MSE/regression\n    # __FOOTNOTE 2__\n    \n    if opt is not None: # if opt\n        opt.zero_grad() ## moved zero grad up to make sure it's not accumulating grads from previous batches\n        loss.backward()\n        opt.step()\n    return loss.item(), len(xb)\n\n\ndef train_epoch(model, train_dl, loss_func, device, opt):\n    '''\n    Execute 1 set of batched training within an epoch\n    '''\n    # Set model to Training mode\n    model.train()\n    tl = [] # train losses\n    ns = [] # batch sizes, n\n    \n    # loop through train DataLoader\n    for xb, yb in train_dl:\n        # put on GPU\n        xb, yb = xb.to(device),yb.to(device)\n        \n        # provide opt so backprop happens\n        t, n = process_batch(model, loss_func, xb, yb, opt=opt)\n        \n        # collect train loss and batch sizes\n        tl.append(t)\n        ns.append(n)\n    \n    # average the losses over all batches    \n    train_loss = np.sum(np.multiply(tl, ns)) / np.sum(ns)\n    \n    return train_loss\n\ndef val_epoch(model, val_dl, loss_func, device):\n    '''\n    Execute 1 set of batched validation within an epoch\n    '''\n    # Set model to Evaluation mode\n    model.eval()\n    with torch.no_grad():\n        vl = [] # val losses\n        ns = [] # batch sizes, n\n        \n        # loop through validation DataLoader\n        for xb, yb in val_dl:\n            # put on GPU\n            xb, yb = xb.to(device),yb.to(device)\n\n            # Do NOT provide opt here, so backprop does not happen\n            v, n = process_batch(model, loss_func, xb, yb)\n\n            # collect val loss and batch sizes\n            vl.append(v)\n            ns.append(n)\n\n    # average the losses over all batches\n    val_loss = np.sum(np.multiply(vl, ns)) / np.sum(ns)\n    \n    return val_loss\n\n\ndef train_loop(epochs, model, loss_func, opt, train_dl, val_dl,device,patience=1000):\n    '''\n    Fit the model params to the training data, eval on unseen data.\n    Loop for a number of epochs and keep train of train and val losses \n    along the way\n    '''\n    # keep track of losses\n    train_losses = []    \n    val_losses = []\n    \n    # loop through epochs\n    for epoch in range(epochs):\n        # take a training step\n        train_loss = train_epoch(model,train_dl,loss_func,device,opt)\n        train_losses.append(train_loss)\n\n        # take a validation step\n        val_loss = val_epoch(model,val_dl,loss_func,device)\n        val_losses.append(val_loss)\n        \n        print(f\"E{epoch} | train loss: {train_loss:.3f} | val loss: {val_loss:.3f}\")\n\n    return train_losses, val_losses\n\n\ndef run_training(train_dl,val_dl,model,device,\n              lr=0.01, epochs=50, \n              lossf=None,opt=None\n             ):\n    '''\n    Given train and val DataLoaders and a NN model, fit the model to the training\n    data. By default, use MSE loss and an SGD optimizer\n    '''\n    # define optimizer\n    if opt:\n        optimizer = opt\n    else: # if no opt provided, just use SGD\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n    \n    # define loss function\n    if lossf:\n        loss_func = lossf\n    else: # if no loss function provided, just use MSE\n        loss_func = torch.nn.MSELoss()\n    \n    # run the training loop\n    train_losses, val_losses = train_loop(\n                                epochs, \n                                model, \n                                loss_func, \n                                optimizer, \n                                train_dl, \n                                val_dl, \n                                device)\n\n    return train_losses, val_losses"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#explanation-of-the-functions-that-make-up-the-training-process",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#explanation-of-the-functions-that-make-up-the-training-process",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "Explanation of the functions that make up the training process",
    "text": "Explanation of the functions that make up the training process\n\nrun_training(): This is the top-level function that orchestrates the entire training process. It handles the initialization of the optimizer and loss function if they are not explicitly provided. It calls the train_loop() function to begin the iterative training loop.\ntrain_loop(): This function manages the training loop across a specified number of epochs. For each epoch, it calls train_epoch() and val_epoch() to perform training and validation, respectively. It also prints the training and validation losses for each epoch.\ntrain_epoch(): This function performs one full pass over the training dataset. It iterates through the train_dl (DataLoader) to get batches of training data. For each batch, it calls process_batch() with the optimizer, enabling backpropagation. It accumulates and averages the training losses.\nval_epoch(): This function performs one full pass over the validation dataset. It iterates through the val_dl (DataLoader) to get batches of validation data. For each batch, it calls process_batch() without the optimizer, preventing gradient updates. It accumulates and averages the validation losses.\nprocess_batch(): This function processes a single batch of data. It calculates the model’s predictions and computes the loss. If an optimizer is provided, it performs backpropagation and updates the model’s weights. If no optimiser is provided, it just returns the loss and batch size. This function is used for both training and validation, with the presence of the optimiser parameter determining if back propagation occurs.\n\n ## 5. Train the models First let’s try running a Linear Model on our 8-mer sequences\n\n# get the sequence length from the first seq in the df\nseq_len = len(train_df['seq'].values[0])\n\n# create Linear model object\nmodel_lin = DNA_Linear(seq_len)\n# use float32 since mps cannot handle 64\nmodel_lin = model_lin.type(torch.float32)\nmodel_lin.to(DEVICE) # put on GPU\n\n\n# run the training pipeline with default settings!\nlin_train_losses, lin_val_losses = run_training(\n    train_dl, \n    val_dl, \n    model_lin,\n    DEVICE\n)\n\nE0 | train loss: 21.238 | val loss: 12.980\nE1 | train loss: 12.969 | val loss: 12.826\nE2 | train loss: 12.918 | val loss: 12.832\nE3 | train loss: 12.915 | val loss: 12.847\nE4 | train loss: 12.916 | val loss: 12.833\nE5 | train loss: 12.918 | val loss: 12.837\nE6 | train loss: 12.915 | val loss: 12.828\nE7 | train loss: 12.917 | val loss: 12.826\nE8 | train loss: 12.917 | val loss: 12.827\nE9 | train loss: 12.917 | val loss: 12.827\nE10 | train loss: 12.918 | val loss: 12.831\nE11 | train loss: 12.914 | val loss: 12.836\nE12 | train loss: 12.918 | val loss: 12.834\nE13 | train loss: 12.916 | val loss: 12.830\nE14 | train loss: 12.917 | val loss: 12.832\nE15 | train loss: 12.917 | val loss: 12.831\nE16 | train loss: 12.917 | val loss: 12.833\nE17 | train loss: 12.915 | val loss: 12.882\nE18 | train loss: 12.916 | val loss: 12.834\nE19 | train loss: 12.916 | val loss: 12.833\nE20 | train loss: 12.917 | val loss: 12.830\nE21 | train loss: 12.918 | val loss: 12.830\nE22 | train loss: 12.917 | val loss: 12.826\nE23 | train loss: 12.914 | val loss: 12.826\nE24 | train loss: 12.915 | val loss: 12.828\nE25 | train loss: 12.916 | val loss: 12.833\nE26 | train loss: 12.916 | val loss: 12.829\nE27 | train loss: 12.916 | val loss: 12.828\nE28 | train loss: 12.918 | val loss: 12.848\nE29 | train loss: 12.916 | val loss: 12.830\nE30 | train loss: 12.916 | val loss: 12.841\nE31 | train loss: 12.917 | val loss: 12.823\nE32 | train loss: 12.917 | val loss: 12.833\nE33 | train loss: 12.916 | val loss: 12.825\nE34 | train loss: 12.918 | val loss: 12.822\nE35 | train loss: 12.916 | val loss: 12.838\nE36 | train loss: 12.917 | val loss: 12.833\nE37 | train loss: 12.914 | val loss: 12.837\nE38 | train loss: 12.917 | val loss: 12.834\nE39 | train loss: 12.917 | val loss: 12.846\nE40 | train loss: 12.916 | val loss: 12.826\nE41 | train loss: 12.917 | val loss: 12.820\nE42 | train loss: 12.917 | val loss: 12.835\nE43 | train loss: 12.916 | val loss: 12.832\nE44 | train loss: 12.916 | val loss: 12.827\nE45 | train loss: 12.915 | val loss: 12.827\nE46 | train loss: 12.916 | val loss: 12.827\nE47 | train loss: 12.918 | val loss: 12.829\nE48 | train loss: 12.915 | val loss: 12.824\nE49 | train loss: 12.917 | val loss: 12.824\n\n\nLet’s look at the loss in quick plot:\n\ndef quick_loss_plot(data_label_list,loss_type=\"MSE Loss\",sparse_n=0):\n    '''\n    For each train/test loss trajectory, plot loss by epoch\n    '''\n    for i,(train_data,test_data,label) in enumerate(data_label_list):    \n        plt.plot(train_data,linestyle='--',color=f\"C{i}\", label=f\"{label} Train\")\n        plt.plot(test_data,color=f\"C{i}\", label=f\"{label} Val\",linewidth=3.0)\n\n    plt.legend()\n    plt.ylabel(loss_type)\n    plt.xlabel(\"Epoch\")\n    plt.legend(bbox_to_anchor=(1,1),loc='upper left')\n    plt.show()\n\n\nlin_data_label = (lin_train_losses,lin_val_losses,\"Lin\")\nquick_loss_plot([lin_data_label])\n\n\n\n\n\n\n\n\nAt first glance, not much learning appears to be happening.\nNext let’s try the CNN.\n\nseq_len = len(train_df['seq'].values[0])\n\n# create Linear model object\nmodel_cnn = DNA_CNN(seq_len)\nmodel_cnn.to(DEVICE) # put on GPU\n\n# run the model with default settings!\ncnn_train_losses, cnn_val_losses = run_training(\n    train_dl, \n    val_dl, \n    model_cnn,\n    DEVICE\n)\n\nE0 | train loss: 14.640 | val loss: 10.167\nE1 | train loss: 8.625 | val loss: 7.035\nE2 | train loss: 6.305 | val loss: 4.967\nE3 | train loss: 4.507 | val loss: 3.257\nE4 | train loss: 3.123 | val loss: 2.256\nE5 | train loss: 2.408 | val loss: 2.627\nE6 | train loss: 1.997 | val loss: 2.078\nE7 | train loss: 1.827 | val loss: 4.446\nE8 | train loss: 1.547 | val loss: 1.297\nE9 | train loss: 1.438 | val loss: 1.185\nE10 | train loss: 1.249 | val loss: 1.108\nE11 | train loss: 1.200 | val loss: 1.149\nE12 | train loss: 1.140 | val loss: 1.096\nE13 | train loss: 1.011 | val loss: 1.102\nE14 | train loss: 1.022 | val loss: 1.188\nE15 | train loss: 1.027 | val loss: 1.119\nE16 | train loss: 1.045 | val loss: 1.040\nE17 | train loss: 0.999 | val loss: 1.052\nE18 | train loss: 0.965 | val loss: 1.069\nE19 | train loss: 0.944 | val loss: 1.208\nE20 | train loss: 0.945 | val loss: 1.175\nE21 | train loss: 0.925 | val loss: 1.038\nE22 | train loss: 0.927 | val loss: 1.249\nE23 | train loss: 0.938 | val loss: 1.022\nE24 | train loss: 0.917 | val loss: 1.042\nE25 | train loss: 0.930 | val loss: 1.062\nE26 | train loss: 0.917 | val loss: 1.089\nE27 | train loss: 0.913 | val loss: 1.286\nE28 | train loss: 0.932 | val loss: 1.041\nE29 | train loss: 0.890 | val loss: 1.126\nE30 | train loss: 0.903 | val loss: 1.038\nE31 | train loss: 0.918 | val loss: 1.033\nE32 | train loss: 0.914 | val loss: 1.048\nE33 | train loss: 0.913 | val loss: 1.060\nE34 | train loss: 0.900 | val loss: 1.236\nE35 | train loss: 0.890 | val loss: 1.030\nE36 | train loss: 0.901 | val loss: 1.066\nE37 | train loss: 0.895 | val loss: 1.055\nE38 | train loss: 0.914 | val loss: 1.030\nE39 | train loss: 0.894 | val loss: 1.024\nE40 | train loss: 0.903 | val loss: 1.056\nE41 | train loss: 0.897 | val loss: 1.089\nE42 | train loss: 0.917 | val loss: 1.031\nE43 | train loss: 0.904 | val loss: 1.105\nE44 | train loss: 0.894 | val loss: 1.284\nE45 | train loss: 0.903 | val loss: 1.648\nE46 | train loss: 0.904 | val loss: 1.042\nE47 | train loss: 0.909 | val loss: 1.052\nE48 | train loss: 0.891 | val loss: 1.186\nE49 | train loss: 0.891 | val loss: 1.075\n\n\n\ncnn_data_label = (cnn_train_losses,cnn_val_losses,\"CNN\")\nquick_loss_plot([lin_data_label,cnn_data_label])\n\n\n\n\n\n\n\n\nIt seems clear from the loss curves that the CNN is able to capture a pattern in the data that the Linear model is not! Let’s spot check a few sequences to see what’s going on.\n\n# oracle dict of true score for each seq\noracle = dict(mer8[['seq','score']].values)\n\ndef quick_seq_pred(model, desc, seqs, oracle):\n    '''\n    Given a model and some sequences, get the model's predictions\n    for those sequences and compare to the oracle (true) output\n    '''\n    print(f\"__{desc}__\")\n    for dna in seqs:\n        s = torch.tensor(one_hot_encode(dna)).unsqueeze(0).to(DEVICE)\n        pred = model(s.float())\n        actual = oracle[dna]\n        diff = pred.item() - actual\n        print(f\"{dna}: pred:{pred.item():.3f} actual:{actual:.3f} ({diff:.3f})\")\n\ndef quick_8mer_pred(model, oracle):\n    seqs1 = (\"poly-X seqs\",['AAAAAAAA', 'CCCCCCCC','GGGGGGGG','TTTTTTTT'])\n    seqs2 = (\"other seqs\", ['AACCAACA','CCGGTGAG','GGGTAAGG', 'TTTCGTTT'])\n    seqsTAT = (\"with TAT motif\", ['TATAAAAA','CCTATCCC','GTATGGGG','TTTATTTT'])\n    seqsGCG = (\"with GCG motif\", ['AAGCGAAA','CGCGCCCC','GGGCGGGG','TTGCGTTT'])\n    TATGCG =  (\"both TAT and GCG\",['ATATGCGA','TGCGTATT'])\n\n    for desc,seqs in [seqs1, seqs2, seqsTAT, seqsGCG, TATGCG]:\n        quick_seq_pred(model, desc, seqs, oracle)\n        print()\n\n\n# Ask the trained Linear model to make \n# predictions for some 8-mers\nquick_8mer_pred(model_lin, oracle)\n\n__poly-X seqs__\nAAAAAAAA: pred:23.415 actual:20.000 (3.415)\nCCCCCCCC: pred:13.762 actual:17.000 (-3.238)\nGGGGGGGG: pred:7.189 actual:14.000 (-6.811)\nTTTTTTTT: pred:17.841 actual:11.000 (6.841)\n\n__other seqs__\nAACCAACA: pred:18.987 actual:18.875 (0.112)\nCCGGTGAG: pred:12.330 actual:15.125 (-2.795)\nGGGTAAGG: pred:14.006 actual:15.125 (-1.119)\nTTTCGTTT: pred:14.945 actual:12.125 (2.820)\n\n__with TAT motif__\nTATAAAAA: pred:22.317 actual:27.750 (-5.433)\nCCTATCCC: pred:17.059 actual:25.875 (-8.816)\nGTATGGGG: pred:12.329 actual:24.000 (-11.671)\nTTTATTTT: pred:18.331 actual:22.125 (-3.794)\n\n__with GCG motif__\nAAGCGAAA: pred:16.972 actual:8.125 (8.847)\nCGCGCCCC: pred:12.467 actual:6.250 (6.217)\nGGGCGGGG: pred:8.121 actual:4.375 (3.746)\nTTGCGTTT: pred:13.014 actual:2.500 (10.514)\n\n__both TAT and GCG__\nATATGCGA: pred:15.874 actual:15.875 (-0.001)\nTGCGTATT: pred:14.779 actual:13.625 (1.154)\n\n\n\nFrom the above examples, it appears that the Linear model is really underpredicting sequences with a lot of G’s and overpredicting those with many T’s. This is probably because it noticed GCG made sequences have unusually low scores and TAT made sequences have unusually high scores, however since the Linear model doesn’t have a way to take into account the different context of GCG vs GAG, it just predicts that sequences with G’s should be lower. We know from our scoring scheme that this isn’t the case: it’s not that G’s in general are detrimental, but specifically GCG is.\n\n# Ask the trained CNN model to make \n# predictions for some 8-mers\nquick_8mer_pred(model_cnn, oracle)\n\n__poly-X seqs__\nAAAAAAAA: pred:19.649 actual:20.000 (-0.351)\nCCCCCCCC: pred:16.750 actual:17.000 (-0.250)\nGGGGGGGG: pred:13.576 actual:14.000 (-0.424)\nTTTTTTTT: pred:10.895 actual:11.000 (-0.105)\n\n__other seqs__\nAACCAACA: pred:18.645 actual:18.875 (-0.230)\nCCGGTGAG: pred:14.759 actual:15.125 (-0.366)\nGGGTAAGG: pred:15.118 actual:15.125 (-0.007)\nTTTCGTTT: pred:11.789 actual:12.125 (-0.336)\n\n__with TAT motif__\nTATAAAAA: pred:26.103 actual:27.750 (-1.647)\nCCTATCCC: pred:24.256 actual:25.875 (-1.619)\nGTATGGGG: pred:22.838 actual:24.000 (-1.162)\nTTTATTTT: pred:20.555 actual:22.125 (-1.570)\n\n__with GCG motif__\nAAGCGAAA: pred:9.007 actual:8.125 (0.882)\nCGCGCCCC: pred:7.091 actual:6.250 (0.841)\nGGGCGGGG: pred:5.275 actual:4.375 (0.900)\nTTGCGTTT: pred:3.411 actual:2.500 (0.911)\n\n__both TAT and GCG__\nATATGCGA: pred:15.389 actual:15.875 (-0.486)\nTGCGTATT: pred:13.185 actual:13.625 (-0.440)\n\n\n\nThe CNN however is better able to adapt to the differences between 3-mer motifs! It predicts quite well on both the sequences with and without motifs."
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-2",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-2",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "Question 2",
    "text": "Question 2\nCompare the performance of the Linear and CNN models by using different learning rates. First run both models with higher learning rates (0.05, 0.1) and lower learning rates (0.005, 0.001), then create loss plots showing: - Linear model with these learning rates - CNN model with these learning rates\nThen analyze your results by answering: 1. How does changing the learning rate affect convergence for each model? 2. Which model is more sensitive to learning rate changes, and why? 3. Based on your analysis, what learning rate would you recommend for each model type, and why?\n ## 6. Check model predictions on the test set An important evaluation step in machine learning tasks is to check if your model can make good predictions on the test set, which it never saw during training. Here, we can use a parity plot to visualize the difference between the actual sequence scores vs the model’s predicted scores.\n\n#%pip install altair ## datapane\n\n\nimport altair as alt\nfrom sklearn.metrics import r2_score\n## import datapane as dp ## compatibility issues with pandas version\nimport os\n\n\n\ndef parity_plot(model_name,df,r2):\n    '''\n    Given a dataframe of samples with their true and predicted values,\n    make a scatterplot.\n    '''\n    plt.scatter(df['truth'].values, df['pred'].values, alpha=0.2)\n    \n    # y=x line\n    xpoints = ypoints = plt.xlim()\n    plt.plot(xpoints, ypoints, linestyle='--', color='k', lw=2, scalex=False, scaley=False)\n\n    plt.ylim(xpoints)\n    plt.ylabel(\"Predicted Score\",fontsize=14)\n    plt.xlabel(\"Actual Score\",fontsize=14)\n    plt.title(f\"{model_name} (r2:{r2:.3f})\",fontsize=20)\n    plt.show()\n\n\ndef alt_parity_plot(model, df, r2, datapane=False):\n    '''\n    Make an interactive parity plot with altair\n    '''\n    import os\n    import altair as alt\n    \n    os.makedirs('alt_out', exist_ok=True)\n    \n    # Convert model name to string to avoid any issues\n    model = str(model)\n    \n    # Create a clean version of the dataframe\n    plot_df = pd.DataFrame({\n        'truth': df['truth'].astype(float),\n        'pred': df['pred'].astype(float),\n        'seq': df['seq'].astype(str)\n    })\n    \n    # Create chart\n    chart = alt.Chart(plot_df).mark_point().encode(\n        x=alt.X('truth', type='quantitative', title='True Values'),\n        y=alt.Y('pred', type='quantitative', title='Predictions'),\n        tooltip=['seq']\n    ).properties(\n        title=str(f'{model} (r2:{r2:.3f})')\n    )\n    \n    chart.save(f'alt_out/parity_plot_{model}.html')\n    display(chart)\n\n\ndef parity_pred(models, seqs, oracle,alt=False,datapane=False):\n    '''Given some sequences, get the model's predictions '''\n    dfs = {} # key: model name, value: parity_df\n    \n    \n    for model_name,model in models:\n        print(f\"Running {model_name}\")\n        data = []\n        for dna in seqs:\n            s = torch.tensor(one_hot_encode(dna)).unsqueeze(0).to(DEVICE)\n            actual = oracle[dna]\n            pred = model(s.float())\n            data.append([dna,actual,pred.item()])\n        df = pd.DataFrame(data, columns=['seq','truth','pred'])\n        r2 = r2_score(df['truth'],df['pred'])\n        dfs[model_name] = (r2,df)\n        \n        #plot parity plot\n        if alt: # make an altair plot\n            alt_parity_plot(model_name, df, r2,datapane=datapane)\n            \n        else:\n            parity_plot(model_name, df, r2)\n\n\nseqs = test_df['seq'].values\nmodels = [\n    (\"Linear\", model_lin),\n    (\"CNN\", model_cnn)\n]\nparity_pred(models, seqs, oracle)\n\nRunning Linear\n\n\n\n\n\n\n\n\n\nRunning CNN\n\n\n\n\n\n\n\n\n\nParity plots are useful for visualizing how well your model predicts individual sequences: in a perfect model, they would all land on the y=x line, meaning that the model prediction was exactly the sequence’s actual value. But if it is off the y=x line, it means the model is over- or under-predicting.\nIn the Linear model, we can see that it can somewhat predict a trend in the Test set sequences, but really gets confused by these buckets of sequences in the high and low areas of the distribution (the ones with a motif).\nHowever for the CNN, it is much better at predicting scores close to the actual value! This is expected, given that the architecture of our CNN uses 3-mer kernels to scan along the sequence for influential motifs.\nBut the CNN isn’t perfect. We could probably train it longer or adjust the hyperparameters, but the goal here isn’t perfection - this is a very simple task relative to actual regulatory grammars. Instead, I thought it would be interesting to use the Altair visualization library to interactively inspect which sequences the models get wrong:\n\nalt.data_transformers.disable_max_rows() # disable altair warning\nparity_pred(models, seqs, oracle,alt=True)\n\nRunning Linear\n\n\n\n\n\n\n\n\nRunning CNN\n\n\n\n\n\n\n\n\nIf you’re viewing this notebook in interactive mode and run the above cell (just viewing via the github preview will omit the altair plot in the rendering), you can hover over the points and see the individual 8-mer sequences (you can also pan and zoom in this plot).\nNotice that the sequences that are off the diagonal tend to have multiple instance of the motifs! In the scoring function, we only gave the sequence a +/- bump if it had at least 1 motif, but it certainly would have been reasonable to decide to add multiple bonuses if the motif was present multiple times. In this example, I arbitrarily only added the bonus for at least 1 motif occurrence, but we could have made a different scoring function.\nIn any case, I thought it was cool that the model noticed the multiple occurrences and predicted them to be important. I suppose we did fool it a little, though an R2 of 0.95 is pretty respectable :)\n ## 7. Visualize convolutional filters When training CNN models, it can be useful to visualize the first layer convolutional filters to try to understand more about what the model is learning. With image data, the first layer convolutional filters often learn patterns such as borders or colors or textures - basic image elements that can be recombined to make more complex features.\nIn DNA, convolutional filters can be thought of like motif scanners. Similar to a position weight matrix for visualizing sequence logos, a convolutional filter is like a matrix showing a particular DNA pattern, but instead of being an exact sequence, it can hold some uncertainty about which nucleotides show up in which part of the pattern. Some positions might be very certain (i.e., there’s always an A in position 2; high information content) while other positions could hold a variety of nucleotides with about equal probability (high entropy; low information content).\nThe calculations that occur within the hidden layers of neural networks can get very complex and not every convolutional filter will be an obviously relevant pattern, but sometimes patterns in the filters do emerge and can be informative for helping to explain the model’s predictions.\nBelow are some functions to visualize the first layer convolutional filters, both as a raw heatmap and as a motif logo."
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#section",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#section",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "===",
    "text": "===\nVisualizing Convolutional Filters When training convolutional neural networks (CNNs), it’s often helpful to visualize the filters in the first convolutional layer to better understand what features the model is learning from the input data.\nIn image data:\nThe first-layer filters commonly learn simple, low-level features like:\nEdges or borders Color gradients Textures or basic shapes\nThese simple elements can be combined in deeper layers to form more complex patterns, such as object parts or entire shapes.\n🧬 In DNA sequence data:\nConvolutional filters act similarly to motif detectors. They can learn biologically meaningful patterns in the sequence, much like:\n\nPosition Weight Matrices (PWMs)\nSequence logos\n\nEach filter can be thought of as a matrix scanning for a specific DNA pattern — not necessarily a fixed sequence, but one that may allow for some variation in certain positions.\nFor example:\nOne position in the filter might strongly prefer an “A” (indicating low entropy and high information content). Another position might tolerate any nucleotide (high entropy, low information content). These learned filters can sometimes correspond to known biological motifs, or reveal novel sequence patterns that are predictive for the task at hand (e.g., enhancer activity, binding sites, expression levels)."
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-3",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-3",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "Question 3",
    "text": "Question 3\nDesign an approach to improve the model’s prediction accuracy, particularly focusing on the sequences where the current model performs poorly:\n\nAfter identifying sequences where the CNN model has high prediction errors, propose and implement a modification to either the model architecture, the loss function, the training process, or the data representation\nRetrain the model with your modifications\nCreate comparative visualizations (such as parity plots, error histograms, or other appropriate plots) to demonstrate the impact of your changes\nAnalyze your results by discussing how your modification addresses the specific weaknesses you identified. What are the trade-offs involved in your approach?\n\n\nimport logomaker\n\n\ndef get_conv_layers_from_model(model):\n    '''\n    Given a trained model, extract its convolutional layers\n    '''\n    model_children = list(model.children())\n    \n    # counter to keep count of the conv layers\n    model_weights = [] # we will save the conv layer weights in this list\n    conv_layers = [] # we will save the actual conv layers in this list\n    bias_weights = []\n    counter = 0 \n\n    # append all the conv layers and their respective weights to the list\n    for i in range(len(model_children)):\n        # get model type of Conv1d\n        if type(model_children[i]) == nn.Conv1d:\n            counter += 1\n            model_weights.append(model_children[i].weight)\n            conv_layers.append(model_children[i])\n            bias_weights.append(model_children[i].bias)\n\n        # also check sequential objects' children for conv1d\n        elif type(model_children[i]) == nn.Sequential:\n            for child in model_children[i]:\n                if type(child) == nn.Conv1d:\n                    counter += 1\n                    model_weights.append(child.weight)\n                    conv_layers.append(child)\n                    bias_weights.append(child.bias)\n\n    print(f\"Total convolutional layers: {counter}\")\n    return conv_layers, model_weights, bias_weights\n\ndef view_filters(model_weights, num_cols=8):\n    model_weights = model_weights[0]\n    num_filt = model_weights.shape[0]\n    filt_width = model_weights[0].shape[1]\n    num_rows = int(np.ceil(num_filt/num_cols))\n    \n    # visualize the first conv layer filters\n    plt.figure(figsize=(20, 17))\n\n    for i, filter in enumerate(model_weights):\n        ax = plt.subplot(num_rows, num_cols, i+1)\n        ax.imshow(filter.cpu().detach(), cmap='gray')\n        ax.set_yticks(np.arange(4))\n        ax.set_yticklabels(['A', 'C', 'G','T'])\n        ax.set_xticks(np.arange(filt_width))\n        ax.set_title(f\"Filter {i}\")\n\n    plt.tight_layout()\n    plt.show()\n\nFirst, we can take a peek at the raw filters.\n\nconv_layers, model_weights, bias_weights = get_conv_layers_from_model(model_cnn)\nview_filters(model_weights)\n\nTotal convolutional layers: 1\n\n\n\n\n\n\n\n\n\n\n\ndef get_conv_output_for_seq(seq, conv_layer):\n    '''\n    Given an input sequeunce and a convolutional layer, \n    get the output tensor containing the conv filter \n    activations along each position in the sequence\n    '''\n    # format seq for input to conv layer (OHE, reshape)\n    seq = torch.tensor(one_hot_encode(seq)).unsqueeze(0).permute(0,2,1).to(DEVICE)\n\n    # run seq through conv layer\n    with torch.no_grad(): # don't want as part of gradient graph\n        # apply learned filters to input seq\n        res = conv_layer(seq.float())\n        return res[0]\n    \n\ndef get_filter_activations(seqs, conv_layer,act_thresh=0):\n    '''\n    Given a set of input sequences and a trained convolutional layer, \n    determine the subsequences for which each filter in the conv layer \n    activate most strongly. \n    \n    1.) Run seq inputs through conv layer. \n    2.) Loop through filter activations of the resulting tensor, saving the\n            position where filter activations were &gt; act_thresh. \n    3.) Compile a count matrix for each filter by accumulating subsequences which\n            activate the filter above the threshold act_thresh\n    '''\n    # initialize dict of pwms for each filter in the conv layer\n    # pwm shape: 4 nucleotides X filter width, initialize to 0.0s\n    num_filters = conv_layer.out_channels\n    filt_width = conv_layer.kernel_size[0]\n    filter_pwms = dict((i,torch.zeros(4,filt_width)) for i in range(num_filters))\n    \n    print(\"Num filters\", num_filters)\n    print(\"filt_width\", filt_width)\n    \n    # loop through a set of sequences and collect subseqs where each filter activated\n    for seq in seqs:\n        # get a tensor of each conv filter activation along the input seq\n        res = get_conv_output_for_seq(seq, conv_layer)\n\n        # for each filter and it's activation vector\n        for filt_id,act_vec in enumerate(res):\n            # collect the indices where the activation level \n            # was above the threshold\n            act_idxs = torch.where(act_vec&gt;act_thresh)[0]\n            activated_positions = [x.item() for x in act_idxs]\n\n            # use activated indicies to extract the actual DNA\n            # subsequences that caused filter to activate\n            for pos in activated_positions:\n                subseq = seq[pos:pos+filt_width]\n                #print(\"subseq\",pos, subseq)\n                # transpose OHE to match PWM orientation\n                subseq_tensor = torch.tensor(one_hot_encode(subseq)).T\n\n                # add this subseq to the pwm count for this filter\n                filter_pwms[filt_id] += subseq_tensor            \n            \n    return filter_pwms\n\ndef view_filters_and_logos(model_weights,filter_activations, num_cols=8):\n    '''\n    Given some convolutional model weights and filter activation PWMs, \n    visualize the heatmap and motif logo pairs in a simple grid\n    '''\n    model_weights = model_weights[0].squeeze(1)\n    print(model_weights.shape)\n\n    # make sure the model weights agree with the number of filters\n    assert(model_weights.shape[0] == len(filter_activations))\n    \n    num_filts = len(filter_activations)\n    num_rows = int(np.ceil(num_filts/num_cols))*2+1 \n    # ^ not sure why +1 is needed... complained otherwise\n    \n    plt.figure(figsize=(20, 17))\n\n    j=0 # use to make sure a filter and it's logo end up vertically paired\n    for i, filter in enumerate(model_weights):\n        if (i)%num_cols == 0:\n            j += num_cols\n\n        # display raw filter\n        ax1 = plt.subplot(num_rows, num_cols, i+j+1)\n        ax1.imshow(filter.cpu().detach(), cmap='gray')\n        ax1.set_yticks(np.arange(4))\n        ax1.set_yticklabels(['A', 'C', 'G','T'])\n        ax1.set_xticks(np.arange(model_weights.shape[2]))\n        ax1.set_title(f\"Filter {i}\")\n\n        # display sequence logo\n        ax2 = plt.subplot(num_rows, num_cols, i+j+1+num_cols)\n        filt_df = pd.DataFrame(filter_activations[i].T.numpy(),columns=['A','C','G','T'])\n        filt_df_info = logomaker.transform_matrix(filt_df,from_type='counts',to_type='information')\n        logo = logomaker.Logo(filt_df_info,ax=ax2)\n        ax2.set_ylim(0,2)\n        ax2.set_title(f\"Filter {i}\")\n\n    plt.tight_layout()\n\n\n# just use some seqs from test_df to activate filters\nsome_seqs = random.choices(seqs, k=3000)\n\nfilter_activations = get_filter_activations(some_seqs, conv_layers[0])\nview_filters_and_logos(model_weights,filter_activations)\n\nNum filters 32\nfilt_width 3\ntorch.Size([32, 4, 3])\n\n\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n\n\n\n\n\n\n\n\n\n\nVisualize filters using a stronger activation threshold\nact_thresh = 1 instead of 0. (Some filters have no subsequence matches above the threshold and result in an empty motif logo)\n\nfilter_activations = get_filter_activations(some_seqs, conv_layers[0],act_thresh=1)\nview_filters_and_logos(model_weights,filter_activations)\n\nNum filters 32\nfilt_width 3\ntorch.Size([32, 4, 3])\n\n\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n\n\n\n\n\n\n\n\n\nFrom this particular CNN training, we can see a few filters have picked up on the strong TAT and GCG motifs, but other filters have focused on other patterns as well. There is some debate about how relevant convolutional filter visualizations are for model interpretability. In deep models with multiple convolutional layers, convolutional filters can be recombined in more complex ways inside the hidden layers, so the first layer filters may not be as informative on their own (Koo and Eddy, 2019). Much of the field has since moved towards attention mechanisms and other explainability methods, but should you be curious to visualize your filters as potential motifs, these functions may help get you started!\n ## 8. Conclusion This tutorial shows some basic Pytorch structure for building CNN models that work with DNA sequences. The practice task used in this demo is not reflective of real biological signals; rather, we designed the scoring method to simulate the presence of regulatory motifs in very short sequences that were easy for us humans to inspect and verify that Pytorch was behaving as expected. From this small example, we observed how a basic CNN with sliding filters was able to predict our scoring scheme better than a basic linear model that only accounted for absolute nucleotide position (without local context).\nTo read more about CNN’s applied to DNA in the wild, check out the following foundational papers: * DeepBind: Alipanahi et al 2015 * DeepSea: Zhou and Troyanskaya 2015 * Basset: Kelley et al 2016\nI hope other new-to-ML folks interested in tackling biological questions may find this helpful for getting started with using Pytorch to model DNA sequences :)"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html",
    "href": "post/2025-03-25-unit00/index.html",
    "title": "preparing environment for unit 00",
    "section": "",
    "text": "## download miniconda from the command line\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\n\n## or manually from https://www.anaconda.com/docs/getting-started/miniconda/main\n\n## install miniconda\nbash Miniconda3-latest-Linux-x86_64.sh\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#install-conda",
    "href": "post/2025-03-25-unit00/index.html#install-conda",
    "title": "preparing environment for unit 00",
    "section": "",
    "text": "## download miniconda from the command line\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\n\n## or manually from https://www.anaconda.com/docs/getting-started/miniconda/main\n\n## install miniconda\nbash Miniconda3-latest-Linux-x86_64.sh"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#create-conda-environment",
    "href": "post/2025-03-25-unit00/index.html#create-conda-environment",
    "title": "preparing environment for unit 00",
    "section": "create conda environment",
    "text": "create conda environment\nconda create -n gene46100 python=3.12\nconda activate gene46100"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#do-not-install-packages-with-conda",
    "href": "post/2025-03-25-unit00/index.html#do-not-install-packages-with-conda",
    "title": "preparing environment for unit 00",
    "section": "do not install packages with conda",
    "text": "do not install packages with conda\n(I ran into lots of issues with torch version incompatibility with torchvision and torchmetrics)\n# DONT USE CONDA TO INSTALL PYTORCH at least for now\n# conda install scikit-learn plotnine pytorch \n\n## installing torchvision and torchmetrics forced downgrading torch to 2.3.1 which was not compatible with mps"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#install-all-packages-within-jupyter-notebook-with-pip",
    "href": "post/2025-03-25-unit00/index.html#install-all-packages-within-jupyter-notebook-with-pip",
    "title": "preparing environment for unit 00",
    "section": "install all packages within jupyter notebook with %pip",
    "text": "install all packages within jupyter notebook with %pip\nas of 2025-03-24\nusing %pip will make sure that the packages are accessible by the kernel you are using for the jupyter notebook\n%pip install scikit-learn plotnine tqdm pandas \n%pip install torch\n%pip install torchvision\n%pip install torchmetrics"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#install-cursor-or-vscode-to-run-the-jupyter-notebook",
    "href": "post/2025-03-25-unit00/index.html#install-cursor-or-vscode-to-run-the-jupyter-notebook",
    "title": "preparing environment for unit 00",
    "section": "install cursor or vscode to run the jupyter notebook",
    "text": "install cursor or vscode to run the jupyter notebook\n\ninstall the python extension for cursor or vscode.\nselect the python interpreter to be the one in the conda environment you created (gene46100)"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#save-the-environment-for-reproducibility",
    "href": "post/2025-03-25-unit00/index.html#save-the-environment-for-reproducibility",
    "title": "preparing environment for unit 00",
    "section": "Save the environment for reproducibility",
    "text": "Save the environment for reproducibility\n\nto reproduce the environment exactly, save the environment\n# Save conda packages with their sources\nconda env export --from-history &gt; environment-gene46100.yml\n\n# Save pip-installed packages separately\npip list --format=freeze &gt; requirements-gene46100.txt\n\n# Save full environment state (for reference)\nconda env export &gt; environment_full-gene46100.yml"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#do-not-run-the-following-unless-need-to-reinstall-the-environment-for-some-reason",
    "href": "post/2025-03-25-unit00/index.html#do-not-run-the-following-unless-need-to-reinstall-the-environment-for-some-reason",
    "title": "preparing environment for unit 00",
    "section": "DO NOT RUN THE FOLLOWING UNLESS NEED TO REINSTALL THE ENVIRONMENT FOR SOME REASON",
    "text": "DO NOT RUN THE FOLLOWING UNLESS NEED TO REINSTALL THE ENVIRONMENT FOR SOME REASON\n\nreinstall the environment\n# Create environment from conda packages\nconda env create -f environment-gene46100.yml\n\n# Activate the environment\nconda activate gene46100\n\n# Install pip packages\npip install -r requirements-gene46100.txt"
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-prediction-starter.html",
    "href": "post/2025-03-25-unit00/tf-binding-prediction-starter.html",
    "title": "TF Binding prediction project",
    "section": "",
    "text": "TF binding prediction model\nThe goal of this project is to create a neural network model that predicts TF binding strength in a DNA sequence.\nTo do this, we have extracted 300 base pair-long DNA sequences that have a predicted binding site(s) from a TF, from a couple of chromosomes.\nThe training data is the following:\n\nThe sequences files: chr#_sequences.txt.gz store the 300 bp-long DNA sequences. A “window_name” in the format chr#_start_end has been assigned to each one.\nThe scores files: chr#_scores.txt.gz store a 300 bases long vector for each DNA sequence. Each position in these vectors correspond to a the sequence position. The values for each position represent the “binding score” that was predicted to that site by Homer, which is a widely used tool to discover motif binding sites for a given TF across the genome.\n\n\n1. Read-in the data\nThe data files for a couple of chromosomes are stored in the following link. Download them to your local folder.\nLet’s explore how the sequence and score data look like:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\n\nDefine data paths, this should be changed to your personal paths:\n\nPROJECT = '/Users/sofiasalazar/Library/CloudStorage/Box-Box/imlab-data/Courses/AI-in-Genomics-2025/'\nDATA = os.path.join(PROJECT, 'data')\n\nSequence data for chromosome 22\n\nsequences = pd.read_csv(os.path.join(DATA, 'chr22_sequences.txt.gz'), sep=\"\\t\", compression='gzip')\n\n\nsequences.head()\n\n\n\n\n\n\n\n\nsequence\nwindow_name\n\n\n\n\n0\nGCAAGACTCAGTCTCAAGGAAAAAAAAAAGCTCGAAAAATGTTTGC...\nchr22_10510500_10510799\n\n\n1\nAATCAAAAAGAATATTAGAAAACAAGCTGACAAAAAAATAAAAAAA...\nchr22_10512900_10513199\n\n\n2\nAGAAAAAGATATAAAGGCATCCAAATTGGAAAGGAAGAAGTAAGTA...\nchr22_10514100_10514399\n\n\n3\nCAAATGGATTGAAGACTTAAATGTAAGAACTAAAGCTGTAAAACTA...\nchr22_10515300_10515599\n\n\n4\nAAAATAGACCTACCATATGATGCAGCAATCCCACTTGTGGGCATTT...\nchr22_10515900_10516199\n\n\n\n\n\n\n\n\nsequences.shape\n\n(23139, 2)\n\n\nTF binding scores for chromosome 22\nHere, each column has 300 values for each sequence, each value is the TF binding score for each position of the sequence. Most positions have ‘0’ as no motif is predicted to bind at those positions. One motif is a couple of bp-long and all of those bp will have the same score since they belong to the same motif.\n\nscores = pd.read_csv(os.path.join(DATA, 'chr22_scores.txt.gz'), sep=\"\\t\", compression='gzip')\n\n\nnp.array(scores.iloc[:, 0])\n\narray([0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 9.708916, 9.708916, 9.708916,\n       9.708916, 9.708916, 9.708916, 9.708916, 9.708916, 9.708916,\n       9.708916, 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       7.859208, 7.859208, 7.859208, 7.859208, 7.859208, 7.859208,\n       7.859208, 7.859208, 7.859208, 7.859208, 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       7.693852, 7.693852, 7.693852, 7.693852, 7.693852, 7.693852,\n       7.693852, 7.693852, 7.693852, 7.693852, 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ])\n\n\nTake a look at how these score vectors look like, the blue sections represent the predicted binding sites for this TF:\n\nx = np.arange(300)\nbar_width = 0.4\nplt.figure(figsize=(12, 5))\nplt.bar(x - bar_width, scores.iloc[:, 0], width=bar_width, label=\"Predicted\", alpha=0.7, color='b')\nplt.xlabel(\"Position in sequence window\")\nplt.ylabel(\"Homer score\")\nplt.grid(axis='y', linestyle='--', alpha=0.6)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2. Model training\nNow, the goal is to use these sequences to train a neural network model that predicts the scores vectors. Overall, the structure of the code should more or less follow these steps:\n\nOne-hot-encode the DNA sequences\nSplit sequences and their corresponding scores into training, test and validation sets\nBuild dataloaders for the training and test sets using sequences as predictor features and the scores as targets\nDefine a NN model architecture\nTrain the model\nTest the model on the test sequences\n\nThis process will be iterative as you find an optimal set of hyperparameters. Please share your best-performing model and we will test it on a set of held-out-data.\nAdditional notes\n\nNote how the scores are values greater than 1, you can try binarizing these values so they are between 0 and 1 and compare between models\nTo assess performance, you can use the following code to compute correlations between predicted scores and the ground truth\n\n\nfrom scipy.stats import pearsonr\ndef plot_comparison(pred, obs):\n  r_value = pearsonr(pred, obs)\n  x = np.arange(len(pred))\n  bar_width = 0.4\n  plt.figure(figsize=(12, 5))\n  plt.bar(x - bar_width, pred, width=bar_width, label=\"Predicted\", alpha=0.7, color='b')\n  plt.bar(x + bar_width, obs, width=bar_width, label=\"Observed\", alpha=0.7, color='r')\n  plt.xlabel(\"Position sequence window\")\n  plt.ylabel(\"Value\")\n  plt.title(\"Comparison of sequence scores\")\n  plt.legend(title=f\"Pearson R: {r_value:.2f}\")\n  plt.grid(axis='y', linestyle='--', alpha=0.6)\n  plt.show()\n\n\n\n\n\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "index-all.html",
    "href": "index-all.html",
    "title": "Deep Learning in Genomics Course Material - ALL",
    "section": "",
    "text": "This page contains ALL material for the GENE46100 Deep learning in genomics course, including test notes.\nFind the 2025 syllabus here.\nEdit github source here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTF binding prediction challenge\n\n\n\n\n\n\ngene46100\n\n\nproject\n\n\n\nCompetition details for TF binding prediction challenge\n\n\n\n\n\nApr 8, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 2\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nApr 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nUpdated - DNA score prediction with Pytorch\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nApr 4, 2025\n\n\nErin Wilson\n\n\n\n\n\n\n\n\n\n\n\n\nTF Binding prediction project\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2025\n\n\nSofia Salazar\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring information in DNA sequence\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nFit linear mode to linear data\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 1\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMar 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nQuick introduction to deep learning\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nMar 25, 2025\n\n\nBoxiang Liu, modified by Haky Im\n\n\n\n\n\n\n\n\n\n\n\n\npreparing environment for unit 00\n\n\n\n\n\n\ngene46100\n\n\nhowto\n\n\n\nsetting up conda environement and installing packages\n\n\n\n\n\nMar 24, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nGradient descent illustration\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nSlides unit 00\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTODO\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2020\n\n\n\n\n\n\nNo matching items\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/homework-02.html",
    "href": "post/2025-03-25-unit00/homework-02.html",
    "title": "homework 2",
    "section": "",
    "text": "Homework 2\n\nrun all the code in the DNA scoring tutorial notebook. You should also be able to find the notebook in your cloned course repository. (20 points)\ncomplete the exercises in the notebook (30 points)\nsubmit the jupyter notebook with the results to canvas assignment\n\n\n\n\n\n\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/homework-01.html",
    "href": "post/2025-03-25-unit00/homework-01.html",
    "title": "homework 1",
    "section": "",
    "text": "Homework 1\n\nset up the environment on macos as described in the page (10 points)\n\nUpload the environment-gene46100.yml and requirements-gene46100.txt with the canvas assignment\nIf you have any issues, please let me know.\n\ngit clone the course repository to your local machine (10 points)\n\nUpload the screenshot of the terminal command you used to clone the repository with the canvas assignment\n\nrun all the code in the notebook. You should also be able to find the notebook in your cloned course repository. (20 points)\ncomplete the exercises in the notebook (30 points)\nsubmit the jupyter notebook with the results to canvas assignment\n\n\n\n\n\n\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-challenge.html",
    "href": "post/2025-03-25-unit00/tf-binding-challenge.html",
    "title": "TF binding prediction challenge",
    "section": "",
    "text": "Goal: Predict transcription factor (TF) binding scores in DNA sequences\nInput: 300bp human DNA sequences\nTarget: Binding scores for transcription factors\n\n\n\n\nThe challenge uses real genomic data to predict TF binding scores:\n\nSequence Data: chr#_sequences.txt.gz files containing 300bp DNA sequences\n\nEach sequence has a unique identifier in format chr#_start_end\n\nTarget Data: chr#_scores.txt.gz files containing binding scores\n\nEach sequence has a corresponding 300-long vector of binding scores\nScores are predicted using Homer, a widely used motif discovery tool\nEach position in the vector represents the binding score at that position in the sequence\n\n\n\n\n\n\nData download link\nStarter notebook\nExample implementation\n\n\n\n\n\nTraining Sessions:\n\nTuesday, April 8: Sofia will review implementation of basic DNA scoring model\nThursday, April 10: Sofia will explain weights and biases usage\n\nPresentation Day (Thursday, April 10): Students will present:\n\nModel architecture\nModel performance\nFilter interpretation (time permitting)\nLessons learned\n\nSubmission Deadline: April 18\n\nSubmit best model to Canvas\nTA will test on held-out data\nLeaderboard will be created\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-challenge.html#overview",
    "href": "post/2025-03-25-unit00/tf-binding-challenge.html#overview",
    "title": "TF binding prediction challenge",
    "section": "",
    "text": "Goal: Predict transcription factor (TF) binding scores in DNA sequences\nInput: 300bp human DNA sequences\nTarget: Binding scores for transcription factors"
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-challenge.html#data-description",
    "href": "post/2025-03-25-unit00/tf-binding-challenge.html#data-description",
    "title": "TF binding prediction challenge",
    "section": "",
    "text": "The challenge uses real genomic data to predict TF binding scores:\n\nSequence Data: chr#_sequences.txt.gz files containing 300bp DNA sequences\n\nEach sequence has a unique identifier in format chr#_start_end\n\nTarget Data: chr#_scores.txt.gz files containing binding scores\n\nEach sequence has a corresponding 300-long vector of binding scores\nScores are predicted using Homer, a widely used motif discovery tool\nEach position in the vector represents the binding score at that position in the sequence"
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-challenge.html#getting-started",
    "href": "post/2025-03-25-unit00/tf-binding-challenge.html#getting-started",
    "title": "TF binding prediction challenge",
    "section": "",
    "text": "Data download link\nStarter notebook\nExample implementation"
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-challenge.html#timeline",
    "href": "post/2025-03-25-unit00/tf-binding-challenge.html#timeline",
    "title": "TF binding prediction challenge",
    "section": "",
    "text": "Training Sessions:\n\nTuesday, April 8: Sofia will review implementation of basic DNA scoring model\nThursday, April 10: Sofia will explain weights and biases usage\n\nPresentation Day (Thursday, April 10): Students will present:\n\nModel architecture\nModel performance\nFilter interpretation (time permitting)\nLessons learned\n\nSubmission Deadline: April 18\n\nSubmit best model to Canvas\nTA will test on held-out data\nLeaderboard will be created"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html",
    "title": "Quick introduction to deep learning",
    "section": "",
    "text": "Understand and implement gradient descent to fit a linear model\nBuild and train a multi-layer perceptron (MLP) using PyTorch\nLearn the basics of PyTorch, including model definition, forward pass, and optimization\n\n\n\nWe’ll start by making sure we have all the required Python packages installed and ready to go. PyTorch is the main library we’ll use for deep learning.\n\n## install packages if needed\nif False:\n    %pip install scikit-learn plotnine tqdm pandas\n\n\nfrom sklearn.datasets import make_regression\nimport numpy as np\nfrom numpy.linalg import inv\nfrom plotnine import qplot, ggplot, geom_point, geom_line, aes, geom_abline\nfrom plotnine.themes import theme_bw\nfrom plotnine.geoms import annotate\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#simulate-linear-data",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#simulate-linear-data",
    "title": "Quick introduction to deep learning",
    "section": "Simulate linear data",
    "text": "Simulate linear data\nWe generate 1000 samples, each with 2 features. We’ll define the “true” coefficients and add a bit of noise to make it realistic.\n\nx: predictors/features (shape: 1000×2)\ny: response variable (shape: 1000×1)\ncoef = [\\(\\beta_1 \\beta_2\\)]: the true coefficients used to generate \\(y\\)\n\n\nnp.random.seed(42)\nbias = 0\nnoise = 10\nx, y, coef = make_regression(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_targets=1,\n    bias=bias,\n    noise=noise,\n    coef=True\n    )\n\n\nshow simulated x, y, and coef= [\\(\\beta_1 \\beta_2\\)]\n\nprint('x.shape:', x.shape)\nprint('y.shape:', y.shape)\nprint('coef.shape:', coef.shape)\n\nx.shape: (1000, 2)\ny.shape: (1000,)\ncoef.shape: (2,)\n\n\n\nprint('x:\\n',x,'\\n')\nprint('y[0:10]:\\n',y[0:10],'\\n')\nprint('coef:\\n',coef,'\\n')\n\nx:\n [[-0.16711808  0.14671369]\n [-0.02090159  0.11732738]\n [ 0.15041891  0.364961  ]\n ...\n [ 0.30263547 -0.75427585]\n [ 0.38193545  0.43004165]\n [ 0.07736831 -0.8612842 ]] \n\ny[0:10]:\n [-14.99694989 -12.67808888  17.77545452   6.66146467 -14.19552996\n -25.24484815 -39.23162627 -52.01803821   5.76368853 -50.11860295] \n\ncoef:\n [40.71064891  6.60098441]"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#predict-y-with-ground-truth-parameters",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#predict-y-with-ground-truth-parameters",
    "title": "Quick introduction to deep learning",
    "section": "Predict y with ground truth parameters",
    "text": "Predict y with ground truth parameters\nSince we know the true coefficients, we can directly compute the predicted y and visualize how well they fit.\n\ny_hat = x.dot(coef) + bias\n\n\ncompare \\(\\hat{y}\\) and \\(y\\)\n\n(qplot(x=y, y=y_hat, geom=\"point\", xlab=\"y\", ylab=\"y_hat\") + geom_abline(intercept=0, slope=1, color='gray', linetype='dashed'))"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#compute-analytical-solution",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#compute-analytical-solution",
    "title": "Quick introduction to deep learning",
    "section": "Compute analytical solution",
    "text": "Compute analytical solution\nFor linear regression, there’s a closed-form solution known as the normal equation:\n\\(\\hat{\\beta} = (X^T X)^{-1} X^T y\\)\nThis gives us the optimal coefficients that minimize mean square errors.\nLet’s get \\(\\hat{\\beta}\\) and compare it with the ground truth.\n\nExercise\nDerive the normal equation from the model, using matrix algebra\n\nvar = x.transpose().dot(x)\ncov = x.transpose().dot(y)\nb_hat = inv(var).dot(cov)\n\n\nprint(\"Estimated\")\nprint(b_hat)\n\nprint(\"Ground truth\")\nprint(coef)\n\nEstimated\n[41.06972678  6.79965716]\nGround truth\n[40.71064891  6.60098441]\n\n\n\n\nResult Comparison\nEstimated coefficients using the normal equation are close to the true ones, despite noise. That’s a good sanity check.\n\n\nExercise\n\nPlot the prediction with the analystical estimates vs the target \\(y\\)"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#define-stochastic-gradient-descent-sgd",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#define-stochastic-gradient-descent-sgd",
    "title": "Quick introduction to deep learning",
    "section": "Define Stochastic Gradient Descent (SGD)",
    "text": "Define Stochastic Gradient Descent (SGD)\nWhen analytical solutions aren’t available (which is often), we rely on numerical optimization like gradient descent.\nAlthough linear regression has analytical solutions, this is unfortunately not the case for many other models. We may need to resort to numerical approximations to find the optimal parameters. One of the most popular numerical optimizers is called stochastic gradient descent."
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-a-gradient",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-a-gradient",
    "title": "Quick introduction to deep learning",
    "section": "What is a Gradient?",
    "text": "What is a Gradient?\nA gradient is the derivative of a function, and it tells us how to adjust parameters to reduce the error.\n$f’() = _{→ 0} = $"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-gradient-descent",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-gradient-descent",
    "title": "Quick introduction to deep learning",
    "section": "What is Gradient Descent?",
    "text": "What is Gradient Descent?\nAn optimization algorithm that iteratively updates parameters in the direction of steepest descent (negative gradient) to minimize a loss function.\n\nLearning rate (\\(\\alpha\\)): step size. It is a hyperparameters that determines how fast we move in the direction of the gradient.\nLoss surface: the landscape we are optimizing over\n\nAt a particular \\(β_i\\), we find the gradient \\(f'(\\beta)\\), and take a step along the direction of the gradient to find the next point \\(\\beta_{i+1}\\).\n\\(\\beta_{i+1} = \\beta_i - \\alpha f'(\\beta_i)\\)\nThe \\(\\alpha\\) is called the learning rate.\n\n\n\ngradient descent"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-stochastic-gradient-descent",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-stochastic-gradient-descent",
    "title": "Quick introduction to deep learning",
    "section": "What is stochastic gradient descent?",
    "text": "What is stochastic gradient descent?\nIn real-world applications, the size of our dataset is so large that it is impossible to calculate the gradient using all data points. Therefore, we take a small chunk (called a “batch”) of the dataset to calcalate the gradient. This approximates the full-data gradients, thus the word stochastic."
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#three-components-of-machine-learning",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#three-components-of-machine-learning",
    "title": "Quick introduction to deep learning",
    "section": "Three Components of Machine Learning",
    "text": "Three Components of Machine Learning\nTo build a machine learning model, we need:\n\nModel: Defines the hypothesis space (e.g., linear model)\nLoss Function: Measures how well the model fits\nOptimizer: Updates parameters to reduce the loss (e.g., SGD)"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#define-the-loss-function",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#define-the-loss-function",
    "title": "Quick introduction to deep learning",
    "section": "Define the Loss Function",
    "text": "Define the Loss Function\nCommon choice for regression: Mean Squared Error (MSE)\n\\(\\ell(\\beta) = \\frac{1}{2}\\sum\\limits_{i=1}^m (f(\\beta)^{i} - y^{i})^2 / m\\)"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#compute-the-gradient",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#compute-the-gradient",
    "title": "Quick introduction to deep learning",
    "section": "Compute the gradient",
    "text": "Compute the gradient\n\\(\\frac{\\partial}{\\partial \\beta_j}\\ell(\\beta) = \\frac{\\partial}{\\partial \\beta_j} \\frac{1}{2} \\sum_i (f(\\beta)^i - y^i)^2 =  \\sum_i (f(\\beta)^i - y^i) x_j\\)\nRecall that in our example $ f() = = _1 x_1 + _2 x_2$. Here \\(\\beta_j\\) is either \\(\\beta_1\\) or \\(\\beta_2\\).\n\nExercise\nShow that the derivative of the loss function is as stated above."
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#optimize-estimate-parameters-with-gradient-descent",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#optimize-estimate-parameters-with-gradient-descent",
    "title": "Quick introduction to deep learning",
    "section": "Optimize: estimate parameters with gradient descent",
    "text": "Optimize: estimate parameters with gradient descent\n\nlr = 0.1 # learning rate\nb = [0.0, 0.0] # initialize all betas to 0\nn_examples = len(y)\ntrajectory = [b]\n\nfor _ in range(50): # 50 steps\n  diff = x.dot(b) - y\n  grad = diff.dot(x) / n_examples\n  b -= lr*grad\n  trajectory.append(b.copy())\n\ntrajectory = np.stack(trajectory, axis=1)\n\n\nqplot(x=trajectory[0], y=trajectory[1], xlab=\"beta_1\", ylab=\"beta_2\", geom=[\"point\", \"line\"]) + theme_bw() + annotate(geom=\"point\", x=coef[0], y=coef[1], size=8, color=\"blue\",shape='+',stroke=1)\n\n\n\n\n\n\n\n\n\nExercise\n\nAdd to the plot the normal equation estimates (traditional linear regression) of the coefficients in a different green\n\n\n\nExercise\n\nSimulate \\(y\\) with larger noise, estimate the regression coefficients using the normal equation and the gradient descent method, and plot the trajectory, the ground truth, and the two estimates for comparison."
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#simulate-non-linear-data",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#simulate-non-linear-data",
    "title": "Quick introduction to deep learning",
    "section": "Simulate non linear data",
    "text": "Simulate non linear data\nLet’s start by simulating data according to the generative model:\n$ y = x_1^3$\n\nx = np.random.normal(size=1000)\ny = x ** 3"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#predict-with-simple-linear-model",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#predict-with-simple-linear-model",
    "title": "Quick introduction to deep learning",
    "section": "Predict with simple linear model",
    "text": "Predict with simple linear model\nLet’s try to predict with a simple linear regression model\n$ y = X $\n\nQuestion\nhow many parameters do we need to estimate for this simple linear model?\n\nlr = 0.1 # learning rate\nb = 0.0 # initialize all betas to 0\nn_examples = len(y)\n\nfor _ in range(50): # 50 steps\n  diff = x.dot(b) - y\n  grad = diff.dot(x) / n_examples\n  b -= lr*grad\n\n\ny_hat = x.dot(b)\n( qplot(x = y, y=y_hat, geom=\"point\", xlab=\"y\", ylab=\"y_hat\") +\ngeom_abline(intercept=0, slope=1, color='gray', linetype='dashed') )\n\n\n\n\n\n\n\n\n\n\nWe would like to be closer to the dashed gray, identity line! Let’s try MLP."
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#install-packages-if-needed",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#install-packages-if-needed",
    "title": "Quick introduction to deep learning",
    "section": "Install packages if needed",
    "text": "Install packages if needed\n\n## install packages if needed\nif False:  # Change to True to run the commands\n    %pip install tqdm\n    %pip install torch\n    %pip install torchvision torchmetrics\n\n\nimport torch\nprint(torch.__version__)\n## if on a mac check if mps is available so you use gpu\n## print(torch.backends.mps.is_available())\n\n2.6.0\n\n\n\nDefine a non linear MLP\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n#device = torch.device(\"cuda\")\ndevice = torch.device(\"mps\") ## use this line instead of the one above withb cuda\n\n\nclass MLP(nn.Module):\n  def __init__(self, input_dim, hid_dim, output_dim):\n    super().__init__()\n    self.fc1 = nn.Linear(input_dim, hid_dim)\n    self.fc2 = nn.Linear(hid_dim, output_dim)\n\n  def forward(self, x):\n    x = F.relu(self.fc1(x))\n    y = self.fc2(x)\n\n    return y.squeeze(1)\n\nmlp = MLP(input_dim=1, hid_dim=1024, output_dim=1).to(device)"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#train-mlp-model-using-pytorch",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#train-mlp-model-using-pytorch",
    "title": "Quick introduction to deep learning",
    "section": "train MLP model using pytorch",
    "text": "train MLP model using pytorch\nWe are now ready to train our model. To understand how our model is doing, we record the loss vs step, which is called the learning curve in ML literature.\n\nx_tensor = torch.Tensor(x).unsqueeze(1).to(device)\ny_tensor = torch.Tensor(y).to(device)\nlearning_curve = []\n\noptimizer = torch.optim.SGD(mlp.parameters(), lr=0.001)\nloss_fn = nn.MSELoss()\n\nfor epoch in range(10000):\n  optimizer.zero_grad()\n  y_hat = mlp(x_tensor)\n  loss = loss_fn(y_hat, y_tensor)\n  learning_curve.append(loss.item())\n  loss.backward()\n  optimizer.step()\n\n\nqplot(x=range(10000), y=learning_curve, xlab=\"epoch\", ylab=\"loss\")\n\n\n\n\n\n\n\n\n\ny_hat = mlp(x_tensor)\ny_hat = y_hat.detach().cpu().numpy()\n#qplot(x=y, y=y_hat, geom=\"point\", xlab=\"y\", ylab=\"y_hat\")\nqplot(x=y, y=y_hat, geom=[\"point\", \"abline\"],\n      xlab=\"y\", ylab=\"y_hat\",\n      abline=dict(slope=1, intercept=0, color='red', linetype='dashed'))\n\n\n\n\n\n\n\n\nNow this looks much better!"
  },
  {
    "objectID": "DRAFTS/slides-00.html",
    "href": "DRAFTS/slides-00.html",
    "title": "Slides unit 00",
    "section": "",
    "text": "An MLP is a fully connected feedforward neural network — the classic deep learning architecture.\n\n\n\nInput Layer → Hidden Layer(s) → Output Layer\n\n\n\nMLP with 2 features in the input layer, one hidden layer, one dimentional output layer\n\n\nEach layer performs:\noutput = activation(Wx + b)\nWhere:\nW = weight matrix\n\nx = input vector\n\nb = bias\n\nactivation = non-linear function like ReLU\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "DRAFTS/slides-00.html#core-idea",
    "href": "DRAFTS/slides-00.html#core-idea",
    "title": "Slides unit 00",
    "section": "",
    "text": "An MLP is a fully connected feedforward neural network — the classic deep learning architecture."
  },
  {
    "objectID": "DRAFTS/slides-00.html#architecture",
    "href": "DRAFTS/slides-00.html#architecture",
    "title": "Slides unit 00",
    "section": "",
    "text": "Input Layer → Hidden Layer(s) → Output Layer\n\n\n\nMLP with 2 features in the input layer, one hidden layer, one dimentional output layer\n\n\nEach layer performs:\noutput = activation(Wx + b)\nWhere:\nW = weight matrix\n\nx = input vector\n\nb = bias\n\nactivation = non-linear function like ReLU"
  },
  {
    "objectID": "DRAFTS/slides-00.html#data---model-input",
    "href": "DRAFTS/slides-00.html#data---model-input",
    "title": "Slides unit 00",
    "section": "1. Data -> Model Input",
    "text": "1. Data -&gt; Model Input\nDNA/RNA/protein sequences → One-hot encoded or embedded tensors\n\nWrapped in custom Dataset + DataLoader for batching"
  },
  {
    "objectID": "DRAFTS/slides-00.html#training-loop",
    "href": "DRAFTS/slides-00.html#training-loop",
    "title": "Slides unit 00",
    "section": "2. Training Loop",
    "text": "2. Training Loop\nfor xb, yb in train_loader: optimizer.zero_grad() # Step 1: Clear old gradients preds = model(xb) # Step 2: Forward pass loss = loss_fn(preds, yb) # Step 3: Compute loss loss.backward() # Step 4: Backprop: compute gradients optimizer.step() # Step 5: Update weights"
  },
  {
    "objectID": "DRAFTS/slides-00.html#model-components",
    "href": "DRAFTS/slides-00.html#model-components",
    "title": "Slides unit 00",
    "section": "3. Model Components",
    "text": "3. Model Components\nnn.Module: Defines layers and forward pass\n\nnn.Conv1d, nn.Linear, nn.ReLU, etc.\n\nOutputs: regression (e.g. expression levels) or classification (e.g. binding sites)"
  },
  {
    "objectID": "DRAFTS/slides-00.html#loss-function",
    "href": "DRAFTS/slides-00.html#loss-function",
    "title": "Slides unit 00",
    "section": "4. Loss Function",
    "text": "4. Loss Function\nnn.MSELoss() for expression prediction\n\nnn.CrossEntropyLoss() for classification\n\nnn.BCELoss() for binary classification, this is just -log likelihood for a bernoulli distribution"
  },
  {
    "objectID": "DRAFTS/slides-00.html#optimizer",
    "href": "DRAFTS/slides-00.html#optimizer",
    "title": "Slides unit 00",
    "section": "5. Optimizer",
    "text": "5. Optimizer\nCommon: torch.optim.Adam, SGD, etc.\n\nControls learning rate and parameter updates"
  },
  {
    "objectID": "DRAFTS/slides-00.html#evaluation",
    "href": "DRAFTS/slides-00.html#evaluation",
    "title": "Slides unit 00",
    "section": "6. Evaluation",
    "text": "6. Evaluation\nUse model.eval() + torch.no_grad() during validation\n\nTrack metrics like loss, R², accuracy, AUC"
  },
  {
    "objectID": "DRAFTS/2025-02-20-testing/gradient-descent-illustration.html",
    "href": "DRAFTS/2025-02-20-testing/gradient-descent-illustration.html",
    "title": "Gradient descent illustration",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom plotnine import *\n\n\n# Loss function and its gradient\ndef loss(beta):\n    return beta ** 2\n\ndef grad(beta):\n    return 2 * beta\n\n# Simulate gradient descent\nbeta = 3.5  # random initial value\nlearning_rate = 0.2\nsteps = []\n\nfor i in range(10):\n    current_loss = loss(beta)\n    steps.append({'step': i, 'beta': beta, 'loss': current_loss})\n    beta -= learning_rate * grad(beta)\n\ndf = pd.DataFrame(steps)\n\n# Create full curve for loss function\nbeta_vals = np.linspace(-4, 4, 200)\nloss_vals = loss(beta_vals)\ndf_curve = pd.DataFrame({'beta': beta_vals, 'loss': loss_vals})\n\n# Create arrows for learning steps\ndf_arrows = df[:-1].copy()\ndf_arrows['beta_end'] = df['beta'][1:].values\ndf_arrows['loss_end'] = df['loss'][1:].values\n\n# Plot\np = (\n    ggplot(df_curve, aes('beta', 'loss')) +\n    geom_line(size=1.5, color=\"gray\") +\n    geom_point(df, aes('beta', 'loss'), color='purple', size=3) +\n    geom_point(df.tail(1), aes('beta', 'loss'), color='yellow', size=4) +\n    geom_segment(df_arrows,\n                 aes(x='beta', y='loss', xend='beta_end', yend='loss_end'),\n                 arrow=arrow(length=0.15, type='closed'),\n                 color='gray') +\n    annotate('text', x=3.5, y=12, label='Random\\ninitial value', ha='left') +\n#    annotate('text', x=1.5, y=5, label='Learning step', ha='left') +\n#    annotate('text', x=0, y=0.5, label='Minimum', ha='left') +\n    geom_vline(xintercept=0, linetype='dashed', color='gray') +\n    labs(x=r'$\\beta$', y='Loss') +\n    theme_minimal() +\n    theme(\n        figure_size=(7, 4),\n        axis_title=element_text(size=12),\n        axis_text=element_text(size=10)\n    )\n)\n\np\n\n\n\n\n\n\n\n\n\n\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "DRAFTS/2020-01-01-TODO/index.html",
    "href": "DRAFTS/2020-01-01-TODO/index.html",
    "title": "TODO",
    "section": "",
    "text": "TODO\n\n\n\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  }
]