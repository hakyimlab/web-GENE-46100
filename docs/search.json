[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning in Genomics Course Material",
    "section": "",
    "text": "This page contains material for the GENE46100 Deep learning in genomics course.\nFind the 2025 syllabus here.\nEdit github source here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUpdated - DNA score prediction with Pytorch\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nApr 4, 2025\n\n\nErin Wilson\n\n\n\n\n\n\n\n\n\n\n\n\nFit linear mode to linear data\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 1\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMar 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nDNA score prediction with Pytorch\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nMar 25, 2025\n\n\nErin Wilson\n\n\n\n\n\n\n\n\n\n\n\n\nQuick introduction to deep learning\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nMar 25, 2025\n\n\nBoxiang Liu, modified by Haky Im\n\n\n\n\n\n\n\n\n\n\n\n\npreparing environment for unit 00\n\n\n\n\n\n\ngene46100\n\n\nhowto\n\n\n\nsetting up conda environement and installing packages\n\n\n\n\n\nMar 24, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nGradient descent illustration\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nSlides unit 00\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTODO\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2020\n\n\n\n\n\n\nNo matching items\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html",
    "title": "Quick introduction to deep learning",
    "section": "",
    "text": "Understand and implement gradient descent to fit a linear model\nBuild and train a multi-layer perceptron (MLP) using PyTorch\nLearn the basics of PyTorch, including model definition, forward pass, and optimization\n\n\n\nWe’ll start by making sure we have all the required Python packages installed and ready to go. PyTorch is the main library we’ll use for deep learning.\n\n## install packages if needed\nif False:\n    %pip install scikit-learn plotnine tqdm pandas\n\n\nfrom sklearn.datasets import make_regression\nimport numpy as np\nfrom numpy.linalg import inv\nfrom plotnine import qplot, ggplot, geom_point, geom_line, aes, geom_abline\nfrom plotnine.themes import theme_bw\nfrom plotnine.geoms import annotate\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#simulate-linear-data",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#simulate-linear-data",
    "title": "Quick introduction to deep learning",
    "section": "Simulate linear data",
    "text": "Simulate linear data\nWe generate 1000 samples, each with 2 features. We’ll define the “true” coefficients and add a bit of noise to make it realistic.\n\nx: predictors/features (shape: 1000×2)\ny: response variable (shape: 1000×1)\ncoef = [\\(\\beta_1 \\beta_2\\)]: the true coefficients used to generate \\(y\\)\n\n\nnp.random.seed(42)\nbias = 0\nnoise = 10\nx, y, coef = make_regression(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_targets=1,\n    bias=bias,\n    noise=noise,\n    coef=True\n    )\n\n\nshow simulated x, y, and coef= [\\(\\beta_1 \\beta_2\\)]\n\nprint('x.shape:', x.shape)\nprint('y.shape:', y.shape)\nprint('coef.shape:', coef.shape)\n\nx.shape: (1000, 2)\ny.shape: (1000,)\ncoef.shape: (2,)\n\n\n\nprint('x:\\n',x,'\\n')\nprint('y[0:10]:\\n',y[0:10],'\\n')\nprint('coef:\\n',coef,'\\n')\n\nx:\n [[-0.16711808  0.14671369]\n [-0.02090159  0.11732738]\n [ 0.15041891  0.364961  ]\n ...\n [ 0.30263547 -0.75427585]\n [ 0.38193545  0.43004165]\n [ 0.07736831 -0.8612842 ]] \n\ny[0:10]:\n [-14.99694989 -12.67808888  17.77545452   6.66146467 -14.19552996\n -25.24484815 -39.23162627 -52.01803821   5.76368853 -50.11860295] \n\ncoef:\n [40.71064891  6.60098441]"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#predict-y-with-ground-truth-parameters",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#predict-y-with-ground-truth-parameters",
    "title": "Quick introduction to deep learning",
    "section": "Predict y with ground truth parameters",
    "text": "Predict y with ground truth parameters\nSince we know the true coefficients, we can directly compute the predicted y and visualize how well they fit.\n\ny_hat = x.dot(coef) + bias\n\n\ncompare \\(\\hat{y}\\) and \\(y\\)\n\n(qplot(x=y, y=y_hat, geom=\"point\", xlab=\"y\", ylab=\"y_hat\") + geom_abline(intercept=0, slope=1, color='gray', linetype='dashed'))"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#compute-analytical-solution",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#compute-analytical-solution",
    "title": "Quick introduction to deep learning",
    "section": "Compute analytical solution",
    "text": "Compute analytical solution\nFor linear regression, there’s a closed-form solution known as the normal equation:\n\\(\\hat{\\beta} = (X^T X)^{-1} X^T y\\)\nThis gives us the optimal coefficients that minimize mean square errors.\nLet’s get \\(\\hat{\\beta}\\) and compare it with the ground truth.\n\nExercise\nDerive the normal equation from the model, using matrix algebra\n\nvar = x.transpose().dot(x)\ncov = x.transpose().dot(y)\nb_hat = inv(var).dot(cov)\n\n\nprint(\"Estimated\")\nprint(b_hat)\n\nprint(\"Ground truth\")\nprint(coef)\n\nEstimated\n[41.06972678  6.79965716]\nGround truth\n[40.71064891  6.60098441]\n\n\n\n\nResult Comparison\nEstimated coefficients using the normal equation are close to the true ones, despite noise. That’s a good sanity check.\n\n\nExercise\n\nPlot the prediction with the analystical estimates vs the target \\(y\\)"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#define-stochastic-gradient-descent-sgd",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#define-stochastic-gradient-descent-sgd",
    "title": "Quick introduction to deep learning",
    "section": "Define Stochastic Gradient Descent (SGD)",
    "text": "Define Stochastic Gradient Descent (SGD)\nWhen analytical solutions aren’t available (which is often), we rely on numerical optimization like gradient descent.\nAlthough linear regression has analytical solutions, this is unfortunately not the case for many other models. We may need to resort to numerical approximations to find the optimal parameters. One of the most popular numerical optimizers is called stochastic gradient descent."
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-a-gradient",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-a-gradient",
    "title": "Quick introduction to deep learning",
    "section": "What is a Gradient?",
    "text": "What is a Gradient?\nA gradient is the derivative of a function, and it tells us how to adjust parameters to reduce the error.\n$f’() = _{→ 0} = $"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-gradient-descent",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-gradient-descent",
    "title": "Quick introduction to deep learning",
    "section": "What is Gradient Descent?",
    "text": "What is Gradient Descent?\nAn optimization algorithm that iteratively updates parameters in the direction of steepest descent (negative gradient) to minimize a loss function.\n\nLearning rate (\\(\\alpha\\)): step size. It is a hyperparameters that determines how fast we move in the direction of the gradient.\nLoss surface: the landscape we are optimizing over\n\nAt a particular \\(β_i\\), we find the gradient \\(f'(\\beta)\\), and take a step along the direction of the gradient to find the next point \\(\\beta_{i+1}\\).\n\\(\\beta_{i+1} = \\beta_i - \\alpha f'(\\beta_i)\\)\nThe \\(\\alpha\\) is called the learning rate.\n\n\n\ngradient descent"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-stochastic-gradient-descent",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-stochastic-gradient-descent",
    "title": "Quick introduction to deep learning",
    "section": "What is stochastic gradient descent?",
    "text": "What is stochastic gradient descent?\nIn real-world applications, the size of our dataset is so large that it is impossible to calculate the gradient using all data points. Therefore, we take a small chunk (called a “batch”) of the dataset to calcalate the gradient. This approximates the full-data gradients, thus the word stochastic."
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#three-components-of-machine-learning",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#three-components-of-machine-learning",
    "title": "Quick introduction to deep learning",
    "section": "Three Components of Machine Learning",
    "text": "Three Components of Machine Learning\nTo build a machine learning model, we need:\n\nModel: Defines the hypothesis space (e.g., linear model)\nLoss Function: Measures how well the model fits\nOptimizer: Updates parameters to reduce the loss (e.g., SGD)"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#define-the-loss-function",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#define-the-loss-function",
    "title": "Quick introduction to deep learning",
    "section": "Define the Loss Function",
    "text": "Define the Loss Function\nCommon choice for regression: Mean Squared Error (MSE)\n\\(\\ell(\\beta) = \\frac{1}{2}\\sum\\limits_{i=1}^m (f(\\beta)^{i} - y^{i})^2 / m\\)"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#compute-the-gradient",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#compute-the-gradient",
    "title": "Quick introduction to deep learning",
    "section": "Compute the gradient",
    "text": "Compute the gradient\n\\(\\frac{\\partial}{\\partial \\beta_j}\\ell(\\beta) = \\frac{\\partial}{\\partial \\beta_j} \\frac{1}{2} \\sum_i (f(\\beta)^i - y^i)^2 =  \\sum_i (f(\\beta)^i - y^i) x_j\\)\nRecall that in our example $ f() = = _1 x_1 + _2 x_2$. Here \\(\\beta_j\\) is either \\(\\beta_1\\) or \\(\\beta_2\\).\n\nExercise\nShow that the derivative of the loss function is as stated above."
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#optimize-estimate-parameters-with-gradient-descent",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#optimize-estimate-parameters-with-gradient-descent",
    "title": "Quick introduction to deep learning",
    "section": "Optimize: estimate parameters with gradient descent",
    "text": "Optimize: estimate parameters with gradient descent\n\nlr = 0.1 # learning rate\nb = [0.0, 0.0] # initialize all betas to 0\nn_examples = len(y)\ntrajectory = [b]\n\nfor _ in range(50): # 50 steps\n  diff = x.dot(b) - y\n  grad = diff.dot(x) / n_examples\n  b -= lr*grad\n  trajectory.append(b.copy())\n\ntrajectory = np.stack(trajectory, axis=1)\n\n\nqplot(x=trajectory[0], y=trajectory[1], xlab=\"beta_1\", ylab=\"beta_2\", geom=[\"point\", \"line\"]) + theme_bw() + annotate(geom=\"point\", x=coef[0], y=coef[1], size=8, color=\"blue\",shape='+',stroke=1)\n\n\n\n\n\n\n\n\n\nExercise\n\nAdd to the plot the normal equation estimates (traditional linear regression) of the coefficients in a different green\n\n\n\nExercise\n\nSimulate \\(y\\) with larger noise, estimate the regression coefficients using the normal equation and the gradient descent method, and plot the trajectory, the ground truth, and the two estimates for comparison."
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#simulate-non-linear-data",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#simulate-non-linear-data",
    "title": "Quick introduction to deep learning",
    "section": "Simulate non linear data",
    "text": "Simulate non linear data\nLet’s start by simulating data according to the generative model:\n$ y = x_1^3$\n\nx = np.random.normal(size=1000)\ny = x ** 3"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#predict-with-simple-linear-model",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#predict-with-simple-linear-model",
    "title": "Quick introduction to deep learning",
    "section": "Predict with simple linear model",
    "text": "Predict with simple linear model\nLet’s try to predict with a simple linear regression model\n$ y = X $\n\nQuestion\nhow many parameters do we need to estimate for this simple linear model?\n\nlr = 0.1 # learning rate\nb = 0.0 # initialize all betas to 0\nn_examples = len(y)\n\nfor _ in range(50): # 50 steps\n  diff = x.dot(b) - y\n  grad = diff.dot(x) / n_examples\n  b -= lr*grad\n\n\ny_hat = x.dot(b)\n( qplot(x = y, y=y_hat, geom=\"point\", xlab=\"y\", ylab=\"y_hat\") +\ngeom_abline(intercept=0, slope=1, color='gray', linetype='dashed') )\n\n\n\n\n\n\n\n\n\n\nWe would like to be closer to the dashed gray, identity line! Let’s try MLP."
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#install-packages-if-needed",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#install-packages-if-needed",
    "title": "Quick introduction to deep learning",
    "section": "Install packages if needed",
    "text": "Install packages if needed\n\n## install packages if needed\nif False:  # Change to True to run the commands\n    %pip install tqdm\n    %pip install torch\n    %pip install torchvision torchmetrics\n\n\nimport torch\nprint(torch.__version__)\n## if on a mac check if mps is available so you use gpu\n## print(torch.backends.mps.is_available())\n\n2.6.0\n\n\n\nDefine a non linear MLP\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n#device = torch.device(\"cuda\")\ndevice = torch.device(\"mps\") ## use this line instead of the one above withb cuda\n\n\nclass MLP(nn.Module):\n  def __init__(self, input_dim, hid_dim, output_dim):\n    super().__init__()\n    self.fc1 = nn.Linear(input_dim, hid_dim)\n    self.fc2 = nn.Linear(hid_dim, output_dim)\n\n  def forward(self, x):\n    x = F.relu(self.fc1(x))\n    y = self.fc2(x)\n\n    return y.squeeze(1)\n\nmlp = MLP(input_dim=1, hid_dim=1024, output_dim=1).to(device)"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#train-mlp-model-using-pytorch",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#train-mlp-model-using-pytorch",
    "title": "Quick introduction to deep learning",
    "section": "train MLP model using pytorch",
    "text": "train MLP model using pytorch\nWe are now ready to train our model. To understand how our model is doing, we record the loss vs step, which is called the learning curve in ML literature.\n\nx_tensor = torch.Tensor(x).unsqueeze(1).to(device)\ny_tensor = torch.Tensor(y).to(device)\nlearning_curve = []\n\noptimizer = torch.optim.SGD(mlp.parameters(), lr=0.001)\nloss_fn = nn.MSELoss()\n\nfor epoch in range(10000):\n  optimizer.zero_grad()\n  y_hat = mlp(x_tensor)\n  loss = loss_fn(y_hat, y_tensor)\n  learning_curve.append(loss.item())\n  loss.backward()\n  optimizer.step()\n\n\nqplot(x=range(10000), y=learning_curve, xlab=\"epoch\", ylab=\"loss\")\n\n\n\n\n\n\n\n\n\ny_hat = mlp(x_tensor)\ny_hat = y_hat.detach().cpu().numpy()\n#qplot(x=y, y=y_hat, geom=\"point\", xlab=\"y\", ylab=\"y_hat\")\nqplot(x=y, y=y_hat, geom=[\"point\", \"abline\"],\n      xlab=\"y\", ylab=\"y_hat\",\n      abline=dict(slope=1, intercept=0, color='red', linetype='dashed'))\n\n\n\n\n\n\n\n\nNow this looks much better!"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html",
    "href": "post/2025-03-25-unit00/index.html",
    "title": "preparing environment for unit 00",
    "section": "",
    "text": "## download miniconda from the command line\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\n\n## or manually from https://www.anaconda.com/docs/getting-started/miniconda/main\n\n## install miniconda\nbash Miniconda3-latest-Linux-x86_64.sh\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#install-conda",
    "href": "post/2025-03-25-unit00/index.html#install-conda",
    "title": "preparing environment for unit 00",
    "section": "",
    "text": "## download miniconda from the command line\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\n\n## or manually from https://www.anaconda.com/docs/getting-started/miniconda/main\n\n## install miniconda\nbash Miniconda3-latest-Linux-x86_64.sh"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#create-conda-environment",
    "href": "post/2025-03-25-unit00/index.html#create-conda-environment",
    "title": "preparing environment for unit 00",
    "section": "create conda environment",
    "text": "create conda environment\nconda create -n gene46100 python=3.12\nconda activate gene46100"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#do-not-install-packages-with-conda",
    "href": "post/2025-03-25-unit00/index.html#do-not-install-packages-with-conda",
    "title": "preparing environment for unit 00",
    "section": "do not install packages with conda",
    "text": "do not install packages with conda\n(I ran into lots of issues with torch version incompatibility with torchvision and torchmetrics)\n# DONT USE CONDA TO INSTALL PYTORCH at least for now\n# conda install scikit-learn plotnine pytorch \n\n## installing torchvision and torchmetrics forced downgrading torch to 2.3.1 which was not compatible with mps"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#install-all-packages-within-jupyter-notebook-with-pip",
    "href": "post/2025-03-25-unit00/index.html#install-all-packages-within-jupyter-notebook-with-pip",
    "title": "preparing environment for unit 00",
    "section": "install all packages within jupyter notebook with %pip",
    "text": "install all packages within jupyter notebook with %pip\nas of 2025-03-24\nusing %pip will make sure that the packages are accessible by the kernel you are using for the jupyter notebook\n%pip install scikit-learn plotnine tqdm pandas \n%pip install torch\n%pip install torchvision\n%pip install torchmetrics"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#install-cursor-or-vscode-to-run-the-jupyter-notebook",
    "href": "post/2025-03-25-unit00/index.html#install-cursor-or-vscode-to-run-the-jupyter-notebook",
    "title": "preparing environment for unit 00",
    "section": "install cursor or vscode to run the jupyter notebook",
    "text": "install cursor or vscode to run the jupyter notebook\n\ninstall the python extension for cursor or vscode.\nselect the python interpreter to be the one in the conda environment you created (gene46100)"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#save-the-environment-for-reproducibility",
    "href": "post/2025-03-25-unit00/index.html#save-the-environment-for-reproducibility",
    "title": "preparing environment for unit 00",
    "section": "Save the environment for reproducibility",
    "text": "Save the environment for reproducibility\n\nto reproduce the environment exactly, save the environment\n# Save conda packages with their sources\nconda env export --from-history &gt; environment-gene46100.yml\n\n# Save pip-installed packages separately\npip list --format=freeze &gt; requirements-gene46100.txt\n\n# Save full environment state (for reference)\nconda env export &gt; environment_full-gene46100.yml"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#do-not-run-the-following-unless-need-to-reinstall-the-environment-for-some-reason",
    "href": "post/2025-03-25-unit00/index.html#do-not-run-the-following-unless-need-to-reinstall-the-environment-for-some-reason",
    "title": "preparing environment for unit 00",
    "section": "DO NOT RUN THE FOLLOWING UNLESS NEED TO REINSTALL THE ENVIRONMENT FOR SOME REASON",
    "text": "DO NOT RUN THE FOLLOWING UNLESS NEED TO REINSTALL THE ENVIRONMENT FOR SOME REASON\n\nreinstall the environment\n# Create environment from conda packages\nconda env create -f environment-gene46100.yml\n\n# Activate the environment\nconda activate gene46100\n\n# Install pip packages\npip install -r requirements-gene46100.txt"
  },
  {
    "objectID": "post/2025-03-25-unit00/slides-00.html",
    "href": "post/2025-03-25-unit00/slides-00.html",
    "title": "Slides unit 00",
    "section": "",
    "text": "An MLP is a fully connected feedforward neural network — the classic deep learning architecture.\n\n\n\nInput Layer → Hidden Layer(s) → Output Layer\n\n\n\nMLP with 2 features in the input layer, one hidden layer, one dimentional output layer\n\n\nEach layer performs:\noutput = activation(Wx + b)\nWhere:\nW = weight matrix\n\nx = input vector\n\nb = bias\n\nactivation = non-linear function like ReLU\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/slides-00.html#core-idea",
    "href": "post/2025-03-25-unit00/slides-00.html#core-idea",
    "title": "Slides unit 00",
    "section": "",
    "text": "An MLP is a fully connected feedforward neural network — the classic deep learning architecture."
  },
  {
    "objectID": "post/2025-03-25-unit00/slides-00.html#architecture",
    "href": "post/2025-03-25-unit00/slides-00.html#architecture",
    "title": "Slides unit 00",
    "section": "",
    "text": "Input Layer → Hidden Layer(s) → Output Layer\n\n\n\nMLP with 2 features in the input layer, one hidden layer, one dimentional output layer\n\n\nEach layer performs:\noutput = activation(Wx + b)\nWhere:\nW = weight matrix\n\nx = input vector\n\nb = bias\n\nactivation = non-linear function like ReLU"
  },
  {
    "objectID": "post/2025-03-25-unit00/slides-00.html#data---model-input",
    "href": "post/2025-03-25-unit00/slides-00.html#data---model-input",
    "title": "Slides unit 00",
    "section": "1. Data -> Model Input",
    "text": "1. Data -&gt; Model Input\nDNA/RNA/protein sequences → One-hot encoded or embedded tensors\n\nWrapped in custom Dataset + DataLoader for batching"
  },
  {
    "objectID": "post/2025-03-25-unit00/slides-00.html#training-loop",
    "href": "post/2025-03-25-unit00/slides-00.html#training-loop",
    "title": "Slides unit 00",
    "section": "2. Training Loop",
    "text": "2. Training Loop\nfor xb, yb in train_loader: optimizer.zero_grad() # Step 1: Clear old gradients preds = model(xb) # Step 2: Forward pass loss = loss_fn(preds, yb) # Step 3: Compute loss loss.backward() # Step 4: Backprop: compute gradients optimizer.step() # Step 5: Update weights"
  },
  {
    "objectID": "post/2025-03-25-unit00/slides-00.html#model-components",
    "href": "post/2025-03-25-unit00/slides-00.html#model-components",
    "title": "Slides unit 00",
    "section": "3. Model Components",
    "text": "3. Model Components\nnn.Module: Defines layers and forward pass\n\nnn.Conv1d, nn.Linear, nn.ReLU, etc.\n\nOutputs: regression (e.g. expression levels) or classification (e.g. binding sites)"
  },
  {
    "objectID": "post/2025-03-25-unit00/slides-00.html#loss-function",
    "href": "post/2025-03-25-unit00/slides-00.html#loss-function",
    "title": "Slides unit 00",
    "section": "4. Loss Function",
    "text": "4. Loss Function\nnn.MSELoss() for expression prediction\n\nnn.CrossEntropyLoss() for classification\n\nnn.BCELoss() for binary classification, this is just -log likelihood for a bernoulli distribution"
  },
  {
    "objectID": "post/2025-03-25-unit00/slides-00.html#optimizer",
    "href": "post/2025-03-25-unit00/slides-00.html#optimizer",
    "title": "Slides unit 00",
    "section": "5. Optimizer",
    "text": "5. Optimizer\nCommon: torch.optim.Adam, SGD, etc.\n\nControls learning rate and parameter updates"
  },
  {
    "objectID": "post/2025-03-25-unit00/slides-00.html#evaluation",
    "href": "post/2025-03-25-unit00/slides-00.html#evaluation",
    "title": "Slides unit 00",
    "section": "6. Evaluation",
    "text": "6. Evaluation\nUse model.eval() + torch.no_grad() during validation\n\nTrack metrics like loss, R², accuracy, AUC"
  },
  {
    "objectID": "post/2020-01-01-TODO/fit-linear-model-to-linear.html",
    "href": "post/2020-01-01-TODO/fit-linear-model-to-linear.html",
    "title": "Fit linear mode to linear data",
    "section": "",
    "text": "We generate a data Y linearly dependent on features X: \\(Y = X\\beta + \\epsilon\\)\nWe fit a linear model\n\nwith gradient descent\nwith pytorch framework\nwith traditional linear regression estimate \\(\\hat\\beta = (X^TX)^{-1}X^TY\\)\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nfrom plotnine import *\n\n# Generate synthetic data\nnp.random.seed(42)\nX = np.random.randn(100, 1) * 2  # 100 samples, 1 feature\ny = 3 * X + 2 + np.random.randn(100, 1) * 0.5  # y = 3x + 2 + noise\n\n# Convert to PyTorch tensors\nX_tensor = torch.FloatTensor(X)\ny_tensor = torch.FloatTensor(y)\n\n# define a linear model\nclass LinearModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(1, 1)  # 1 input feature, 1 output\n    def forward(self, x):\n        return self.linear(x)\n\n# instantiate the model\nmodel = LinearModel()\n\n# define a loss function\nloss_fn = nn.MSELoss()\n\n# define an optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Train: iterate over the data and update the model\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Forward pass: compute model predictions and loss\n    y_pred = model(X_tensor)\n    loss = loss_fn(y_pred, y_tensor)\n    \n    # Backward pass and parameter update\n    optimizer.zero_grad()  # Clear previous gradients\n    loss.backward()        # Compute gradients via backpropagation\n    optimizer.step()       # Update model parameters\n    \n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# check performance\n\n# compute PyTorch model predictions\nmodel.eval()\nwith torch.no_grad():\n    y_pred_pytorch = model(X_tensor).numpy()\n\n# compute scikit-learn predictions\nlr = LinearRegression()\nlr.fit(X, y)\ny_pred_sklearn = lr.predict(X)\n\n# compare coefficients\nprint(\"\\nPyTorch model coefficients:\")\nprint(f\"Slope: {model.linear.weight.item():.4f}\")\nprint(f\"Intercept: {model.linear.bias.item():.4f}\")\n\nprint(\"\\nScikit-learn coefficients:\")\nprint(f\"Slope: {lr.coef_[0][0]:.4f}\")\nprint(f\"Intercept: {lr.intercept_[0]:.4f}\")\n\n# compare MSE\nmse_pytorch = np.mean((y - y_pred_pytorch) ** 2)\nmse_sklearn = np.mean((y - y_pred_sklearn) ** 2)\nprint(f\"\\nPyTorch MSE: {mse_pytorch:.4f}\")\nprint(f\"Scikit-learn MSE: {mse_sklearn:.4f}\")\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2020-01-01-TODO/fit-linear-model-to-linear.html#fit-a-linear-model-to-linear-data",
    "href": "post/2020-01-01-TODO/fit-linear-model-to-linear.html#fit-a-linear-model-to-linear-data",
    "title": "Fit linear mode to linear data",
    "section": "",
    "text": "We generate a data Y linearly dependent on features X: \\(Y = X\\beta + \\epsilon\\)\nWe fit a linear model\n\nwith gradient descent\nwith pytorch framework\nwith traditional linear regression estimate \\(\\hat\\beta = (X^TX)^{-1}X^TY\\)\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nfrom plotnine import *\n\n# Generate synthetic data\nnp.random.seed(42)\nX = np.random.randn(100, 1) * 2  # 100 samples, 1 feature\ny = 3 * X + 2 + np.random.randn(100, 1) * 0.5  # y = 3x + 2 + noise\n\n# Convert to PyTorch tensors\nX_tensor = torch.FloatTensor(X)\ny_tensor = torch.FloatTensor(y)\n\n# define a linear model\nclass LinearModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(1, 1)  # 1 input feature, 1 output\n    def forward(self, x):\n        return self.linear(x)\n\n# instantiate the model\nmodel = LinearModel()\n\n# define a loss function\nloss_fn = nn.MSELoss()\n\n# define an optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Train: iterate over the data and update the model\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Forward pass: compute model predictions and loss\n    y_pred = model(X_tensor)\n    loss = loss_fn(y_pred, y_tensor)\n    \n    # Backward pass and parameter update\n    optimizer.zero_grad()  # Clear previous gradients\n    loss.backward()        # Compute gradients via backpropagation\n    optimizer.step()       # Update model parameters\n    \n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# check performance\n\n# compute PyTorch model predictions\nmodel.eval()\nwith torch.no_grad():\n    y_pred_pytorch = model(X_tensor).numpy()\n\n# compute scikit-learn predictions\nlr = LinearRegression()\nlr.fit(X, y)\ny_pred_sklearn = lr.predict(X)\n\n# compare coefficients\nprint(\"\\nPyTorch model coefficients:\")\nprint(f\"Slope: {model.linear.weight.item():.4f}\")\nprint(f\"Intercept: {model.linear.bias.item():.4f}\")\n\nprint(\"\\nScikit-learn coefficients:\")\nprint(f\"Slope: {lr.coef_[0][0]:.4f}\")\nprint(f\"Intercept: {lr.intercept_[0]:.4f}\")\n\n# compare MSE\nmse_pytorch = np.mean((y - y_pred_pytorch) ** 2)\nmse_sklearn = np.mean((y - y_pred_sklearn) ** 2)\nprint(f\"\\nPyTorch MSE: {mse_pytorch:.4f}\")\nprint(f\"Scikit-learn MSE: {mse_sklearn:.4f}\")"
  },
  {
    "objectID": "post/2020-01-01-TODO/fit-linear-model-to-linear.html#plot-predictions-vs-true",
    "href": "post/2020-01-01-TODO/fit-linear-model-to-linear.html#plot-predictions-vs-true",
    "title": "Fit linear mode to linear data",
    "section": "plot predictions vs true",
    "text": "plot predictions vs true\n\n# Create a plot comparing predictions\n# Create a DataFrame for plotting\nplot_df = pd.DataFrame({\n    'y_true': y.flatten(),\n    'y_pred_pytorch': y_pred_pytorch.flatten(),\n    'y_pred_sklearn': y_pred_sklearn.flatten()\n})\n\n# Create the plot\np = (ggplot(plot_df)\n    + geom_point(aes(x='y_true', y='y_pred_pytorch', color='\"PyTorch\"'), size=1)\n    + geom_point(aes(x='y_true', y='y_pred_sklearn', color='\"Scikit-learn\"'), size=1)\n    + geom_abline(intercept=0, slope=1, color='gray', linetype='dashed', alpha=0.5)  # Identity line\n    + scale_color_manual(values=['red', 'blue'])\n    + labs(x='True Values', y='Predicted Values', \n           title='True vs Predicted Values',\n           color='Model')\n    + theme_minimal()\n)\n\np\n\n\n\n\n\n\n\n\n\n(ggplot(plot_df)\n    + geom_point(aes(x='y_pred_sklearn', y='y_pred_pytorch'), size=1)\n    + geom_abline(intercept=0, slope=1, color='gray', linetype='dashed', alpha=0.5)  # Identity line\n    + labs(x='Y sklearn', y='Y pytorch', \n           title='ptorch vs sklearn predictions',\n           color='Model')\n    + theme_minimal()\n)"
  },
  {
    "objectID": "post/2020-01-01-TODO/fit-linear-model-to-linear.html#fit-linear-model-using-the-gpu",
    "href": "post/2020-01-01-TODO/fit-linear-model-to-linear.html#fit-linear-model-using-the-gpu",
    "title": "Fit linear mode to linear data",
    "section": "Fit linear model using the GPU",
    "text": "Fit linear model using the GPU\n\n# Train on GPU (checking for both MPS and CUDA)\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nfrom plotnine import *\n\n# Check for available GPU devices\nif torch.backends.mps.is_available():\n    device = torch.device('mps')  # Apple Silicon\n    print(\"Using MPS (Apple Silicon GPU)\")\nelif torch.cuda.is_available():\n    device = torch.device('cuda')  # NVIDIA GPU\n    print(\"Using CUDA (NVIDIA GPU)\")\nelse:\n    device = torch.device('cpu')  # Fallback to CPU\n    print(\"Using CPU (no GPU available)\")\n\n# Generate synthetic data (stays on CPU)\nnp.random.seed(42)\nX = np.random.randn(100, 1) * 2\ny = 3 * X + 2 + np.random.randn(100, 1) * 0.5\n\n# Convert to PyTorch tensors and move to GPU\nX_tensor = torch.FloatTensor(X).to(device)\ny_tensor = torch.FloatTensor(y).to(device)\n\n# Define and move model to GPU\nclass LinearModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(1, 1)\n\n    def forward(self, x):\n        return self.linear(x)\n\nmodel = LinearModel().to(device)\nloss_fn = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Training loop (same as before, but data is on GPU)\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Forward pass\n    y_pred = model(X_tensor)\n    loss = loss_fn(y_pred, y_tensor)\n    \n    # Backward pass and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Get predictions back to CPU for comparison\nmodel.eval()\nwith torch.no_grad():\n    y_pred_pytorch = model(X_tensor).cpu().numpy()  # Move back to CPU before converting to numpy\n\n# Compare with scikit-learn (on CPU)\nlr = LinearRegression()\nlr.fit(X, y)\ny_pred_sklearn = lr.predict(X)\n\n# Compare coefficients\nprint(\"\\nPyTorch model coefficients:\")\nprint(f\"Slope: {model.linear.weight.item():.4f}\")\nprint(f\"Intercept: {model.linear.bias.item():.4f}\")\n\nprint(\"\\nScikit-learn coefficients:\")\nprint(f\"Slope: {lr.coef_[0][0]:.4f}\")\nprint(f\"Intercept: {lr.intercept_[0]:.4f}\")\n\n# Compare MSE\nmse_pytorch = np.mean((y - y_pred_pytorch) ** 2)\nmse_sklearn = np.mean((y - y_pred_sklearn) ** 2)\nprint(f\"\\nPyTorch MSE: {mse_pytorch:.4f}\")\nprint(f\"Scikit-learn MSE: {mse_sklearn:.4f}\")\n\n# Create a plot comparing predictions\nplot_df = pd.DataFrame({\n    'y_true': y.flatten(),\n    'y_pred_pytorch': y_pred_pytorch.flatten(),\n    'y_pred_sklearn': y_pred_sklearn.flatten()\n})\n\n# Create the plot\np = (ggplot(plot_df)\n    + geom_point(aes(x='y_true', y='y_pred_pytorch', color='\"PyTorch\"'), size=1)\n    + geom_point(aes(x='y_true', y='y_pred_sklearn', color='\"Scikit-learn\"'), size=1)\n    + geom_abline(intercept=0, slope=1, color='gray', linetype='dashed', alpha=0.5)  # Identity line\n    + scale_color_manual(values=['red', 'blue'])\n    + labs(x='True Values', y='Predicted Values', \n           title=f'True vs Predicted Values (Training on {device})',\n           color='Model')\n    + theme_minimal()\n)\n\np"
  },
  {
    "objectID": "post/2025-02-20-testing/gradient-descent-illustration.html",
    "href": "post/2025-02-20-testing/gradient-descent-illustration.html",
    "title": "Gradient descent illustration",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom plotnine import *\n\n\n# Loss function and its gradient\ndef loss(beta):\n    return beta ** 2\n\ndef grad(beta):\n    return 2 * beta\n\n# Simulate gradient descent\nbeta = 3.5  # random initial value\nlearning_rate = 0.2\nsteps = []\n\nfor i in range(10):\n    current_loss = loss(beta)\n    steps.append({'step': i, 'beta': beta, 'loss': current_loss})\n    beta -= learning_rate * grad(beta)\n\ndf = pd.DataFrame(steps)\n\n# Create full curve for loss function\nbeta_vals = np.linspace(-4, 4, 200)\nloss_vals = loss(beta_vals)\ndf_curve = pd.DataFrame({'beta': beta_vals, 'loss': loss_vals})\n\n# Create arrows for learning steps\ndf_arrows = df[:-1].copy()\ndf_arrows['beta_end'] = df['beta'][1:].values\ndf_arrows['loss_end'] = df['loss'][1:].values\n\n# Plot\np = (\n    ggplot(df_curve, aes('beta', 'loss')) +\n    geom_line(size=1.5, color=\"gray\") +\n    geom_point(df, aes('beta', 'loss'), color='purple', size=3) +\n    geom_point(df.tail(1), aes('beta', 'loss'), color='yellow', size=4) +\n    geom_segment(df_arrows,\n                 aes(x='beta', y='loss', xend='beta_end', yend='loss_end'),\n                 arrow=arrow(length=0.15, type='closed'),\n                 color='gray') +\n    annotate('text', x=3.5, y=12, label='Random\\ninitial value', ha='left') +\n#    annotate('text', x=1.5, y=5, label='Learning step', ha='left') +\n#    annotate('text', x=0, y=0.5, label='Minimum', ha='left') +\n    geom_vline(xintercept=0, linetype='dashed', color='gray') +\n    labs(x=r'$\\beta$', y='Loss') +\n    theme_minimal() +\n    theme(\n        figure_size=(7, 4),\n        axis_title=element_text(size=12),\n        axis_text=element_text(size=10)\n    )\n)\n\np\n\n\n\n\n\n\n\n\n\n\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2020-01-01-TODO/index.html",
    "href": "post/2020-01-01-TODO/index.html",
    "title": "TODO",
    "section": "",
    "text": "TODO\n\n\n\n\n\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/homework-01.html",
    "href": "post/2025-03-25-unit00/homework-01.html",
    "title": "homework 1",
    "section": "",
    "text": "Homework 1\n\nset up the environment on macos as described in the page (10 points)\n\nUpload the environment-gene46100.yml and requirements-gene46100.txt with the canvas assignment\nIf you have any issues, please let me know.\n\ngit clone the course repository to your local machine (10 points)\n\nUpload the screenshot of the terminal command you used to clone the repository with the canvas assignment\n\nrun all the code in the notebook. You should also be able to find the notebook in your cloned course repository. (20 points)\ncomplete the exercises in the notebook (30 points)\nsubmit the jupyter notebook with the results to canvas assignment\n\n\n\n\n\n\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/basic_DNA_tutorial.html",
    "href": "post/2025-03-25-unit00/basic_DNA_tutorial.html",
    "title": "DNA score prediction with Pytorch",
    "section": "",
    "text": "Pytorch tutorial using DNA created by Erin Wilson. Downloaded from here. Minor edits by Haky Im for compatibility with Apple Silicon (M1/M2/M3) MacBooks using MPS\nKey changes: 1. Using float32 instead of float64 (MPS doesn’t support double precision) 2. Setting device to ‘mps’ when available 3. Ensuring tensor data types are compatible with Metal Performance Shaders (MPS)\nThese modifications ensure compatibility with Apple’s built-in GPU acceleration while maintaining compatibility with other systems (CUDA, CPU).\nBelow is the environment by erinhwilson but not recommended here, we use %pip, which yielded fewer version incompatibility issues\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/basic_DNA_tutorial.html#question-1",
    "href": "post/2025-03-25-unit00/basic_DNA_tutorial.html#question-1",
    "title": "DNA score prediction with Pytorch",
    "section": "Question 1",
    "text": "Question 1\nModify the scoring function to create a more complex pattern. Instead of giving fixed bonuses for “TAT” and “GCG”, implement a position-dependent scoring where a motif gets a higher bonus if it appears at the beginning of the sequence compared to the end. How does this change the distribution of scores?\n ## 2. Prepare data for Pytorch training\nFor neural networks to make predictions, you have to give it your input as a matrix of numbers. For example, to classify images by whether or not they contain a cat, a network “sees” the image as a matrix of pixel values and learns relevant patterns in the relative arrangement of pixels (e.g. patterns that correspond to cat ears, or a nose with whiskers).\nWe similarly need to turn our DNA sequences (strings of ACGTs) into a matrix of numbers. So how do we pretend our DNA is a cat?\nOne common strategy is to one-hot encode the DNA: treat each nucleotide as a vector of length 4, where 3 positions are 0 and one position is a 1, depending on the nucleotide.\n\nThis one-hot encoding has the nice property that it makes your DNA appear like how a computer sees a picture of a cat!\n\ndef one_hot_encode(seq):\n    \"\"\"\n    Given a DNA sequence, return its one-hot encoding\n    \"\"\"\n    # Make sure seq has only allowed bases\n    allowed = set(\"ACTGN\")\n    if not set(seq).issubset(allowed):\n        invalid = set(seq) - allowed\n        raise ValueError(f\"Sequence contains chars not in allowed DNA alphabet (ACGTN): {invalid}\")\n        \n    # Dictionary returning one-hot encoding for each nucleotide \n    nuc_d = {'A':[1.0,0.0,0.0,0.0],\n             'C':[0.0,1.0,0.0,0.0],\n             'G':[0.0,0.0,1.0,0.0],\n             'T':[0.0,0.0,0.0,1.0],\n             'N':[0.0,0.0,0.0,0.0]}\n    \n    # Create array from nucleotide sequence\n    vec=np.array([nuc_d[x] for x in seq], dtype=np.float32)\n        \n    return vec\n\n\n# look at DNA seq of 8 As\na8 = one_hot_encode(\"AAAAAAAA\")\nprint(\"AAAAAA:\\n\",a8)\n\n# look at DNA seq of random nucleotides\ns = one_hot_encode(\"AGGTACCT\")\nprint(\"AGGTACC:\\n\",s)\nprint(\"shape:\",s.shape)\n\nAAAAAA:\n [[1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]]\nAGGTACC:\n [[1. 0. 0. 0.]\n [0. 0. 1. 0.]\n [0. 0. 1. 0.]\n [0. 0. 0. 1.]\n [1. 0. 0. 0.]\n [0. 1. 0. 0.]\n [0. 1. 0. 0.]\n [0. 0. 0. 1.]]\nshape: (8, 4)\n\n\nWith this one-hot encoding scheme, we can prepare our train, val, and test sets. This quick_split just randomly picks some indices in the pandas dataframe to split (sklearn has a function to do this too).\nNote: In real/non-synthetic tasks, you might need to be more clever about your splitting strategy depending on your prediction task: often papers will create train/test splits by chromosome or other genome location features\n\ndef quick_split(df, split_frac=0.8, verbose=False):\n    '''\n    Given a df of samples, randomly split indices between\n    train and test at the desired fraction\n    '''\n    cols = df.columns # original columns, use to clean up reindexed cols\n    df = df.reset_index()\n\n    # shuffle indices\n    idxs = list(range(df.shape[0]))\n    random.shuffle(idxs)\n\n    # split shuffled index list by split_frac\n    split = int(len(idxs)*split_frac)\n    train_idxs = idxs[:split]\n    test_idxs = idxs[split:]\n    \n    # split dfs and return\n    train_df = df[df.index.isin(train_idxs)]\n    test_df = df[df.index.isin(test_idxs)]\n        \n    return train_df[cols], test_df[cols]\n\n\nfull_train_df, test_df = quick_split(mer8)\ntrain_df, val_df = quick_split(full_train_df)\n\nprint(\"Train:\", train_df.shape)\nprint(\"Val:\", val_df.shape)\nprint(\"Test:\", test_df.shape)\n\ntrain_df.head()\n\nTrain: (41942, 2)\nVal: (10486, 2)\nTest: (13108, 2)\n\n\n\n\n\n\n\n\n\nseq\nscore\n\n\n\n\n0\nAAAAAAAA\n20.000\n\n\n1\nAAAAAAAC\n19.625\n\n\n2\nAAAAAAAG\n19.250\n\n\n3\nAAAAAAAT\n18.875\n\n\n4\nAAAAAACC\n19.250\n\n\n\n\n\n\n\n\ndef plot_train_test_hist(train_df, val_df,test_df,bins=20):\n    ''' Check distribution of train/test scores, sanity check that its not skewed'''\n    plt.hist(train_df['score'].values,bins=bins,label='train',alpha=0.5)\n    plt.hist(val_df['score'].values,bins=bins,label='val',alpha=0.75)\n    plt.hist(test_df['score'].values,bins=bins,label='test',alpha=0.4)\n    plt.legend()\n    plt.xlabel(\"seq score\",fontsize=14)\n    plt.ylabel(\"count\",fontsize=14)\n    plt.show()\n\nWith the below histogram, we can confirm that the train, test, and val sets contain example sequences from each bucket of the distribution (each set has some examples with each kind of motif)\n\nplot_train_test_hist(train_df, val_df,test_df)\n\n\n\n\n\n\n\n\nA big step here when preparing your data for Pytorch is using DataLoader and Dataset objects. It took me a lot of googling around to figure something out, but this is a solution I was able to concoct from a lot of combing through docs and stack overflow posts!\nIn short, a Dataset wraps your data in an object that can smoothly give your properly formatted X examples and Y labels to the model you’re training. The DataLoader accepts a Dataset and some other details about how to form batches from your data and makes it easier to iterate through training steps.\n\nfrom torch.utils.data import Dataset, DataLoader\n\n\nHere is a custom defined Dataset object specialized for one-hot encoded DNA:\n\nclass SeqDatasetOHE(Dataset):\n    '''\n    Dataset for one-hot-encoded sequences\n    '''\n    def __init__(self,\n                 df,\n                 seq_col='seq',\n                 target_col='score'\n                ):\n        # +--------------------+\n        # | Get the X examples |\n        # +--------------------+\n        # extract the DNA from the appropriate column in the df\n        self.seqs = list(df[seq_col].values)\n        self.seq_len = len(self.seqs[0])\n        \n        # one-hot encode sequences, then stack in a torch tensor\n        self.ohe_seqs = torch.stack([torch.tensor(one_hot_encode(x)) for x in self.seqs])\n    \n        # +------------------+\n        # | Get the Y labels |\n        # +------------------+\n        self.labels = torch.tensor(list(df[target_col].values)).unsqueeze(1)\n        \n    def __len__(self): return len(self.seqs)\n    \n    def __getitem__(self,idx):\n        # Given an index, return a tuple of an X with it's associated Y\n        # This is called inside DataLoader\n        seq = self.ohe_seqs[idx]\n        label = self.labels[idx]\n        \n        return seq, label\n\n\n\nHere is how I constructed DataLoaders from Datasets.\n\ndef build_dataloaders(train_df,\n                      test_df,\n                      seq_col='seq',\n                      target_col='score',\n                      batch_size=128,\n                      shuffle=True\n                     ):\n    '''\n    Given a train and test df with some batch construction\n    details, put them into custom SeqDatasetOHE() objects. \n    Give the Datasets to the DataLoaders and return.\n    '''\n    \n    # create Datasets    \n    train_ds = SeqDatasetOHE(train_df,seq_col=seq_col,target_col=target_col)\n    test_ds = SeqDatasetOHE(test_df,seq_col=seq_col,target_col=target_col)\n\n    # Put DataSets into DataLoaders\n    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle)\n    test_dl = DataLoader(test_ds, batch_size=batch_size)\n\n    \n    return train_dl,test_dl\n\n\ntrain_dl, val_dl = build_dataloaders(train_df, val_df)\n\nThese dataloaders are now ready to be used in a training loop!\n ## 3. Define Pytorch models The primary model I was interested in trying was a Convolutional Neural Network, as these have been shown to be useful for learning motifs from genomic data. But as a point of comparison, I included a simple Linear model. Here are some model definitions:\n\n# very simple linear model\nclass DNA_Linear(nn.Module):\n    def __init__(self, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        # the 4 is for our one-hot encoded vector length 4!\n        self.lin = nn.Linear(4*seq_len, 1)\n\n    def forward(self, xb):\n        # reshape to flatten sequence dimension\n        xb = xb.view(xb.shape[0],self.seq_len*4)\n        # Linear wraps up the weights/bias dot product operations\n        out = self.lin(xb)\n        return out\n\n    \n# basic CNN model\nclass DNA_CNN(nn.Module):\n    def __init__(self,\n                 seq_len,\n                 num_filters=32,\n                 kernel_size=3):\n        super().__init__()\n        self.seq_len = seq_len\n        \n        self.conv_net = nn.Sequential(\n            # 4 is for the 4 nucleotides\n            nn.Conv1d(4, num_filters, kernel_size=kernel_size),\n            nn.ReLU(inplace=True),\n            nn.Flatten(),\n            nn.Linear(num_filters*(seq_len-kernel_size+1), 1)\n        ) \n\n    def forward(self, xb):\n        # reshape view to batch_size x 4channel x seq_len\n        # permute to put channel in correct order\n        xb = xb.permute(0,2,1) \n        \n        #print(xb.shape)\n        out = self.conv_net(xb)\n        return out\n    \n    # __FOOTNOTE 1__\n\n\nclass DNA_CNN2(nn.Module):\n    def __init__(self, seq_len, num_filters=32, kernel_size=3):\n        super().__init__()\n        self.seq_len = seq_len\n        \n        # Define layers individually\n        self.conv = nn.Conv1d(4, num_filters, kernel_size=kernel_size)\n        self.linear = nn.Linear(num_filters*(seq_len-kernel_size+1), 1)\n\n    def forward(self, xb):\n        # reshape view to batch_size x 4channel x seq_len\n        xb = xb.permute(0, 2, 1)\n        \n        # Apply layers step by step\n        x = self.conv(xb)\n        x = F.relu(x, inplace=True)\n        x = x.flatten(1)  # flatten all dimensions except batch\n        out = self.linear(x)\n        return out \n\nThese aren’t optimized models, just something to start with (again, we’re just practicing connecting the Pytorch tubes in the context of DNA!). * The Linear model tries to predict the score by simply weighting the nucleotides that appears in each position. * The CNN model uses 32 filters of length (kernel_size) 3 to scan across the 8-mer sequences for informative 3-mer patterns.\n ## 4. Define the training loop functions Next, we need to define the training/fit loop. I admit I’m not super confident here and spent a lot of time wading through matrix dimension mismatch errors - there are likely more elegant ways to do this! But maybe this is ok? -shrug- (Shoot me a message if you have feedback :) )\nIn any case, I defined functions that stack like this:\n# adds default optimizer and loss function\nrun_model()\n    # loops through epochs\n    fit()\n        # loop through batches\n        train_step()\n            # calc train loss for batch\n            loss_batch()\n        val_step()\n            # calc val loss for batch\n            loss_batch()\n\n# +--------------------------------+\n# | Training and fitting functions |\n# +--------------------------------+\n\ndef loss_batch(model, loss_func, xb, yb, opt=None,verbose=False):\n    '''\n    Apply loss function to a batch of inputs. If no optimizer\n    is provided, skip the back prop step.\n    '''\n    if verbose:\n        print('loss batch ****')\n        print(\"xb shape:\",xb.shape)\n        print(\"yb shape:\",yb.shape)\n        print(\"yb shape:\",yb.squeeze(1).shape)\n        #print(\"yb\",yb)\n\n    # get the batch output from the model given your input batch \n    # ** This is the model's prediction for the y labels! **\n    xb_out = model(xb.float())\n    \n    if verbose:\n        print(\"model out pre loss\", xb_out.shape)\n        #print('xb_out', xb_out)\n        print(\"xb_out:\",xb_out.shape)\n        print(\"yb:\",yb.shape)\n        print(\"yb.long:\",yb.long().shape)\n    \n    loss = loss_func(xb_out, yb.float()) # for MSE/regression\n    # __FOOTNOTE 2__\n    \n    if opt is not None: # if opt\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    return loss.item(), len(xb)\n\n\ndef train_step(model, train_dl, loss_func, device, opt):\n    '''\n    Execute 1 set of batched training within an epoch\n    '''\n    # Set model to Training mode\n    model.train()\n    tl = [] # train losses\n    ns = [] # batch sizes, n\n    \n    # loop through train DataLoader\n    for xb, yb in train_dl:\n        # put on GPU\n        xb, yb = xb.to(device),yb.to(device)\n        \n        # provide opt so backprop happens\n        t, n = loss_batch(model, loss_func, xb, yb, opt=opt)\n        \n        # collect train loss and batch sizes\n        tl.append(t)\n        ns.append(n)\n    \n    # average the losses over all batches    \n    train_loss = np.sum(np.multiply(tl, ns)) / np.sum(ns)\n    \n    return train_loss\n\ndef val_step(model, val_dl, loss_func, device):\n    '''\n    Execute 1 set of batched validation within an epoch\n    '''\n    # Set model to Evaluation mode\n    model.eval()\n    with torch.no_grad():\n        vl = [] # val losses\n        ns = [] # batch sizes, n\n        \n        # loop through validation DataLoader\n        for xb, yb in val_dl:\n            # put on GPU\n            xb, yb = xb.to(device),yb.to(device)\n\n            # Do NOT provide opt here, so backprop does not happen\n            v, n = loss_batch(model, loss_func, xb, yb)\n\n            # collect val loss and batch sizes\n            vl.append(v)\n            ns.append(n)\n\n    # average the losses over all batches\n    val_loss = np.sum(np.multiply(vl, ns)) / np.sum(ns)\n    \n    return val_loss\n\n\ndef fit(epochs, model, loss_func, opt, train_dl, val_dl,device,patience=1000):\n    '''\n    Fit the model params to the training data, eval on unseen data.\n    Loop for a number of epochs and keep train of train and val losses \n    along the way\n    '''\n    # keep track of losses\n    train_losses = []    \n    val_losses = []\n    \n    # loop through epochs\n    for epoch in range(epochs):\n        # take a training step\n        train_loss = train_step(model,train_dl,loss_func,device,opt)\n        train_losses.append(train_loss)\n\n        # take a validation step\n        val_loss = val_step(model,val_dl,loss_func,device)\n        val_losses.append(val_loss)\n        \n        print(f\"E{epoch} | train loss: {train_loss:.3f} | val loss: {val_loss:.3f}\")\n\n    return train_losses, val_losses\n\n\ndef run_model(train_dl,val_dl,model,device,\n              lr=0.01, epochs=50, \n              lossf=None,opt=None\n             ):\n    '''\n    Given train and val DataLoaders and a NN model, fit the mode to the training\n    data. By default, use MSE loss and an SGD optimizer\n    '''\n    # define optimizer\n    if opt:\n        optimizer = opt\n    else: # if no opt provided, just use SGD\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n    \n    # define loss function\n    if lossf:\n        loss_func = lossf\n    else: # if no loss function provided, just use MSE\n        loss_func = torch.nn.MSELoss()\n    \n    # run the training loop\n    train_losses, val_losses = fit(\n                                epochs, \n                                model, \n                                loss_func, \n                                optimizer, \n                                train_dl, \n                                val_dl, \n                                device)\n\n    return train_losses, val_losses\n\n ## 5. Run the models First let’s try running a Linear Model on our 8-mer sequences\n\n# get the sequence length from the first seq in the df\nseq_len = len(train_df['seq'].values[0])\n\n# create Linear model object\nmodel_lin = DNA_Linear(seq_len)\n# use float32 since mps cannot handle 64\nmodel_lin = model_lin.type(torch.float32)\nmodel_lin.to(DEVICE) # put on GPU\n\n\n# run the model with default settings!\nlin_train_losses, lin_val_losses = run_model(\n    train_dl, \n    val_dl, \n    model_lin,\n    DEVICE\n)\n\nE0 | train loss: 21.238 | val loss: 12.980\nE1 | train loss: 12.969 | val loss: 12.826\nE2 | train loss: 12.918 | val loss: 12.832\nE3 | train loss: 12.915 | val loss: 12.847\nE4 | train loss: 12.916 | val loss: 12.833\nE5 | train loss: 12.918 | val loss: 12.837\nE6 | train loss: 12.915 | val loss: 12.828\nE7 | train loss: 12.917 | val loss: 12.826\nE8 | train loss: 12.917 | val loss: 12.827\nE9 | train loss: 12.917 | val loss: 12.827\nE10 | train loss: 12.918 | val loss: 12.831\nE11 | train loss: 12.914 | val loss: 12.836\nE12 | train loss: 12.918 | val loss: 12.834\nE13 | train loss: 12.916 | val loss: 12.830\nE14 | train loss: 12.917 | val loss: 12.832\nE15 | train loss: 12.917 | val loss: 12.831\nE16 | train loss: 12.917 | val loss: 12.833\nE17 | train loss: 12.915 | val loss: 12.882\nE18 | train loss: 12.916 | val loss: 12.834\nE19 | train loss: 12.916 | val loss: 12.833\nE20 | train loss: 12.917 | val loss: 12.830\nE21 | train loss: 12.918 | val loss: 12.830\nE22 | train loss: 12.917 | val loss: 12.826\nE23 | train loss: 12.914 | val loss: 12.826\nE24 | train loss: 12.915 | val loss: 12.828\nE25 | train loss: 12.916 | val loss: 12.833\nE26 | train loss: 12.916 | val loss: 12.829\nE27 | train loss: 12.916 | val loss: 12.828\nE28 | train loss: 12.918 | val loss: 12.848\nE29 | train loss: 12.916 | val loss: 12.830\nE30 | train loss: 12.916 | val loss: 12.841\nE31 | train loss: 12.917 | val loss: 12.823\nE32 | train loss: 12.917 | val loss: 12.833\nE33 | train loss: 12.916 | val loss: 12.825\nE34 | train loss: 12.918 | val loss: 12.822\nE35 | train loss: 12.916 | val loss: 12.838\nE36 | train loss: 12.917 | val loss: 12.833\nE37 | train loss: 12.914 | val loss: 12.837\nE38 | train loss: 12.917 | val loss: 12.834\nE39 | train loss: 12.917 | val loss: 12.846\nE40 | train loss: 12.916 | val loss: 12.826\nE41 | train loss: 12.917 | val loss: 12.820\nE42 | train loss: 12.917 | val loss: 12.835\nE43 | train loss: 12.916 | val loss: 12.832\nE44 | train loss: 12.916 | val loss: 12.827\nE45 | train loss: 12.915 | val loss: 12.827\nE46 | train loss: 12.916 | val loss: 12.827\nE47 | train loss: 12.918 | val loss: 12.829\nE48 | train loss: 12.915 | val loss: 12.824\nE49 | train loss: 12.917 | val loss: 12.824\n\n\nLet’s look at the loss in quick plot:\n\ndef quick_loss_plot(data_label_list,loss_type=\"MSE Loss\",sparse_n=0):\n    '''\n    For each train/test loss trajectory, plot loss by epoch\n    '''\n    for i,(train_data,test_data,label) in enumerate(data_label_list):    \n        plt.plot(train_data,linestyle='--',color=f\"C{i}\", label=f\"{label} Train\")\n        plt.plot(test_data,color=f\"C{i}\", label=f\"{label} Val\",linewidth=3.0)\n\n    plt.legend()\n    plt.ylabel(loss_type)\n    plt.xlabel(\"Epoch\")\n    plt.legend(bbox_to_anchor=(1,1),loc='upper left')\n    plt.show()\n\n\nlin_data_label = (lin_train_losses,lin_val_losses,\"Lin\")\nquick_loss_plot([lin_data_label])\n\n\n\n\n\n\n\n\nAt first glance, not much learning appears to be happening.\nNext let’s try the CNN.\n\nseq_len = len(train_df['seq'].values[0])\n\n# create Linear model object\nmodel_cnn = DNA_CNN(seq_len)\nmodel_cnn.to(DEVICE) # put on GPU\n\n# run the model with default settings!\ncnn_train_losses, cnn_val_losses = run_model(\n    train_dl, \n    val_dl, \n    model_cnn,\n    DEVICE\n)\n\nE0 | train loss: 14.640 | val loss: 10.167\nE1 | train loss: 8.625 | val loss: 7.035\nE2 | train loss: 6.305 | val loss: 4.965\nE3 | train loss: 4.507 | val loss: 3.267\nE4 | train loss: 3.118 | val loss: 2.280\nE5 | train loss: 2.419 | val loss: 2.693\nE6 | train loss: 1.996 | val loss: 2.415\nE7 | train loss: 1.825 | val loss: 3.567\nE8 | train loss: 1.542 | val loss: 1.295\nE9 | train loss: 1.451 | val loss: 1.189\nE10 | train loss: 1.225 | val loss: 1.107\nE11 | train loss: 1.205 | val loss: 1.135\nE12 | train loss: 1.146 | val loss: 1.104\nE13 | train loss: 1.028 | val loss: 1.108\nE14 | train loss: 1.030 | val loss: 1.204\nE15 | train loss: 1.037 | val loss: 1.115\nE16 | train loss: 1.036 | val loss: 1.039\nE17 | train loss: 1.000 | val loss: 1.051\nE18 | train loss: 0.968 | val loss: 1.069\nE19 | train loss: 0.945 | val loss: 1.174\nE20 | train loss: 0.945 | val loss: 1.170\nE21 | train loss: 0.926 | val loss: 1.039\nE22 | train loss: 0.929 | val loss: 1.257\nE23 | train loss: 0.937 | val loss: 1.021\nE24 | train loss: 0.918 | val loss: 1.040\nE25 | train loss: 0.931 | val loss: 1.059\nE26 | train loss: 0.918 | val loss: 1.088\nE27 | train loss: 0.914 | val loss: 1.281\nE28 | train loss: 0.931 | val loss: 1.037\nE29 | train loss: 0.890 | val loss: 1.125\nE30 | train loss: 0.902 | val loss: 1.037\nE31 | train loss: 0.917 | val loss: 1.034\nE32 | train loss: 0.914 | val loss: 1.046\nE33 | train loss: 0.908 | val loss: 1.055\nE34 | train loss: 0.900 | val loss: 1.233\nE35 | train loss: 0.890 | val loss: 1.030\nE36 | train loss: 0.901 | val loss: 1.067\nE37 | train loss: 0.895 | val loss: 1.057\nE38 | train loss: 0.914 | val loss: 1.029\nE39 | train loss: 0.894 | val loss: 1.024\nE40 | train loss: 0.903 | val loss: 1.053\nE41 | train loss: 0.896 | val loss: 1.088\nE42 | train loss: 0.916 | val loss: 1.031\nE43 | train loss: 0.904 | val loss: 1.105\nE44 | train loss: 0.894 | val loss: 1.288\nE45 | train loss: 0.904 | val loss: 1.666\nE46 | train loss: 0.906 | val loss: 1.050\nE47 | train loss: 0.909 | val loss: 1.053\nE48 | train loss: 0.892 | val loss: 1.174\nE49 | train loss: 0.892 | val loss: 1.081\n\n\n\ncnn_data_label = (cnn_train_losses,cnn_val_losses,\"CNN\")\nquick_loss_plot([lin_data_label,cnn_data_label])\n\n\n\n\n\n\n\n\nIt seems clear from the loss curves that the CNN is able to capture a pattern in the data that the Linear model is not! Let’s spot check a few sequences to see what’s going on.\n\n# oracle dict of true score for each seq\noracle = dict(mer8[['seq','score']].values)\n\ndef quick_seq_pred(model, desc, seqs, oracle):\n    '''\n    Given a model and some sequences, get the model's predictions\n    for those sequences and compare to the oracle (true) output\n    '''\n    print(f\"__{desc}__\")\n    for dna in seqs:\n        s = torch.tensor(one_hot_encode(dna)).unsqueeze(0).to(DEVICE)\n        pred = model(s.float())\n        actual = oracle[dna]\n        diff = pred.item() - actual\n        print(f\"{dna}: pred:{pred.item():.3f} actual:{actual:.3f} ({diff:.3f})\")\n\ndef quick_8mer_pred(model, oracle):\n    seqs1 = (\"poly-X seqs\",['AAAAAAAA', 'CCCCCCCC','GGGGGGGG','TTTTTTTT'])\n    seqs2 = (\"other seqs\", ['AACCAACA','CCGGTGAG','GGGTAAGG', 'TTTCGTTT'])\n    seqsTAT = (\"with TAT motif\", ['TATAAAAA','CCTATCCC','GTATGGGG','TTTATTTT'])\n    seqsGCG = (\"with GCG motif\", ['AAGCGAAA','CGCGCCCC','GGGCGGGG','TTGCGTTT'])\n    TATGCG =  (\"both TAT and GCG\",['ATATGCGA','TGCGTATT'])\n\n    for desc,seqs in [seqs1, seqs2, seqsTAT, seqsGCG, TATGCG]:\n        quick_seq_pred(model, desc, seqs, oracle)\n        print()\n\n\n# Ask the trained Linear model to make \n# predictions for some 8-mers\nquick_8mer_pred(model_lin, oracle)\n\n__poly-X seqs__\nAAAAAAAA: pred:23.415 actual:20.000 (3.415)\nCCCCCCCC: pred:13.762 actual:17.000 (-3.238)\nGGGGGGGG: pred:7.189 actual:14.000 (-6.811)\nTTTTTTTT: pred:17.841 actual:11.000 (6.841)\n\n__other seqs__\nAACCAACA: pred:18.987 actual:18.875 (0.112)\nCCGGTGAG: pred:12.330 actual:15.125 (-2.795)\nGGGTAAGG: pred:14.006 actual:15.125 (-1.119)\nTTTCGTTT: pred:14.945 actual:12.125 (2.820)\n\n__with TAT motif__\nTATAAAAA: pred:22.317 actual:27.750 (-5.433)\nCCTATCCC: pred:17.059 actual:25.875 (-8.816)\nGTATGGGG: pred:12.329 actual:24.000 (-11.671)\nTTTATTTT: pred:18.331 actual:22.125 (-3.794)\n\n__with GCG motif__\nAAGCGAAA: pred:16.972 actual:8.125 (8.847)\nCGCGCCCC: pred:12.467 actual:6.250 (6.217)\nGGGCGGGG: pred:8.121 actual:4.375 (3.746)\nTTGCGTTT: pred:13.014 actual:2.500 (10.514)\n\n__both TAT and GCG__\nATATGCGA: pred:15.874 actual:15.875 (-0.001)\nTGCGTATT: pred:14.779 actual:13.625 (1.154)\n\n\n\nFrom the above examples, it appears that the Linear model is really underpredicting sequences with a lot of G’s and overpredicting those with many T’s. This is probably because it noticed GCG made sequences have unusually low scores and TAT made sequences have unusually high scores, however since the Linear model doesn’t have a way to take into account the different context of GCG vs GAG, it just predicts that sequences with G’s should be lower. We know from our scoring scheme that this isn’t the case: it’s not that G’s in general are detrimental, but specifically GCG is.\n\n# Ask the trained CNN model to make \n# predictions for some 8-mers\nquick_8mer_pred(model_cnn, oracle)\n\n__poly-X seqs__\nAAAAAAAA: pred:19.629 actual:20.000 (-0.371)\nCCCCCCCC: pred:16.705 actual:17.000 (-0.295)\nGGGGGGGG: pred:13.571 actual:14.000 (-0.429)\nTTTTTTTT: pred:10.887 actual:11.000 (-0.113)\n\n__other seqs__\nAACCAACA: pred:18.592 actual:18.875 (-0.283)\nCCGGTGAG: pred:14.767 actual:15.125 (-0.358)\nGGGTAAGG: pred:15.091 actual:15.125 (-0.034)\nTTTCGTTT: pred:11.759 actual:12.125 (-0.366)\n\n__with TAT motif__\nTATAAAAA: pred:26.081 actual:27.750 (-1.669)\nCCTATCCC: pred:24.200 actual:25.875 (-1.675)\nGTATGGGG: pred:22.796 actual:24.000 (-1.204)\nTTTATTTT: pred:20.518 actual:22.125 (-1.607)\n\n__with GCG motif__\nAAGCGAAA: pred:9.008 actual:8.125 (0.883)\nCGCGCCCC: pred:7.093 actual:6.250 (0.843)\nGGGCGGGG: pred:5.264 actual:4.375 (0.889)\nTTGCGTTT: pred:3.397 actual:2.500 (0.897)\n\n__both TAT and GCG__\nATATGCGA: pred:15.341 actual:15.875 (-0.534)\nTGCGTATT: pred:13.181 actual:13.625 (-0.444)\n\n\n\nThe CNN however is better able to adapt to the differences between 3-mer motifs! It predicts quite well on both the sequences with and without motifs."
  },
  {
    "objectID": "post/2025-03-25-unit00/basic_DNA_tutorial.html#question-2",
    "href": "post/2025-03-25-unit00/basic_DNA_tutorial.html#question-2",
    "title": "DNA score prediction with Pytorch",
    "section": "Question 2",
    "text": "Question 2\nCompare the performance of the Linear and CNN models by using different learning rates. First run both models with higher learning rates (0.05, 0.1) and lower learning rates (0.005, 0.001), then create loss plots showing: - Linear model with these learning rates - CNN model with these learning rates\nThen analyze your results by answering: 1. How does changing the learning rate affect convergence for each model? 2. Which model is more sensitive to learning rate changes, and why? 3. Based on your analysis, what learning rate would you recommend for each model type, and why?\n ## 6. Check model predictions on the test set An important evaluation step in machine learning tasks is to check if your model can make good predictions on the test set, which it never saw during training. Here, we can use a parity plot to visualize the difference between the actual sequence scores vs the model’s predicted scores.\n\n%pip install altair ## datapane\n\nCollecting altair\n  Downloading altair-5.5.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: jinja2 in /opt/anaconda3/envs/gene46100/lib/python3.12/site-packages (from altair) (3.1.6)\nCollecting jsonschema&gt;=3.0 (from altair)\n  Downloading jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\nCollecting narwhals&gt;=1.14.2 (from altair)\n  Downloading narwhals-1.33.0-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: packaging in /opt/anaconda3/envs/gene46100/lib/python3.12/site-packages (from altair) (24.2)\nRequirement already satisfied: typing-extensions&gt;=4.10.0 in /opt/anaconda3/envs/gene46100/lib/python3.12/site-packages (from altair) (4.12.2)\nCollecting attrs&gt;=22.2.0 (from jsonschema&gt;=3.0-&gt;altair)\n  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\nCollecting jsonschema-specifications&gt;=2023.03.6 (from jsonschema&gt;=3.0-&gt;altair)\n  Downloading jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\nCollecting referencing&gt;=0.28.4 (from jsonschema&gt;=3.0-&gt;altair)\n  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\nCollecting rpds-py&gt;=0.7.1 (from jsonschema&gt;=3.0-&gt;altair)\n  Downloading rpds_py-0.24.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.1 kB)\nRequirement already satisfied: MarkupSafe&gt;=2.0 in /opt/anaconda3/envs/gene46100/lib/python3.12/site-packages (from jinja2-&gt;altair) (3.0.2)\nDownloading altair-5.5.0-py3-none-any.whl (731 kB)\n   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.2/731.2 kB 1.0 MB/s eta 0:00:00ta 0:00:01\nDownloading jsonschema-4.23.0-py3-none-any.whl (88 kB)\nDownloading narwhals-1.33.0-py3-none-any.whl (322 kB)\nDownloading attrs-25.3.0-py3-none-any.whl (63 kB)\nDownloading jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\nDownloading referencing-0.36.2-py3-none-any.whl (26 kB)\nDownloading rpds_py-0.24.0-cp312-cp312-macosx_11_0_arm64.whl (351 kB)\nInstalling collected packages: rpds-py, narwhals, attrs, referencing, jsonschema-specifications, jsonschema, altair\nSuccessfully installed altair-5.5.0 attrs-25.3.0 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 narwhals-1.33.0 referencing-0.36.2 rpds-py-0.24.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nimport altair as alt\nfrom sklearn.metrics import r2_score\n## import datapane as dp ## compatibility issues with pandas version\nimport os\n\n\n\ndef parity_plot(model_name,df,r2):\n    '''\n    Given a dataframe of samples with their true and predicted values,\n    make a scatterplot.\n    '''\n    plt.scatter(df['truth'].values, df['pred'].values, alpha=0.2)\n    \n    # y=x line\n    xpoints = ypoints = plt.xlim()\n    plt.plot(xpoints, ypoints, linestyle='--', color='k', lw=2, scalex=False, scaley=False)\n\n    plt.ylim(xpoints)\n    plt.ylabel(\"Predicted Score\",fontsize=14)\n    plt.xlabel(\"Actual Score\",fontsize=14)\n    plt.title(f\"{model_name} (r2:{r2:.3f})\",fontsize=20)\n    plt.show()\n\n\ndef alt_parity_plot(model, df, r2, datapane=False):\n    '''\n    Make an interactive parity plot with altair\n    '''\n    import os\n    import altair as alt\n    \n    os.makedirs('alt_out', exist_ok=True)\n    \n    # Convert model name to string to avoid any issues\n    model = str(model)\n    \n    # Create a clean version of the dataframe\n    plot_df = pd.DataFrame({\n        'truth': df['truth'].astype(float),\n        'pred': df['pred'].astype(float),\n        'seq': df['seq'].astype(str)\n    })\n    \n    # Create chart\n    chart = alt.Chart(plot_df).mark_point().encode(\n        x=alt.X('truth', type='quantitative', title='True Values'),\n        y=alt.Y('pred', type='quantitative', title='Predictions'),\n        tooltip=['seq']\n    ).properties(\n        title=str(f'{model} (r2:{r2:.3f})')\n    )\n    \n    chart.save(f'alt_out/parity_plot_{model}.html')\n    display(chart)\n\n# def alt_parity_plot(model, df, r2, datapane=False):\n#     '''\n#     Make an interactive parity plot with altair\n#     '''\n#     import os\n#     import altair as alt\n    \n#     # Debug: Print data info\n#     print(\"DataFrame info:\")\n#     print(df.dtypes)\n#     print(\"\\nFirst few rows:\")\n#     print(df.head())\n    \n#     os.makedirs('alt_out', exist_ok=True)\n    \n#     # Convert model name to string to avoid any issues\n#     model = str(model)\n    \n#     # Create a clean version of the dataframe\n#     plot_df = pd.DataFrame({\n#         'truth': df['truth'].astype(float),\n#         'pred': df['pred'].astype(float),\n#         'seq': df['seq'].astype(str)\n#     })\n    \n#     # Create chart with simplified specification\n#     chart = alt.Chart(plot_df).mark_point().encode(\n#         x=alt.X('truth', type='quantitative', title='True Values'),\n#         y=alt.Y('pred', type='quantitative', title='Predictions'),\n#         tooltip=['seq']\n#     ).properties(\n#         title=str(f'{model} (r2:{r2:.3f})')\n#     )\n    \n#     # Try to display and save\n#     try:\n#         chart.save(f'alt_out/parity_plot_{model}.html')\n#         display(chart)\n#     except Exception as e:\n#         print(f\"Error saving/displaying chart: {e}\")\n\n\n    \n# def alt_parity_plot(model,df, r2,datapane=False):\n#     '''\n#     Make an interactive parity plot with altair\n#     '''\n#     chart = alt.Chart(df).mark_circle(size=100,opacity=0.4).encode(\n#         alt.X('truth:Q'),\n#         alt.Y('pred:Q'),\n#         tooltip=['seq:N']\n#     ).properties(\n#         title=f'{model} (r2:{r2:.3f})',\n#     ).interactive()\n    \n#     chart.save(f'alt_out/parity_plot_{model}.html')\n#     display(chart)\n    \n#     # if datapane:\n#     #     report = dp.Report(dp.Plot(chart) ) #Create a report\n#     #     report.upload(name=f'dna_pytorch_tutorial_altair_{model}', open=True, visibility='PUBLIC') #Publish the report\n\n\n\ndef parity_pred(models, seqs, oracle,alt=False,datapane=False):\n    '''Given some sequences, get the model's predictions '''\n    dfs = {} # key: model name, value: parity_df\n    \n    \n    for model_name,model in models:\n        print(f\"Running {model_name}\")\n        data = []\n        for dna in seqs:\n            s = torch.tensor(one_hot_encode(dna)).unsqueeze(0).to(DEVICE)\n            actual = oracle[dna]\n            pred = model(s.float())\n            data.append([dna,actual,pred.item()])\n        df = pd.DataFrame(data, columns=['seq','truth','pred'])\n        r2 = r2_score(df['truth'],df['pred'])\n        dfs[model_name] = (r2,df)\n        \n        #plot parity plot\n        if alt: # make an altair plot\n            alt_parity_plot(model_name, df, r2,datapane=datapane)\n            \n        else:\n            parity_plot(model_name, df, r2)\n\n\nseqs = test_df['seq'].values\nmodels = [\n    (\"Linear\", model_lin),\n    (\"CNN\", model_cnn)\n]\nparity_pred(models, seqs, oracle)\n\nRunning Linear\n\n\n\n\n\n\n\n\n\nRunning CNN\n\n\n\n\n\n\n\n\n\nParity plots are useful for visualizing how well your model predicts individual sequences: in a perfect model, they would all land on the y=x line, meaning that the model prediction was exactly the sequence’s actual value. But if it is off the y=x line, it means the model is over- or under-predicting.\nIn the Linear model, we can see that it can somewhat predict a trend in the Test set sequences, but really gets confused by these buckets of sequences in the high and low areas of the distribution (the ones with a motif).\nHowever for the CNN, it is much better at predicting scores close to the actual value! This is expected, given that the architecture of our CNN uses 3-mer kernels to scan along the sequence for influential motifs.\nBut the CNN isn’t perfect. We could probably train it longer or adjust the hyperparameters, but the goal here isn’t perfection - this is a very simple task relative to actual regulatory grammars. Instead, I thought it would be interesting to use the Altair visualization library to interactively inspect which sequences the models get wrong:\n\nalt.data_transformers.disable_max_rows() # disable altair warning\nparity_pred(models, seqs, oracle,alt=True)\n\nRunning Linear\n\n\n\n\n\n\n\n\nRunning CNN\n\n\n\n\n\n\n\n\nIf you’re viewing this notebook in interactive mode and run the above cell (just viewing via the github preview will omit the altair plot in the rendering), you can hover over the points and see the individual 8-mer sequences (you can also pan and zoom in this plot).\nNotice that the sequences that are off the diagonal tend to have multiple instance of the motifs! In the scoring function, we only gave the sequence a +/- bump if it had at least 1 motif, but it certainly would have been reasonable to decide to add multiple bonuses if the motif was present multiple times. In this example, I arbitrarily only added the bonus for at least 1 motif occurrence, but we could have made a different scoring function.\nIn any case, I thought it was cool that the model noticed the multiple occurrences and predicted them to be important. I suppose we did fool it a little, though an R2 of 0.95 is pretty respectable :)\n\n(A quick screenshot of the Altair plot with tooltips from above if you can’t see it in the Github preview)\n\n# Compatibility issues with datapane, commented out here\n# upload CNN altair viz to Datapane for publishing\n# parity_pred([(\"CNN\",model_cnn)], seqs, oracle,alt=True,datapane=True)"
  },
  {
    "objectID": "post/2025-03-25-unit00/basic_DNA_tutorial.html#question-3",
    "href": "post/2025-03-25-unit00/basic_DNA_tutorial.html#question-3",
    "title": "DNA score prediction with Pytorch",
    "section": "Question 3",
    "text": "Question 3\nDesign an approach to improve the model’s prediction accuracy, particularly focusing on the sequences where the current model performs poorly:\n\nAfter identifying sequences where the CNN model has high prediction errors, propose and implement a modification to either the model architecture, the loss function, the training process, or the data representation\nRetrain the model with your modifications\nCreate comparative visualizations (such as parity plots, error histograms, or other appropriate plots) to demonstrate the impact of your changes\nAnalyze your results by discussing how your modification addresses the specific weaknesses you identified. What are the trade-offs involved in your approach?\n\n ## 7. Visualize convolutional filters When training CNN models, it can be useful to visualize the first layer convolutional filters to try to understand more about what the model is learning. With image data, the first layer convolutional filters often learn patterns such as borders or colors or textures - basic image elements that can be recombined to make more complex features.\nIn DNA, convolutional filters can be thought of like motif scanners. Similar to a position weight matrix for visualizing sequence logos, a convolutional filter is like a matrix showing a particular DNA pattern, but instead of being an exact sequence, it can hold some uncertainty about which nucleotides show up in which part of the pattern. Some positions might be very certain (i.e., there’s always an A in position 2; high information content) while other positions could hold a variety of nucleotides with about equal probability (high entropy; low information content).\nThe calculations that occur within the hidden layers of neural networks can get very complex and not every convolutional filter will be an obviously relevant pattern, but sometimes patterns in the filters do emerge and can be informative for helping to explain the model’s predictions.\nBelow are some functions to visualize the first layer convolutional filters, both as a raw heatmap and as a motif logo.\n\nimport logomaker\n\n\ndef get_conv_layers_from_model(model):\n    '''\n    Given a trained model, extract its convolutional layers\n    '''\n    model_children = list(model.children())\n    \n    # counter to keep count of the conv layers\n    model_weights = [] # we will save the conv layer weights in this list\n    conv_layers = [] # we will save the actual conv layers in this list\n    bias_weights = []\n    counter = 0 \n\n    # append all the conv layers and their respective weights to the list\n    for i in range(len(model_children)):\n        # get model type of Conv1d\n        if type(model_children[i]) == nn.Conv1d:\n            counter += 1\n            model_weights.append(model_children[i].weight)\n            conv_layers.append(model_children[i])\n            bias_weights.append(model_children[i].bias)\n\n        # also check sequential objects' children for conv1d\n        elif type(model_children[i]) == nn.Sequential:\n            for child in model_children[i]:\n                if type(child) == nn.Conv1d:\n                    counter += 1\n                    model_weights.append(child.weight)\n                    conv_layers.append(child)\n                    bias_weights.append(child.bias)\n\n    print(f\"Total convolutional layers: {counter}\")\n    return conv_layers, model_weights, bias_weights\n\ndef view_filters(model_weights, num_cols=8):\n    model_weights = model_weights[0]\n    num_filt = model_weights.shape[0]\n    filt_width = model_weights[0].shape[1]\n    num_rows = int(np.ceil(num_filt/num_cols))\n    \n    # visualize the first conv layer filters\n    plt.figure(figsize=(20, 17))\n\n    for i, filter in enumerate(model_weights):\n        ax = plt.subplot(num_rows, num_cols, i+1)\n        ax.imshow(filter.cpu().detach(), cmap='gray')\n        ax.set_yticks(np.arange(4))\n        ax.set_yticklabels(['A', 'C', 'G','T'])\n        ax.set_xticks(np.arange(filt_width))\n        ax.set_title(f\"Filter {i}\")\n\n    plt.tight_layout()\n    plt.show()\n\nFirst, we can take a peek at the raw filters.\n\nconv_layers, model_weights, bias_weights = get_conv_layers_from_model(model_cnn)\nview_filters(model_weights)\n\nTotal convolutional layers: 1\n\n\n\n\n\n\n\n\n\n\n\ndef get_conv_output_for_seq(seq, conv_layer):\n    '''\n    Given an input sequeunce and a convolutional layer, \n    get the output tensor containing the conv filter \n    activations along each position in the sequence\n    '''\n    # format seq for input to conv layer (OHE, reshape)\n    seq = torch.tensor(one_hot_encode(seq)).unsqueeze(0).permute(0,2,1).to(DEVICE)\n\n    # run seq through conv layer\n    with torch.no_grad(): # don't want as part of gradient graph\n        # apply learned filters to input seq\n        res = conv_layer(seq.float())\n        return res[0]\n    \n\ndef get_filter_activations(seqs, conv_layer,act_thresh=0):\n    '''\n    Given a set of input sequences and a trained convolutional layer, \n    determine the subsequences for which each filter in the conv layer \n    activate most strongly. \n    \n    1.) Run seq inputs through conv layer. \n    2.) Loop through filter activations of the resulting tensor, saving the\n            position where filter activations were &gt; act_thresh. \n    3.) Compile a count matrix for each filter by accumulating subsequences which\n            activate the filter above the threshold act_thresh\n    '''\n    # initialize dict of pwms for each filter in the conv layer\n    # pwm shape: 4 nucleotides X filter width, initialize to 0.0s\n    num_filters = conv_layer.out_channels\n    filt_width = conv_layer.kernel_size[0]\n    filter_pwms = dict((i,torch.zeros(4,filt_width)) for i in range(num_filters))\n    \n    print(\"Num filters\", num_filters)\n    print(\"filt_width\", filt_width)\n    \n    # loop through a set of sequences and collect subseqs where each filter activated\n    for seq in seqs:\n        # get a tensor of each conv filter activation along the input seq\n        res = get_conv_output_for_seq(seq, conv_layer)\n\n        # for each filter and it's activation vector\n        for filt_id,act_vec in enumerate(res):\n            # collect the indices where the activation level \n            # was above the threshold\n            act_idxs = torch.where(act_vec&gt;act_thresh)[0]\n            activated_positions = [x.item() for x in act_idxs]\n\n            # use activated indicies to extract the actual DNA\n            # subsequences that caused filter to activate\n            for pos in activated_positions:\n                subseq = seq[pos:pos+filt_width]\n                #print(\"subseq\",pos, subseq)\n                # transpose OHE to match PWM orientation\n                subseq_tensor = torch.tensor(one_hot_encode(subseq)).T\n\n                # add this subseq to the pwm count for this filter\n                filter_pwms[filt_id] += subseq_tensor            \n            \n    return filter_pwms\n\ndef view_filters_and_logos(model_weights,filter_activations, num_cols=8):\n    '''\n    Given some convolutional model weights and filter activation PWMs, \n    visualize the heatmap and motif logo pairs in a simple grid\n    '''\n    model_weights = model_weights[0].squeeze(1)\n    print(model_weights.shape)\n\n    # make sure the model weights agree with the number of filters\n    assert(model_weights.shape[0] == len(filter_activations))\n    \n    num_filts = len(filter_activations)\n    num_rows = int(np.ceil(num_filts/num_cols))*2+1 \n    # ^ not sure why +1 is needed... complained otherwise\n    \n    plt.figure(figsize=(20, 17))\n\n    j=0 # use to make sure a filter and it's logo end up vertically paired\n    for i, filter in enumerate(model_weights):\n        if (i)%num_cols == 0:\n            j += num_cols\n\n        # display raw filter\n        ax1 = plt.subplot(num_rows, num_cols, i+j+1)\n        ax1.imshow(filter.cpu().detach(), cmap='gray')\n        ax1.set_yticks(np.arange(4))\n        ax1.set_yticklabels(['A', 'C', 'G','T'])\n        ax1.set_xticks(np.arange(model_weights.shape[2]))\n        ax1.set_title(f\"Filter {i}\")\n\n        # display sequence logo\n        ax2 = plt.subplot(num_rows, num_cols, i+j+1+num_cols)\n        filt_df = pd.DataFrame(filter_activations[i].T.numpy(),columns=['A','C','G','T'])\n        filt_df_info = logomaker.transform_matrix(filt_df,from_type='counts',to_type='information')\n        logo = logomaker.Logo(filt_df_info,ax=ax2)\n        ax2.set_ylim(0,2)\n        ax2.set_title(f\"Filter {i}\")\n\n    plt.tight_layout()\n\n\n# just use some seqs from test_df to activate filters\nsome_seqs = random.choices(seqs, k=3000)\n\nfilter_activations = get_filter_activations(some_seqs, conv_layers[0])\nview_filters_and_logos(model_weights,filter_activations)\n\nNum filters 32\nfilt_width 3\ntorch.Size([32, 4, 3])\n\n\n/opt/anaconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n\n\n\n\n\n\n\n\n\n\nVisualize filters using a stronger activation threshold\nact_thresh = 1 instead of 0. (Some filters have no subsequence matches above the threshold and result in an empty motif logo)\n\nfilter_activations = get_filter_activations(some_seqs, conv_layers[0],act_thresh=1)\nview_filters_and_logos(model_weights,filter_activations)\n\nNum filters 32\nfilt_width 3\ntorch.Size([32, 4, 3])\n\n\n/opt/anaconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/opt/anaconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/opt/anaconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/opt/anaconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/opt/anaconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/opt/anaconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/opt/anaconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/opt/anaconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n\n\n\n\n\n\n\n\n\nFrom this particular CNN training, we can see a few filters have picked up on the strong TAT and GCG motifs, but other filters have focused on other patterns as well. There is some debate about how relevant convolutional filter visualizations are for model interpretability. In deep models with multiple convolutional layers, convolutional filters can be recombined in more complex ways inside the hidden layers, so the first layer filters may not be as informative on their own (Koo and Eddy, 2019). Much of the field has since moved towards attention mechanisms and other explainability methods, but should you be curious to visualize your filters as potential motifs, these functions may help get you started!\n\n\nQuestion 4\nModify the scoring function in Part 1 to use a longer motif (e.g., change “TAT” to “TATAT” and “GCG” to “GCGCG”). Keep everything else the same (same sequence length, same CNN architecture with kernel_size=3). Train the model and visualize the convolutional filters. Answer these questions in your analysis:\n\nCan the model with kernel_size=3 still detect the longer motifs? How?\nHow do the filter visualizations differ from the original 3-base motif case?\n\n ## 8. Conclusion This tutorial shows some basic Pytorch structure for building CNN models that work with DNA sequences. The practice task used in this demo is not reflective of real biological signals; rather, we designed the scoring method to simulate the presence of regulatory motifs in very short sequences that were easy for us humans to inspect and verify that Pytorch was behaving as expected. From this small example, we observed how a basic CNN with sliding filters was able to predict our scoring scheme better than a basic linear model that only accounted for absolute nucleotide position (without local context).\nTo read more about CNN’s applied to DNA in the wild, check out the following foundational papers: * DeepBind: Alipanahi et al 2015 * DeepSea: Zhou and Troyanskaya 2015 * Basset: Kelley et al 2016\nI hope other new-to-ML folks interested in tackling biological questions may find this helpful for getting started with using Pytorch to model DNA sequences :)"
  },
  {
    "objectID": "post/2025-03-25-unit00/basic_DNA_tutorial.html#foot-notes",
    "href": "post/2025-03-25-unit00/basic_DNA_tutorial.html#foot-notes",
    "title": "DNA score prediction with Pytorch",
    "section": "9. Foot notes",
    "text": "9. Foot notes\n\nFOOTNOTE 1\nIn this tutorial, the CNN model definition uses a 1D convolutional layer - since DNA is not an image with 2 dimensions, Conv1D is sufficient to just slide along the length dimension and not scan up and down. (In fact, sliding a filter “up” and “down” doesn’t apply to one-hot encoded DNA matrices: separating the A and C rows from the G and T rows doesn’t make sense - you need all 4 rows to accurately represent a DNA sequence.)\nHowever, I once found myself needing to use an analysis tool built with keras and found a pytorch2keras conversion script. The conversion script only knew how to handle Conv2d layers and gave errors for models with Conv1d layers :(\nIn case this happens to you, here is an example of how to reformat the CNN definition using a Conv2D while ensuring that it still scans along the DNA as if it were a Conv1D:\n\nclass DNA_CNN_2D(nn.Module):\n    def __init__(self,\n                 seq_len,\n                 num_filters=31,\n                 kernel_size=3,\n                ):\n        super().__init__()\n        self.seq_len = seq_len\n        \n        self.conv_net = nn.Sequential(\n            nn.Conv2d(1, num_filters, kernel_size=(4,kernel_size)),\n            # ^^ changed from 4 to 1 channel, moved 4 to kernel_size\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(num_filters*(seq_len-kernel_size+1), 1)\n        ) \n\n    def forward(self, xb):\n        # reshape view to batch_ssize x 4channel x seq_len\n        # permute to put channel in correct order\n        xb = xb.permute(0,2,1).unsqueeze(1)\n        # ^^ Conv2D input fix\n        \n        #print(xb.shape)\n        out = self.conv_net(xb)\n        return out\n\n\n\nFOOTNOTE 2\nIf you’re doing a classification task instead of a regression task, you may want to use CrossEntropyLoss. However, CrossEntropyLoss expects a slightly different format than MSELoss - try this:\n\nloss = loss_func(xb_out, yb.long().squeeze(1))"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "",
    "text": "created by Erin Wilson. Downloaded from here.\nSome edits by Haky Im and Ran Blekhman for the deep learning in genomics gene46100 course.\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#import-necessary-modules",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#import-necessary-modules",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "import necessary modules",
    "text": "import necessary modules\n\nfrom collections import defaultdict\nfrom itertools import product\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport random\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nif torch.backends.mps.is_available():\n    torch.set_default_dtype(torch.float32)\n    print(\"Set default to float32 for MPS compatibility\")\n\nSet default to float32 for MPS compatibility"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#set-seeds-for-reproduciblity-across-runs",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#set-seeds-for-reproduciblity-across-runs",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "set seeds for reproduciblity across runs",
    "text": "set seeds for reproduciblity across runs\n\n# Set a random seed in a bunch of different places\ndef set_seed(seed: int = 42) -&gt; None:\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    \n    if torch.backends.mps.is_available():\n        # For MacBooks with Apple Silicon\n        torch.mps.manual_seed(seed)\n    elif torch.cuda.is_available():\n        # For CUDA GPUs\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    print(f\"Random seed set as {seed}\")\n    \nset_seed(17)\n\nRandom seed set as 17"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#define-gpu-device",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#define-gpu-device",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "define GPU device",
    "text": "define GPU device\nAre you working on a GPU? If so, you can put your data/models on DEVICE (and have to do so explicity)! If not, you can probably remove all instances of foo.to(DEVICE) and it should still work fine on a CPU.\n\nDEVICE = torch.device('mps' if torch.backends.mps.is_available() \n                     else 'cuda' if torch.cuda.is_available() \n                     else 'cpu')\nDEVICE\n\ndevice(type='mps')\n\n\n ## 1. Generate synthetic DNA data\nUsually scientists might be interested in predicting something like a binding score, an expression strength, or classifying a TF binding event. But here, we are going to keep it simple: the goal in this tutorial is to observe if a deep learning model can learn to detect a very small, simple pattern in a DNA sequence and score it appropriately (again, just a practice task to convince ourselves that we have actually set up the Pytorch pieces correctly such that it can learn from input that looks like a DNA sequence).\nSo arbitrarily, let’s say that given an 8-mer DNA sequence, we will score it based on the following rules: * A = +20 points * C = +17 points * G = +14 points * T = +11 points\nFor every 8-mer, let’s sum up its total points based on the nucleotides in its sequence, then take the average. For example,\n\nAAAAAAAA would score 20.0\n\n(mean(20 + 20 + 20 + 20 + 20 + 20 + 20 + 20) = 20.0)\n\nACAAAAAA would score 19.625\n\n(mean(20 + 17 + 20 + 20 + 20 + 20 + 20 + 20) = 19.625)\n\n\nThese values for the nucleotides are arbitrary - there’s no real biology here! It’s just a way to assign sequences a score for the purposes of our Pytorch practice.\nHowever, since many recent papers use methods like CNNs to automatically detect “motifs,” or short patterns in the DNA that can activate or repress a biological response, let’s add one more piece to our scoring system. To simulate something like motifs influencing gene expression, let’s say a given sequence gets a +10 bump if TAT appears anywhere in the 8-mer, and a -10 bump if it has a GCG in it. Again, these motifs don’t mean anything in real life, they are just a mechanism for simulating a really simple activation or repression effect.\n\nSo let’s implement this basic scoring function!\n\n# define function for generating all k-mers of length k\ndef kmers(k):\n    '''Generate a list of all k-mers for a given k'''\n    \n    return [''.join(x) for x in product(['A','C','G','T'], repeat=k)]\n\n\ngenerate all 8-mers\n\n# generate all 8-mers\nseqs8 = kmers(8)\nprint('Total 8mers:',len(seqs8))\n\nTotal 8mers: 65536\n\n\n\n\ndefine scoring function for the DNA sequences\n\n# define score_dict\nscore_dict = {\n    'A':20,\n    'C':17,\n    'G':14,\n    'T':11\n}\n# define function for scoring sequences\ndef score_seqs_motif(seqs):\n    '''\n    Calculate the scores for a list of sequences based on \n    the above score_dict\n    '''\n    data = []\n    for seq in seqs:\n        # get the average score by nucleotide\n        score = np.mean([score_dict[base] for base in seq],dtype=np.float32)\n        \n        # give a + or - bump if this k-mer has a specific motif\n        if 'TAT' in seq:\n            score += 10\n        if 'GCG' in seq:\n            score -= 10\n        data.append([seq,score])\n        \n    df = pd.DataFrame(data, columns=['seq','score'])\n    return df\n\n\nmer8 = score_seqs_motif(seqs8)\nmer8.head()\n\n\n\n\n\n\n\n\nseq\nscore\n\n\n\n\n0\nAAAAAAAA\n20.000\n\n\n1\nAAAAAAAC\n19.625\n\n\n2\nAAAAAAAG\n19.250\n\n\n3\nAAAAAAAT\n18.875\n\n\n4\nAAAAAACA\n19.625\n\n\n\n\n\n\n\nSpot check scores of a couple seqs with motifs:\n\nmer8[mer8['seq'].isin(['TGCGTTTT','CCCCCTAT'])]\n\n\n\n\n\n\n\n\nseq\nscore\n\n\n\n\n21875\nCCCCCTAT\n25.875\n\n\n59135\nTGCGTTTT\n2.500"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#plot-distribution-of-motif-scores",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#plot-distribution-of-motif-scores",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "plot distribution of motif scores",
    "text": "plot distribution of motif scores\n\nplt.hist(mer8['score'].values,bins=20)\nplt.title(\"8-mer with Motifs score distribution\")\nplt.xlabel(\"seq score\",fontsize=14)\nplt.ylabel(\"count\",fontsize=14)\nplt.show()\n\n\n\n\n\n\n\n\nAs expected, the distribution of scores across all 8-mers has 3 groups: * No motif (centered around ~15) * contains TAT (~25) * contains GCG (~5)"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-1",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-1",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "Question 1",
    "text": "Question 1\nModify the scoring function to create a more complex pattern. Instead of giving fixed bonuses for “TAT” and “GCG”, implement a position-dependent scoring where a motif gets a higher bonus if it appears at the beginning of the sequence compared to the end. How does this change the distribution of scores?"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#next-we-want-to-train-a-model-to-predict-this-score-by-from-the-dna-sequence",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#next-we-want-to-train-a-model-to-predict-this-score-by-from-the-dna-sequence",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "Next, we want to train a model to predict this score by from the DNA sequence",
    "text": "Next, we want to train a model to predict this score by from the DNA sequence\n ## 2. Prepare data for Pytorch training\nFor neural networks to make predictions, you have to give it your input as a matrix of numbers. For example, to classify images by whether or not they contain a cat, a network “sees” the image as a matrix of pixel values and learns relevant patterns in the relative arrangement of pixels (e.g. patterns that correspond to cat ears, or a nose with whiskers).\nWe similarly need to turn our DNA sequences (strings of ACGTs) into a matrix of numbers. So how do we pretend our DNA is a cat?\nOne common strategy is to one-hot encode the DNA: treat each nucleotide as a vector of length 4, where 3 positions are 0 and one position is a 1, depending on the nucleotide.\n\nturn DNA sequences into numbers with one hot encoding\n\nThis one-hot encoding has the nice property that it makes your DNA appear like how a computer sees a picture of a cat!\n\ndef one_hot_encode(seq):\n    \"\"\"\n    Given a DNA sequence, return its one-hot encoding\n    \"\"\"\n    # Make sure seq has only allowed bases\n    allowed = set(\"ACTGN\")\n    if not set(seq).issubset(allowed):\n        invalid = set(seq) - allowed\n        raise ValueError(f\"Sequence contains chars not in allowed DNA alphabet (ACGTN): {invalid}\")\n        \n    # Dictionary returning one-hot encoding for each nucleotide \n    nuc_d = {'A':[1.0,0.0,0.0,0.0],\n             'C':[0.0,1.0,0.0,0.0],\n             'G':[0.0,0.0,1.0,0.0],\n             'T':[0.0,0.0,0.0,1.0],\n             'N':[0.0,0.0,0.0,0.0]}\n    \n    # Create array from nucleotide sequence\n    vec=np.array([nuc_d[x] for x in seq], dtype=np.float32)\n        \n    return vec\n\n\n# one hot encoding of 8 As\na8 = one_hot_encode(\"AAAAAAAA\")\nprint(\"AAAAAA:\\n\",a8)\n\n\nAAAAAA:\n [[1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]]\n\n\n\n# one hot encoding of another DNA\ns = one_hot_encode(\"AGGTACCT\")\nprint(\"AGGTACC:\\n\",s)\nprint(\"shape:\",s.shape)\n\nAGGTACC:\n [[1. 0. 0. 0.]\n [0. 0. 1. 0.]\n [0. 0. 1. 0.]\n [0. 0. 0. 1.]\n [1. 0. 0. 0.]\n [0. 1. 0. 0.]\n [0. 1. 0. 0.]\n [0. 0. 0. 1.]]\nshape: (8, 4)\n\n\n\n\nsplit data into train, validation, and test\nData are typically split into training, validation, and test sets. This helps avoid overfitting and achieve better generalization to new data.\n\nTraining set (e.g. 70% of the data)\n\nused to train the model\nmodel learns patterns from this data\nanalogy: studying for an exam\n\nValidation or tuning set (e.g. 15% of the data)\n\nused to tune hypterparameters\nhelps prevent overfitting\nanalogy: pratice problems while studying\n\nTest set or held out set (e.g. 15% of the data)\n\nused only to evaluate final model performance\nnever used during training or tuning\nanalogy: actual exam\n\n\nNote: in this tutorial, that uses the quick_split function defined here splits the data into 20% for the test set, 80% of the remaining 80% as the training set (i.e. 64% of the total) and 20% of the non test sets as validation (i.e. 16%).\n\ndefine quick splitting function\n\n# define function for splitting data\ndef quick_split(df, split_frac=0.8, verbose=False):\n    '''\n    Given a df of samples, randomly split indices between\n    train and test at the desired fraction\n    '''\n    cols = df.columns # original columns, use to clean up reindexed cols\n    df = df.reset_index()\n\n    # shuffle indices\n    idxs = list(range(df.shape[0]))\n    random.shuffle(idxs)\n\n    # split shuffled index list by split_frac\n    split = int(len(idxs)*split_frac)\n    train_idxs = idxs[:split]\n    test_idxs = idxs[split:]\n    \n    # split dfs and return\n    train_df = df[df.index.isin(train_idxs)]\n    test_df = df[df.index.isin(test_idxs)]\n        \n    return train_df[cols], test_df[cols]\n\n\n\nsplit data into train, validation, and test sets\n\n# split data into train, validation, and test\nfull_train_df, test_df = quick_split(mer8)\ntrain_df, val_df = quick_split(full_train_df)\n\nprint(\"Train:\", train_df.shape)\nprint(\"Val:\", val_df.shape)\nprint(\"Test:\", test_df.shape)\n\ntrain_df.head()\n\nTrain: (41942, 2)\nVal: (10486, 2)\nTest: (13108, 2)\n\n\n\n\n\n\n\n\n\nseq\nscore\n\n\n\n\n0\nAAAAAAAA\n20.000\n\n\n1\nAAAAAAAC\n19.625\n\n\n2\nAAAAAAAG\n19.250\n\n\n3\nAAAAAAAT\n18.875\n\n\n4\nAAAAAACC\n19.250\n\n\n\n\n\n\n\n\n\n\nplot distribution of train, validation, and test data and check that they are similarly distributed\n\ndef plot_train_test_hist(train_df, val_df,test_df,bins=20):\n    ''' Check distribution of train/test scores, sanity check that its not skewed'''\n    plt.hist(train_df['score'].values,bins=bins,label='train',alpha=0.5)\n    plt.hist(val_df['score'].values,bins=bins,label='val',alpha=0.75)\n    plt.hist(test_df['score'].values,bins=bins,label='test',alpha=0.4)\n    plt.legend()\n    plt.xlabel(\"seq score\",fontsize=14)\n    plt.ylabel(\"count\",fontsize=14)\n    plt.show()\n\nWith the below histogram, we can confirm that the train, test, and val sets contain example sequences from each bucket of the distribution (each set has some examples with each kind of motif)\n\nplot_train_test_hist(train_df, val_df,test_df)\n\n\n\n\n\n\n\n\n\n\ndefine dataset and dataloader classes\nDataset and DataLoader classes allow efficient data handling in deep learning.\nDataset class allows standardized way to access and preprocess data.\nDataloader handles batching, shuffling, parallel data loading to make it easier to feed the data for training.\nYou can read more about DataLoader and Dataset objects.\n\nfrom torch.utils.data import Dataset, DataLoader\n\n\ndefine one hot encoded dataset class\nThis class is essential for preparing DNA sequence data for deep learning models, converting the DNA sequences into a numerical format that neural networks can process.\n\nclass SeqDatasetOHE(Dataset):\n    '''\n    Dataset for one-hot-encoded sequences\n    '''\n    def __init__(self, df, seq_col='seq', target_col='score'):\n        # Input: DataFrame with DNA sequences and their scores\n        self.seqs = list(df[seq_col].values)  # Get DNA sequences\n        self.seq_len = len(self.seqs[0])      # Length of each sequence\n        \n        # Convert DNA sequences to one-hot encoding\n        self.ohe_seqs = torch.stack([torch.tensor(one_hot_encode(x)) for x in self.seqs])\n        \n        # Get target scores\n        self.labels = torch.tensor(list(df[target_col].values)).unsqueeze(1)\n        \n    def __len__(self): return len(self.seqs)\n    \n    def __getitem__(self,idx):\n        # Given an index, return a tuple of an X with it's associated Y\n        # This is called inside DataLoader\n        seq = self.ohe_seqs[idx]\n        label = self.labels[idx]\n        \n        return seq, label\n\n\n\nconstruct DataLoaders from Datasets.\n\ndef build_dataloaders(train_df,\n                      test_df,\n                      seq_col='seq',\n                      target_col='score',\n                      batch_size=128,\n                      shuffle=True\n                     ):\n    '''\n    Given a train and test df with some batch construction\n    details, put them into custom SeqDatasetOHE() objects. \n    Give the Datasets to the DataLoaders and return.\n    '''\n    \n    # create Datasets    \n    train_ds = SeqDatasetOHE(train_df,seq_col=seq_col,target_col=target_col)\n    test_ds = SeqDatasetOHE(test_df,seq_col=seq_col,target_col=target_col)\n\n    # Put DataSets into DataLoaders\n    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle)\n    test_dl = DataLoader(test_ds, batch_size=batch_size)\n\n    \n    return train_dl,test_dl\n\n\ntrain_dl, val_dl = build_dataloaders(train_df, val_df)\n\nThese dataloaders are now ready to be used in a training loop!\n ## 3. Define Pytorch models The primary model I was interested in trying was a Convolutional Neural Network, as these have been shown to be useful for learning motifs from genomic data. But as a point of comparison, I included a simple Linear model. Here are some model definitions:\n\n# very simple linear model\nclass DNA_Linear(nn.Module):\n    def __init__(self, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        # the 4 is for our one-hot encoded vector length 4!\n        self.lin = nn.Linear(4*seq_len, 1)\n\n    def forward(self, xb):\n        # reshape to flatten sequence dimension\n        xb = xb.view(xb.shape[0],self.seq_len*4)\n        # Linear wraps up the weights/bias dot product operations\n        out = self.lin(xb)\n        return out\n    \n## CNN model\nclass DNA_CNN(nn.Module):\n    def __init__(self, seq_len, num_filters=32, kernel_size=3):\n        super().__init__()\n        self.seq_len = seq_len\n        \n        # Define layers individually\n        self.conv = nn.Conv1d(4, num_filters, kernel_size=kernel_size)\n        self.relu = nn.ReLU(inplace=True)\n        self.linear = nn.Linear(num_filters*(seq_len-kernel_size+1), 1)\n\n    def forward(self, xb):\n        # reshape view to batch_size x 4channel x seq_len\n        xb = xb.permute(0, 2, 1)\n        \n        # Apply layers step by step\n        x = self.conv(xb)\n        x = self.relu(x)\n        x = x.flatten(1)  # flatten all dimensions except batch\n        out = self.linear(x)\n        return out \n\nThese aren’t optimized models, just something to start with (again, we’re just practicing connecting the Pytorch tubes in the context of DNA!). * The Linear model tries to predict the score by simply weighting the nucleotides that appears in each position. * The CNN model uses 32 filters of length (kernel_size) 3 to scan across the 8-mer sequences for informative 3-mer patterns.\n ## 4. Define the training loop functions\nThe model training process is structured into a series of modular functions, each responsible for a specific part of the workflow. This design improves clarity, reusability, and flexibility when working with different models, optimizers, or loss functions.\nThe overall structure is as follows:\n# Initializes optimizer and loss function if not provided, then trains the model\nrun_training()\n\n    # Iterates over multiple epochs\n    train_loop()\n\n        # Performs one complete pass over the training dataset\n        train_epoch()\n\n            # Computes loss and backprops for a single batch\n            process_batch()\n\n        # Performs one complete pass over the validation dataset\n        val_epoch()\n\n            # Computes loss for a single batch no gradient updates\n            process_batch()\n\n# +--------------------------------+\n# | Training and fitting functions |\n# +--------------------------------+\n\ndef process_batch(model, loss_func, xb, yb, opt=None,verbose=False):\n    '''\n    Apply loss function to a batch of inputs. If no optimizer\n    is provided, skip the back prop step.\n    '''\n    if verbose:\n        print('loss batch ****')\n        print(\"xb shape:\",xb.shape)\n        print(\"yb shape:\",yb.shape)\n        print(\"yb shape:\",yb.squeeze(1).shape)\n        #print(\"yb\",yb)\n\n    # get the batch output from the model given your input batch \n    # ** This is the model's prediction for the y labels! **\n    xb_out = model(xb.float())\n    \n    if verbose:\n        print(\"model out pre loss\", xb_out.shape)\n        #print('xb_out', xb_out)\n        print(\"xb_out:\",xb_out.shape)\n        print(\"yb:\",yb.shape)\n        print(\"yb.long:\",yb.long().shape)\n    \n    loss = loss_func(xb_out, yb.float()) # for MSE/regression\n    # __FOOTNOTE 2__\n    \n    if opt is not None: # if opt\n        opt.zero_grad() ## moved zero grad up to make sure it's not accumulating grads from previous batches\n        loss.backward()\n        opt.step()\n    return loss.item(), len(xb)\n\n\ndef train_epoch(model, train_dl, loss_func, device, opt):\n    '''\n    Execute 1 set of batched training within an epoch\n    '''\n    # Set model to Training mode\n    model.train()\n    tl = [] # train losses\n    ns = [] # batch sizes, n\n    \n    # loop through train DataLoader\n    for xb, yb in train_dl:\n        # put on GPU\n        xb, yb = xb.to(device),yb.to(device)\n        \n        # provide opt so backprop happens\n        t, n = process_batch(model, loss_func, xb, yb, opt=opt)\n        \n        # collect train loss and batch sizes\n        tl.append(t)\n        ns.append(n)\n    \n    # average the losses over all batches    \n    train_loss = np.sum(np.multiply(tl, ns)) / np.sum(ns)\n    \n    return train_loss\n\ndef val_epoch(model, val_dl, loss_func, device):\n    '''\n    Execute 1 set of batched validation within an epoch\n    '''\n    # Set model to Evaluation mode\n    model.eval()\n    with torch.no_grad():\n        vl = [] # val losses\n        ns = [] # batch sizes, n\n        \n        # loop through validation DataLoader\n        for xb, yb in val_dl:\n            # put on GPU\n            xb, yb = xb.to(device),yb.to(device)\n\n            # Do NOT provide opt here, so backprop does not happen\n            v, n = process_batch(model, loss_func, xb, yb)\n\n            # collect val loss and batch sizes\n            vl.append(v)\n            ns.append(n)\n\n    # average the losses over all batches\n    val_loss = np.sum(np.multiply(vl, ns)) / np.sum(ns)\n    \n    return val_loss\n\n\ndef train_loop(epochs, model, loss_func, opt, train_dl, val_dl,device,patience=1000):\n    '''\n    Fit the model params to the training data, eval on unseen data.\n    Loop for a number of epochs and keep train of train and val losses \n    along the way\n    '''\n    # keep track of losses\n    train_losses = []    \n    val_losses = []\n    \n    # loop through epochs\n    for epoch in range(epochs):\n        # take a training step\n        train_loss = train_epoch(model,train_dl,loss_func,device,opt)\n        train_losses.append(train_loss)\n\n        # take a validation step\n        val_loss = val_epoch(model,val_dl,loss_func,device)\n        val_losses.append(val_loss)\n        \n        print(f\"E{epoch} | train loss: {train_loss:.3f} | val loss: {val_loss:.3f}\")\n\n    return train_losses, val_losses\n\n\ndef run_training(train_dl,val_dl,model,device,\n              lr=0.01, epochs=50, \n              lossf=None,opt=None\n             ):\n    '''\n    Given train and val DataLoaders and a NN model, fit the model to the training\n    data. By default, use MSE loss and an SGD optimizer\n    '''\n    # define optimizer\n    if opt:\n        optimizer = opt\n    else: # if no opt provided, just use SGD\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n    \n    # define loss function\n    if lossf:\n        loss_func = lossf\n    else: # if no loss function provided, just use MSE\n        loss_func = torch.nn.MSELoss()\n    \n    # run the training loop\n    train_losses, val_losses = train_loop(\n                                epochs, \n                                model, \n                                loss_func, \n                                optimizer, \n                                train_dl, \n                                val_dl, \n                                device)\n\n    return train_losses, val_losses"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#explanation-of-the-functions-that-make-up-the-training-process",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#explanation-of-the-functions-that-make-up-the-training-process",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "Explanation of the functions that make up the training process",
    "text": "Explanation of the functions that make up the training process\n\nrun_training(): This is the top-level function that orchestrates the entire training process. It handles the initialization of the optimizer and loss function if they are not explicitly provided. It calls the train_loop() function to begin the iterative training loop.\ntrain_loop(): This function manages the training loop across a specified number of epochs. For each epoch, it calls train_epoch() and val_epoch() to perform training and validation, respectively. It also prints the training and validation losses for each epoch.\ntrain_epoch(): This function performs one full pass over the training dataset. It iterates through the train_dl (DataLoader) to get batches of training data. For each batch, it calls process_batch() with the optimizer, enabling backpropagation. It accumulates and averages the training losses.\nval_epoch(): This function performs one full pass over the validation dataset. It iterates through the val_dl (DataLoader) to get batches of validation data. For each batch, it calls process_batch() without the optimizer, preventing gradient updates. It accumulates and averages the validation losses.\nprocess_batch(): This function processes a single batch of data. It calculates the model’s predictions and computes the loss. If an optimizer is provided, it performs backpropagation and updates the model’s weights. If no optimiser is provided, it just returns the loss and batch size. This function is used for both training and validation, with the presence of the optimiser parameter determining if back propagation occurs.\n\n ## 5. Train the models First let’s try running a Linear Model on our 8-mer sequences\n\n# get the sequence length from the first seq in the df\nseq_len = len(train_df['seq'].values[0])\n\n# create Linear model object\nmodel_lin = DNA_Linear(seq_len)\n# use float32 since mps cannot handle 64\nmodel_lin = model_lin.type(torch.float32)\nmodel_lin.to(DEVICE) # put on GPU\n\n\n# run the training pipeline with default settings!\nlin_train_losses, lin_val_losses = run_training(\n    train_dl, \n    val_dl, \n    model_lin,\n    DEVICE\n)\n\nE0 | train loss: 21.238 | val loss: 12.980\nE1 | train loss: 12.969 | val loss: 12.826\nE2 | train loss: 12.918 | val loss: 12.832\nE3 | train loss: 12.915 | val loss: 12.847\nE4 | train loss: 12.916 | val loss: 12.833\nE5 | train loss: 12.918 | val loss: 12.837\nE6 | train loss: 12.915 | val loss: 12.828\nE7 | train loss: 12.917 | val loss: 12.826\nE8 | train loss: 12.917 | val loss: 12.827\nE9 | train loss: 12.917 | val loss: 12.827\nE10 | train loss: 12.918 | val loss: 12.831\nE11 | train loss: 12.914 | val loss: 12.836\nE12 | train loss: 12.918 | val loss: 12.834\nE13 | train loss: 12.916 | val loss: 12.830\nE14 | train loss: 12.917 | val loss: 12.832\nE15 | train loss: 12.917 | val loss: 12.831\nE16 | train loss: 12.917 | val loss: 12.833\nE17 | train loss: 12.915 | val loss: 12.882\nE18 | train loss: 12.916 | val loss: 12.834\nE19 | train loss: 12.916 | val loss: 12.833\nE20 | train loss: 12.917 | val loss: 12.830\nE21 | train loss: 12.918 | val loss: 12.830\nE22 | train loss: 12.917 | val loss: 12.826\nE23 | train loss: 12.914 | val loss: 12.826\nE24 | train loss: 12.915 | val loss: 12.828\nE25 | train loss: 12.916 | val loss: 12.833\nE26 | train loss: 12.916 | val loss: 12.829\nE27 | train loss: 12.916 | val loss: 12.828\nE28 | train loss: 12.918 | val loss: 12.848\nE29 | train loss: 12.916 | val loss: 12.830\nE30 | train loss: 12.916 | val loss: 12.841\nE31 | train loss: 12.917 | val loss: 12.823\nE32 | train loss: 12.917 | val loss: 12.833\nE33 | train loss: 12.916 | val loss: 12.825\nE34 | train loss: 12.918 | val loss: 12.822\nE35 | train loss: 12.916 | val loss: 12.838\nE36 | train loss: 12.917 | val loss: 12.833\nE37 | train loss: 12.914 | val loss: 12.837\nE38 | train loss: 12.917 | val loss: 12.834\nE39 | train loss: 12.917 | val loss: 12.846\nE40 | train loss: 12.916 | val loss: 12.826\nE41 | train loss: 12.917 | val loss: 12.820\nE42 | train loss: 12.917 | val loss: 12.835\nE43 | train loss: 12.916 | val loss: 12.832\nE44 | train loss: 12.916 | val loss: 12.827\nE45 | train loss: 12.915 | val loss: 12.827\nE46 | train loss: 12.916 | val loss: 12.827\nE47 | train loss: 12.918 | val loss: 12.829\nE48 | train loss: 12.915 | val loss: 12.824\nE49 | train loss: 12.917 | val loss: 12.824\n\n\nLet’s look at the loss in quick plot:\n\ndef quick_loss_plot(data_label_list,loss_type=\"MSE Loss\",sparse_n=0):\n    '''\n    For each train/test loss trajectory, plot loss by epoch\n    '''\n    for i,(train_data,test_data,label) in enumerate(data_label_list):    \n        plt.plot(train_data,linestyle='--',color=f\"C{i}\", label=f\"{label} Train\")\n        plt.plot(test_data,color=f\"C{i}\", label=f\"{label} Val\",linewidth=3.0)\n\n    plt.legend()\n    plt.ylabel(loss_type)\n    plt.xlabel(\"Epoch\")\n    plt.legend(bbox_to_anchor=(1,1),loc='upper left')\n    plt.show()\n\n\nlin_data_label = (lin_train_losses,lin_val_losses,\"Lin\")\nquick_loss_plot([lin_data_label])\n\n\n\n\n\n\n\n\nAt first glance, not much learning appears to be happening.\nNext let’s try the CNN.\n\nseq_len = len(train_df['seq'].values[0])\n\n# create Linear model object\nmodel_cnn = DNA_CNN(seq_len)\nmodel_cnn.to(DEVICE) # put on GPU\n\n# run the model with default settings!\ncnn_train_losses, cnn_val_losses = run_training(\n    train_dl, \n    val_dl, \n    model_cnn,\n    DEVICE\n)\n\nE0 | train loss: 14.640 | val loss: 10.167\nE1 | train loss: 8.625 | val loss: 7.035\nE2 | train loss: 6.305 | val loss: 4.967\nE3 | train loss: 4.507 | val loss: 3.257\nE4 | train loss: 3.123 | val loss: 2.256\nE5 | train loss: 2.408 | val loss: 2.627\nE6 | train loss: 1.997 | val loss: 2.078\nE7 | train loss: 1.827 | val loss: 4.446\nE8 | train loss: 1.547 | val loss: 1.297\nE9 | train loss: 1.438 | val loss: 1.185\nE10 | train loss: 1.249 | val loss: 1.108\nE11 | train loss: 1.200 | val loss: 1.149\nE12 | train loss: 1.140 | val loss: 1.096\nE13 | train loss: 1.011 | val loss: 1.102\nE14 | train loss: 1.022 | val loss: 1.188\nE15 | train loss: 1.027 | val loss: 1.119\nE16 | train loss: 1.045 | val loss: 1.040\nE17 | train loss: 0.999 | val loss: 1.052\nE18 | train loss: 0.965 | val loss: 1.069\nE19 | train loss: 0.944 | val loss: 1.208\nE20 | train loss: 0.945 | val loss: 1.175\nE21 | train loss: 0.925 | val loss: 1.038\nE22 | train loss: 0.927 | val loss: 1.249\nE23 | train loss: 0.938 | val loss: 1.022\nE24 | train loss: 0.917 | val loss: 1.042\nE25 | train loss: 0.930 | val loss: 1.062\nE26 | train loss: 0.917 | val loss: 1.089\nE27 | train loss: 0.913 | val loss: 1.286\nE28 | train loss: 0.932 | val loss: 1.041\nE29 | train loss: 0.890 | val loss: 1.126\nE30 | train loss: 0.903 | val loss: 1.038\nE31 | train loss: 0.918 | val loss: 1.033\nE32 | train loss: 0.914 | val loss: 1.048\nE33 | train loss: 0.913 | val loss: 1.060\nE34 | train loss: 0.900 | val loss: 1.236\nE35 | train loss: 0.890 | val loss: 1.030\nE36 | train loss: 0.901 | val loss: 1.066\nE37 | train loss: 0.895 | val loss: 1.055\nE38 | train loss: 0.914 | val loss: 1.030\nE39 | train loss: 0.894 | val loss: 1.024\nE40 | train loss: 0.903 | val loss: 1.056\nE41 | train loss: 0.897 | val loss: 1.089\nE42 | train loss: 0.917 | val loss: 1.031\nE43 | train loss: 0.904 | val loss: 1.105\nE44 | train loss: 0.894 | val loss: 1.284\nE45 | train loss: 0.903 | val loss: 1.648\nE46 | train loss: 0.904 | val loss: 1.042\nE47 | train loss: 0.909 | val loss: 1.052\nE48 | train loss: 0.891 | val loss: 1.186\nE49 | train loss: 0.891 | val loss: 1.075\n\n\n\ncnn_data_label = (cnn_train_losses,cnn_val_losses,\"CNN\")\nquick_loss_plot([lin_data_label,cnn_data_label])\n\n\n\n\n\n\n\n\nIt seems clear from the loss curves that the CNN is able to capture a pattern in the data that the Linear model is not! Let’s spot check a few sequences to see what’s going on.\n\n# oracle dict of true score for each seq\noracle = dict(mer8[['seq','score']].values)\n\ndef quick_seq_pred(model, desc, seqs, oracle):\n    '''\n    Given a model and some sequences, get the model's predictions\n    for those sequences and compare to the oracle (true) output\n    '''\n    print(f\"__{desc}__\")\n    for dna in seqs:\n        s = torch.tensor(one_hot_encode(dna)).unsqueeze(0).to(DEVICE)\n        pred = model(s.float())\n        actual = oracle[dna]\n        diff = pred.item() - actual\n        print(f\"{dna}: pred:{pred.item():.3f} actual:{actual:.3f} ({diff:.3f})\")\n\ndef quick_8mer_pred(model, oracle):\n    seqs1 = (\"poly-X seqs\",['AAAAAAAA', 'CCCCCCCC','GGGGGGGG','TTTTTTTT'])\n    seqs2 = (\"other seqs\", ['AACCAACA','CCGGTGAG','GGGTAAGG', 'TTTCGTTT'])\n    seqsTAT = (\"with TAT motif\", ['TATAAAAA','CCTATCCC','GTATGGGG','TTTATTTT'])\n    seqsGCG = (\"with GCG motif\", ['AAGCGAAA','CGCGCCCC','GGGCGGGG','TTGCGTTT'])\n    TATGCG =  (\"both TAT and GCG\",['ATATGCGA','TGCGTATT'])\n\n    for desc,seqs in [seqs1, seqs2, seqsTAT, seqsGCG, TATGCG]:\n        quick_seq_pred(model, desc, seqs, oracle)\n        print()\n\n\n# Ask the trained Linear model to make \n# predictions for some 8-mers\nquick_8mer_pred(model_lin, oracle)\n\n__poly-X seqs__\nAAAAAAAA: pred:23.415 actual:20.000 (3.415)\nCCCCCCCC: pred:13.762 actual:17.000 (-3.238)\nGGGGGGGG: pred:7.189 actual:14.000 (-6.811)\nTTTTTTTT: pred:17.841 actual:11.000 (6.841)\n\n__other seqs__\nAACCAACA: pred:18.987 actual:18.875 (0.112)\nCCGGTGAG: pred:12.330 actual:15.125 (-2.795)\nGGGTAAGG: pred:14.006 actual:15.125 (-1.119)\nTTTCGTTT: pred:14.945 actual:12.125 (2.820)\n\n__with TAT motif__\nTATAAAAA: pred:22.317 actual:27.750 (-5.433)\nCCTATCCC: pred:17.059 actual:25.875 (-8.816)\nGTATGGGG: pred:12.329 actual:24.000 (-11.671)\nTTTATTTT: pred:18.331 actual:22.125 (-3.794)\n\n__with GCG motif__\nAAGCGAAA: pred:16.972 actual:8.125 (8.847)\nCGCGCCCC: pred:12.467 actual:6.250 (6.217)\nGGGCGGGG: pred:8.121 actual:4.375 (3.746)\nTTGCGTTT: pred:13.014 actual:2.500 (10.514)\n\n__both TAT and GCG__\nATATGCGA: pred:15.874 actual:15.875 (-0.001)\nTGCGTATT: pred:14.779 actual:13.625 (1.154)\n\n\n\nFrom the above examples, it appears that the Linear model is really underpredicting sequences with a lot of G’s and overpredicting those with many T’s. This is probably because it noticed GCG made sequences have unusually low scores and TAT made sequences have unusually high scores, however since the Linear model doesn’t have a way to take into account the different context of GCG vs GAG, it just predicts that sequences with G’s should be lower. We know from our scoring scheme that this isn’t the case: it’s not that G’s in general are detrimental, but specifically GCG is.\n\n# Ask the trained CNN model to make \n# predictions for some 8-mers\nquick_8mer_pred(model_cnn, oracle)\n\n__poly-X seqs__\nAAAAAAAA: pred:19.649 actual:20.000 (-0.351)\nCCCCCCCC: pred:16.750 actual:17.000 (-0.250)\nGGGGGGGG: pred:13.576 actual:14.000 (-0.424)\nTTTTTTTT: pred:10.895 actual:11.000 (-0.105)\n\n__other seqs__\nAACCAACA: pred:18.645 actual:18.875 (-0.230)\nCCGGTGAG: pred:14.759 actual:15.125 (-0.366)\nGGGTAAGG: pred:15.118 actual:15.125 (-0.007)\nTTTCGTTT: pred:11.789 actual:12.125 (-0.336)\n\n__with TAT motif__\nTATAAAAA: pred:26.103 actual:27.750 (-1.647)\nCCTATCCC: pred:24.256 actual:25.875 (-1.619)\nGTATGGGG: pred:22.838 actual:24.000 (-1.162)\nTTTATTTT: pred:20.555 actual:22.125 (-1.570)\n\n__with GCG motif__\nAAGCGAAA: pred:9.007 actual:8.125 (0.882)\nCGCGCCCC: pred:7.091 actual:6.250 (0.841)\nGGGCGGGG: pred:5.275 actual:4.375 (0.900)\nTTGCGTTT: pred:3.411 actual:2.500 (0.911)\n\n__both TAT and GCG__\nATATGCGA: pred:15.389 actual:15.875 (-0.486)\nTGCGTATT: pred:13.185 actual:13.625 (-0.440)\n\n\n\nThe CNN however is better able to adapt to the differences between 3-mer motifs! It predicts quite well on both the sequences with and without motifs."
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-2",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-2",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "Question 2",
    "text": "Question 2\nCompare the performance of the Linear and CNN models by using different learning rates. First run both models with higher learning rates (0.05, 0.1) and lower learning rates (0.005, 0.001), then create loss plots showing: - Linear model with these learning rates - CNN model with these learning rates\nThen analyze your results by answering: 1. How does changing the learning rate affect convergence for each model? 2. Which model is more sensitive to learning rate changes, and why? 3. Based on your analysis, what learning rate would you recommend for each model type, and why?\n ## 6. Check model predictions on the test set An important evaluation step in machine learning tasks is to check if your model can make good predictions on the test set, which it never saw during training. Here, we can use a parity plot to visualize the difference between the actual sequence scores vs the model’s predicted scores.\n\n#%pip install altair ## datapane\n\n\nimport altair as alt\nfrom sklearn.metrics import r2_score\n## import datapane as dp ## compatibility issues with pandas version\nimport os\n\n\n\ndef parity_plot(model_name,df,r2):\n    '''\n    Given a dataframe of samples with their true and predicted values,\n    make a scatterplot.\n    '''\n    plt.scatter(df['truth'].values, df['pred'].values, alpha=0.2)\n    \n    # y=x line\n    xpoints = ypoints = plt.xlim()\n    plt.plot(xpoints, ypoints, linestyle='--', color='k', lw=2, scalex=False, scaley=False)\n\n    plt.ylim(xpoints)\n    plt.ylabel(\"Predicted Score\",fontsize=14)\n    plt.xlabel(\"Actual Score\",fontsize=14)\n    plt.title(f\"{model_name} (r2:{r2:.3f})\",fontsize=20)\n    plt.show()\n\n\ndef alt_parity_plot(model, df, r2, datapane=False):\n    '''\n    Make an interactive parity plot with altair\n    '''\n    import os\n    import altair as alt\n    \n    os.makedirs('alt_out', exist_ok=True)\n    \n    # Convert model name to string to avoid any issues\n    model = str(model)\n    \n    # Create a clean version of the dataframe\n    plot_df = pd.DataFrame({\n        'truth': df['truth'].astype(float),\n        'pred': df['pred'].astype(float),\n        'seq': df['seq'].astype(str)\n    })\n    \n    # Create chart\n    chart = alt.Chart(plot_df).mark_point().encode(\n        x=alt.X('truth', type='quantitative', title='True Values'),\n        y=alt.Y('pred', type='quantitative', title='Predictions'),\n        tooltip=['seq']\n    ).properties(\n        title=str(f'{model} (r2:{r2:.3f})')\n    )\n    \n    chart.save(f'alt_out/parity_plot_{model}.html')\n    display(chart)\n\n\ndef parity_pred(models, seqs, oracle,alt=False,datapane=False):\n    '''Given some sequences, get the model's predictions '''\n    dfs = {} # key: model name, value: parity_df\n    \n    \n    for model_name,model in models:\n        print(f\"Running {model_name}\")\n        data = []\n        for dna in seqs:\n            s = torch.tensor(one_hot_encode(dna)).unsqueeze(0).to(DEVICE)\n            actual = oracle[dna]\n            pred = model(s.float())\n            data.append([dna,actual,pred.item()])\n        df = pd.DataFrame(data, columns=['seq','truth','pred'])\n        r2 = r2_score(df['truth'],df['pred'])\n        dfs[model_name] = (r2,df)\n        \n        #plot parity plot\n        if alt: # make an altair plot\n            alt_parity_plot(model_name, df, r2,datapane=datapane)\n            \n        else:\n            parity_plot(model_name, df, r2)\n\n\nseqs = test_df['seq'].values\nmodels = [\n    (\"Linear\", model_lin),\n    (\"CNN\", model_cnn)\n]\nparity_pred(models, seqs, oracle)\n\nRunning Linear\n\n\n\n\n\n\n\n\n\nRunning CNN\n\n\n\n\n\n\n\n\n\nParity plots are useful for visualizing how well your model predicts individual sequences: in a perfect model, they would all land on the y=x line, meaning that the model prediction was exactly the sequence’s actual value. But if it is off the y=x line, it means the model is over- or under-predicting.\nIn the Linear model, we can see that it can somewhat predict a trend in the Test set sequences, but really gets confused by these buckets of sequences in the high and low areas of the distribution (the ones with a motif).\nHowever for the CNN, it is much better at predicting scores close to the actual value! This is expected, given that the architecture of our CNN uses 3-mer kernels to scan along the sequence for influential motifs.\nBut the CNN isn’t perfect. We could probably train it longer or adjust the hyperparameters, but the goal here isn’t perfection - this is a very simple task relative to actual regulatory grammars. Instead, I thought it would be interesting to use the Altair visualization library to interactively inspect which sequences the models get wrong:\n\nalt.data_transformers.disable_max_rows() # disable altair warning\nparity_pred(models, seqs, oracle,alt=True)\n\nRunning Linear\n\n\n\n\n\n\n\n\nRunning CNN\n\n\n\n\n\n\n\n\nIf you’re viewing this notebook in interactive mode and run the above cell (just viewing via the github preview will omit the altair plot in the rendering), you can hover over the points and see the individual 8-mer sequences (you can also pan and zoom in this plot).\nNotice that the sequences that are off the diagonal tend to have multiple instance of the motifs! In the scoring function, we only gave the sequence a +/- bump if it had at least 1 motif, but it certainly would have been reasonable to decide to add multiple bonuses if the motif was present multiple times. In this example, I arbitrarily only added the bonus for at least 1 motif occurrence, but we could have made a different scoring function.\nIn any case, I thought it was cool that the model noticed the multiple occurrences and predicted them to be important. I suppose we did fool it a little, though an R2 of 0.95 is pretty respectable :)\n ## 7. Visualize convolutional filters When training CNN models, it can be useful to visualize the first layer convolutional filters to try to understand more about what the model is learning. With image data, the first layer convolutional filters often learn patterns such as borders or colors or textures - basic image elements that can be recombined to make more complex features.\nIn DNA, convolutional filters can be thought of like motif scanners. Similar to a position weight matrix for visualizing sequence logos, a convolutional filter is like a matrix showing a particular DNA pattern, but instead of being an exact sequence, it can hold some uncertainty about which nucleotides show up in which part of the pattern. Some positions might be very certain (i.e., there’s always an A in position 2; high information content) while other positions could hold a variety of nucleotides with about equal probability (high entropy; low information content).\nThe calculations that occur within the hidden layers of neural networks can get very complex and not every convolutional filter will be an obviously relevant pattern, but sometimes patterns in the filters do emerge and can be informative for helping to explain the model’s predictions.\nBelow are some functions to visualize the first layer convolutional filters, both as a raw heatmap and as a motif logo."
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#section",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#section",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "===",
    "text": "===\nVisualizing Convolutional Filters When training convolutional neural networks (CNNs), it’s often helpful to visualize the filters in the first convolutional layer to better understand what features the model is learning from the input data.\nIn image data:\nThe first-layer filters commonly learn simple, low-level features like:\nEdges or borders Color gradients Textures or basic shapes\nThese simple elements can be combined in deeper layers to form more complex patterns, such as object parts or entire shapes.\n🧬 In DNA sequence data:\nConvolutional filters act similarly to motif detectors. They can learn biologically meaningful patterns in the sequence, much like:\n\nPosition Weight Matrices (PWMs)\nSequence logos\n\nEach filter can be thought of as a matrix scanning for a specific DNA pattern — not necessarily a fixed sequence, but one that may allow for some variation in certain positions.\nFor example:\nOne position in the filter might strongly prefer an “A” (indicating low entropy and high information content). Another position might tolerate any nucleotide (high entropy, low information content). These learned filters can sometimes correspond to known biological motifs, or reveal novel sequence patterns that are predictive for the task at hand (e.g., enhancer activity, binding sites, expression levels)."
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-3",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-3",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "Question 3",
    "text": "Question 3\nDesign an approach to improve the model’s prediction accuracy, particularly focusing on the sequences where the current model performs poorly:\n\nAfter identifying sequences where the CNN model has high prediction errors, propose and implement a modification to either the model architecture, the loss function, the training process, or the data representation\nRetrain the model with your modifications\nCreate comparative visualizations (such as parity plots, error histograms, or other appropriate plots) to demonstrate the impact of your changes\nAnalyze your results by discussing how your modification addresses the specific weaknesses you identified. What are the trade-offs involved in your approach?\n\n\nimport logomaker\n\n\ndef get_conv_layers_from_model(model):\n    '''\n    Given a trained model, extract its convolutional layers\n    '''\n    model_children = list(model.children())\n    \n    # counter to keep count of the conv layers\n    model_weights = [] # we will save the conv layer weights in this list\n    conv_layers = [] # we will save the actual conv layers in this list\n    bias_weights = []\n    counter = 0 \n\n    # append all the conv layers and their respective weights to the list\n    for i in range(len(model_children)):\n        # get model type of Conv1d\n        if type(model_children[i]) == nn.Conv1d:\n            counter += 1\n            model_weights.append(model_children[i].weight)\n            conv_layers.append(model_children[i])\n            bias_weights.append(model_children[i].bias)\n\n        # also check sequential objects' children for conv1d\n        elif type(model_children[i]) == nn.Sequential:\n            for child in model_children[i]:\n                if type(child) == nn.Conv1d:\n                    counter += 1\n                    model_weights.append(child.weight)\n                    conv_layers.append(child)\n                    bias_weights.append(child.bias)\n\n    print(f\"Total convolutional layers: {counter}\")\n    return conv_layers, model_weights, bias_weights\n\ndef view_filters(model_weights, num_cols=8):\n    model_weights = model_weights[0]\n    num_filt = model_weights.shape[0]\n    filt_width = model_weights[0].shape[1]\n    num_rows = int(np.ceil(num_filt/num_cols))\n    \n    # visualize the first conv layer filters\n    plt.figure(figsize=(20, 17))\n\n    for i, filter in enumerate(model_weights):\n        ax = plt.subplot(num_rows, num_cols, i+1)\n        ax.imshow(filter.cpu().detach(), cmap='gray')\n        ax.set_yticks(np.arange(4))\n        ax.set_yticklabels(['A', 'C', 'G','T'])\n        ax.set_xticks(np.arange(filt_width))\n        ax.set_title(f\"Filter {i}\")\n\n    plt.tight_layout()\n    plt.show()\n\nFirst, we can take a peek at the raw filters.\n\nconv_layers, model_weights, bias_weights = get_conv_layers_from_model(model_cnn)\nview_filters(model_weights)\n\nTotal convolutional layers: 1\n\n\n\n\n\n\n\n\n\n\n\ndef get_conv_output_for_seq(seq, conv_layer):\n    '''\n    Given an input sequeunce and a convolutional layer, \n    get the output tensor containing the conv filter \n    activations along each position in the sequence\n    '''\n    # format seq for input to conv layer (OHE, reshape)\n    seq = torch.tensor(one_hot_encode(seq)).unsqueeze(0).permute(0,2,1).to(DEVICE)\n\n    # run seq through conv layer\n    with torch.no_grad(): # don't want as part of gradient graph\n        # apply learned filters to input seq\n        res = conv_layer(seq.float())\n        return res[0]\n    \n\ndef get_filter_activations(seqs, conv_layer,act_thresh=0):\n    '''\n    Given a set of input sequences and a trained convolutional layer, \n    determine the subsequences for which each filter in the conv layer \n    activate most strongly. \n    \n    1.) Run seq inputs through conv layer. \n    2.) Loop through filter activations of the resulting tensor, saving the\n            position where filter activations were &gt; act_thresh. \n    3.) Compile a count matrix for each filter by accumulating subsequences which\n            activate the filter above the threshold act_thresh\n    '''\n    # initialize dict of pwms for each filter in the conv layer\n    # pwm shape: 4 nucleotides X filter width, initialize to 0.0s\n    num_filters = conv_layer.out_channels\n    filt_width = conv_layer.kernel_size[0]\n    filter_pwms = dict((i,torch.zeros(4,filt_width)) for i in range(num_filters))\n    \n    print(\"Num filters\", num_filters)\n    print(\"filt_width\", filt_width)\n    \n    # loop through a set of sequences and collect subseqs where each filter activated\n    for seq in seqs:\n        # get a tensor of each conv filter activation along the input seq\n        res = get_conv_output_for_seq(seq, conv_layer)\n\n        # for each filter and it's activation vector\n        for filt_id,act_vec in enumerate(res):\n            # collect the indices where the activation level \n            # was above the threshold\n            act_idxs = torch.where(act_vec&gt;act_thresh)[0]\n            activated_positions = [x.item() for x in act_idxs]\n\n            # use activated indicies to extract the actual DNA\n            # subsequences that caused filter to activate\n            for pos in activated_positions:\n                subseq = seq[pos:pos+filt_width]\n                #print(\"subseq\",pos, subseq)\n                # transpose OHE to match PWM orientation\n                subseq_tensor = torch.tensor(one_hot_encode(subseq)).T\n\n                # add this subseq to the pwm count for this filter\n                filter_pwms[filt_id] += subseq_tensor            \n            \n    return filter_pwms\n\ndef view_filters_and_logos(model_weights,filter_activations, num_cols=8):\n    '''\n    Given some convolutional model weights and filter activation PWMs, \n    visualize the heatmap and motif logo pairs in a simple grid\n    '''\n    model_weights = model_weights[0].squeeze(1)\n    print(model_weights.shape)\n\n    # make sure the model weights agree with the number of filters\n    assert(model_weights.shape[0] == len(filter_activations))\n    \n    num_filts = len(filter_activations)\n    num_rows = int(np.ceil(num_filts/num_cols))*2+1 \n    # ^ not sure why +1 is needed... complained otherwise\n    \n    plt.figure(figsize=(20, 17))\n\n    j=0 # use to make sure a filter and it's logo end up vertically paired\n    for i, filter in enumerate(model_weights):\n        if (i)%num_cols == 0:\n            j += num_cols\n\n        # display raw filter\n        ax1 = plt.subplot(num_rows, num_cols, i+j+1)\n        ax1.imshow(filter.cpu().detach(), cmap='gray')\n        ax1.set_yticks(np.arange(4))\n        ax1.set_yticklabels(['A', 'C', 'G','T'])\n        ax1.set_xticks(np.arange(model_weights.shape[2]))\n        ax1.set_title(f\"Filter {i}\")\n\n        # display sequence logo\n        ax2 = plt.subplot(num_rows, num_cols, i+j+1+num_cols)\n        filt_df = pd.DataFrame(filter_activations[i].T.numpy(),columns=['A','C','G','T'])\n        filt_df_info = logomaker.transform_matrix(filt_df,from_type='counts',to_type='information')\n        logo = logomaker.Logo(filt_df_info,ax=ax2)\n        ax2.set_ylim(0,2)\n        ax2.set_title(f\"Filter {i}\")\n\n    plt.tight_layout()\n\n\n# just use some seqs from test_df to activate filters\nsome_seqs = random.choices(seqs, k=3000)\n\nfilter_activations = get_filter_activations(some_seqs, conv_layers[0])\nview_filters_and_logos(model_weights,filter_activations)\n\nNum filters 32\nfilt_width 3\ntorch.Size([32, 4, 3])\n\n\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n\n\n\n\n\n\n\n\n\n\nVisualize filters using a stronger activation threshold\nact_thresh = 1 instead of 0. (Some filters have no subsequence matches above the threshold and result in an empty motif logo)\n\nfilter_activations = get_filter_activations(some_seqs, conv_layers[0],act_thresh=1)\nview_filters_and_logos(model_weights,filter_activations)\n\nNum filters 32\nfilt_width 3\ntorch.Size([32, 4, 3])\n\n\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n\n\n\n\n\n\n\n\n\nFrom this particular CNN training, we can see a few filters have picked up on the strong TAT and GCG motifs, but other filters have focused on other patterns as well. There is some debate about how relevant convolutional filter visualizations are for model interpretability. In deep models with multiple convolutional layers, convolutional filters can be recombined in more complex ways inside the hidden layers, so the first layer filters may not be as informative on their own (Koo and Eddy, 2019). Much of the field has since moved towards attention mechanisms and other explainability methods, but should you be curious to visualize your filters as potential motifs, these functions may help get you started!\n ## 8. Conclusion This tutorial shows some basic Pytorch structure for building CNN models that work with DNA sequences. The practice task used in this demo is not reflective of real biological signals; rather, we designed the scoring method to simulate the presence of regulatory motifs in very short sequences that were easy for us humans to inspect and verify that Pytorch was behaving as expected. From this small example, we observed how a basic CNN with sliding filters was able to predict our scoring scheme better than a basic linear model that only accounted for absolute nucleotide position (without local context).\nTo read more about CNN’s applied to DNA in the wild, check out the following foundational papers: * DeepBind: Alipanahi et al 2015 * DeepSea: Zhou and Troyanskaya 2015 * Basset: Kelley et al 2016\nI hope other new-to-ML folks interested in tackling biological questions may find this helpful for getting started with using Pytorch to model DNA sequences :)"
  }
]