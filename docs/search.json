[
  {
    "objectID": "post/2025-05-12-unit04/scGPT/scgpt-quickstart-jupyter.html",
    "href": "post/2025-05-12-unit04/scGPT/scgpt-quickstart-jupyter.html",
    "title": "scgpt quickstart - jupyter notebook",
    "section": "",
    "text": "https://virtualcellmodels.cziscience.com/quickstart/scgpt-quickstart\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-05-12-unit04/scGPT/scgpt-quickstart-jupyter.html#quick-start-scgpt",
    "href": "post/2025-05-12-unit04/scGPT/scgpt-quickstart-jupyter.html#quick-start-scgpt",
    "title": "scgpt quickstart - jupyter notebook",
    "section": "Quick Start: scGPT",
    "text": "Quick Start: scGPT\nThis quick start will guide you through using the scGPT model, trained on 33 million cells (including data from the CZ CELLxGENE Census), to generate embeddings for single-cell transcriptomic data analysis. Learning Goals\nBy the end of this tutorial, you will understand how to:\nAccess and prepare the scGPT model for use.\nGenerate embeddings to analyze and compare your dataset against the CZ CELLxGENE Census.\nVisualize the results using a UMAP, colored by cell type.\n\nPre-requisites and Requirements\nBefore starting, ensure you are familiar with:\nPython and AnnData\nSingle-cell data analysis (see this tutorial for a primer on the subject) You can run this tutorial locally (tested on an M3 MacBook with 32 GiB memory) or in Google Colab using a T4 instance. Environment setup will be covered in a later section.\n\n\nOverview\nThis notebook provides a step-by-step guide to:\nSetting up your environment\nDownloading the necessary model checkpoints and h5ad dataset\nPerforming model inference to create embeddings\nVisualizing the results with UMAP\n\n\nSetup\nLet‚Äôs start by setting up dependencies. The released version of scGPT requires PyTorch 2.1.2, so we will remove the existing PyTorch installation and replace it with the required one. If you want to run this on another environment, this step might not be necessary.\n** but then I got error about torch version. So I installed torch 2.3.0 and torchvision 0.16.2. and then reinstalled torch 2.1.2.**\n```{bash}\nconda create -n scgpt python=3.9\nconda activate scgpt\n```\n\nfirst_time = False\nif first_time:\n    # First uninstall the conflicting packages\n    %pip uninstall -y -q torch torchvision\n    %pip uninstall -y numpy pandas scipy scikit-learn anndata cell-gears datasets dcor\n    #%pip install -q torchvision==0.16.2 torch==2.1.2\n    %pip install -q torch==2.3.0 torchvision==0.16.2\n    %pip install -q scgpt scanpy gdown\n\n\n    # Then install them in the correct order with specific versions\n    %pip install numpy==1.23.5\n    %pip install pandas==1.5.3  # This version is compatible with anndata 0.10.9\n    %pip install scipy==1.10.1  # This version is &gt;1.8 as required by anndata\n    %pip install scikit-learn==1.2.2\n    %pip install anndata==0.10.9\n    %pip install cell-gears==0.0.2\n    %pip install dcor==0.6\n\n    %pip install datasets==2.3.0\n\n\n    # First uninstall both packages\n    %pip uninstall -y torch torchtext\n\n    # Then install compatible versions\n    %pip install torch==2.1.2 torchtext==0.16.2\n\nWe can install the rest of our dependencies and import the relevant libraries.\n\n# Import libraries\n\n# Import required packages\nimport os\nimport multiprocessing\n\n# Set MPS fallback for unimplemented operations\nos.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n\n# Monkey-patch os.sched_getaffinity for macOS\nif not hasattr(os, 'sched_getaffinity'):\n    def sched_getaffinity(pid):\n        return set(range(multiprocessing.cpu_count()))\n    os.sched_getaffinity = sched_getaffinity\n\nimport warnings\nimport urllib.request\nfrom pathlib import Path\n\nimport scgpt as scg\nimport scanpy as sc\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Check for MPS availability\ndevice = (\n    torch.device(\"mps\")\n    if torch.backends.mps.is_available()\n    else torch.device(\"cpu\")\n)\nprint(f\"Using device: {device}\")\nprint(\"Note: Some operations may fall back to CPU due to MPS limitations\")\n\nwarnings.filterwarnings(\"ignore\")\n\n/Users/haekyungim/miniconda3/envs/scgpt/lib/python3.9/site-packages/scgpt/model/model.py:21: UserWarning: flash_attn is not installed\n  warnings.warn(\"flash_attn is not installed\")\n/Users/haekyungim/miniconda3/envs/scgpt/lib/python3.9/site-packages/scgpt/model/multiomic_model.py:19: UserWarning: flash_attn is not installed\n  warnings.warn(\"flash_attn is not installed\")\n/Users/haekyungim/miniconda3/envs/scgpt/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nUsing device: mps\nNote: Some operations may fall back to CPU due to MPS limitations\n\n\n\n# Define the base working directory\nWORKDIR = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100/scgpt\"\n# Convert to Path objects for better path handling\nWORKDIR = Path(WORKDIR)\nDATA_DIR = WORKDIR / \"data\"\nMODEL_DIR = WORKDIR / \"model\" \n\nDownload Model Checkpoints and Data\nLet‚Äôs download the checkpoints from the scGPT repository.\n\nwarnings.simplefilter(\"ignore\", ResourceWarning)\nwarnings.filterwarnings(\"ignore\", category=ImportWarning)\n\n# Use gdown with the recursive flag to download the folder\n# Replace the folder ID with the ID of your folder\nfolder_id = '1oWh_-ZRdhtoGQ2Fw24HP41FgLoomVo-y'\n\n# Check if model files already exist\nif not (MODEL_DIR / \"args.json\").exists():\n    print(\"Downloading model checkpoint...\")\n    !gdown --folder {folder_id} -O {MODEL_DIR}\nelse:\n    print(\"Model files already exist in\", MODEL_DIR)\n\nModel files already exist in /Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100/scgpt/model\n\n\nWe will now download an H5AD dataset from CZ CELLxGENE. To reduce memory utilization, we will also perform a reduction to the top 3000 highly variable genes using scanpy‚Äôs highly_variable_genes function.\n\nuri = \"https://datasets.cellxgene.cziscience.com/f50deffa-43ae-4f12-85ed-33e45040a1fa.h5ad\"\nsource_path = DATA_DIR / \"source.h5ad\"\n\n# Check if file exists before downloading\nif not source_path.exists():\n    print(f\"Downloading dataset to {source_path}...\")\n    urllib.request.urlretrieve(uri, filename=str(source_path))\nelse:\n    print(f\"Dataset already exists at {source_path}\")\n\n# Read the data\nadata = sc.read_h5ad(source_path)\n\nbatch_key = \"sample\"\nN_HVG = 3000\n\nsc.pp.highly_variable_genes(adata, n_top_genes=N_HVG, flavor='seurat_v3')\nadata_hvg = adata[:, adata.var['highly_variable']]\n\nDataset already exists at /Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100/scgpt/data/source.h5ad\n\n\nWe can now use embed_data to generate the embeddings. Note that gene_col needs to point to the column where the gene names (not symbols!) are defined. For CZ CELLxGENE datasets, they are stored in the feature_name column.\n\n# Monkey patch get_batch_cell_embeddings to force single processor\nimport types\nfrom scgpt.tasks.cell_emb import get_batch_cell_embeddings as original_get_batch_cell_embeddings\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, SequentialSampler\nfrom scgpt.data_collator import DataCollator\nimport numpy as np\nfrom tqdm import tqdm\n\n# Define Dataset class at module level\nclass CellEmbeddingDataset(Dataset):\n    def __init__(self, count_matrix, gene_ids, batch_ids=None, vocab=None, model_configs=None):\n        self.count_matrix = count_matrix\n        self.gene_ids = gene_ids\n        self.batch_ids = batch_ids\n        self.vocab = vocab\n        self.model_configs = model_configs\n\n    def __len__(self):\n        return len(self.count_matrix)\n\n    def __getitem__(self, idx):\n        row = self.count_matrix[idx]\n        nonzero_idx = np.nonzero(row)[0]\n        values = row[nonzero_idx]\n        genes = self.gene_ids[nonzero_idx]\n        # append &lt;cls&gt; token at the beginning\n        genes = np.insert(genes, 0, self.vocab[\"&lt;cls&gt;\"])\n        values = np.insert(values, 0, self.model_configs[\"pad_value\"])\n        genes = torch.from_numpy(genes).long()\n        values = torch.from_numpy(values).float()\n        output = {\n            \"id\": idx,\n            \"genes\": genes,\n            \"expressions\": values,\n        }\n        if self.batch_ids is not None:\n            output[\"batch_labels\"] = self.batch_ids[idx]\n        return output\n\ndef patched_get_batch_cell_embeddings(\n    adata,\n    cell_embedding_mode: str = \"cls\",\n    model=None,\n    vocab=None,\n    max_length=1200,\n    batch_size=64,\n    model_configs=None,\n    gene_ids=None,\n    use_batch_labels=False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Patched version of get_batch_cell_embeddings that uses the module-level Dataset class\n    and forces num_workers=0.\n    \"\"\"\n    count_matrix = adata.X\n    count_matrix = (\n        count_matrix if isinstance(count_matrix, np.ndarray) else count_matrix.toarray()\n    )\n\n    # gene vocabulary ids\n    if gene_ids is None:\n        gene_ids = np.array(adata.var[\"id_in_vocab\"])\n        assert np.all(gene_ids &gt;= 0)\n\n    if use_batch_labels:\n        batch_ids = np.array(adata.obs[\"batch_id\"].tolist())\n\n    if cell_embedding_mode == \"cls\":\n        dataset = CellEmbeddingDataset(\n            count_matrix, \n            gene_ids, \n            batch_ids if use_batch_labels else None,\n            vocab=vocab,\n            model_configs=model_configs\n        )\n        collator = DataCollator(\n            do_padding=True,\n            pad_token_id=vocab[model_configs[\"pad_token\"]],\n            pad_value=model_configs[\"pad_value\"],\n            do_mlm=False,\n            do_binning=True,\n            max_length=max_length,\n            sampling=True,\n            keep_first_n_tokens=1,\n        )\n        data_loader = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            sampler=SequentialSampler(dataset),\n            collate_fn=collator,\n            drop_last=False,\n            num_workers=0,  # Force single worker\n            pin_memory=True,\n        )\n\n        # Use the global device variable instead of getting it from model\n        cell_embeddings = np.zeros(\n            (len(dataset), model_configs[\"embsize\"]), dtype=np.float32\n        )\n        with torch.no_grad():\n            # Disable autocast for MPS as it's not supported\n            count = 0\n            for data_dict in tqdm(data_loader, desc=\"Embedding cells\"):\n                input_gene_ids = data_dict[\"gene\"].to(device)\n                src_key_padding_mask = input_gene_ids.eq(\n                    vocab[model_configs[\"pad_token\"]]\n                )\n                embeddings = model._encode(\n                    input_gene_ids,\n                    data_dict[\"expr\"].to(device),\n                    src_key_padding_mask=src_key_padding_mask,\n                    batch_labels=data_dict[\"batch_labels\"].to(device)\n                    if use_batch_labels\n                    else None,\n                )\n\n                embeddings = embeddings[:, 0, :]  # get the &lt;cls&gt; position embedding\n                embeddings = embeddings.cpu().numpy()\n                cell_embeddings[count : count + len(embeddings)] = embeddings\n                count += len(embeddings)\n        cell_embeddings = cell_embeddings / np.linalg.norm(\n            cell_embeddings, axis=1, keepdims=True\n        )\n    else:\n        raise ValueError(f\"Unknown cell embedding mode: {cell_embedding_mode}\")\n    return cell_embeddings\n\n# Replace the original function with our patched version\nimport scgpt.tasks.cell_emb\nscgpt.tasks.cell_emb.get_batch_cell_embeddings = patched_get_batch_cell_embeddings\n\nos.environ['PYTHONWARNINGS'] = 'ignore'\n\n\nmodel_dir = MODEL_DIR #/ \"scGPT_human\"\ngene_col = \"feature_name\"\ncell_type_key = \"cell_type\"\n\nembedding_file = DATA_DIR / \"ref_embed_adata.h5ad\"\n\nif embedding_file.exists():\n    print(f\"Loading existing embeddings from {embedding_file}\")\n    ref_embed_adata = sc.read_h5ad(str(embedding_file))\nelse:\n    print(\"Computing new embeddings...\")\n    ref_embed_adata = scg.tasks.embed_data(\n        adata_hvg,\n        model_dir,\n        gene_col=gene_col,\n        obs_to_save=cell_type_key,\n        batch_size=64,\n        return_new_adata=True,\n        device=device,  # Pass the device to embed_data\n    )\n    print(f\"Saving embeddings to {embedding_file}\")\n    ref_embed_adata.write(str(embedding_file))\n\nLoading existing embeddings from /Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100/scgpt/data/ref_embed_adata.h5ad\n\n\nOur scGPT embeddings are stored in the .X attribute of the returned AnnData object and have a dimensionality of 512.\n\nref_embed_adata.X.shape\n\n(11103, 512)\n\n\nWe can now calculate neighbors based on scGPT embeddings.\n\nsc.pp.neighbors(ref_embed_adata, use_rep=\"X\")\nsc.tl.umap(ref_embed_adata)\n\nWe will put our calculated UMAP and embeddings in our original adata object with our original annotations.\n\nadata.obsm[\"X_scgpt\"] = ref_embed_adata.X\nadata.obsm[\"X_umap\"] = ref_embed_adata.obsm[\"X_umap\"]\n\nWe can also switch our .var index which is currently set to Ensembl ID‚Äôs, to be gene symbols, allowing us to plot gene expression more easily.\n\n# Add the current index ('ensembl_id') as a new column\nadata.var['ensembl_id'] = adata.var.index\n\n# Set the new index to the 'feature_name' column\nadata.var.set_index('feature_name', inplace=True)\n\n\n# Add a copy of the gene symbols back to the var dataframe\nadata.var['gene_symbol'] = adata.var.index\n\nWe can now plot a UMAP, coloring it by cell type to visualize our embeddings. Below, we color by both the standard cell type labels provided by CZ CELLxGENE and the original cell type annotations from the authors. The embeddings generated by scGPT effectively capture the structure of the data, closely aligning with the original author annotations.\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\")\n    #sc.pp.neighbors(ref_embed_adata, use_rep=\"X\")\n    #sc.tl.umap(ref_embed_adata)\n    sc.pl.umap(adata, color=[\"cell_type\", \"annotation_res0.34_new2\"], wspace = 0.6)\n\n\n\n\n\n\n\n\nWe can also take a look at some markers of the major cell types represented in the dataset.\n\nsc.pl.umap(adata, color=['cell_type', 'MKI67', 'LYZ', 'RBP2', 'MUC2', 'CHGA', 'TAGLN', 'ELAVL3'], frameon=False, use_raw=False, legend_fontsize =\"xx-small\", legend_loc=\"none\")\n\n\n\n\n\n\n\n\nReferences\nPlease refer to the following papers for information about:\nscGPT: Toward building a foundation model for single-cell multi-omics using generative AI\nCui, H., Wang, C., Maan, H. et al.¬†scGPT: toward building a foundation model for single-cell multi-omics using generative AI. Nat Methods 21, 1470‚Äì1480 (2024). https://doi.org/10.1038/s41592-024-02201-0\nThe dataset used in this tutorial\nMoerkens, R., Mooiweer, J., Ram√≠rez-S√°nchez, A. D., Oelen, R., Franke, L., Wijmenga, C., Barrett, R. J., Jonkers, I. H., & Withoff, S. (2024). An iPSC-derived small intestine-on-chip with self-organizing epithelial, mesenchymal, and neural cells. Cell Reports, 43(7). https://doi.org/10.1016/j.celrep.2024.114247\nCZ CELLxGENE Discover and Census\nCZ CELLxGENE Discover: A single-cell data platform for scalable exploration, analysis and modeling of aggregated data CZI Single-Cell Biology, et al.¬†bioRxiv 2023.10.30; doi: https://doi.org/10.1101/2023.10.30.563174"
  },
  {
    "objectID": "post/2025-05-01-unit03/borzoi_personal_gen_prediction-jupyter.html",
    "href": "post/2025-05-01-unit03/borzoi_personal_gen_prediction-jupyter.html",
    "title": "Borzoi prediction from personal genome - jupyter notebook",
    "section": "",
    "text": "Recommended: create a new conda environment, borzoi is not compatible with python &gt; 3.10\nconda create ‚Äìname borzoi46100 python=3.10\nThen, use borzoi46100 environment to run the notebook\nInstall software repositories\n\n\nShow the code\nimport os\nGITHUB_DIR = '/Users/haekyungim/Github'\n\n# clone baskerville and borzoi if not already cloned\nbaskerville_path = os.path.join(GITHUB_DIR, 'baskerville')\nborzoi_path = os.path.join(GITHUB_DIR, 'borzoi')\nif not os.path.exists(baskerville_path):\n    !git clone https://github.com/calico/baskerville.git {GITHUB_DIR}/baskerville\n\nif not os.path.exists(borzoi_path):\n    !git clone https://github.com/calico/borzoi.git {GITHUB_DIR}/borzoi\n\n\nAfter loading baskerville, restart runtime, run code from here\nInstall libraries\n\n\nShow the code\ntry:\n    import baskerville\nexcept ImportError:\n    !pip install -e {GITHUB_DIR}/baskerville\n\n\n\n\nShow the code\ntry:\n    import borzoi\nexcept ImportError:\n    !pip install -e {GITHUB_DIR}/borzoi\n\n\nNOTE: install tensorflow-metal if running on a mac\n\n\nShow the code\n# import platform\n# if platform.processor() == 'arm':\n#     print(\"Apple Silicon detected, installing tensorflow-metal...\")\n#     %pip install tensorflow-metal\n# else:\n#     print(\"Not running on Apple Silicon, skipping tensorflow-metal installation\")\n\n\nApple Silicon detected, installing tensorflow-metal...\nCollecting tensorflow-metal\n  Downloading tensorflow_metal-1.2.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (1.3 kB)\nRequirement already satisfied: wheel~=0.35 in /Users/haekyungim/miniconda3/envs/borzoi46100/lib/python3.10/site-packages (from tensorflow-metal) (0.45.1)\nRequirement already satisfied: six&gt;=1.15.0 in /Users/haekyungim/miniconda3/envs/borzoi46100/lib/python3.10/site-packages (from tensorflow-metal) (1.17.0)\nDownloading tensorflow_metal-1.2.0-cp310-cp310-macosx_12_0_arm64.whl (1.4 MB)\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.4/1.4 MB 23.8 MB/s eta 0:00:00\nInstalling collected packages: tensorflow-metal\nSuccessfully installed tensorflow-metal-1.2.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\n\n\nShow the code\ntry:\n    import kipoiseq\nexcept ImportError:\n    %pip install kipoiseq\nfrom kipoiseq import Interval\ntry:\n    import cyvcf2\nexcept ImportError:\n    %pip install cyvcf2\nimport os\nimport time\nimport io\nimport gzip\n\n#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n\nimport h5py\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport baskerville\nfrom baskerville import seqnn\nfrom baskerville import gene as bgene\nfrom baskerville import dna\n\nimport json\n\nimport pysam\n\nimport pyfaidx\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n\n\n\n\n\n\nShow the code\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\ngpu_device = tf.config.list_physical_devices('GPU')[0]\ntf.config.experimental.set_memory_growth(gpu_device, True)\n\n\nNum GPUs Available:  1\n\n\n\n\n\n\n\nShow the code\nPRE = '/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100'\nCONTENT_DIR = PRE + '/borzoi'\n# Create borzoi/data directory if it doesn't exist\nif not os.path.exists(os.path.join(CONTENT_DIR)):\n    !mkdir {CONTENT_DIR}\nif not os.path.exists(os.path.join(CONTENT_DIR, '/data')):\n    !mkdir {CONTENT_DIR}/data\n\n# Create model file structure\nsaved_models_path = os.path.join(CONTENT_DIR, 'data/saved_models')\nif not os.path.exists(saved_models_path):\n    !mkdir {saved_models_path}\nif not os.path.exists(os.path.join(saved_models_path, 'f0')):\n    !mkdir {saved_models_path}/f0\nif not os.path.exists(os.path.join(saved_models_path, 'f1')):\n    !mkdir {saved_models_path}/f1\nif not os.path.exists(os.path.join(saved_models_path, 'f2')):\n    !mkdir {saved_models_path}/f2\nif not os.path.exists(os.path.join(saved_models_path, 'f3')):\n    !mkdir {saved_models_path}/f3\n    \n#%cd {CONTENT_DIR}/data\n\n\nmkdir: /Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100/borzoi/data: File exists\n/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100/borzoi/data\n\n\n\n\nShow the code\n#DELET\nos.path.join(CONTENT_DIR, 'data/hg38.fa')\n\n\n'/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100/borzoi/data/hg38.fa'\n\n\n\n\nShow the code\n#Download model weights\nif not os.path.exists(os.path.join(saved_models_path, 'f0', 'model0_best.h5')):\n    !wget https://storage.googleapis.com/seqnn-share/borzoi/f0/model0_best.h5 -O {saved_models_path}/f0/model0_best.h5\nif not os.path.exists(os.path.join(saved_models_path, 'f1', 'model0_best.h5')): \n    !wget https://storage.googleapis.com/seqnn-share/borzoi/f1/model0_best.h5 -O {saved_models_path}/f1/model0_best.h5\nif not os.path.exists(os.path.join(saved_models_path, 'f2', 'model0_best.h5')):\n    !wget https://storage.googleapis.com/seqnn-share/borzoi/f2/model0_best.h5 -O {saved_models_path}/f2/model0_best.h5\nif not os.path.exists(os.path.join(saved_models_path, 'f3', 'model0_best.h5')):\n    !wget https://storage.googleapis.com/seqnn-share/borzoi/f3/model0_best.h5 -O {saved_models_path}/f3/model0_best.h5\n\n\n\n\nShow the code\n#Download and uncompress annotation files\nif not os.path.exists(os.path.join(CONTENT_DIR, 'data', 'gencode41_basic_nort.gtf')):\n    !wget  -O - https://storage.googleapis.com/seqnn-share/helper/gencode41_basic_nort.gtf.gz | gunzip -c &gt; {os.path.join(CONTENT_DIR, 'data', 'gencode41_basic_nort.gtf')}\nif not os.path.exists(os.path.join(CONTENT_DIR, 'data', 'gencode41_basic_protein_splice.csv.gz')):\n    !wget https://storage.googleapis.com/seqnn-share/helper/gencode41_basic_protein_splice.csv.gz -O {os.path.join(CONTENT_DIR, 'data', 'gencode41_basic_protein_splice.csv.gz')}\nif not os.path.exists(os.path.join(CONTENT_DIR, 'data', 'polyadb_human_v3.csv.gz')):\n    !wget https://storage.googleapis.com/seqnn-share/helper/polyadb_human_v3.csv.gz -O {os. path.join(CONTENT_DIR, 'data', 'polyadb_human_v3.csv.gz')}\n\n\n\n\n\n\n\nShow the code\n#fasta_file = '/content/data/genome.fa'\nfasta_file = os.path.join(CONTENT_DIR, 'data/hg38.fa')\nif not os.path.exists(fasta_file):\n    !wget -O - http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz | gunzip -c &gt; {fasta_file}\n\n\n\n\nShow the code\n### download vcf file and index for chromosome 22\nVCF_FILE = os.path.join(CONTENT_DIR, 'data/ALL.chr22.shapeit2_integrated_SNPs_v2a_27022019.GRCh38.phased.gz')\nif not os.path.exists(VCF_FILE):\n    !wget https://uchicago.box.com/shared/static/u526pjh29w93ufn65yomlch4z8lkb1sp.gz -O {VCF_FILE}\n\n\n--2025-05-04 23:18:01--  https://uchicago.box.com/shared/static/u526pjh29w93ufn65yomlch4z8lkb1sp.gz\nResolving uchicago.box.com (uchicago.box.com)... 74.112.186.157\nConnecting to uchicago.box.com (uchicago.box.com)|74.112.186.157|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /public/static/u526pjh29w93ufn65yomlch4z8lkb1sp.gz [following]\n--2025-05-04 23:18:05--  https://uchicago.box.com/public/static/u526pjh29w93ufn65yomlch4z8lkb1sp.gz\nReusing existing connection to uchicago.box.com:443.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://uchicago.app.box.com/public/static/u526pjh29w93ufn65yomlch4z8lkb1sp.gz [following]\n--2025-05-04 23:18:06--  https://uchicago.app.box.com/public/static/u526pjh29w93ufn65yomlch4z8lkb1sp.gz\nResolving uchicago.app.box.com (uchicago.app.box.com)... 74.112.186.157\nConnecting to uchicago.app.box.com (uchicago.app.box.com)|74.112.186.157|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://public.boxcloud.com/d/1/b1!RHvdIthE36FYyn0QLtnT0Bd4TIsiBQIoBEWvZGN7fVlSkjJkj5zRGnxAgcyWT75KdCvMqjeykC5OtsuTJP9LtQazK-z4ZyX6W9K4IWYfpcPQ67INeF0g8ST99TB4sR-fsAZxsGiDA80d3RwZom5lXnpC8__U5y9y5MkwgaaUV14pLDVPP7tNkFtPppWdkrcIo_78YFKZ5hB2J4ZqdH1_vZsfAVD7HeXu69e8rmSIeiSrGI2RlvPRH6ArsbuJSmMQ6GTjHLCgeAW2zq-EwzqvWYVvyra6-BfS1WGFZ02PiLIsSP4mxL9l7BR95kJqzk9zTmejGYBVxFGX1Q2fjpbQtKBBRxx0wjsnUwlQ1dzV3I251e5PSRgsZt-Ln03YYtOQZCiCLa7tPHt1Lj-5Zq_RS-8uPqaEaIaVxO3BL9H1XLOCvo16rQF5TWJ5cP5uv49l6z1llm37VYm-E_9wKg3r-VHqGZqJDcoB-EObe-UqMgSn0dQJyzsMmeI4eYhx8crMaGYFP18HGeA403GYC_VAqZ3T7dJnxyxAOCnL_3tvzJO9n5zNb6cDCuUSO6jvCKYuQ62fDgqKdtGK4Gc3BTfLZt6ikXKBTdp3Iqv9fxV8c3ecSV92UWyM95YiFftzYDzYKK8K4Qg68JhPE8BGDQCaElazdGWUHSv6aG9FofMB-YInY2YexGCeYWA5VRFkaUSEFv19TtqLjUB_zksXQQdfhnm9D9Ri2aV1iZSw7EhcI8SX7BbHxWp5-4KaVdQ4Da2_iq-mp4dzQUOCiZBULNIcAov1Upu3pSOGmSqTDuuAGm2a9yakG-D__hiSoPL2W-3gKX2XOEqqAypS10cVq0iE7WfDro0xf3el1UfN9DnSBx_i1lkspAF1gwWoMBbt-PCjTQn1QrC4brBNsHXXgs-4YPiJhtxitocc3rmQ7QiBpwufdihSnEb-gxxQwP_P2ixrVpcBn4GEfErjkpOYrjN2Kvuedu_raeWS2c8Ek2sQkJpM9A0aiwxblg-ZRjFszKntmL9auguHq5l8QKVCcvz5Okr49dpd1dxTZsK4HCC0DuclTebjWKUmE3d5jZDk5zM6aJ2NcADklr5H-cuZSi4fdbcCHeufIE92k_6bahN9WK0MQrcv4yTlKsGkaveWgxDpu4tNqnFTe_ckgr6OWzppsITVOiBMO5SilHeCRNfaBdSXIPZ4R76ATrIySz51YLGCsQpkeB-qXEgAMXDIQaKHkN0_wpqsQlJlsXOchvJ3Zs_g8ZtooEM14-sYhP7jwDYK30_Gq5PMAhYBTT33XUh_NOJiHOLFU-3jgi7thJ_AeMBWfgedpXC0coX7isbLsqhPOXewyWmQxKFxauYz7jv6GmomfUgsmAcAuMKmKcbUZX10DfWgvlH3LYGgfdLsXuDY8j3B2On0Ya_mMfkLZVgR8enXtrvQzCjV1SKsh85oVPUJF_GVojV_MtEgIHINRfK_DuqhO6LfMLjSzwCKfdN0C8vBYluHH_bIgTvVm9SkasNQCb0hpnRdrgpNXDp0M49qHok1UCFmbT1xHWUk7_ZSWPerEpUQQ9dRM8ngDU6pq6FgLaKiM8ksIIF4VZMg/download [following]\n--2025-05-04 23:18:10--  https://public.boxcloud.com/d/1/b1!RHvdIthE36FYyn0QLtnT0Bd4TIsiBQIoBEWvZGN7fVlSkjJkj5zRGnxAgcyWT75KdCvMqjeykC5OtsuTJP9LtQazK-z4ZyX6W9K4IWYfpcPQ67INeF0g8ST99TB4sR-fsAZxsGiDA80d3RwZom5lXnpC8__U5y9y5MkwgaaUV14pLDVPP7tNkFtPppWdkrcIo_78YFKZ5hB2J4ZqdH1_vZsfAVD7HeXu69e8rmSIeiSrGI2RlvPRH6ArsbuJSmMQ6GTjHLCgeAW2zq-EwzqvWYVvyra6-BfS1WGFZ02PiLIsSP4mxL9l7BR95kJqzk9zTmejGYBVxFGX1Q2fjpbQtKBBRxx0wjsnUwlQ1dzV3I251e5PSRgsZt-Ln03YYtOQZCiCLa7tPHt1Lj-5Zq_RS-8uPqaEaIaVxO3BL9H1XLOCvo16rQF5TWJ5cP5uv49l6z1llm37VYm-E_9wKg3r-VHqGZqJDcoB-EObe-UqMgSn0dQJyzsMmeI4eYhx8crMaGYFP18HGeA403GYC_VAqZ3T7dJnxyxAOCnL_3tvzJO9n5zNb6cDCuUSO6jvCKYuQ62fDgqKdtGK4Gc3BTfLZt6ikXKBTdp3Iqv9fxV8c3ecSV92UWyM95YiFftzYDzYKK8K4Qg68JhPE8BGDQCaElazdGWUHSv6aG9FofMB-YInY2YexGCeYWA5VRFkaUSEFv19TtqLjUB_zksXQQdfhnm9D9Ri2aV1iZSw7EhcI8SX7BbHxWp5-4KaVdQ4Da2_iq-mp4dzQUOCiZBULNIcAov1Upu3pSOGmSqTDuuAGm2a9yakG-D__hiSoPL2W-3gKX2XOEqqAypS10cVq0iE7WfDro0xf3el1UfN9DnSBx_i1lkspAF1gwWoMBbt-PCjTQn1QrC4brBNsHXXgs-4YPiJhtxitocc3rmQ7QiBpwufdihSnEb-gxxQwP_P2ixrVpcBn4GEfErjkpOYrjN2Kvuedu_raeWS2c8Ek2sQkJpM9A0aiwxblg-ZRjFszKntmL9auguHq5l8QKVCcvz5Okr49dpd1dxTZsK4HCC0DuclTebjWKUmE3d5jZDk5zM6aJ2NcADklr5H-cuZSi4fdbcCHeufIE92k_6bahN9WK0MQrcv4yTlKsGkaveWgxDpu4tNqnFTe_ckgr6OWzppsITVOiBMO5SilHeCRNfaBdSXIPZ4R76ATrIySz51YLGCsQpkeB-qXEgAMXDIQaKHkN0_wpqsQlJlsXOchvJ3Zs_g8ZtooEM14-sYhP7jwDYK30_Gq5PMAhYBTT33XUh_NOJiHOLFU-3jgi7thJ_AeMBWfgedpXC0coX7isbLsqhPOXewyWmQxKFxauYz7jv6GmomfUgsmAcAuMKmKcbUZX10DfWgvlH3LYGgfdLsXuDY8j3B2On0Ya_mMfkLZVgR8enXtrvQzCjV1SKsh85oVPUJF_GVojV_MtEgIHINRfK_DuqhO6LfMLjSzwCKfdN0C8vBYluHH_bIgTvVm9SkasNQCb0hpnRdrgpNXDp0M49qHok1UCFmbT1xHWUk7_ZSWPerEpUQQ9dRM8ngDU6pq6FgLaKiM8ksIIF4VZMg/download\nResolving public.boxcloud.com (public.boxcloud.com)... 74.112.186.164\nConnecting to public.boxcloud.com (public.boxcloud.com)|74.112.186.164|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 374610665 (357M) [application/octet-stream]\nSaving to: ‚Äò/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100/borzoi/data/ALL.chr22.shapeit2_integrated_SNPs_v2a_27022019.GRCh38.phased.gz‚Äô\n\n/Users/haekyungim/L 100%[===================&gt;] 357.26M  97.6MB/s    in 4.0s    \n\n2025-05-04 23:18:19 (90.1 MB/s) - ‚Äò/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100/borzoi/data/ALL.chr22.shapeit2_integrated_SNPs_v2a_27022019.GRCh38.phased.gz‚Äô saved [374610665/374610665]\n\n\n\n\n\nShow the code\n#!ls /content/data\n!ls {CONTENT_DIR}/data\n\n\nALL.chr22.shapeit2_integrated_SNPs_v2a_27022019.GRCh38.phased.gz\ngencode41_basic_nort.gtf\ngencode41_basic_nort.gtf.gz\ngencode41_basic_protein_splice.csv.gz\nhg38.fa\npolyadb_human_v3.csv.gz\nsaved_models\n\n\n\n\nShow the code\n%cd {CONTENT_DIR}\n\n\n/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100/borzoi\n\n\n\n\n\n\nShow the code\n!wget https://github.com/samtools/htslib/releases/download/1.9/htslib-1.9.tar.bz2\n!tar -vxjf htslib-1.9.tar.bz2\n%cd htslib-1.9\n!autoreconf -i\n!./configure\n!make\n!make install\n\n\n\n\n\nShow the code\n## if you Permission denied error on a mac, try\n!brew install htslib\n\n\n==&gt; Auto-updating Homebrew...\nAdjust how often this is run with HOMEBREW_AUTO_UPDATE_SECS or disable with\nHOMEBREW_NO_AUTO_UPDATE. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n==&gt; Downloading https://ghcr.io/v2/homebrew/portable-ruby/portable-ruby/blobs/sha256:40e7f5d7514a7e9757facdd39006f7a351d3d7986d3a228be13c8b1c3216727b\n######################################################################### 100.0%\n==&gt; Pouring portable-ruby-3.4.3.arm64_big_sur.bottle.tar.gz\n==&gt; Auto-updated Homebrew!\nUpdated 2 taps (homebrew/core and homebrew/cask).\n==&gt; New Formulae\nbkmr                       newsraft                   tdom\ncamlpdf                    policy-engine              tmex\nelfio                      readerwriterqueue          undercutf1\niccmax                     rofi                       unoserver\ninfat                      sequoia-chameleon-gnupg\nminiflux                   sexpect\n==&gt; New Casks\ncloudpouch                               font-harmonyos-sans-sc\nclover-chord-systems                     font-harmonyos-sans-tc\nelemental                                font-noto-serif-dives-akuru\nelemental@6                              font-wdxl-lubrifont-jp-n\nfont-asta-sans                           font-wdxl-lubrifont-sc\nfont-bizter                              font-wdxl-lubrifont-tc\nfont-harmonyos-sans                      meru\nfont-harmonyos-sans-naskh-arabic         sc-menu\n\nYou have 13 outdated formulae installed.\n\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/htslib/manifests/1.21\n######################################################################### 100.0%\n==&gt; Fetching htslib\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/htslib/blobs/sha256:5666590893b\n######################################################################### 100.0%\n==&gt; Pouring htslib--1.21.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/htslib/1.21: 51 files, 5.6MB\n==&gt; Running `brew cleanup htslib`...\nDisable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\nHide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n\n\n\n\n\n\n\nShow the code\n!tabix {VCF_FILE}\n\n\n\n\n\n\n\n\nShow the code\nmodels_path = CONTENT_DIR + '/data/saved_models/'\nparams_file = GITHUB_DIR + '/borzoi/examples/params_pred.json'\ntargets_file = GITHUB_DIR + '/borzoi/examples/targets_human.txt'\n\n\nLoad splice site annotation\n\n\nShow the code\nsplice_df = pd.read_csv(CONTENT_DIR+'/data/gencode41_basic_protein_splice.csv.gz', sep='\\t', compression='gzip')\nprint(\"len(splice_df) = \" + str(len(splice_df)))\n\n\nlen(splice_df) = 404837\n\n\nLoad transcriptome\n\n\nShow the code\ntranscriptome = bgene.Transcriptome(CONTENT_DIR + '/data/gencode41_basic_nort.gtf')\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-05-01-unit03/borzoi_personal_gen_prediction-jupyter.html#set-up",
    "href": "post/2025-05-01-unit03/borzoi_personal_gen_prediction-jupyter.html#set-up",
    "title": "Borzoi prediction from personal genome - jupyter notebook",
    "section": "",
    "text": "Recommended: create a new conda environment, borzoi is not compatible with python &gt; 3.10\nconda create ‚Äìname borzoi46100 python=3.10\nThen, use borzoi46100 environment to run the notebook\nInstall software repositories\n\n\nShow the code\nimport os\nGITHUB_DIR = '/Users/haekyungim/Github'\n\n# clone baskerville and borzoi if not already cloned\nbaskerville_path = os.path.join(GITHUB_DIR, 'baskerville')\nborzoi_path = os.path.join(GITHUB_DIR, 'borzoi')\nif not os.path.exists(baskerville_path):\n    !git clone https://github.com/calico/baskerville.git {GITHUB_DIR}/baskerville\n\nif not os.path.exists(borzoi_path):\n    !git clone https://github.com/calico/borzoi.git {GITHUB_DIR}/borzoi\n\n\nAfter loading baskerville, restart runtime, run code from here\nInstall libraries\n\n\nShow the code\ntry:\n    import baskerville\nexcept ImportError:\n    !pip install -e {GITHUB_DIR}/baskerville\n\n\n\n\nShow the code\ntry:\n    import borzoi\nexcept ImportError:\n    !pip install -e {GITHUB_DIR}/borzoi\n\n\nNOTE: install tensorflow-metal if running on a mac\n\n\nShow the code\n# import platform\n# if platform.processor() == 'arm':\n#     print(\"Apple Silicon detected, installing tensorflow-metal...\")\n#     %pip install tensorflow-metal\n# else:\n#     print(\"Not running on Apple Silicon, skipping tensorflow-metal installation\")\n\n\nApple Silicon detected, installing tensorflow-metal...\nCollecting tensorflow-metal\n  Downloading tensorflow_metal-1.2.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (1.3 kB)\nRequirement already satisfied: wheel~=0.35 in /Users/haekyungim/miniconda3/envs/borzoi46100/lib/python3.10/site-packages (from tensorflow-metal) (0.45.1)\nRequirement already satisfied: six&gt;=1.15.0 in /Users/haekyungim/miniconda3/envs/borzoi46100/lib/python3.10/site-packages (from tensorflow-metal) (1.17.0)\nDownloading tensorflow_metal-1.2.0-cp310-cp310-macosx_12_0_arm64.whl (1.4 MB)\n   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.4/1.4 MB 23.8 MB/s eta 0:00:00\nInstalling collected packages: tensorflow-metal\nSuccessfully installed tensorflow-metal-1.2.0\nNote: you may need to restart the kernel to use updated packages.\n\n\n\n\n\n\nShow the code\ntry:\n    import kipoiseq\nexcept ImportError:\n    %pip install kipoiseq\nfrom kipoiseq import Interval\ntry:\n    import cyvcf2\nexcept ImportError:\n    %pip install cyvcf2\nimport os\nimport time\nimport io\nimport gzip\n\n#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n\nimport h5py\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport baskerville\nfrom baskerville import seqnn\nfrom baskerville import gene as bgene\nfrom baskerville import dna\n\nimport json\n\nimport pysam\n\nimport pyfaidx\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n\n\n\n\n\n\nShow the code\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\ngpu_device = tf.config.list_physical_devices('GPU')[0]\ntf.config.experimental.set_memory_growth(gpu_device, True)\n\n\nNum GPUs Available:  1\n\n\n\n\n\n\n\nShow the code\nPRE = '/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100'\nCONTENT_DIR = PRE + '/borzoi'\n# Create borzoi/data directory if it doesn't exist\nif not os.path.exists(os.path.join(CONTENT_DIR)):\n    !mkdir {CONTENT_DIR}\nif not os.path.exists(os.path.join(CONTENT_DIR, '/data')):\n    !mkdir {CONTENT_DIR}/data\n\n# Create model file structure\nsaved_models_path = os.path.join(CONTENT_DIR, 'data/saved_models')\nif not os.path.exists(saved_models_path):\n    !mkdir {saved_models_path}\nif not os.path.exists(os.path.join(saved_models_path, 'f0')):\n    !mkdir {saved_models_path}/f0\nif not os.path.exists(os.path.join(saved_models_path, 'f1')):\n    !mkdir {saved_models_path}/f1\nif not os.path.exists(os.path.join(saved_models_path, 'f2')):\n    !mkdir {saved_models_path}/f2\nif not os.path.exists(os.path.join(saved_models_path, 'f3')):\n    !mkdir {saved_models_path}/f3\n    \n#%cd {CONTENT_DIR}/data\n\n\nmkdir: /Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100/borzoi/data: File exists\n/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100/borzoi/data\n\n\n\n\nShow the code\n#DELET\nos.path.join(CONTENT_DIR, 'data/hg38.fa')\n\n\n'/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100/borzoi/data/hg38.fa'\n\n\n\n\nShow the code\n#Download model weights\nif not os.path.exists(os.path.join(saved_models_path, 'f0', 'model0_best.h5')):\n    !wget https://storage.googleapis.com/seqnn-share/borzoi/f0/model0_best.h5 -O {saved_models_path}/f0/model0_best.h5\nif not os.path.exists(os.path.join(saved_models_path, 'f1', 'model0_best.h5')): \n    !wget https://storage.googleapis.com/seqnn-share/borzoi/f1/model0_best.h5 -O {saved_models_path}/f1/model0_best.h5\nif not os.path.exists(os.path.join(saved_models_path, 'f2', 'model0_best.h5')):\n    !wget https://storage.googleapis.com/seqnn-share/borzoi/f2/model0_best.h5 -O {saved_models_path}/f2/model0_best.h5\nif not os.path.exists(os.path.join(saved_models_path, 'f3', 'model0_best.h5')):\n    !wget https://storage.googleapis.com/seqnn-share/borzoi/f3/model0_best.h5 -O {saved_models_path}/f3/model0_best.h5\n\n\n\n\nShow the code\n#Download and uncompress annotation files\nif not os.path.exists(os.path.join(CONTENT_DIR, 'data', 'gencode41_basic_nort.gtf')):\n    !wget  -O - https://storage.googleapis.com/seqnn-share/helper/gencode41_basic_nort.gtf.gz | gunzip -c &gt; {os.path.join(CONTENT_DIR, 'data', 'gencode41_basic_nort.gtf')}\nif not os.path.exists(os.path.join(CONTENT_DIR, 'data', 'gencode41_basic_protein_splice.csv.gz')):\n    !wget https://storage.googleapis.com/seqnn-share/helper/gencode41_basic_protein_splice.csv.gz -O {os.path.join(CONTENT_DIR, 'data', 'gencode41_basic_protein_splice.csv.gz')}\nif not os.path.exists(os.path.join(CONTENT_DIR, 'data', 'polyadb_human_v3.csv.gz')):\n    !wget https://storage.googleapis.com/seqnn-share/helper/polyadb_human_v3.csv.gz -O {os. path.join(CONTENT_DIR, 'data', 'polyadb_human_v3.csv.gz')}\n\n\n\n\n\n\n\nShow the code\n#fasta_file = '/content/data/genome.fa'\nfasta_file = os.path.join(CONTENT_DIR, 'data/hg38.fa')\nif not os.path.exists(fasta_file):\n    !wget -O - http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz | gunzip -c &gt; {fasta_file}\n\n\n\n\nShow the code\n### download vcf file and index for chromosome 22\nVCF_FILE = os.path.join(CONTENT_DIR, 'data/ALL.chr22.shapeit2_integrated_SNPs_v2a_27022019.GRCh38.phased.gz')\nif not os.path.exists(VCF_FILE):\n    !wget https://uchicago.box.com/shared/static/u526pjh29w93ufn65yomlch4z8lkb1sp.gz -O {VCF_FILE}\n\n\n--2025-05-04 23:18:01--  https://uchicago.box.com/shared/static/u526pjh29w93ufn65yomlch4z8lkb1sp.gz\nResolving uchicago.box.com (uchicago.box.com)... 74.112.186.157\nConnecting to uchicago.box.com (uchicago.box.com)|74.112.186.157|:443... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: /public/static/u526pjh29w93ufn65yomlch4z8lkb1sp.gz [following]\n--2025-05-04 23:18:05--  https://uchicago.box.com/public/static/u526pjh29w93ufn65yomlch4z8lkb1sp.gz\nReusing existing connection to uchicago.box.com:443.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://uchicago.app.box.com/public/static/u526pjh29w93ufn65yomlch4z8lkb1sp.gz [following]\n--2025-05-04 23:18:06--  https://uchicago.app.box.com/public/static/u526pjh29w93ufn65yomlch4z8lkb1sp.gz\nResolving uchicago.app.box.com (uchicago.app.box.com)... 74.112.186.157\nConnecting to uchicago.app.box.com (uchicago.app.box.com)|74.112.186.157|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://public.boxcloud.com/d/1/b1!RHvdIthE36FYyn0QLtnT0Bd4TIsiBQIoBEWvZGN7fVlSkjJkj5zRGnxAgcyWT75KdCvMqjeykC5OtsuTJP9LtQazK-z4ZyX6W9K4IWYfpcPQ67INeF0g8ST99TB4sR-fsAZxsGiDA80d3RwZom5lXnpC8__U5y9y5MkwgaaUV14pLDVPP7tNkFtPppWdkrcIo_78YFKZ5hB2J4ZqdH1_vZsfAVD7HeXu69e8rmSIeiSrGI2RlvPRH6ArsbuJSmMQ6GTjHLCgeAW2zq-EwzqvWYVvyra6-BfS1WGFZ02PiLIsSP4mxL9l7BR95kJqzk9zTmejGYBVxFGX1Q2fjpbQtKBBRxx0wjsnUwlQ1dzV3I251e5PSRgsZt-Ln03YYtOQZCiCLa7tPHt1Lj-5Zq_RS-8uPqaEaIaVxO3BL9H1XLOCvo16rQF5TWJ5cP5uv49l6z1llm37VYm-E_9wKg3r-VHqGZqJDcoB-EObe-UqMgSn0dQJyzsMmeI4eYhx8crMaGYFP18HGeA403GYC_VAqZ3T7dJnxyxAOCnL_3tvzJO9n5zNb6cDCuUSO6jvCKYuQ62fDgqKdtGK4Gc3BTfLZt6ikXKBTdp3Iqv9fxV8c3ecSV92UWyM95YiFftzYDzYKK8K4Qg68JhPE8BGDQCaElazdGWUHSv6aG9FofMB-YInY2YexGCeYWA5VRFkaUSEFv19TtqLjUB_zksXQQdfhnm9D9Ri2aV1iZSw7EhcI8SX7BbHxWp5-4KaVdQ4Da2_iq-mp4dzQUOCiZBULNIcAov1Upu3pSOGmSqTDuuAGm2a9yakG-D__hiSoPL2W-3gKX2XOEqqAypS10cVq0iE7WfDro0xf3el1UfN9DnSBx_i1lkspAF1gwWoMBbt-PCjTQn1QrC4brBNsHXXgs-4YPiJhtxitocc3rmQ7QiBpwufdihSnEb-gxxQwP_P2ixrVpcBn4GEfErjkpOYrjN2Kvuedu_raeWS2c8Ek2sQkJpM9A0aiwxblg-ZRjFszKntmL9auguHq5l8QKVCcvz5Okr49dpd1dxTZsK4HCC0DuclTebjWKUmE3d5jZDk5zM6aJ2NcADklr5H-cuZSi4fdbcCHeufIE92k_6bahN9WK0MQrcv4yTlKsGkaveWgxDpu4tNqnFTe_ckgr6OWzppsITVOiBMO5SilHeCRNfaBdSXIPZ4R76ATrIySz51YLGCsQpkeB-qXEgAMXDIQaKHkN0_wpqsQlJlsXOchvJ3Zs_g8ZtooEM14-sYhP7jwDYK30_Gq5PMAhYBTT33XUh_NOJiHOLFU-3jgi7thJ_AeMBWfgedpXC0coX7isbLsqhPOXewyWmQxKFxauYz7jv6GmomfUgsmAcAuMKmKcbUZX10DfWgvlH3LYGgfdLsXuDY8j3B2On0Ya_mMfkLZVgR8enXtrvQzCjV1SKsh85oVPUJF_GVojV_MtEgIHINRfK_DuqhO6LfMLjSzwCKfdN0C8vBYluHH_bIgTvVm9SkasNQCb0hpnRdrgpNXDp0M49qHok1UCFmbT1xHWUk7_ZSWPerEpUQQ9dRM8ngDU6pq6FgLaKiM8ksIIF4VZMg/download [following]\n--2025-05-04 23:18:10--  https://public.boxcloud.com/d/1/b1!RHvdIthE36FYyn0QLtnT0Bd4TIsiBQIoBEWvZGN7fVlSkjJkj5zRGnxAgcyWT75KdCvMqjeykC5OtsuTJP9LtQazK-z4ZyX6W9K4IWYfpcPQ67INeF0g8ST99TB4sR-fsAZxsGiDA80d3RwZom5lXnpC8__U5y9y5MkwgaaUV14pLDVPP7tNkFtPppWdkrcIo_78YFKZ5hB2J4ZqdH1_vZsfAVD7HeXu69e8rmSIeiSrGI2RlvPRH6ArsbuJSmMQ6GTjHLCgeAW2zq-EwzqvWYVvyra6-BfS1WGFZ02PiLIsSP4mxL9l7BR95kJqzk9zTmejGYBVxFGX1Q2fjpbQtKBBRxx0wjsnUwlQ1dzV3I251e5PSRgsZt-Ln03YYtOQZCiCLa7tPHt1Lj-5Zq_RS-8uPqaEaIaVxO3BL9H1XLOCvo16rQF5TWJ5cP5uv49l6z1llm37VYm-E_9wKg3r-VHqGZqJDcoB-EObe-UqMgSn0dQJyzsMmeI4eYhx8crMaGYFP18HGeA403GYC_VAqZ3T7dJnxyxAOCnL_3tvzJO9n5zNb6cDCuUSO6jvCKYuQ62fDgqKdtGK4Gc3BTfLZt6ikXKBTdp3Iqv9fxV8c3ecSV92UWyM95YiFftzYDzYKK8K4Qg68JhPE8BGDQCaElazdGWUHSv6aG9FofMB-YInY2YexGCeYWA5VRFkaUSEFv19TtqLjUB_zksXQQdfhnm9D9Ri2aV1iZSw7EhcI8SX7BbHxWp5-4KaVdQ4Da2_iq-mp4dzQUOCiZBULNIcAov1Upu3pSOGmSqTDuuAGm2a9yakG-D__hiSoPL2W-3gKX2XOEqqAypS10cVq0iE7WfDro0xf3el1UfN9DnSBx_i1lkspAF1gwWoMBbt-PCjTQn1QrC4brBNsHXXgs-4YPiJhtxitocc3rmQ7QiBpwufdihSnEb-gxxQwP_P2ixrVpcBn4GEfErjkpOYrjN2Kvuedu_raeWS2c8Ek2sQkJpM9A0aiwxblg-ZRjFszKntmL9auguHq5l8QKVCcvz5Okr49dpd1dxTZsK4HCC0DuclTebjWKUmE3d5jZDk5zM6aJ2NcADklr5H-cuZSi4fdbcCHeufIE92k_6bahN9WK0MQrcv4yTlKsGkaveWgxDpu4tNqnFTe_ckgr6OWzppsITVOiBMO5SilHeCRNfaBdSXIPZ4R76ATrIySz51YLGCsQpkeB-qXEgAMXDIQaKHkN0_wpqsQlJlsXOchvJ3Zs_g8ZtooEM14-sYhP7jwDYK30_Gq5PMAhYBTT33XUh_NOJiHOLFU-3jgi7thJ_AeMBWfgedpXC0coX7isbLsqhPOXewyWmQxKFxauYz7jv6GmomfUgsmAcAuMKmKcbUZX10DfWgvlH3LYGgfdLsXuDY8j3B2On0Ya_mMfkLZVgR8enXtrvQzCjV1SKsh85oVPUJF_GVojV_MtEgIHINRfK_DuqhO6LfMLjSzwCKfdN0C8vBYluHH_bIgTvVm9SkasNQCb0hpnRdrgpNXDp0M49qHok1UCFmbT1xHWUk7_ZSWPerEpUQQ9dRM8ngDU6pq6FgLaKiM8ksIIF4VZMg/download\nResolving public.boxcloud.com (public.boxcloud.com)... 74.112.186.164\nConnecting to public.boxcloud.com (public.boxcloud.com)|74.112.186.164|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 374610665 (357M) [application/octet-stream]\nSaving to: ‚Äò/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100/borzoi/data/ALL.chr22.shapeit2_integrated_SNPs_v2a_27022019.GRCh38.phased.gz‚Äô\n\n/Users/haekyungim/L 100%[===================&gt;] 357.26M  97.6MB/s    in 4.0s    \n\n2025-05-04 23:18:19 (90.1 MB/s) - ‚Äò/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100/borzoi/data/ALL.chr22.shapeit2_integrated_SNPs_v2a_27022019.GRCh38.phased.gz‚Äô saved [374610665/374610665]\n\n\n\n\n\nShow the code\n#!ls /content/data\n!ls {CONTENT_DIR}/data\n\n\nALL.chr22.shapeit2_integrated_SNPs_v2a_27022019.GRCh38.phased.gz\ngencode41_basic_nort.gtf\ngencode41_basic_nort.gtf.gz\ngencode41_basic_protein_splice.csv.gz\nhg38.fa\npolyadb_human_v3.csv.gz\nsaved_models\n\n\n\n\nShow the code\n%cd {CONTENT_DIR}\n\n\n/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100/borzoi\n\n\n\n\n\n\nShow the code\n!wget https://github.com/samtools/htslib/releases/download/1.9/htslib-1.9.tar.bz2\n!tar -vxjf htslib-1.9.tar.bz2\n%cd htslib-1.9\n!autoreconf -i\n!./configure\n!make\n!make install\n\n\n\n\n\nShow the code\n## if you Permission denied error on a mac, try\n!brew install htslib\n\n\n==&gt; Auto-updating Homebrew...\nAdjust how often this is run with HOMEBREW_AUTO_UPDATE_SECS or disable with\nHOMEBREW_NO_AUTO_UPDATE. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n==&gt; Downloading https://ghcr.io/v2/homebrew/portable-ruby/portable-ruby/blobs/sha256:40e7f5d7514a7e9757facdd39006f7a351d3d7986d3a228be13c8b1c3216727b\n######################################################################### 100.0%\n==&gt; Pouring portable-ruby-3.4.3.arm64_big_sur.bottle.tar.gz\n==&gt; Auto-updated Homebrew!\nUpdated 2 taps (homebrew/core and homebrew/cask).\n==&gt; New Formulae\nbkmr                       newsraft                   tdom\ncamlpdf                    policy-engine              tmex\nelfio                      readerwriterqueue          undercutf1\niccmax                     rofi                       unoserver\ninfat                      sequoia-chameleon-gnupg\nminiflux                   sexpect\n==&gt; New Casks\ncloudpouch                               font-harmonyos-sans-sc\nclover-chord-systems                     font-harmonyos-sans-tc\nelemental                                font-noto-serif-dives-akuru\nelemental@6                              font-wdxl-lubrifont-jp-n\nfont-asta-sans                           font-wdxl-lubrifont-sc\nfont-bizter                              font-wdxl-lubrifont-tc\nfont-harmonyos-sans                      meru\nfont-harmonyos-sans-naskh-arabic         sc-menu\n\nYou have 13 outdated formulae installed.\n\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/htslib/manifests/1.21\n######################################################################### 100.0%\n==&gt; Fetching htslib\n==&gt; Downloading https://ghcr.io/v2/homebrew/core/htslib/blobs/sha256:5666590893b\n######################################################################### 100.0%\n==&gt; Pouring htslib--1.21.arm64_sequoia.bottle.tar.gz\nüç∫  /opt/homebrew/Cellar/htslib/1.21: 51 files, 5.6MB\n==&gt; Running `brew cleanup htslib`...\nDisable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\nHide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n\n\n\n\n\n\n\nShow the code\n!tabix {VCF_FILE}\n\n\n\n\n\n\n\n\nShow the code\nmodels_path = CONTENT_DIR + '/data/saved_models/'\nparams_file = GITHUB_DIR + '/borzoi/examples/params_pred.json'\ntargets_file = GITHUB_DIR + '/borzoi/examples/targets_human.txt'\n\n\nLoad splice site annotation\n\n\nShow the code\nsplice_df = pd.read_csv(CONTENT_DIR+'/data/gencode41_basic_protein_splice.csv.gz', sep='\\t', compression='gzip')\nprint(\"len(splice_df) = \" + str(len(splice_df)))\n\n\nlen(splice_df) = 404837\n\n\nLoad transcriptome\n\n\nShow the code\ntranscriptome = bgene.Transcriptome(CONTENT_DIR + '/data/gencode41_basic_nort.gtf')"
  },
  {
    "objectID": "post/2025-05-01-unit03/borzoi_personal_gen_prediction-jupyter.html#define-functions",
    "href": "post/2025-05-01-unit03/borzoi_personal_gen_prediction-jupyter.html#define-functions",
    "title": "Borzoi prediction from personal genome - jupyter notebook",
    "section": "2. Define functions",
    "text": "2. Define functions\n\nFastaStringExtractor\nFrom reference seq fasta: - Indexes ref sequence - Gets chromosome sizes into a dictionary\nExtract function: - gets target chromosome length from kipoiseq interval and chr sizes dictionary - truncates interval if it extends beyond chromosome lengths - extracts sequence\n\n\nmake_seq_1hot\n\none hot encodes given sequence\n\n\n\npredict_tracks\n\nmakes prediction of ALL tracks given an encoded sequence\n\n\n\nShow the code\nclass FastaStringExtractor:\n\n    def __init__(self, fasta_file):\n        self.fasta = pyfaidx.Fasta(fasta_file)\n        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n    #import pd.Interval as Interval\n\n    def extract(self, interval: Interval, **kwargs) -&gt; str:\n        # Truncate interval if it extends beyond the chromosome lengths.\n        chromosome_length = self._chromosome_sizes[interval.chrom]\n        trimmed_interval = Interval(interval.chrom,\n                                    max(interval.start, 0),\n                                    min(interval.end, chromosome_length),\n                                    )\n        # pyfaidx wants a 1-based interval\n        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n                                          trimmed_interval.start + 1,\n                                          trimmed_interval.stop).seq).upper()\n        # Fill truncated values with N's.\n        pad_upstream = 'N' * max(-interval.start, 0)\n        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n        return pad_upstream + sequence + pad_downstream # returns padded sequence\n\n    def close(self):\n        return self.fasta.close()\n\n\n\n\nShow the code\n# Make one-hot coded sequence (modified from borzoi)\ndef make_seq_1hot(sequence):\n    return dna.dna_1hot(sequence).astype(\"float32\")\n\ndef predict_tracks(models, sequence_one_hot):\n\n  predicted_tracks = []\n  for fold_ix in range(len(models)):\n\n    yh = models[fold_ix](sequence_one_hot[None, ...])[:, None, ...].astype(\"float16\")\n\n    predicted_tracks.append(yh)\n\n  predicted_tracks = np.concatenate(predicted_tracks, axis=1)\n\n  return predicted_tracks\n\n\n\n\nvcf_to_seq_faster\n\nModifies ref sequence to personal sequence given the vcf\n\n\n\nShow the code\ndef vcf_to_seq_faster(target_interval, individual, vcf_file, fasta_extractor):\n  # target_inverval is a kipoiseq interval, e.g. kipoiseq.Interval(\"chr22\", 18118779, 18145669)\n  # individual is the id of the individual in the vcf file\n\n  target_fa = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))\n  window_coords = target_interval.resize(SEQUENCE_LENGTH)\n      # two haplotypes per individual\n  haplo_1 = list(target_fa[:])\n  haplo_2 = list(target_fa[:])\n\n  # Open the VCF file\n  vcf_reader = cyvcf2.VCF(vcf_file)\n\n  # Specific genomic region\n  CHROM = window_coords.chrom\n  START = max(1,window_coords.start)\n  END = min(window_coords.end, fasta_extractor._chromosome_sizes[CHROM]-1)\n\n  count1 = 0\n  count2 = 0\n\n  # Iterate through variants in the specified region\n  for variant in vcf_reader(CHROM + ':' + str(START) + '-' + str(END)):\n      # Access the individual's genotype using index (0-based) of the sample\n      individual_index = vcf_reader.samples.index(individual)\n      genotype = variant.genotypes[individual_index]\n      ALT=variant.ALT[0]\n      REF=variant.REF\n      POS=variant.POS\n      if REF == target_fa[POS - window_coords.start - 1]:\n        if genotype[0] == 1:\n          haplo_1[POS - window_coords.start - 1] = ALT\n          count1 = count1 + 1\n        if genotype[1] == 1:\n          haplo_2[POS - window_coords.start - 1] = ALT\n          count2 = count2 + 1\n      else:\n        print(\"ERROR: REF in vcf \"+ REF + \"!= REF from the build\" + target_fa[POS - window_coords.start - 1])\n  print(\"number of changes haplo1:\")\n  print(count1)\n  print(\"number of changes haplo2:\")\n  print(count2)\n  return haplo_1, haplo_2\n\n\n\n\nmake_prediction function\n\n\nShow the code\ndef make_prediction(gene, interval, haplo, sequence_one_hot, window = 131072): # snp_pos, alt_allele\n    with tf.device('/GPU:0'):\n        search_gene = gene # gene to predict\n\n        resized_interval = interval.resize(SEQUENCE_LENGTH)\n        start = resized_interval.start\n        end = resized_interval.end\n        center_pos = start + SEQUENCE_LENGTH//2 # figure center position\n\n        chrom = resized_interval.chr # chromosome\n\n        #Get exon bin range\n        gene_keys = [gene_key for gene_key in transcriptome.genes.keys() if search_gene in gene_key]\n\n        gene = transcriptome.genes[gene_keys[0]]\n\n        #Determine output sequence start\n        seq_out_start = start + seqnn_model.model_strides[0]*seqnn_model.target_crops[0]\n        seq_out_len = seqnn_model.model_strides[0]*seqnn_model.target_lengths[0]\n\n        #Determine output positions of gene exons\n        gene_slice = gene.output_slice(seq_out_start, seq_out_len, seqnn_model.model_strides[0], False)\n\n        #Make predictions\n        #y_wt\n        return predict_tracks(models, sequence_one_hot)\n\n\n\n\nGet indexes of tracks function\n\n\nShow the code\n# Get indexes of track\ndef get_track_idx(tracks):\n  track_idx = []\n  targets_df['local_index'] = np.arange(len(targets_df))\n  for track in tracks:\n    track_idx.append(targets_df.loc[targets_df['description'] == track]['local_index'].tolist())\n  return track_idx\n\n\n\n\nPlot tracks function\n\n\nShow the code\ndef plot_tracks(prediction, interval, haplo, anno_df=None, tracks=[],\n                log = [False, False, False],\n                sqrt_scale = [False, False, False]):\n  # Gene slice not included\n  bin_size = 32\n  pad = 16\n\n  resized_interval = interval.resize(SEQUENCE_LENGTH)\n  start = resized_interval.start\n  end = resized_interval.end\n  center_pos = start + SEQUENCE_LENGTH//2 # figure center position\n  window=131072\n\n  plot_start = center_pos - window // 2\n  plot_end = center_pos + window // 2\n\n  plot_start_bin = (plot_start - start) // bin_size - pad\n  plot_end_bin = (plot_end - start) // bin_size - pad\n\n  track_idx = get_track_idx(tracks)\n  track_scales = [0.01, 0.01, 0.01]\n  track_transforms = [3./4., 3./4., 3./4.]\n  soft_clips = [384., 384., 384.]\n\n  # Get annotation positions\n  anno_poses = []\n  if anno_df is not None:\n      anno_poses = anno_df.query(\n          \"chrom == '\"\n          + interval.chr\n          + \"' and position_hg38 &gt;= \"\n          + str(plot_start)\n          + \" and position_hg38 &lt; \"\n          + str(plot_end)\n        )[\"position_hg38\"].values.tolist()\n  # BIG plot\n  fig, axes = plt.subplots(len(tracks), figsize=(20, 1.5 * len(tracks)), sharex = True)\n\n  for ax, track, track_index, track_scale, track_transform, clip_soft in zip(axes,\n        tracks, track_idx, track_scales, track_transforms, soft_clips):\n    # Plot track densities\n    y = np.array(np.copy(prediction), dtype = np.float32)\n    y = np.mean(y[..., track_index], axis=(0, 1, 3))\n    y = y[plot_start_bin:plot_end_bin]\n    signal = np.max(y)\n    if log:\n      y = np.log2(y + 1.0)\n    elif sqrt_scale:\n      y = np.sqrt(y + 1.0)\n\n    max_y = np.max(y)\n    print(\"Max signal for \"+ str(track) + \" = \" + str(round(signal, 4)))\n    print(\"Max transformed signal for \"+ str(track) + \" = \" + str(round(max_y, 4)))\n    print(\"---\")\n    ax.bar(np.arange(plot_end_bin - plot_start_bin) + plot_start_bin,\n            y,\n            width=1.0,\n            color=\"green\",\n            alpha=0.5)\n            # label=\"Prediction\",)\n    # X axis tick annotations\n    xtick_vals = []\n    for _, anno_pos in enumerate(anno_poses):\n      pas_bin = int((anno_pos - start) // 32) - 16\n      xtick_vals.append(pas_bin)\n      bin_end = pas_bin + 3 - 0.5\n      bin_start = bin_end - 5\n\n      ax.axvline(x=pas_bin,\n                color=\"cyan\",\n                linewidth=2,\n                alpha=0.5,\n                linestyle=\"-\",\n                zorder=-1,)\n\n    plt.xlim(plot_start_bin, plot_end_bin - 1)\n    plt.xticks([], [])\n    plt.yticks([], [])\n\n    ax.set(ylabel = \"Signal(log)\" if log else \"Signal\")# , fontsize = 8)\n    ax.set_title(\"Track: \" + str(track)+ \" in \"+ str(haplo), fontsize=10)\n    # plt.legend(fontsize=8)\n    plt.tight_layout()\n  ax.set_xlabel(\n            interval.chr\n            + \":\"\n            + str(plot_start)\n            + \"-\"\n            + str(plot_end)\n            + \" (\"\n            + str(window)\n            + \"bp window)\")\n            # fontsize=8,)\n  plt.show()\n\n\n\n\nExtract fasta\n\n\nShow the code\nfasta_extractor = FastaStringExtractor(fasta_file)"
  },
  {
    "objectID": "post/2025-05-01-unit03/borzoi_personal_gen_prediction-jupyter.html#model-configuration",
    "href": "post/2025-05-01-unit03/borzoi_personal_gen_prediction-jupyter.html#model-configuration",
    "title": "Borzoi prediction from personal genome - jupyter notebook",
    "section": "3.Model configuration",
    "text": "3.Model configuration\n\n\nShow the code\nSEQUENCE_LENGTH = 524288\nn_folds = 4       #To use only one model fold, change to 'n_folds = 1'\nrc = True         #Average across reverse-complement prediction\n\n#Read model parameters\n\nwith open(params_file) as params_open :\n\n    params = json.load(params_open)\n\n    params_model = params['model']\n    params_train = params['train']\n\n#Read targets\n\ntargets_df = pd.read_csv(targets_file, index_col=0, sep='\\t')\ntarget_index = targets_df.index\n\n#Create local index of strand_pair (relative to sliced targets)\nif rc :\n    strand_pair = targets_df.strand_pair\n\n    target_slice_dict = {ix : i for i, ix in enumerate(target_index.values.tolist())}\n    slice_pair = np.array([\n        target_slice_dict[ix] if ix in target_slice_dict else ix for ix in strand_pair.values.tolist()\n    ], dtype='int32')\n\n\n\n\nShow the code\n\n#Initialize model ensemble\n\nmodels = []\nfor fold_ix in range(n_folds) :\n\n    model_file = models_path + \"f\" + str(fold_ix) + \"/model0_best.h5\"\n\n    seqnn_model = seqnn.SeqNN(params_model)\n    seqnn_model.restore(model_file, 0)\n    seqnn_model.build_slice(target_index)\n    if rc :\n        seqnn_model.strand_pair.append(slice_pair)\n    #seqnn_model.build_ensemble(rc, '0')\n    seqnn_model.build_ensemble(rc, [0])\n\n    models.append(seqnn_model)"
  },
  {
    "objectID": "post/2025-05-01-unit03/borzoi_personal_gen_prediction-jupyter.html#build-sequence",
    "href": "post/2025-05-01-unit03/borzoi_personal_gen_prediction-jupyter.html#build-sequence",
    "title": "Borzoi prediction from personal genome - jupyter notebook",
    "section": "4. Build sequence",
    "text": "4. Build sequence\n\n\nShow the code\n# read VCFs and encode haplotypes\nCHROM = 'chr22'\nvcf_file = CONTENT_DIR + \"/data/ALL.\" + CHROM + \".shapeit2_integrated_SNPs_v2a_27022019.GRCh38.phased.gz\"\ntarget_interval = kipoiseq.Interval(CHROM, 18118779, 18145669)\nhaplo1, haplo2 = vcf_to_seq_faster(target_interval, 'HG00097', vcf_file, fasta_extractor)\nhaplo0 = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH)) # ref seq\n\n# removed extra dimension as input layer is of shape=(None, 524288, 4)\nhaplo1_enc = make_seq_1hot(\"\".join(haplo1))\nhaplo2_enc = make_seq_1hot(\"\".join(haplo2))\nhaplo0_enc = make_seq_1hot(\"\".join(haplo0))\n\n\nnumber of changes haplo1:\n414\nnumber of changes haplo2:\n133"
  },
  {
    "objectID": "post/2025-05-01-unit03/borzoi_personal_gen_prediction-jupyter.html#predict",
    "href": "post/2025-05-01-unit03/borzoi_personal_gen_prediction-jupyter.html#predict",
    "title": "Borzoi prediction from personal genome - jupyter notebook",
    "section": "5. Predict",
    "text": "5. Predict\nENSG00000099968\n\n\nShow the code\nsequences = [haplo0_enc, haplo1_enc, haplo2_enc]\nmy_tracks = ['CAGE:B lymphoblastoid cell line: GM12878 ENCODE, biol_',\n                          'DNASE:CD14-positive monocyte female',\n                          'RNA:blood']\n\n\n\n\nShow the code\nfor h in range(0, len(sequences)):\n  haplotypes = [\"Reference\", \"Haplotype 1\", \"Haplotype 2\"]\n  prediction = make_prediction(gene = 'ENSG00000099968', interval = target_interval,\n                               haplo = haplotypes[h], sequence_one_hot=sequences[h])\n  print(haplotypes[h])\n  plot_tracks(prediction, interval = target_interval, haplo = haplotypes[h],\n              tracks = my_tracks, log = [True, True, False])\n\n\nReference\nMax signal for CAGE:B lymphoblastoid cell line: GM12878 ENCODE, biol_ = 38.1346\nMax transformed signal for CAGE:B lymphoblastoid cell line: GM12878 ENCODE, biol_ = 5.2904\n---\nMax signal for DNASE:CD14-positive monocyte female = 11.8511\nMax transformed signal for DNASE:CD14-positive monocyte female = 3.6838\n---\nMax signal for RNA:blood = 2.7051\nMax transformed signal for RNA:blood = 1.8895\n---\n\n\n\n\n\n\n\n\n\nHaplotype 1\nMax signal for CAGE:B lymphoblastoid cell line: GM12878 ENCODE, biol_ = 39.2737\nMax transformed signal for CAGE:B lymphoblastoid cell line: GM12878 ENCODE, biol_ = 5.3318\n---\nMax signal for DNASE:CD14-positive monocyte female = 11.8428\nMax transformed signal for DNASE:CD14-positive monocyte female = 3.6829\n---\nMax signal for RNA:blood = 2.689\nMax transformed signal for RNA:blood = 1.8832\n---\n\n\n\n\n\n\n\n\n\nHaplotype 2\nMax signal for CAGE:B lymphoblastoid cell line: GM12878 ENCODE, biol_ = 38.5659\nMax transformed signal for CAGE:B lymphoblastoid cell line: GM12878 ENCODE, biol_ = 5.3062\n---\nMax signal for DNASE:CD14-positive monocyte female = 11.8301\nMax transformed signal for DNASE:CD14-positive monocyte female = 3.6815\n---\nMax signal for RNA:blood = 2.7414\nMax transformed signal for RNA:blood = 1.9036\n---\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nprediction.shape\n\n\n(1, 4, 16352, 7611)"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "",
    "text": "created by Erin Wilson. Downloaded from here.\nSome edits by Haky Im and Ran Blekhman for the deep learning in genomics gene46100 course.\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#import-necessary-modules",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#import-necessary-modules",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "import necessary modules",
    "text": "import necessary modules\n\nfrom collections import defaultdict\nfrom itertools import product\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport random\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nif torch.backends.mps.is_available():\n    torch.set_default_dtype(torch.float32)\n    print(\"Set default to float32 for MPS compatibility\")\n\nSet default to float32 for MPS compatibility"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#set-seeds-for-reproduciblity-across-runs",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#set-seeds-for-reproduciblity-across-runs",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "set seeds for reproduciblity across runs",
    "text": "set seeds for reproduciblity across runs\n\n# Set a random seed in a bunch of different places\ndef set_seed(seed: int = 42) -&gt; None:\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    \n    if torch.backends.mps.is_available():\n        # For MacBooks with Apple Silicon\n        torch.mps.manual_seed(seed)\n    elif torch.cuda.is_available():\n        # For CUDA GPUs\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    print(f\"Random seed set as {seed}\")\n    \nset_seed(17)\n\nRandom seed set as 17"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#define-gpu-device",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#define-gpu-device",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "define GPU device",
    "text": "define GPU device\nAre you working on a GPU? If so, you can put your data/models on DEVICE (and have to do so explicity)! If not, you can probably remove all instances of foo.to(DEVICE) and it should still work fine on a CPU.\n\nDEVICE = torch.device('mps' if torch.backends.mps.is_available() \n                     else 'cuda' if torch.cuda.is_available() \n                     else 'cpu')\nDEVICE\n\ndevice(type='mps')\n\n\n ## 1. Generate synthetic DNA data\nUsually scientists might be interested in predicting something like a binding score, an expression strength, or classifying a TF binding event. But here, we are going to keep it simple: the goal in this tutorial is to observe if a deep learning model can learn to detect a very small, simple pattern in a DNA sequence and score it appropriately (again, just a practice task to convince ourselves that we have actually set up the Pytorch pieces correctly such that it can learn from input that looks like a DNA sequence).\nSo arbitrarily, let‚Äôs say that given an 8-mer DNA sequence, we will score it based on the following rules: * A = +20 points * C = +17 points * G = +14 points * T = +11 points\nFor every 8-mer, let‚Äôs sum up its total points based on the nucleotides in its sequence, then take the average. For example,\n\nAAAAAAAA would score 20.0\n\n(mean(20 + 20 + 20 + 20 + 20 + 20 + 20 + 20) = 20.0)\n\nACAAAAAA would score 19.625\n\n(mean(20 + 17 + 20 + 20 + 20 + 20 + 20 + 20) = 19.625)\n\n\nThese values for the nucleotides are arbitrary - there‚Äôs no real biology here! It‚Äôs just a way to assign sequences a score for the purposes of our Pytorch practice.\nHowever, since many recent papers use methods like CNNs to automatically detect ‚Äúmotifs,‚Äù or short patterns in the DNA that can activate or repress a biological response, let‚Äôs add one more piece to our scoring system. To simulate something like motifs influencing gene expression, let‚Äôs say a given sequence gets a +10 bump if TAT appears anywhere in the 8-mer, and a -10 bump if it has a GCG in it. Again, these motifs don‚Äôt mean anything in real life, they are just a mechanism for simulating a really simple activation or repression effect.\n\nSo let‚Äôs implement this basic scoring function!\n\n# define function for generating all k-mers of length k\ndef kmers(k):\n    '''Generate a list of all k-mers for a given k'''\n    \n    return [''.join(x) for x in product(['A','C','G','T'], repeat=k)]\n\n\ngenerate all 8-mers\n\n# generate all 8-mers\nseqs8 = kmers(8)\nprint('Total 8mers:',len(seqs8))\n\nTotal 8mers: 65536\n\n\n\n\ndefine scoring function for the DNA sequences\n\n# define score_dict\nscore_dict = {\n    'A':20,\n    'C':17,\n    'G':14,\n    'T':11\n}\n# define function for scoring sequences\ndef score_seqs_motif(seqs):\n    '''\n    Calculate the scores for a list of sequences based on \n    the above score_dict\n    '''\n    data = []\n    for seq in seqs:\n        # get the average score by nucleotide\n        score = np.mean([score_dict[base] for base in seq],dtype=np.float32)\n        \n        # give a + or - bump if this k-mer has a specific motif\n        if 'TAT' in seq:\n            score += 10\n        if 'GCG' in seq:\n            score -= 10\n        data.append([seq,score])\n        \n    df = pd.DataFrame(data, columns=['seq','score'])\n    return df\n\n\nmer8 = score_seqs_motif(seqs8)\nmer8.head()\n\n\n\n\n\n\n\n\nseq\nscore\n\n\n\n\n0\nAAAAAAAA\n20.000\n\n\n1\nAAAAAAAC\n19.625\n\n\n2\nAAAAAAAG\n19.250\n\n\n3\nAAAAAAAT\n18.875\n\n\n4\nAAAAAACA\n19.625\n\n\n\n\n\n\n\nSpot check scores of a couple seqs with motifs:\n\nmer8[mer8['seq'].isin(['TGCGTTTT','CCCCCTAT'])]\n\n\n\n\n\n\n\n\nseq\nscore\n\n\n\n\n21875\nCCCCCTAT\n25.875\n\n\n59135\nTGCGTTTT\n2.500"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#plot-distribution-of-motif-scores",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#plot-distribution-of-motif-scores",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "plot distribution of motif scores",
    "text": "plot distribution of motif scores\n\nplt.hist(mer8['score'].values,bins=20)\nplt.title(\"8-mer with Motifs score distribution\")\nplt.xlabel(\"seq score\",fontsize=14)\nplt.ylabel(\"count\",fontsize=14)\nplt.show()\n\n\n\n\n\n\n\n\nAs expected, the distribution of scores across all 8-mers has 3 groups: * No motif (centered around ~15) * contains TAT (~25) * contains GCG (~5)"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-1",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-1",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "Question 1",
    "text": "Question 1\nModify the scoring function to create a more complex pattern. Instead of giving fixed bonuses for ‚ÄúTAT‚Äù and ‚ÄúGCG‚Äù, implement a position-dependent scoring where a motif gets a higher bonus if it appears at the beginning of the sequence compared to the end. How does this change the distribution of scores?"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#next-we-want-to-train-a-model-to-predict-this-score-by-from-the-dna-sequence",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#next-we-want-to-train-a-model-to-predict-this-score-by-from-the-dna-sequence",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "Next, we want to train a model to predict this score by from the DNA sequence",
    "text": "Next, we want to train a model to predict this score by from the DNA sequence\n ## 2. Prepare data for Pytorch training\nFor neural networks to make predictions, you have to give it your input as a matrix of numbers. For example, to classify images by whether or not they contain a cat, a network ‚Äúsees‚Äù the image as a matrix of pixel values and learns relevant patterns in the relative arrangement of pixels (e.g.¬†patterns that correspond to cat ears, or a nose with whiskers).\nWe similarly need to turn our DNA sequences (strings of ACGTs) into a matrix of numbers. So how do we pretend our DNA is a cat?\nOne common strategy is to one-hot encode the DNA: treat each nucleotide as a vector of length 4, where 3 positions are 0 and one position is a 1, depending on the nucleotide.\n\nturn DNA sequences into numbers with one hot encoding\n\nThis one-hot encoding has the nice property that it makes your DNA appear like how a computer sees a picture of a cat!\n\ndef one_hot_encode(seq):\n    \"\"\"\n    Given a DNA sequence, return its one-hot encoding\n    \"\"\"\n    # Make sure seq has only allowed bases\n    allowed = set(\"ACTGN\")\n    if not set(seq).issubset(allowed):\n        invalid = set(seq) - allowed\n        raise ValueError(f\"Sequence contains chars not in allowed DNA alphabet (ACGTN): {invalid}\")\n        \n    # Dictionary returning one-hot encoding for each nucleotide \n    nuc_d = {'A':[1.0,0.0,0.0,0.0],\n             'C':[0.0,1.0,0.0,0.0],\n             'G':[0.0,0.0,1.0,0.0],\n             'T':[0.0,0.0,0.0,1.0],\n             'N':[0.0,0.0,0.0,0.0]}\n    \n    # Create array from nucleotide sequence\n    vec=np.array([nuc_d[x] for x in seq], dtype=np.float32)\n        \n    return vec\n\n\n# one hot encoding of 8 As\na8 = one_hot_encode(\"AAAAAAAA\")\nprint(\"AAAAAA:\\n\",a8)\n\n\nAAAAAA:\n [[1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]]\n\n\n\n# one hot encoding of another DNA\ns = one_hot_encode(\"AGGTACCT\")\nprint(\"AGGTACC:\\n\",s)\nprint(\"shape:\",s.shape)\n\nAGGTACC:\n [[1. 0. 0. 0.]\n [0. 0. 1. 0.]\n [0. 0. 1. 0.]\n [0. 0. 0. 1.]\n [1. 0. 0. 0.]\n [0. 1. 0. 0.]\n [0. 1. 0. 0.]\n [0. 0. 0. 1.]]\nshape: (8, 4)\n\n\n\n\nsplit data into train, validation, and test\nData are typically split into training, validation, and test sets. This helps avoid overfitting and achieve better generalization to new data.\n\nTraining set (e.g.¬†70% of the data)\n\nused to train the model\nmodel learns patterns from this data\nanalogy: studying for an exam\n\nValidation or tuning set (e.g.¬†15% of the data)\n\nused to tune hypterparameters\nhelps prevent overfitting\nanalogy: pratice problems while studying\n\nTest set or held out set (e.g.¬†15% of the data)\n\nused only to evaluate final model performance\nnever used during training or tuning\nanalogy: actual exam\n\n\nNote: in this tutorial, that uses the quick_split function defined here splits the data into 20% for the test set, 80% of the remaining 80% as the training set (i.e.¬†64% of the total) and 20% of the non test sets as validation (i.e.¬†16%).\n\ndefine quick splitting function\n\n# define function for splitting data\ndef quick_split(df, split_frac=0.8, verbose=False):\n    '''\n    Given a df of samples, randomly split indices between\n    train and test at the desired fraction\n    '''\n    cols = df.columns # original columns, use to clean up reindexed cols\n    df = df.reset_index()\n\n    # shuffle indices\n    idxs = list(range(df.shape[0]))\n    random.shuffle(idxs)\n\n    # split shuffled index list by split_frac\n    split = int(len(idxs)*split_frac)\n    train_idxs = idxs[:split]\n    test_idxs = idxs[split:]\n    \n    # split dfs and return\n    train_df = df[df.index.isin(train_idxs)]\n    test_df = df[df.index.isin(test_idxs)]\n        \n    return train_df[cols], test_df[cols]\n\n\n\nsplit data into train, validation, and test sets\n\n# split data into train, validation, and test\nfull_train_df, test_df = quick_split(mer8)\ntrain_df, val_df = quick_split(full_train_df)\n\nprint(\"Train:\", train_df.shape)\nprint(\"Val:\", val_df.shape)\nprint(\"Test:\", test_df.shape)\n\ntrain_df.head()\n\nTrain: (41942, 2)\nVal: (10486, 2)\nTest: (13108, 2)\n\n\n\n\n\n\n\n\n\nseq\nscore\n\n\n\n\n0\nAAAAAAAA\n20.000\n\n\n1\nAAAAAAAC\n19.625\n\n\n2\nAAAAAAAG\n19.250\n\n\n3\nAAAAAAAT\n18.875\n\n\n4\nAAAAAACC\n19.250\n\n\n\n\n\n\n\n\n\n\nplot distribution of train, validation, and test data and check that they are similarly distributed\n\ndef plot_train_test_hist(train_df, val_df,test_df,bins=20):\n    ''' Check distribution of train/test scores, sanity check that its not skewed'''\n    plt.hist(train_df['score'].values,bins=bins,label='train',alpha=0.5)\n    plt.hist(val_df['score'].values,bins=bins,label='val',alpha=0.75)\n    plt.hist(test_df['score'].values,bins=bins,label='test',alpha=0.4)\n    plt.legend()\n    plt.xlabel(\"seq score\",fontsize=14)\n    plt.ylabel(\"count\",fontsize=14)\n    plt.show()\n\nWith the below histogram, we can confirm that the train, test, and val sets contain example sequences from each bucket of the distribution (each set has some examples with each kind of motif)\n\nplot_train_test_hist(train_df, val_df,test_df)\n\n\n\n\n\n\n\n\n\n\ndefine dataset and dataloader classes\nDataset and DataLoader classes allow efficient data handling in deep learning.\nDataset class allows standardized way to access and preprocess data.\nDataloader handles batching, shuffling, parallel data loading to make it easier to feed the data for training.\nYou can read more about DataLoader and Dataset objects.\n\nfrom torch.utils.data import Dataset, DataLoader\n\n\ndefine one hot encoded dataset class\nThis class is essential for preparing DNA sequence data for deep learning models, converting the DNA sequences into a numerical format that neural networks can process.\n\nclass SeqDatasetOHE(Dataset):\n    '''\n    Dataset for one-hot-encoded sequences\n    '''\n    def __init__(self, df, seq_col='seq', target_col='score'):\n        # Input: DataFrame with DNA sequences and their scores\n        self.seqs = list(df[seq_col].values)  # Get DNA sequences\n        self.seq_len = len(self.seqs[0])      # Length of each sequence\n        \n        # Convert DNA sequences to one-hot encoding\n        self.ohe_seqs = torch.stack([torch.tensor(one_hot_encode(x)) for x in self.seqs])\n        \n        # Get target scores\n        self.labels = torch.tensor(list(df[target_col].values)).unsqueeze(1)\n        \n    def __len__(self): return len(self.seqs)\n    \n    def __getitem__(self,idx):\n        # Given an index, return a tuple of an X with it's associated Y\n        # This is called inside DataLoader\n        seq = self.ohe_seqs[idx]\n        label = self.labels[idx]\n        \n        return seq, label\n\n\n\nconstruct DataLoaders from Datasets.\n\ndef build_dataloaders(train_df,\n                      test_df,\n                      seq_col='seq',\n                      target_col='score',\n                      batch_size=128,\n                      shuffle=True\n                     ):\n    '''\n    Given a train and test df with some batch construction\n    details, put them into custom SeqDatasetOHE() objects. \n    Give the Datasets to the DataLoaders and return.\n    '''\n    \n    # create Datasets    \n    train_ds = SeqDatasetOHE(train_df,seq_col=seq_col,target_col=target_col)\n    test_ds = SeqDatasetOHE(test_df,seq_col=seq_col,target_col=target_col)\n\n    # Put DataSets into DataLoaders\n    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle)\n    test_dl = DataLoader(test_ds, batch_size=batch_size)\n\n    \n    return train_dl,test_dl\n\n\ntrain_dl, val_dl = build_dataloaders(train_df, val_df)\n\nThese dataloaders are now ready to be used in a training loop!\n ## 3. Define Pytorch models The primary model I was interested in trying was a Convolutional Neural Network, as these have been shown to be useful for learning motifs from genomic data. But as a point of comparison, I included a simple Linear model. Here are some model definitions:\n\n# very simple linear model\nclass DNA_Linear(nn.Module):\n    def __init__(self, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        # the 4 is for our one-hot encoded vector length 4!\n        self.lin = nn.Linear(4*seq_len, 1)\n\n    def forward(self, xb):\n        # reshape to flatten sequence dimension\n        xb = xb.view(xb.shape[0],self.seq_len*4)\n        # Linear wraps up the weights/bias dot product operations\n        out = self.lin(xb)\n        return out\n    \n## CNN model\nclass DNA_CNN(nn.Module):\n    def __init__(self, seq_len, num_filters=32, kernel_size=3):\n        super().__init__()\n        self.seq_len = seq_len\n        \n        # Define layers individually\n        self.conv = nn.Conv1d(4, num_filters, kernel_size=kernel_size)\n        self.relu = nn.ReLU(inplace=True)\n        self.linear = nn.Linear(num_filters*(seq_len-kernel_size+1), 1)\n\n    def forward(self, xb):\n        # reshape view to batch_size x 4channel x seq_len\n        xb = xb.permute(0, 2, 1)\n        \n        # Apply layers step by step\n        x = self.conv(xb)\n        x = self.relu(x)\n        x = x.flatten(1)  # flatten all dimensions except batch\n        out = self.linear(x)\n        return out \n\nThese aren‚Äôt optimized models, just something to start with (again, we‚Äôre just practicing connecting the Pytorch tubes in the context of DNA!). * The Linear model tries to predict the score by simply weighting the nucleotides that appears in each position. * The CNN model uses 32 filters of length (kernel_size) 3 to scan across the 8-mer sequences for informative 3-mer patterns.\n ## 4. Define the training loop functions\nThe model training process is structured into a series of modular functions, each responsible for a specific part of the workflow. This design improves clarity, reusability, and flexibility when working with different models, optimizers, or loss functions.\nThe overall structure is as follows:\n# Initializes optimizer and loss function if not provided, then trains the model\nrun_training()\n\n    # Iterates over multiple epochs\n    train_loop()\n\n        # Performs one complete pass over the training dataset\n        train_epoch()\n\n            # Computes loss and backprops for a single batch\n            process_batch()\n\n        # Performs one complete pass over the validation dataset\n        val_epoch()\n\n            # Computes loss for a single batch no gradient updates\n            process_batch()\n\n# +--------------------------------+\n# | Training and fitting functions |\n# +--------------------------------+\n\ndef process_batch(model, loss_func, xb, yb, opt=None,verbose=False):\n    '''\n    Apply loss function to a batch of inputs. If no optimizer\n    is provided, skip the back prop step.\n    '''\n    if verbose:\n        print('loss batch ****')\n        print(\"xb shape:\",xb.shape)\n        print(\"yb shape:\",yb.shape)\n        print(\"yb shape:\",yb.squeeze(1).shape)\n        #print(\"yb\",yb)\n\n    # get the batch output from the model given your input batch \n    # ** This is the model's prediction for the y labels! **\n    xb_out = model(xb.float())\n    \n    if verbose:\n        print(\"model out pre loss\", xb_out.shape)\n        #print('xb_out', xb_out)\n        print(\"xb_out:\",xb_out.shape)\n        print(\"yb:\",yb.shape)\n        print(\"yb.long:\",yb.long().shape)\n    \n    loss = loss_func(xb_out, yb.float()) # for MSE/regression\n    # __FOOTNOTE 2__\n    \n    if opt is not None: # if opt\n        opt.zero_grad() ## moved zero grad up to make sure it's not accumulating grads from previous batches\n        loss.backward()\n        opt.step()\n    return loss.item(), len(xb)\n\n\ndef train_epoch(model, train_dl, loss_func, device, opt):\n    '''\n    Execute 1 set of batched training within an epoch\n    '''\n    # Set model to Training mode\n    model.train()\n    tl = [] # train losses\n    ns = [] # batch sizes, n\n    \n    # loop through train DataLoader\n    for xb, yb in train_dl:\n        # put on GPU\n        xb, yb = xb.to(device),yb.to(device)\n        \n        # provide opt so backprop happens\n        t, n = process_batch(model, loss_func, xb, yb, opt=opt)\n        \n        # collect train loss and batch sizes\n        tl.append(t)\n        ns.append(n)\n    \n    # average the losses over all batches    \n    train_loss = np.sum(np.multiply(tl, ns)) / np.sum(ns)\n    \n    return train_loss\n\ndef val_epoch(model, val_dl, loss_func, device):\n    '''\n    Execute 1 set of batched validation within an epoch\n    '''\n    # Set model to Evaluation mode\n    model.eval()\n    with torch.no_grad():\n        vl = [] # val losses\n        ns = [] # batch sizes, n\n        \n        # loop through validation DataLoader\n        for xb, yb in val_dl:\n            # put on GPU\n            xb, yb = xb.to(device),yb.to(device)\n\n            # Do NOT provide opt here, so backprop does not happen\n            v, n = process_batch(model, loss_func, xb, yb)\n\n            # collect val loss and batch sizes\n            vl.append(v)\n            ns.append(n)\n\n    # average the losses over all batches\n    val_loss = np.sum(np.multiply(vl, ns)) / np.sum(ns)\n    \n    return val_loss\n\n\ndef train_loop(epochs, model, loss_func, opt, train_dl, val_dl,device,patience=1000):\n    '''\n    Fit the model params to the training data, eval on unseen data.\n    Loop for a number of epochs and keep train of train and val losses \n    along the way\n    '''\n    # keep track of losses\n    train_losses = []    \n    val_losses = []\n    \n    # loop through epochs\n    for epoch in range(epochs):\n        # take a training step\n        train_loss = train_epoch(model,train_dl,loss_func,device,opt)\n        train_losses.append(train_loss)\n\n        # take a validation step\n        val_loss = val_epoch(model,val_dl,loss_func,device)\n        val_losses.append(val_loss)\n        \n        print(f\"E{epoch} | train loss: {train_loss:.3f} | val loss: {val_loss:.3f}\")\n\n    return train_losses, val_losses\n\n\ndef run_training(train_dl,val_dl,model,device,\n              lr=0.01, epochs=50, \n              lossf=None,opt=None\n             ):\n    '''\n    Given train and val DataLoaders and a NN model, fit the model to the training\n    data. By default, use MSE loss and an SGD optimizer\n    '''\n    # define optimizer\n    if opt:\n        optimizer = opt\n    else: # if no opt provided, just use SGD\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n    \n    # define loss function\n    if lossf:\n        loss_func = lossf\n    else: # if no loss function provided, just use MSE\n        loss_func = torch.nn.MSELoss()\n    \n    # run the training loop\n    train_losses, val_losses = train_loop(\n                                epochs, \n                                model, \n                                loss_func, \n                                optimizer, \n                                train_dl, \n                                val_dl, \n                                device)\n\n    return train_losses, val_losses"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#explanation-of-the-functions-that-make-up-the-training-process",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#explanation-of-the-functions-that-make-up-the-training-process",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "Explanation of the functions that make up the training process",
    "text": "Explanation of the functions that make up the training process\n\nrun_training(): This is the top-level function that orchestrates the entire training process. It handles the initialization of the optimizer and loss function if they are not explicitly provided. It calls the train_loop() function to begin the iterative training loop.\ntrain_loop(): This function manages the training loop across a specified number of epochs. For each epoch, it calls train_epoch() and val_epoch() to perform training and validation, respectively. It also prints the training and validation losses for each epoch.\ntrain_epoch(): This function performs one full pass over the training dataset. It iterates through the train_dl (DataLoader) to get batches of training data. For each batch, it calls process_batch() with the optimizer, enabling backpropagation. It accumulates and averages the training losses.\nval_epoch(): This function performs one full pass over the validation dataset. It iterates through the val_dl (DataLoader) to get batches of validation data. For each batch, it calls process_batch() without the optimizer, preventing gradient updates. It accumulates and averages the validation losses.\nprocess_batch(): This function processes a single batch of data. It calculates the model‚Äôs predictions and computes the loss. If an optimizer is provided, it performs backpropagation and updates the model‚Äôs weights. If no optimiser is provided, it just returns the loss and batch size. This function is used for both training and validation, with the presence of the optimiser parameter determining if back propagation occurs.\n\n ## 5. Train the models First let‚Äôs try running a Linear Model on our 8-mer sequences\n\n# get the sequence length from the first seq in the df\nseq_len = len(train_df['seq'].values[0])\n\n# create Linear model object\nmodel_lin = DNA_Linear(seq_len)\n# use float32 since mps cannot handle 64\nmodel_lin = model_lin.type(torch.float32)\nmodel_lin.to(DEVICE) # put on GPU\n\n\n# run the training pipeline with default settings!\nlin_train_losses, lin_val_losses = run_training(\n    train_dl, \n    val_dl, \n    model_lin,\n    DEVICE\n)\n\nE0 | train loss: 21.238 | val loss: 12.980\nE1 | train loss: 12.969 | val loss: 12.826\nE2 | train loss: 12.918 | val loss: 12.832\nE3 | train loss: 12.915 | val loss: 12.847\nE4 | train loss: 12.916 | val loss: 12.833\nE5 | train loss: 12.918 | val loss: 12.837\nE6 | train loss: 12.915 | val loss: 12.828\nE7 | train loss: 12.917 | val loss: 12.826\nE8 | train loss: 12.917 | val loss: 12.827\nE9 | train loss: 12.917 | val loss: 12.827\nE10 | train loss: 12.918 | val loss: 12.831\nE11 | train loss: 12.914 | val loss: 12.836\nE12 | train loss: 12.918 | val loss: 12.834\nE13 | train loss: 12.916 | val loss: 12.830\nE14 | train loss: 12.917 | val loss: 12.832\nE15 | train loss: 12.917 | val loss: 12.831\nE16 | train loss: 12.917 | val loss: 12.833\nE17 | train loss: 12.915 | val loss: 12.882\nE18 | train loss: 12.916 | val loss: 12.834\nE19 | train loss: 12.916 | val loss: 12.833\nE20 | train loss: 12.917 | val loss: 12.830\nE21 | train loss: 12.918 | val loss: 12.830\nE22 | train loss: 12.917 | val loss: 12.826\nE23 | train loss: 12.914 | val loss: 12.826\nE24 | train loss: 12.915 | val loss: 12.828\nE25 | train loss: 12.916 | val loss: 12.833\nE26 | train loss: 12.916 | val loss: 12.829\nE27 | train loss: 12.916 | val loss: 12.828\nE28 | train loss: 12.918 | val loss: 12.848\nE29 | train loss: 12.916 | val loss: 12.830\nE30 | train loss: 12.916 | val loss: 12.841\nE31 | train loss: 12.917 | val loss: 12.823\nE32 | train loss: 12.917 | val loss: 12.833\nE33 | train loss: 12.916 | val loss: 12.825\nE34 | train loss: 12.918 | val loss: 12.822\nE35 | train loss: 12.916 | val loss: 12.838\nE36 | train loss: 12.917 | val loss: 12.833\nE37 | train loss: 12.914 | val loss: 12.837\nE38 | train loss: 12.917 | val loss: 12.834\nE39 | train loss: 12.917 | val loss: 12.846\nE40 | train loss: 12.916 | val loss: 12.826\nE41 | train loss: 12.917 | val loss: 12.820\nE42 | train loss: 12.917 | val loss: 12.835\nE43 | train loss: 12.916 | val loss: 12.832\nE44 | train loss: 12.916 | val loss: 12.827\nE45 | train loss: 12.915 | val loss: 12.827\nE46 | train loss: 12.916 | val loss: 12.827\nE47 | train loss: 12.918 | val loss: 12.829\nE48 | train loss: 12.915 | val loss: 12.824\nE49 | train loss: 12.917 | val loss: 12.824\n\n\nLet‚Äôs look at the loss in quick plot:\n\ndef quick_loss_plot(data_label_list,loss_type=\"MSE Loss\",sparse_n=0):\n    '''\n    For each train/test loss trajectory, plot loss by epoch\n    '''\n    for i,(train_data,test_data,label) in enumerate(data_label_list):    \n        plt.plot(train_data,linestyle='--',color=f\"C{i}\", label=f\"{label} Train\")\n        plt.plot(test_data,color=f\"C{i}\", label=f\"{label} Val\",linewidth=3.0)\n\n    plt.legend()\n    plt.ylabel(loss_type)\n    plt.xlabel(\"Epoch\")\n    plt.legend(bbox_to_anchor=(1,1),loc='upper left')\n    plt.show()\n\n\nlin_data_label = (lin_train_losses,lin_val_losses,\"Lin\")\nquick_loss_plot([lin_data_label])\n\n\n\n\n\n\n\n\nAt first glance, not much learning appears to be happening.\nNext let‚Äôs try the CNN.\n\nseq_len = len(train_df['seq'].values[0])\n\n# create Linear model object\nmodel_cnn = DNA_CNN(seq_len)\nmodel_cnn.to(DEVICE) # put on GPU\n\n# run the model with default settings!\ncnn_train_losses, cnn_val_losses = run_training(\n    train_dl, \n    val_dl, \n    model_cnn,\n    DEVICE\n)\n\nE0 | train loss: 14.640 | val loss: 10.167\nE1 | train loss: 8.625 | val loss: 7.035\nE2 | train loss: 6.305 | val loss: 4.967\nE3 | train loss: 4.507 | val loss: 3.257\nE4 | train loss: 3.123 | val loss: 2.256\nE5 | train loss: 2.408 | val loss: 2.627\nE6 | train loss: 1.997 | val loss: 2.078\nE7 | train loss: 1.827 | val loss: 4.446\nE8 | train loss: 1.547 | val loss: 1.297\nE9 | train loss: 1.438 | val loss: 1.185\nE10 | train loss: 1.249 | val loss: 1.108\nE11 | train loss: 1.200 | val loss: 1.149\nE12 | train loss: 1.140 | val loss: 1.096\nE13 | train loss: 1.011 | val loss: 1.102\nE14 | train loss: 1.022 | val loss: 1.188\nE15 | train loss: 1.027 | val loss: 1.119\nE16 | train loss: 1.045 | val loss: 1.040\nE17 | train loss: 0.999 | val loss: 1.052\nE18 | train loss: 0.965 | val loss: 1.069\nE19 | train loss: 0.944 | val loss: 1.208\nE20 | train loss: 0.945 | val loss: 1.175\nE21 | train loss: 0.925 | val loss: 1.038\nE22 | train loss: 0.927 | val loss: 1.249\nE23 | train loss: 0.938 | val loss: 1.022\nE24 | train loss: 0.917 | val loss: 1.042\nE25 | train loss: 0.930 | val loss: 1.062\nE26 | train loss: 0.917 | val loss: 1.089\nE27 | train loss: 0.913 | val loss: 1.286\nE28 | train loss: 0.932 | val loss: 1.041\nE29 | train loss: 0.890 | val loss: 1.126\nE30 | train loss: 0.903 | val loss: 1.038\nE31 | train loss: 0.918 | val loss: 1.033\nE32 | train loss: 0.914 | val loss: 1.048\nE33 | train loss: 0.913 | val loss: 1.060\nE34 | train loss: 0.900 | val loss: 1.236\nE35 | train loss: 0.890 | val loss: 1.030\nE36 | train loss: 0.901 | val loss: 1.066\nE37 | train loss: 0.895 | val loss: 1.055\nE38 | train loss: 0.914 | val loss: 1.030\nE39 | train loss: 0.894 | val loss: 1.024\nE40 | train loss: 0.903 | val loss: 1.056\nE41 | train loss: 0.897 | val loss: 1.089\nE42 | train loss: 0.917 | val loss: 1.031\nE43 | train loss: 0.904 | val loss: 1.105\nE44 | train loss: 0.894 | val loss: 1.284\nE45 | train loss: 0.903 | val loss: 1.648\nE46 | train loss: 0.904 | val loss: 1.042\nE47 | train loss: 0.909 | val loss: 1.052\nE48 | train loss: 0.891 | val loss: 1.186\nE49 | train loss: 0.891 | val loss: 1.075\n\n\n\ncnn_data_label = (cnn_train_losses,cnn_val_losses,\"CNN\")\nquick_loss_plot([lin_data_label,cnn_data_label])\n\n\n\n\n\n\n\n\nIt seems clear from the loss curves that the CNN is able to capture a pattern in the data that the Linear model is not! Let‚Äôs spot check a few sequences to see what‚Äôs going on.\n\n# oracle dict of true score for each seq\noracle = dict(mer8[['seq','score']].values)\n\ndef quick_seq_pred(model, desc, seqs, oracle):\n    '''\n    Given a model and some sequences, get the model's predictions\n    for those sequences and compare to the oracle (true) output\n    '''\n    print(f\"__{desc}__\")\n    for dna in seqs:\n        s = torch.tensor(one_hot_encode(dna)).unsqueeze(0).to(DEVICE)\n        pred = model(s.float())\n        actual = oracle[dna]\n        diff = pred.item() - actual\n        print(f\"{dna}: pred:{pred.item():.3f} actual:{actual:.3f} ({diff:.3f})\")\n\ndef quick_8mer_pred(model, oracle):\n    seqs1 = (\"poly-X seqs\",['AAAAAAAA', 'CCCCCCCC','GGGGGGGG','TTTTTTTT'])\n    seqs2 = (\"other seqs\", ['AACCAACA','CCGGTGAG','GGGTAAGG', 'TTTCGTTT'])\n    seqsTAT = (\"with TAT motif\", ['TATAAAAA','CCTATCCC','GTATGGGG','TTTATTTT'])\n    seqsGCG = (\"with GCG motif\", ['AAGCGAAA','CGCGCCCC','GGGCGGGG','TTGCGTTT'])\n    TATGCG =  (\"both TAT and GCG\",['ATATGCGA','TGCGTATT'])\n\n    for desc,seqs in [seqs1, seqs2, seqsTAT, seqsGCG, TATGCG]:\n        quick_seq_pred(model, desc, seqs, oracle)\n        print()\n\n\n# Ask the trained Linear model to make \n# predictions for some 8-mers\nquick_8mer_pred(model_lin, oracle)\n\n__poly-X seqs__\nAAAAAAAA: pred:23.415 actual:20.000 (3.415)\nCCCCCCCC: pred:13.762 actual:17.000 (-3.238)\nGGGGGGGG: pred:7.189 actual:14.000 (-6.811)\nTTTTTTTT: pred:17.841 actual:11.000 (6.841)\n\n__other seqs__\nAACCAACA: pred:18.987 actual:18.875 (0.112)\nCCGGTGAG: pred:12.330 actual:15.125 (-2.795)\nGGGTAAGG: pred:14.006 actual:15.125 (-1.119)\nTTTCGTTT: pred:14.945 actual:12.125 (2.820)\n\n__with TAT motif__\nTATAAAAA: pred:22.317 actual:27.750 (-5.433)\nCCTATCCC: pred:17.059 actual:25.875 (-8.816)\nGTATGGGG: pred:12.329 actual:24.000 (-11.671)\nTTTATTTT: pred:18.331 actual:22.125 (-3.794)\n\n__with GCG motif__\nAAGCGAAA: pred:16.972 actual:8.125 (8.847)\nCGCGCCCC: pred:12.467 actual:6.250 (6.217)\nGGGCGGGG: pred:8.121 actual:4.375 (3.746)\nTTGCGTTT: pred:13.014 actual:2.500 (10.514)\n\n__both TAT and GCG__\nATATGCGA: pred:15.874 actual:15.875 (-0.001)\nTGCGTATT: pred:14.779 actual:13.625 (1.154)\n\n\n\nFrom the above examples, it appears that the Linear model is really underpredicting sequences with a lot of G‚Äôs and overpredicting those with many T‚Äôs. This is probably because it noticed GCG made sequences have unusually low scores and TAT made sequences have unusually high scores, however since the Linear model doesn‚Äôt have a way to take into account the different context of GCG vs GAG, it just predicts that sequences with G‚Äôs should be lower. We know from our scoring scheme that this isn‚Äôt the case: it‚Äôs not that G‚Äôs in general are detrimental, but specifically GCG is.\n\n# Ask the trained CNN model to make \n# predictions for some 8-mers\nquick_8mer_pred(model_cnn, oracle)\n\n__poly-X seqs__\nAAAAAAAA: pred:19.649 actual:20.000 (-0.351)\nCCCCCCCC: pred:16.750 actual:17.000 (-0.250)\nGGGGGGGG: pred:13.576 actual:14.000 (-0.424)\nTTTTTTTT: pred:10.895 actual:11.000 (-0.105)\n\n__other seqs__\nAACCAACA: pred:18.645 actual:18.875 (-0.230)\nCCGGTGAG: pred:14.759 actual:15.125 (-0.366)\nGGGTAAGG: pred:15.118 actual:15.125 (-0.007)\nTTTCGTTT: pred:11.789 actual:12.125 (-0.336)\n\n__with TAT motif__\nTATAAAAA: pred:26.103 actual:27.750 (-1.647)\nCCTATCCC: pred:24.256 actual:25.875 (-1.619)\nGTATGGGG: pred:22.838 actual:24.000 (-1.162)\nTTTATTTT: pred:20.555 actual:22.125 (-1.570)\n\n__with GCG motif__\nAAGCGAAA: pred:9.007 actual:8.125 (0.882)\nCGCGCCCC: pred:7.091 actual:6.250 (0.841)\nGGGCGGGG: pred:5.275 actual:4.375 (0.900)\nTTGCGTTT: pred:3.411 actual:2.500 (0.911)\n\n__both TAT and GCG__\nATATGCGA: pred:15.389 actual:15.875 (-0.486)\nTGCGTATT: pred:13.185 actual:13.625 (-0.440)\n\n\n\nThe CNN however is better able to adapt to the differences between 3-mer motifs! It predicts quite well on both the sequences with and without motifs."
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-2",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-2",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "Question 2",
    "text": "Question 2\nCompare the performance of the Linear and CNN models by using different learning rates. First run both models with higher learning rates (0.05, 0.1) and lower learning rates (0.005, 0.001), then create loss plots showing: - Linear model with these learning rates - CNN model with these learning rates\nThen analyze your results by answering: 1. How does changing the learning rate affect convergence for each model? 2. Which model is more sensitive to learning rate changes, and why? 3. Based on your analysis, what learning rate would you recommend for each model type, and why?\n ## 6. Check model predictions on the test set An important evaluation step in machine learning tasks is to check if your model can make good predictions on the test set, which it never saw during training. Here, we can use a parity plot to visualize the difference between the actual sequence scores vs the model‚Äôs predicted scores.\n\n#%pip install altair ## datapane\n\n\nimport altair as alt\nfrom sklearn.metrics import r2_score\n## import datapane as dp ## compatibility issues with pandas version\nimport os\n\n\n\ndef parity_plot(model_name,df,r2):\n    '''\n    Given a dataframe of samples with their true and predicted values,\n    make a scatterplot.\n    '''\n    plt.scatter(df['truth'].values, df['pred'].values, alpha=0.2)\n    \n    # y=x line\n    xpoints = ypoints = plt.xlim()\n    plt.plot(xpoints, ypoints, linestyle='--', color='k', lw=2, scalex=False, scaley=False)\n\n    plt.ylim(xpoints)\n    plt.ylabel(\"Predicted Score\",fontsize=14)\n    plt.xlabel(\"Actual Score\",fontsize=14)\n    plt.title(f\"{model_name} (r2:{r2:.3f})\",fontsize=20)\n    plt.show()\n\n\ndef alt_parity_plot(model, df, r2, datapane=False):\n    '''\n    Make an interactive parity plot with altair\n    '''\n    import os\n    import altair as alt\n    \n    os.makedirs('alt_out', exist_ok=True)\n    \n    # Convert model name to string to avoid any issues\n    model = str(model)\n    \n    # Create a clean version of the dataframe\n    plot_df = pd.DataFrame({\n        'truth': df['truth'].astype(float),\n        'pred': df['pred'].astype(float),\n        'seq': df['seq'].astype(str)\n    })\n    \n    # Create chart\n    chart = alt.Chart(plot_df).mark_point().encode(\n        x=alt.X('truth', type='quantitative', title='True Values'),\n        y=alt.Y('pred', type='quantitative', title='Predictions'),\n        tooltip=['seq']\n    ).properties(\n        title=str(f'{model} (r2:{r2:.3f})')\n    )\n    \n    chart.save(f'alt_out/parity_plot_{model}.html')\n    display(chart)\n\n\ndef parity_pred(models, seqs, oracle,alt=False,datapane=False):\n    '''Given some sequences, get the model's predictions '''\n    dfs = {} # key: model name, value: parity_df\n    \n    \n    for model_name,model in models:\n        print(f\"Running {model_name}\")\n        data = []\n        for dna in seqs:\n            s = torch.tensor(one_hot_encode(dna)).unsqueeze(0).to(DEVICE)\n            actual = oracle[dna]\n            pred = model(s.float())\n            data.append([dna,actual,pred.item()])\n        df = pd.DataFrame(data, columns=['seq','truth','pred'])\n        r2 = r2_score(df['truth'],df['pred'])\n        dfs[model_name] = (r2,df)\n        \n        #plot parity plot\n        if alt: # make an altair plot\n            alt_parity_plot(model_name, df, r2,datapane=datapane)\n            \n        else:\n            parity_plot(model_name, df, r2)\n\n\nseqs = test_df['seq'].values\nmodels = [\n    (\"Linear\", model_lin),\n    (\"CNN\", model_cnn)\n]\nparity_pred(models, seqs, oracle)\n\nRunning Linear\n\n\n\n\n\n\n\n\n\nRunning CNN\n\n\n\n\n\n\n\n\n\nParity plots are useful for visualizing how well your model predicts individual sequences: in a perfect model, they would all land on the y=x line, meaning that the model prediction was exactly the sequence‚Äôs actual value. But if it is off the y=x line, it means the model is over- or under-predicting.\nIn the Linear model, we can see that it can somewhat predict a trend in the Test set sequences, but really gets confused by these buckets of sequences in the high and low areas of the distribution (the ones with a motif).\nHowever for the CNN, it is much better at predicting scores close to the actual value! This is expected, given that the architecture of our CNN uses 3-mer kernels to scan along the sequence for influential motifs.\nBut the CNN isn‚Äôt perfect. We could probably train it longer or adjust the hyperparameters, but the goal here isn‚Äôt perfection - this is a very simple task relative to actual regulatory grammars. Instead, I thought it would be interesting to use the Altair visualization library to interactively inspect which sequences the models get wrong:\n\nalt.data_transformers.disable_max_rows() # disable altair warning\nparity_pred(models, seqs, oracle,alt=True)\n\nRunning Linear\n\n\n\n\n\n\n\n\nRunning CNN\n\n\n\n\n\n\n\n\nIf you‚Äôre viewing this notebook in interactive mode and run the above cell (just viewing via the github preview will omit the altair plot in the rendering), you can hover over the points and see the individual 8-mer sequences (you can also pan and zoom in this plot).\nNotice that the sequences that are off the diagonal tend to have multiple instance of the motifs! In the scoring function, we only gave the sequence a +/- bump if it had at least 1 motif, but it certainly would have been reasonable to decide to add multiple bonuses if the motif was present multiple times. In this example, I arbitrarily only added the bonus for at least 1 motif occurrence, but we could have made a different scoring function.\nIn any case, I thought it was cool that the model noticed the multiple occurrences and predicted them to be important. I suppose we did fool it a little, though an R2 of 0.95 is pretty respectable :)\n ## 7. Visualize convolutional filters When training CNN models, it can be useful to visualize the first layer convolutional filters to try to understand more about what the model is learning. With image data, the first layer convolutional filters often learn patterns such as borders or colors or textures - basic image elements that can be recombined to make more complex features.\nIn DNA, convolutional filters can be thought of like motif scanners. Similar to a position weight matrix for visualizing sequence logos, a convolutional filter is like a matrix showing a particular DNA pattern, but instead of being an exact sequence, it can hold some uncertainty about which nucleotides show up in which part of the pattern. Some positions might be very certain (i.e., there‚Äôs always an A in position 2; high information content) while other positions could hold a variety of nucleotides with about equal probability (high entropy; low information content).\nThe calculations that occur within the hidden layers of neural networks can get very complex and not every convolutional filter will be an obviously relevant pattern, but sometimes patterns in the filters do emerge and can be informative for helping to explain the model‚Äôs predictions.\nBelow are some functions to visualize the first layer convolutional filters, both as a raw heatmap and as a motif logo."
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#section",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#section",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "===",
    "text": "===\nVisualizing Convolutional Filters When training convolutional neural networks (CNNs), it‚Äôs often helpful to visualize the filters in the first convolutional layer to better understand what features the model is learning from the input data.\nIn image data:\nThe first-layer filters commonly learn simple, low-level features like:\nEdges or borders Color gradients Textures or basic shapes\nThese simple elements can be combined in deeper layers to form more complex patterns, such as object parts or entire shapes.\nüß¨ In DNA sequence data:\nConvolutional filters act similarly to motif detectors. They can learn biologically meaningful patterns in the sequence, much like:\n\nPosition Weight Matrices (PWMs)\nSequence logos\n\nEach filter can be thought of as a matrix scanning for a specific DNA pattern ‚Äî not necessarily a fixed sequence, but one that may allow for some variation in certain positions.\nFor example:\nOne position in the filter might strongly prefer an ‚ÄúA‚Äù (indicating low entropy and high information content). Another position might tolerate any nucleotide (high entropy, low information content). These learned filters can sometimes correspond to known biological motifs, or reveal novel sequence patterns that are predictive for the task at hand (e.g., enhancer activity, binding sites, expression levels)."
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-3",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-3",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "Question 3",
    "text": "Question 3\nDesign an approach to improve the model‚Äôs prediction accuracy, particularly focusing on the sequences where the current model performs poorly:\n\nAfter identifying sequences where the CNN model has high prediction errors, propose and implement a modification to either the model architecture, the loss function, the training process, or the data representation\nRetrain the model with your modifications\nCreate comparative visualizations (such as parity plots, error histograms, or other appropriate plots) to demonstrate the impact of your changes\nAnalyze your results by discussing how your modification addresses the specific weaknesses you identified. What are the trade-offs involved in your approach?\n\n\nimport logomaker\n\n\ndef get_conv_layers_from_model(model):\n    '''\n    Given a trained model, extract its convolutional layers\n    '''\n    model_children = list(model.children())\n    \n    # counter to keep count of the conv layers\n    model_weights = [] # we will save the conv layer weights in this list\n    conv_layers = [] # we will save the actual conv layers in this list\n    bias_weights = []\n    counter = 0 \n\n    # append all the conv layers and their respective weights to the list\n    for i in range(len(model_children)):\n        # get model type of Conv1d\n        if type(model_children[i]) == nn.Conv1d:\n            counter += 1\n            model_weights.append(model_children[i].weight)\n            conv_layers.append(model_children[i])\n            bias_weights.append(model_children[i].bias)\n\n        # also check sequential objects' children for conv1d\n        elif type(model_children[i]) == nn.Sequential:\n            for child in model_children[i]:\n                if type(child) == nn.Conv1d:\n                    counter += 1\n                    model_weights.append(child.weight)\n                    conv_layers.append(child)\n                    bias_weights.append(child.bias)\n\n    print(f\"Total convolutional layers: {counter}\")\n    return conv_layers, model_weights, bias_weights\n\ndef view_filters(model_weights, num_cols=8):\n    model_weights = model_weights[0]\n    num_filt = model_weights.shape[0]\n    filt_width = model_weights[0].shape[1]\n    num_rows = int(np.ceil(num_filt/num_cols))\n    \n    # visualize the first conv layer filters\n    plt.figure(figsize=(20, 17))\n\n    for i, filter in enumerate(model_weights):\n        ax = plt.subplot(num_rows, num_cols, i+1)\n        ax.imshow(filter.cpu().detach(), cmap='gray')\n        ax.set_yticks(np.arange(4))\n        ax.set_yticklabels(['A', 'C', 'G','T'])\n        ax.set_xticks(np.arange(filt_width))\n        ax.set_title(f\"Filter {i}\")\n\n    plt.tight_layout()\n    plt.show()\n\nFirst, we can take a peek at the raw filters.\n\nconv_layers, model_weights, bias_weights = get_conv_layers_from_model(model_cnn)\nview_filters(model_weights)\n\nTotal convolutional layers: 1\n\n\n\n\n\n\n\n\n\n\n\ndef get_conv_output_for_seq(seq, conv_layer):\n    '''\n    Given an input sequeunce and a convolutional layer, \n    get the output tensor containing the conv filter \n    activations along each position in the sequence\n    '''\n    # format seq for input to conv layer (OHE, reshape)\n    seq = torch.tensor(one_hot_encode(seq)).unsqueeze(0).permute(0,2,1).to(DEVICE)\n\n    # run seq through conv layer\n    with torch.no_grad(): # don't want as part of gradient graph\n        # apply learned filters to input seq\n        res = conv_layer(seq.float())\n        return res[0]\n    \n\ndef get_filter_activations(seqs, conv_layer,act_thresh=0):\n    '''\n    Given a set of input sequences and a trained convolutional layer, \n    determine the subsequences for which each filter in the conv layer \n    activate most strongly. \n    \n    1.) Run seq inputs through conv layer. \n    2.) Loop through filter activations of the resulting tensor, saving the\n            position where filter activations were &gt; act_thresh. \n    3.) Compile a count matrix for each filter by accumulating subsequences which\n            activate the filter above the threshold act_thresh\n    '''\n    # initialize dict of pwms for each filter in the conv layer\n    # pwm shape: 4 nucleotides X filter width, initialize to 0.0s\n    num_filters = conv_layer.out_channels\n    filt_width = conv_layer.kernel_size[0]\n    filter_pwms = dict((i,torch.zeros(4,filt_width)) for i in range(num_filters))\n    \n    print(\"Num filters\", num_filters)\n    print(\"filt_width\", filt_width)\n    \n    # loop through a set of sequences and collect subseqs where each filter activated\n    for seq in seqs:\n        # get a tensor of each conv filter activation along the input seq\n        res = get_conv_output_for_seq(seq, conv_layer)\n\n        # for each filter and it's activation vector\n        for filt_id,act_vec in enumerate(res):\n            # collect the indices where the activation level \n            # was above the threshold\n            act_idxs = torch.where(act_vec&gt;act_thresh)[0]\n            activated_positions = [x.item() for x in act_idxs]\n\n            # use activated indicies to extract the actual DNA\n            # subsequences that caused filter to activate\n            for pos in activated_positions:\n                subseq = seq[pos:pos+filt_width]\n                #print(\"subseq\",pos, subseq)\n                # transpose OHE to match PWM orientation\n                subseq_tensor = torch.tensor(one_hot_encode(subseq)).T\n\n                # add this subseq to the pwm count for this filter\n                filter_pwms[filt_id] += subseq_tensor            \n            \n    return filter_pwms\n\ndef view_filters_and_logos(model_weights,filter_activations, num_cols=8):\n    '''\n    Given some convolutional model weights and filter activation PWMs, \n    visualize the heatmap and motif logo pairs in a simple grid\n    '''\n    model_weights = model_weights[0].squeeze(1)\n    print(model_weights.shape)\n\n    # make sure the model weights agree with the number of filters\n    assert(model_weights.shape[0] == len(filter_activations))\n    \n    num_filts = len(filter_activations)\n    num_rows = int(np.ceil(num_filts/num_cols))*2+1 \n    # ^ not sure why +1 is needed... complained otherwise\n    \n    plt.figure(figsize=(20, 17))\n\n    j=0 # use to make sure a filter and it's logo end up vertically paired\n    for i, filter in enumerate(model_weights):\n        if (i)%num_cols == 0:\n            j += num_cols\n\n        # display raw filter\n        ax1 = plt.subplot(num_rows, num_cols, i+j+1)\n        ax1.imshow(filter.cpu().detach(), cmap='gray')\n        ax1.set_yticks(np.arange(4))\n        ax1.set_yticklabels(['A', 'C', 'G','T'])\n        ax1.set_xticks(np.arange(model_weights.shape[2]))\n        ax1.set_title(f\"Filter {i}\")\n\n        # display sequence logo\n        ax2 = plt.subplot(num_rows, num_cols, i+j+1+num_cols)\n        filt_df = pd.DataFrame(filter_activations[i].T.numpy(),columns=['A','C','G','T'])\n        filt_df_info = logomaker.transform_matrix(filt_df,from_type='counts',to_type='information')\n        logo = logomaker.Logo(filt_df_info,ax=ax2)\n        ax2.set_ylim(0,2)\n        ax2.set_title(f\"Filter {i}\")\n\n    plt.tight_layout()\n\n\n# just use some seqs from test_df to activate filters\nsome_seqs = random.choices(seqs, k=3000)\n\nfilter_activations = get_filter_activations(some_seqs, conv_layers[0])\nview_filters_and_logos(model_weights,filter_activations)\n\nNum filters 32\nfilt_width 3\ntorch.Size([32, 4, 3])\n\n\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n\n\n\n\n\n\n\n\n\n\nVisualize filters using a stronger activation threshold\nact_thresh = 1 instead of 0. (Some filters have no subsequence matches above the threshold and result in an empty motif logo)\n\nfilter_activations = get_filter_activations(some_seqs, conv_layers[0],act_thresh=1)\nview_filters_and_logos(model_weights,filter_activations)\n\nNum filters 32\nfilt_width 3\ntorch.Size([32, 4, 3])\n\n\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n\n\n\n\n\n\n\n\n\nFrom this particular CNN training, we can see a few filters have picked up on the strong TAT and GCG motifs, but other filters have focused on other patterns as well. There is some debate about how relevant convolutional filter visualizations are for model interpretability. In deep models with multiple convolutional layers, convolutional filters can be recombined in more complex ways inside the hidden layers, so the first layer filters may not be as informative on their own (Koo and Eddy, 2019). Much of the field has since moved towards attention mechanisms and other explainability methods, but should you be curious to visualize your filters as potential motifs, these functions may help get you started!\n ## 8. Conclusion This tutorial shows some basic Pytorch structure for building CNN models that work with DNA sequences. The practice task used in this demo is not reflective of real biological signals; rather, we designed the scoring method to simulate the presence of regulatory motifs in very short sequences that were easy for us humans to inspect and verify that Pytorch was behaving as expected. From this small example, we observed how a basic CNN with sliding filters was able to predict our scoring scheme better than a basic linear model that only accounted for absolute nucleotide position (without local context).\nTo read more about CNN‚Äôs applied to DNA in the wild, check out the following foundational papers: * DeepBind: Alipanahi et al 2015 * DeepSea: Zhou and Troyanskaya 2015 * Basset: Kelley et al 2016\nI hope other new-to-ML folks interested in tackling biological questions may find this helpful for getting started with using Pytorch to model DNA sequences :)"
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-prediction-starter.html",
    "href": "post/2025-03-25-unit00/tf-binding-prediction-starter.html",
    "title": "TF Binding prediction project",
    "section": "",
    "text": "TF binding prediction model\nThe goal of this project is to create a neural network model that predicts TF binding strength in a DNA sequence.\nTo do this, we have extracted 300 base pair-long DNA sequences that have a predicted binding site(s) from a TF, from a couple of chromosomes.\nThe training data is the following:\n\nThe sequences files: chr#_sequences.txt.gz store the 300 bp-long DNA sequences. A ‚Äúwindow_name‚Äù in the format chr#_start_end has been assigned to each one.\nThe scores files: chr#_scores.txt.gz store a 300 bases long vector for each DNA sequence. Each position in these vectors correspond to a the sequence position. The values for each position represent the ‚Äúbinding score‚Äù that was predicted to that site by Homer, which is a widely used tool to discover motif binding sites for a given TF across the genome.\n\n\n1. Read-in the data\nThe data files for a couple of chromosomes are stored in the following link. Download them to your local folder.\nLet‚Äôs explore how the sequence and score data look like:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\n\nDefine data paths, this should be changed to your personal paths:\n\nPROJECT = '/Users/sofiasalazar/Library/CloudStorage/Box-Box/imlab-data/Courses/AI-in-Genomics-2025/'\nDATA = os.path.join(PROJECT, 'data')\n\nSequence data for chromosome 22\n\nsequences = pd.read_csv(os.path.join(DATA, 'chr22_sequences.txt.gz'), sep=\"\\t\", compression='gzip')\n\n\nsequences.head()\n\n\n\n\n\n\n\n\nsequence\nwindow_name\n\n\n\n\n0\nGCAAGACTCAGTCTCAAGGAAAAAAAAAAGCTCGAAAAATGTTTGC...\nchr22_10510500_10510799\n\n\n1\nAATCAAAAAGAATATTAGAAAACAAGCTGACAAAAAAATAAAAAAA...\nchr22_10512900_10513199\n\n\n2\nAGAAAAAGATATAAAGGCATCCAAATTGGAAAGGAAGAAGTAAGTA...\nchr22_10514100_10514399\n\n\n3\nCAAATGGATTGAAGACTTAAATGTAAGAACTAAAGCTGTAAAACTA...\nchr22_10515300_10515599\n\n\n4\nAAAATAGACCTACCATATGATGCAGCAATCCCACTTGTGGGCATTT...\nchr22_10515900_10516199\n\n\n\n\n\n\n\n\nsequences.shape\n\n(23139, 2)\n\n\nTF binding scores for chromosome 22\nHere, each column has 300 values for each sequence, each value is the TF binding score for each position of the sequence. Most positions have ‚Äò0‚Äô as no motif is predicted to bind at those positions. One motif is a couple of bp-long and all of those bp will have the same score since they belong to the same motif.\n\nscores = pd.read_csv(os.path.join(DATA, 'chr22_scores.txt.gz'), sep=\"\\t\", compression='gzip')\n\n\nnp.array(scores.iloc[:, 0])\n\narray([0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 9.708916, 9.708916, 9.708916,\n       9.708916, 9.708916, 9.708916, 9.708916, 9.708916, 9.708916,\n       9.708916, 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       7.859208, 7.859208, 7.859208, 7.859208, 7.859208, 7.859208,\n       7.859208, 7.859208, 7.859208, 7.859208, 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       7.693852, 7.693852, 7.693852, 7.693852, 7.693852, 7.693852,\n       7.693852, 7.693852, 7.693852, 7.693852, 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ])\n\n\nTake a look at how these score vectors look like, the blue sections represent the predicted binding sites for this TF:\n\nx = np.arange(300)\nbar_width = 0.4\nplt.figure(figsize=(12, 5))\nplt.bar(x - bar_width, scores.iloc[:, 0], width=bar_width, label=\"Predicted\", alpha=0.7, color='b')\nplt.xlabel(\"Position in sequence window\")\nplt.ylabel(\"Homer score\")\nplt.grid(axis='y', linestyle='--', alpha=0.6)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2. Model training\nNow, the goal is to use these sequences to train a neural network model that predicts the scores vectors. Overall, the structure of the code should more or less follow these steps:\n\nOne-hot-encode the DNA sequences\nSplit sequences and their corresponding scores into training, test and validation sets\nBuild dataloaders for the training and test sets using sequences as predictor features and the scores as targets\nDefine a NN model architecture\nTrain the model\nTest the model on the test sequences\n\nThis process will be iterative as you find an optimal set of hyperparameters. Please share your best-performing model and we will test it on a set of held-out-data.\nAdditional notes\n\nNote how the scores are values greater than 1, you can try binarizing these values so they are between 0 and 1 and compare between models\nTo assess performance, you can use the following code to compute correlations between predicted scores and the ground truth\n\n\nfrom scipy.stats import pearsonr\ndef plot_comparison(pred, obs):\n  r_value = pearsonr(pred, obs)\n  x = np.arange(len(pred))\n  bar_width = 0.4\n  plt.figure(figsize=(12, 5))\n  plt.bar(x - bar_width, pred, width=bar_width, label=\"Predicted\", alpha=0.7, color='b')\n  plt.bar(x + bar_width, obs, width=bar_width, label=\"Observed\", alpha=0.7, color='r')\n  plt.xlabel(\"Position sequence window\")\n  plt.ylabel(\"Value\")\n  plt.title(\"Comparison of sequence scores\")\n  plt.legend(title=f\"Pearson R: {r_value:.2f}\")\n  plt.grid(axis='y', linestyle='--', alpha=0.6)\n  plt.show()\n\n\n\n\n\n\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "DRAFTS/measuring-info-in-DNA-seq.html",
    "href": "DRAFTS/measuring-info-in-DNA-seq.html",
    "title": "Measuring information in DNA sequence",
    "section": "",
    "text": "In the analysis of DNA sequences, particularly when studying motifs and patterns, two key concepts from information theory are entropy and information content. These measures help quantify the variability and conservation of nucleotides at specific positions within a set of aligned sequences.\n\nEntropy (H):\n\nEntropy measures the uncertainty or randomness at a given position. For DNA with four possible bases (A, C, G, T) and their respective probabilities at a position being p(A), p(C), p(G), and p(T) (where p(A) + p(C) + p(G) + p(T) = 1), the Shannon entropy (H) in bits is calculated using the following formula:\nH = - (p(A) * log2(p(A)) + p(C) * log2(p(C)) + p(G) * log2(p(G)) + p(T) * log2(p(T))) A higher entropy value indicates greater uncertainty or variability in the nucleotides at that position. A lower entropy value indicates less uncertainty, meaning one or a few nucleotides are more dominant. The maximum possible entropy for DNA (when all four bases are equally likely) is 2 bits. The minimum possible entropy (when only one base is present) is 0 bits. 2. Information Content (IC):\nInformation content measures the amount of information gained by knowing the nucleotide at a particular position. It reflects the conservation of that position relative to a background distribution (often assumed to be uniform). The information content (IC) in bits is calculated as:\nIC = log2(N) - H Where:\nN is the number of possible nucleotides (4 for DNA).\nlog2(N) represents the maximum possible information content (2 bits for DNA).\nH is the Shannon entropy calculated for that position.\nA higher information content value indicates greater conservation and importance of specific nucleotides at that position.\nA lower information content value indicates less conservation and more variability.\nThe maximum possible information content for DNA is 2 bits (when entropy is 0).\nThe minimum possible information content (when entropy is maximum) is 0 bits.\n\nimport math\n\ndef calculate_entropy(pA, pC, pG, pT):\n    \"\"\"\n    Calculates the Shannon entropy for DNA bases given their probabilities.\n\n    Args:\n        pA (float): Probability of Adenine (A).\n        pC (float): Probability of Cytosine (C).\n        pG (float): Probability of Guanine (G).\n        pT (float): Probability of Thymine (T).\n\n    Returns:\n        float: The Shannon entropy in bits.\n    \"\"\"\n    entropy = 0\n    if pA &gt; 0:\n        entropy -= pA * math.log2(pA)\n    if pC &gt; 0:\n        entropy -= pC * math.log2(pC)\n    if pG &gt; 0:\n        entropy -= pG * math.log2(pG)\n    if pT &gt; 0:\n        entropy -= pT * math.log2(pT)\n    return entropy\n\ndef calculate_information_content(pA, pC, pG, pT):\n    \"\"\"\n    Calculates the information content (in bits) at a given position\n    based on the nucleotide probabilities.\n\n    Args:\n        pA (float): Probability of Adenine (A).\n        pC (float): Probability of Cytosine (C).\n        pG (float): Probability of Guanine (G).\n        pT (float): Probability of Thymine (T).\n\n    Returns:\n        float: The information content in bits.\n    \"\"\"\n    num_nucleotides = 4  # A, C, G, T\n    max_entropy = math.log2(num_nucleotides)  # Maximum entropy for 4 equally likely bases\n    entropy = calculate_entropy(pA, pC, pG, pT)\n    information_content = max_entropy - entropy\n    return information_content\n\n# Example Usage:\nprobability_A = 0.3\nprobability_C = 0.2\nprobability_G = 0.25\nprobability_T = 0.25\n\nentropy_value = calculate_entropy(probability_A, probability_C, probability_G, probability_T)\ninformation_content_value = calculate_information_content(probability_A, probability_C, probability_G, probability_T)\n\nprint(f\"Probabilities: p(A)={probability_A:.2f}, p(C)={probability_C:.2f}, p(G)={probability_G:.2f}, p(T)={probability_T:.2f}\")\nprint(f\"Entropy: {entropy_value:.4f} bits\")\nprint(f\"Information Content: {information_content_value:.4f} bits\")\n\n# Example with uniform distribution:\nentropy_uniform = calculate_entropy(0.25, 0.25, 0.25, 0.25)\ninformation_content_uniform = calculate_information_content(0.25, 0.25, 0.25, 0.25)\nprint(f\"\\nProbabilities (uniform): p(A)=0.25, p(C)=0.25, p(G)=0.25, p(T)=0.25\")\nprint(f\"Entropy (uniform): {entropy_uniform:.4f} bits\")\nprint(f\"Information Content (uniform): {information_content_uniform:.4f} bits\")\n\n# Example with a fully conserved position:\nentropy_conserved = calculate_entropy(1.0, 0.0, 0.0, 0.0)\ninformation_content_conserved = calculate_information_content(1.0, 0.0, 0.0, 0.0)\nprint(f\"\\nProbabilities (fully conserved): p(A)=1.00, p(C)=0.00, p(G)=0.00, p(T)=0.00\")\nprint(f\"Entropy (fully conserved): {entropy_conserved:.4f} bits\")\nprint(f\"Information Content (fully conserved): {information_content_conserved:.4f} bits\")\n\nProbabilities: p(A)=0.30, p(C)=0.20, p(G)=0.25, p(T)=0.25\nEntropy: 1.9855 bits\nInformation Content: 0.0145 bits\n\nProbabilities (uniform): p(A)=0.25, p(C)=0.25, p(G)=0.25, p(T)=0.25\nEntropy (uniform): 2.0000 bits\nInformation Content (uniform): 0.0000 bits\n\nProbabilities (fully conserved): p(A)=1.00, p(C)=0.00, p(G)=0.00, p(T)=0.00\nEntropy (fully conserved): 0.0000 bits\nInformation Content (fully conserved): 2.0000 bits\n\n\n** text and code generated with gemini 2.0 via a series of prompting **\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "DRAFTS/measuring-info-in-DNA-seq.html#understanding-entropy-and-information-content-in-dna-sequence-analysis",
    "href": "DRAFTS/measuring-info-in-DNA-seq.html#understanding-entropy-and-information-content-in-dna-sequence-analysis",
    "title": "Measuring information in DNA sequence",
    "section": "",
    "text": "In the analysis of DNA sequences, particularly when studying motifs and patterns, two key concepts from information theory are entropy and information content. These measures help quantify the variability and conservation of nucleotides at specific positions within a set of aligned sequences.\n\nEntropy (H):\n\nEntropy measures the uncertainty or randomness at a given position. For DNA with four possible bases (A, C, G, T) and their respective probabilities at a position being p(A), p(C), p(G), and p(T) (where p(A) + p(C) + p(G) + p(T) = 1), the Shannon entropy (H) in bits is calculated using the following formula:\nH = - (p(A) * log2(p(A)) + p(C) * log2(p(C)) + p(G) * log2(p(G)) + p(T) * log2(p(T))) A higher entropy value indicates greater uncertainty or variability in the nucleotides at that position. A lower entropy value indicates less uncertainty, meaning one or a few nucleotides are more dominant. The maximum possible entropy for DNA (when all four bases are equally likely) is 2 bits. The minimum possible entropy (when only one base is present) is 0 bits. 2. Information Content (IC):\nInformation content measures the amount of information gained by knowing the nucleotide at a particular position. It reflects the conservation of that position relative to a background distribution (often assumed to be uniform). The information content (IC) in bits is calculated as:\nIC = log2(N) - H Where:\nN is the number of possible nucleotides (4 for DNA).\nlog2(N) represents the maximum possible information content (2 bits for DNA).\nH is the Shannon entropy calculated for that position.\nA higher information content value indicates greater conservation and importance of specific nucleotides at that position.\nA lower information content value indicates less conservation and more variability.\nThe maximum possible information content for DNA is 2 bits (when entropy is 0).\nThe minimum possible information content (when entropy is maximum) is 0 bits.\n\nimport math\n\ndef calculate_entropy(pA, pC, pG, pT):\n    \"\"\"\n    Calculates the Shannon entropy for DNA bases given their probabilities.\n\n    Args:\n        pA (float): Probability of Adenine (A).\n        pC (float): Probability of Cytosine (C).\n        pG (float): Probability of Guanine (G).\n        pT (float): Probability of Thymine (T).\n\n    Returns:\n        float: The Shannon entropy in bits.\n    \"\"\"\n    entropy = 0\n    if pA &gt; 0:\n        entropy -= pA * math.log2(pA)\n    if pC &gt; 0:\n        entropy -= pC * math.log2(pC)\n    if pG &gt; 0:\n        entropy -= pG * math.log2(pG)\n    if pT &gt; 0:\n        entropy -= pT * math.log2(pT)\n    return entropy\n\ndef calculate_information_content(pA, pC, pG, pT):\n    \"\"\"\n    Calculates the information content (in bits) at a given position\n    based on the nucleotide probabilities.\n\n    Args:\n        pA (float): Probability of Adenine (A).\n        pC (float): Probability of Cytosine (C).\n        pG (float): Probability of Guanine (G).\n        pT (float): Probability of Thymine (T).\n\n    Returns:\n        float: The information content in bits.\n    \"\"\"\n    num_nucleotides = 4  # A, C, G, T\n    max_entropy = math.log2(num_nucleotides)  # Maximum entropy for 4 equally likely bases\n    entropy = calculate_entropy(pA, pC, pG, pT)\n    information_content = max_entropy - entropy\n    return information_content\n\n# Example Usage:\nprobability_A = 0.3\nprobability_C = 0.2\nprobability_G = 0.25\nprobability_T = 0.25\n\nentropy_value = calculate_entropy(probability_A, probability_C, probability_G, probability_T)\ninformation_content_value = calculate_information_content(probability_A, probability_C, probability_G, probability_T)\n\nprint(f\"Probabilities: p(A)={probability_A:.2f}, p(C)={probability_C:.2f}, p(G)={probability_G:.2f}, p(T)={probability_T:.2f}\")\nprint(f\"Entropy: {entropy_value:.4f} bits\")\nprint(f\"Information Content: {information_content_value:.4f} bits\")\n\n# Example with uniform distribution:\nentropy_uniform = calculate_entropy(0.25, 0.25, 0.25, 0.25)\ninformation_content_uniform = calculate_information_content(0.25, 0.25, 0.25, 0.25)\nprint(f\"\\nProbabilities (uniform): p(A)=0.25, p(C)=0.25, p(G)=0.25, p(T)=0.25\")\nprint(f\"Entropy (uniform): {entropy_uniform:.4f} bits\")\nprint(f\"Information Content (uniform): {information_content_uniform:.4f} bits\")\n\n# Example with a fully conserved position:\nentropy_conserved = calculate_entropy(1.0, 0.0, 0.0, 0.0)\ninformation_content_conserved = calculate_information_content(1.0, 0.0, 0.0, 0.0)\nprint(f\"\\nProbabilities (fully conserved): p(A)=1.00, p(C)=0.00, p(G)=0.00, p(T)=0.00\")\nprint(f\"Entropy (fully conserved): {entropy_conserved:.4f} bits\")\nprint(f\"Information Content (fully conserved): {information_content_conserved:.4f} bits\")\n\nProbabilities: p(A)=0.30, p(C)=0.20, p(G)=0.25, p(T)=0.25\nEntropy: 1.9855 bits\nInformation Content: 0.0145 bits\n\nProbabilities (uniform): p(A)=0.25, p(C)=0.25, p(G)=0.25, p(T)=0.25\nEntropy (uniform): 2.0000 bits\nInformation Content (uniform): 0.0000 bits\n\nProbabilities (fully conserved): p(A)=1.00, p(C)=0.00, p(G)=0.00, p(T)=0.00\nEntropy (fully conserved): 0.0000 bits\nInformation Content (fully conserved): 2.0000 bits\n\n\n** text and code generated with gemini 2.0 via a series of prompting **"
  },
  {
    "objectID": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html",
    "href": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html",
    "title": "Fit linear mode to linear data",
    "section": "",
    "text": "We generate a data Y linearly dependent on features X: Y = X\\beta + \\epsilon\nWe fit a linear model\n\nwith gradient descent\nwith pytorch framework\nwith traditional linear regression estimate \\hat\\beta = (X^TX)^{-1}X^TY\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nfrom plotnine import *\n\n# Generate synthetic data\nnp.random.seed(42)\nX = np.random.randn(100, 1) * 2  # 100 samples, 1 feature\ny = 3 * X + 2 + np.random.randn(100, 1) * 0.5  # y = 3x + 2 + noise\n\n# Convert to PyTorch tensors\nX_tensor = torch.FloatTensor(X)\ny_tensor = torch.FloatTensor(y)\n\n# define a linear model\nclass LinearModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(1, 1)  # 1 input feature, 1 output\n    def forward(self, x):\n        return self.linear(x)\n\n# instantiate the model\nmodel = LinearModel()\n\n# define a loss function\nloss_fn = nn.MSELoss()\n\n# define an optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Train: iterate over the data and update the model\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Forward pass: compute model predictions and loss\n    y_pred = model(X_tensor)\n    loss = loss_fn(y_pred, y_tensor)\n    \n    # Backward pass and parameter update\n    optimizer.zero_grad()  # Clear previous gradients\n    loss.backward()        # Compute gradients via backpropagation\n    optimizer.step()       # Update model parameters\n    \n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# check performance\n\n# compute PyTorch model predictions\nmodel.eval()\nwith torch.no_grad():\n    y_pred_pytorch = model(X_tensor).numpy()\n\n# compute scikit-learn predictions\nlr = LinearRegression()\nlr.fit(X, y)\ny_pred_sklearn = lr.predict(X)\n\n# compare coefficients\nprint(\"\\nPyTorch model coefficients:\")\nprint(f\"Slope: {model.linear.weight.item():.4f}\")\nprint(f\"Intercept: {model.linear.bias.item():.4f}\")\n\nprint(\"\\nScikit-learn coefficients:\")\nprint(f\"Slope: {lr.coef_[0][0]:.4f}\")\nprint(f\"Intercept: {lr.intercept_[0]:.4f}\")\n\n# compare MSE\nmse_pytorch = np.mean((y - y_pred_pytorch) ** 2)\nmse_sklearn = np.mean((y - y_pred_sklearn) ** 2)\nprint(f\"\\nPyTorch MSE: {mse_pytorch:.4f}\")\nprint(f\"Scikit-learn MSE: {mse_sklearn:.4f}\")\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html#fit-a-linear-model-to-linear-data",
    "href": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html#fit-a-linear-model-to-linear-data",
    "title": "Fit linear mode to linear data",
    "section": "",
    "text": "We generate a data Y linearly dependent on features X: Y = X\\beta + \\epsilon\nWe fit a linear model\n\nwith gradient descent\nwith pytorch framework\nwith traditional linear regression estimate \\hat\\beta = (X^TX)^{-1}X^TY\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nfrom plotnine import *\n\n# Generate synthetic data\nnp.random.seed(42)\nX = np.random.randn(100, 1) * 2  # 100 samples, 1 feature\ny = 3 * X + 2 + np.random.randn(100, 1) * 0.5  # y = 3x + 2 + noise\n\n# Convert to PyTorch tensors\nX_tensor = torch.FloatTensor(X)\ny_tensor = torch.FloatTensor(y)\n\n# define a linear model\nclass LinearModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(1, 1)  # 1 input feature, 1 output\n    def forward(self, x):\n        return self.linear(x)\n\n# instantiate the model\nmodel = LinearModel()\n\n# define a loss function\nloss_fn = nn.MSELoss()\n\n# define an optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Train: iterate over the data and update the model\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Forward pass: compute model predictions and loss\n    y_pred = model(X_tensor)\n    loss = loss_fn(y_pred, y_tensor)\n    \n    # Backward pass and parameter update\n    optimizer.zero_grad()  # Clear previous gradients\n    loss.backward()        # Compute gradients via backpropagation\n    optimizer.step()       # Update model parameters\n    \n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# check performance\n\n# compute PyTorch model predictions\nmodel.eval()\nwith torch.no_grad():\n    y_pred_pytorch = model(X_tensor).numpy()\n\n# compute scikit-learn predictions\nlr = LinearRegression()\nlr.fit(X, y)\ny_pred_sklearn = lr.predict(X)\n\n# compare coefficients\nprint(\"\\nPyTorch model coefficients:\")\nprint(f\"Slope: {model.linear.weight.item():.4f}\")\nprint(f\"Intercept: {model.linear.bias.item():.4f}\")\n\nprint(\"\\nScikit-learn coefficients:\")\nprint(f\"Slope: {lr.coef_[0][0]:.4f}\")\nprint(f\"Intercept: {lr.intercept_[0]:.4f}\")\n\n# compare MSE\nmse_pytorch = np.mean((y - y_pred_pytorch) ** 2)\nmse_sklearn = np.mean((y - y_pred_sklearn) ** 2)\nprint(f\"\\nPyTorch MSE: {mse_pytorch:.4f}\")\nprint(f\"Scikit-learn MSE: {mse_sklearn:.4f}\")"
  },
  {
    "objectID": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html#plot-predictions-vs-true",
    "href": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html#plot-predictions-vs-true",
    "title": "Fit linear mode to linear data",
    "section": "plot predictions vs true",
    "text": "plot predictions vs true\n\n# Create a plot comparing predictions\n# Create a DataFrame for plotting\nplot_df = pd.DataFrame({\n    'y_true': y.flatten(),\n    'y_pred_pytorch': y_pred_pytorch.flatten(),\n    'y_pred_sklearn': y_pred_sklearn.flatten()\n})\n\n# Create the plot\np = (ggplot(plot_df)\n    + geom_point(aes(x='y_true', y='y_pred_pytorch', color='\"PyTorch\"'), size=1)\n    + geom_point(aes(x='y_true', y='y_pred_sklearn', color='\"Scikit-learn\"'), size=1)\n    + geom_abline(intercept=0, slope=1, color='gray', linetype='dashed', alpha=0.5)  # Identity line\n    + scale_color_manual(values=['red', 'blue'])\n    + labs(x='True Values', y='Predicted Values', \n           title='True vs Predicted Values',\n           color='Model')\n    + theme_minimal()\n)\n\np\n\n\n\n\n\n\n\n\n\n(ggplot(plot_df)\n    + geom_point(aes(x='y_pred_sklearn', y='y_pred_pytorch'), size=1)\n    + geom_abline(intercept=0, slope=1, color='gray', linetype='dashed', alpha=0.5)  # Identity line\n    + labs(x='Y sklearn', y='Y pytorch', \n           title='ptorch vs sklearn predictions',\n           color='Model')\n    + theme_minimal()\n)"
  },
  {
    "objectID": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html#fit-linear-model-using-the-gpu",
    "href": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html#fit-linear-model-using-the-gpu",
    "title": "Fit linear mode to linear data",
    "section": "Fit linear model using the GPU",
    "text": "Fit linear model using the GPU\n\n# Train on GPU (checking for both MPS and CUDA)\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nfrom plotnine import *\n\n# Check for available GPU devices\nif torch.backends.mps.is_available():\n    device = torch.device('mps')  # Apple Silicon\n    print(\"Using MPS (Apple Silicon GPU)\")\nelif torch.cuda.is_available():\n    device = torch.device('cuda')  # NVIDIA GPU\n    print(\"Using CUDA (NVIDIA GPU)\")\nelse:\n    device = torch.device('cpu')  # Fallback to CPU\n    print(\"Using CPU (no GPU available)\")\n\n# Generate synthetic data (stays on CPU)\nnp.random.seed(42)\nX = np.random.randn(100, 1) * 2\ny = 3 * X + 2 + np.random.randn(100, 1) * 0.5\n\n# Convert to PyTorch tensors and move to GPU\nX_tensor = torch.FloatTensor(X).to(device)\ny_tensor = torch.FloatTensor(y).to(device)\n\n# Define and move model to GPU\nclass LinearModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(1, 1)\n\n    def forward(self, x):\n        return self.linear(x)\n\nmodel = LinearModel().to(device)\nloss_fn = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Training loop (same as before, but data is on GPU)\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Forward pass\n    y_pred = model(X_tensor)\n    loss = loss_fn(y_pred, y_tensor)\n    \n    # Backward pass and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Get predictions back to CPU for comparison\nmodel.eval()\nwith torch.no_grad():\n    y_pred_pytorch = model(X_tensor).cpu().numpy()  # Move back to CPU before converting to numpy\n\n# Compare with scikit-learn (on CPU)\nlr = LinearRegression()\nlr.fit(X, y)\ny_pred_sklearn = lr.predict(X)\n\n# Compare coefficients\nprint(\"\\nPyTorch model coefficients:\")\nprint(f\"Slope: {model.linear.weight.item():.4f}\")\nprint(f\"Intercept: {model.linear.bias.item():.4f}\")\n\nprint(\"\\nScikit-learn coefficients:\")\nprint(f\"Slope: {lr.coef_[0][0]:.4f}\")\nprint(f\"Intercept: {lr.intercept_[0]:.4f}\")\n\n# Compare MSE\nmse_pytorch = np.mean((y - y_pred_pytorch) ** 2)\nmse_sklearn = np.mean((y - y_pred_sklearn) ** 2)\nprint(f\"\\nPyTorch MSE: {mse_pytorch:.4f}\")\nprint(f\"Scikit-learn MSE: {mse_sklearn:.4f}\")\n\n# Create a plot comparing predictions\nplot_df = pd.DataFrame({\n    'y_true': y.flatten(),\n    'y_pred_pytorch': y_pred_pytorch.flatten(),\n    'y_pred_sklearn': y_pred_sklearn.flatten()\n})\n\n# Create the plot\np = (ggplot(plot_df)\n    + geom_point(aes(x='y_true', y='y_pred_pytorch', color='\"PyTorch\"'), size=1)\n    + geom_point(aes(x='y_true', y='y_pred_sklearn', color='\"Scikit-learn\"'), size=1)\n    + geom_abline(intercept=0, slope=1, color='gray', linetype='dashed', alpha=0.5)  # Identity line\n    + scale_color_manual(values=['red', 'blue'])\n    + labs(x='True Values', y='Predicted Values', \n           title=f'True vs Predicted Values (Training on {device})',\n           color='Model')\n    + theme_minimal()\n)\n\np"
  },
  {
    "objectID": "post/2025-05-12-unit04/scanpy-basic-scrna-tutorial.html",
    "href": "post/2025-05-12-unit04/scanpy-basic-scrna-tutorial.html",
    "title": "scanpy tutorial - qmd",
    "section": "",
    "text": "This notebook is not compatible with this python 3.12. Some issues with pickle data and celltypist.\n# set first time to False to avoid multiple downloads\nfirst_time = False\nprint(first_time)\n\nFalse\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-05-12-unit04/scanpy-basic-scrna-tutorial.html#quality-control",
    "href": "post/2025-05-12-unit04/scanpy-basic-scrna-tutorial.html#quality-control",
    "title": "scanpy tutorial - qmd",
    "section": "Quality Control",
    "text": "Quality Control\nThe scanpy function {func}~scanpy.pp.calculate_qc_metrics calculates common quality control (QC) metrics, which are largely based on calculateQCMetrics from scater {cite}McCarthy2017. One can pass specific gene population to {func}~scanpy.pp.calculate_qc_metrics in order to calculate proportions of counts for these populations. Mitochondrial, ribosomal and hemoglobin genes are defined by distinct prefixes as listed below.\n\n# mitochondrial genes\nadata.var[\"mt\"] = adata.var_names.str.startswith(\"MT-\")  # \"MT-\" for human, \"Mt-\" for mouse\n# ribosomal genes\nadata.var[\"ribo\"] = adata.var_names.str.startswith((\"RPS\", \"RPL\"))\n# hemoglobin genes\nadata.var[\"hb\"] = adata.var_names.str.contains(\"^HB[^(P)]\")\n\n\nsc.pp.calculate_qc_metrics(adata, qc_vars=[\"mt\", \"ribo\", \"hb\"], inplace=True, log1p=True)\n\nOne can now inspect violin plots of some of the computed QC metrics:\n\nthe number of genes expressed in the count matrix\nthe total counts per cell\nthe percentage of counts in mitochondrial genes\n\n\nsc.pl.violin(adata, [\"n_genes_by_counts\", \"total_counts\", \"pct_counts_mt\"], jitter=0.4, multi_panel=True)\n\n\n\n\n\n\n\n\nAdditionally, it is useful to consider QC metrics jointly by inspecting a scatter plot colored by pct_counts_mt.\n\nsc.pl.scatter(adata, \"total_counts\", \"n_genes_by_counts\", color=\"pct_counts_mt\")\n\n\n\n\n\n\n\n\nBased on the QC metric plots, one could now remove cells that have too many mitochondrial genes expressed or too many total counts by setting manual or automatic thresholds. However, it proved to be beneficial to apply a very permissive filtering strategy in the beginning for your single-cell analysis and filter low quality cells during clustering or revisit the filtering again at a later point. We therefore now only filter cells with less than 100 genes expressed and genes that are detected in less than 3 cells.\nAdditionally, it is important to note that for datasets with multiple batches, quality control should be performed for each sample individually as quality control thresholds can very substantially between batches.\n\nsc.pp.filter_cells(adata, min_genes=100)\nsc.pp.filter_genes(adata, min_cells=3)\n\n\nDoublet detection\nAs a next step, we run a doublet detection algorithm. Identifying doublets is crucial as they can lead to misclassifications or distortions in downstream analysis steps. Scanpy contains the doublet detection method Scrublet {cite}Wolock2019. Scrublet predicts cell doublets using a nearest-neighbor classifier of observed transcriptomes and simulated doublets. {func}scanpy.pp.scrublet adds doublet_score and predicted_doublet to .obs. One can now either filter directly on predicted_doublet or use the doublet_score later during clustering to filter clusters with high doublet scores.\n\nsc.pp.scrublet(adata, batch_key=\"sample\")\n\n\nAlternative methods for doublet detection within the scverse ecosystem are DoubletDetection and SOLO. You can read more about these in the Doublet Detection chapter of Single Cell Best Practices."
  },
  {
    "objectID": "post/2025-05-12-unit04/scanpy-basic-scrna-tutorial.html#normalization",
    "href": "post/2025-05-12-unit04/scanpy-basic-scrna-tutorial.html#normalization",
    "title": "scanpy tutorial - qmd",
    "section": "Normalization",
    "text": "Normalization\nThe next preprocessing step is normalization. A common approach is count depth scaling with subsequent log plus one (log1p) transformation. Count depth scaling normalizes the data to a ‚Äúsize factor‚Äù such as the median count depth in the dataset, ten thousand (CP10k) or one million (CPM, counts per million). The size factor for count depth scaling can be controlled via target_sum in pp.normalize_total. We are applying median count depth normalization with log1p transformation (AKA log1PF).\n\n# Saving count data\nadata.layers[\"counts\"] = adata.X.copy()\n\n\n# Normalizing to median total counts\nsc.pp.normalize_total(adata)\n# Logarithmize the data:\nsc.pp.log1p(adata)"
  },
  {
    "objectID": "post/2025-05-12-unit04/scanpy-basic-scrna-tutorial.html#feature-selection",
    "href": "post/2025-05-12-unit04/scanpy-basic-scrna-tutorial.html#feature-selection",
    "title": "scanpy tutorial - qmd",
    "section": "Feature selection",
    "text": "Feature selection\nAs a next step, we want to reduce the dimensionality of the dataset and only include the most informative genes. This step is commonly known as feature selection. The scanpy function pp.highly_variable_genes annotates highly variable genes by reproducing the implementations of Seurat {cite}Satija2015, Cell Ranger {cite}Zheng2017, and Seurat v3 {cite}stuart2019comprehensive depending on the chosen flavor.\n\nsc.pp.highly_variable_genes(adata, n_top_genes=2000, batch_key=\"sample\")\n\n\nsc.pl.highly_variable_genes(adata)"
  },
  {
    "objectID": "post/2025-05-12-unit04/scanpy-basic-scrna-tutorial.html#dimensionality-reduction",
    "href": "post/2025-05-12-unit04/scanpy-basic-scrna-tutorial.html#dimensionality-reduction",
    "title": "scanpy tutorial - qmd",
    "section": "Dimensionality Reduction",
    "text": "Dimensionality Reduction\nReduce the dimensionality of the data by running principal component analysis (PCA), which reveals the main axes of variation and denoises the data.\n\nsc.tl.pca(adata)\n\nLet us inspect the contribution of single PCs to the total variance in the data. This gives us information about how many PCs we should consider in order to compute the neighborhood relations of cells, e.g.¬†used in the clustering function {func}~scanpy.tl.leiden or {func}~scanpy.tl.tsne. In our experience, often a rough estimate of the number of PCs does fine.\n\nsc.pl.pca_variance_ratio(adata, n_pcs=50, log=True)"
  },
  {
    "objectID": "post/2025-05-12-unit04/scanpy-basic-scrna-tutorial.html#visualization",
    "href": "post/2025-05-12-unit04/scanpy-basic-scrna-tutorial.html#visualization",
    "title": "scanpy tutorial - qmd",
    "section": "Visualization",
    "text": "Visualization\nLet us compute the neighborhood graph of cells using the PCA representation of the data matrix.\n\nsc.pp.neighbors(adata)\n\n/Users/haekyungim/miniconda3/envs/scanpy-tutorial/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n\n\nWe suggest embedding the graph in two dimensions using UMAP (McInnes et al., 2018), see below.\n\nsc.tl.umap(adata)\n\nWe can now visualize the UMAP according to the sample.\n\nsc.pl.umap(adata, color=\"sample\")\n\n\n\n\n\n\n\n\nEven though the data considered in this tutorial includes two different samples, we only observe a minor batch effect and we can continue with clustering and annotation of our data.\nIf you inspect batch effects in your UMAP it can be beneficial to integrate across samples and perform batch correction/integration."
  },
  {
    "objectID": "post/2025-05-12-unit04/scanpy-basic-scrna-tutorial.html#clustering",
    "href": "post/2025-05-12-unit04/scanpy-basic-scrna-tutorial.html#clustering",
    "title": "scanpy tutorial - qmd",
    "section": "Clustering",
    "text": "Clustering\nAs with Seurat and many other frameworks, we recommend the Leiden graph-clustering method (community detection based on optimizing modularity) {cite}traag2019louvain. Note that Leiden clustering directly clusters the neighborhood graph of cells, which we already computed in the previous section.\n\nsc.tl.leiden(adata, flavor=\"igraph\")\n\n\nsc.pl.umap(adata, color=[\"leiden\"])"
  },
  {
    "objectID": "post/2025-05-12-unit04/scanpy-basic-scrna-tutorial.html#re-assess-quality-control-and-cell-filtering",
    "href": "post/2025-05-12-unit04/scanpy-basic-scrna-tutorial.html#re-assess-quality-control-and-cell-filtering",
    "title": "scanpy tutorial - qmd",
    "section": "Re-assess quality control and cell filtering",
    "text": "Re-assess quality control and cell filtering\nAs indicated before, we will now re-assess our filtering strategy by visualizing different QC metrics using UMAP.\n\nadata.obs[\"predicted_doublet\"] = adata.obs[\"predicted_doublet\"].astype(\"category\")\nsc.pl.umap(\n    adata,\n    color=[\"leiden\", \"predicted_doublet\", \"doublet_score\"],\n    # increase horizontal space between panels\n    wspace=0.5,\n)\n\n\n\n\n\n\n\n\nWe can now subset the AnnData object to exclude cells predicted as doublets:\n\nadata = adata[~adata.obs[\"predicted_doublet\"].to_numpy()].copy()\n\n\nsc.pl.umap(\n    adata, color=[\"leiden\", \"log1p_total_counts\", \"pct_counts_mt\", \"log1p_n_genes_by_counts\"], wspace=0.5, ncols=2\n)"
  },
  {
    "objectID": "post/2025-05-12-unit04/scanpy-basic-scrna-tutorial.html#cell-type-annotation",
    "href": "post/2025-05-12-unit04/scanpy-basic-scrna-tutorial.html#cell-type-annotation",
    "title": "scanpy tutorial - qmd",
    "section": "Cell-type annotation",
    "text": "Cell-type annotation\n\ntry: \n    import celltypist as ct\nexcept ImportError:\n    %pip install celltypist\n    import celltypist as ct\ntry:\n    import decoupler as dc\nexcept ImportError:\n    %pip install decoupler\n    import decoupler as dc\n\nWe have now reached a point where we have obtained a set of cells with decent quality, and we can proceed to their annotation to known cell types. Typically, this is done using genes that are exclusively expressed by a given cell type, or in other words these genes are the marker genes of the cell types, and are thus used to distinguish the heterogeneous groups of cells in our data. Previous efforts have collected and curated various marker genes into available resources, such as CellMarker, TF-Marker, and PanglaoDB.\nCommonly and classically, cell type annotation uses those marker genes subsequent to the grouping of the cells into clusters. So, let‚Äôs generate a set of clustering solutions which we can then use to annotate our cell types. Here, we will use the Leiden clustering algorithm which will extract cell communities from our nearest neighbours graph.\n\nsc.tl.leiden(adata, flavor=\"igraph\", key_added=\"leiden_res0_02\", resolution=0.02)\nsc.tl.leiden(adata, flavor=\"igraph\", key_added=\"leiden_res0_5\", resolution=0.5)\nsc.tl.leiden(adata, flavor=\"igraph\", key_added=\"leiden_res2\", resolution=2)\n\nNotably, the number of clusters that we define is largely arbitrary, and so is the resolution parameter that we use to control for it. As such, the number of clusters is ultimately bound to the stable and biologically-meaningful groups that we can ultimately distringuish, typically done by experts in the corresponding field or by using expert-curated prior knowledge in the form of markers.\n\nsc.pl.umap(\n    adata,\n    color=[\"leiden_res0_02\", \"leiden_res0_5\", \"leiden_res2\"],\n    legend_loc=\"on data\",\n)\n\n\n\n\n\n\n\n\nThough UMAPs should not be over-interpreted, here we can already see that in the highest resolution our data is over-clustered, while the lowest resolution is likely grouping cells which belong to distinct cell identities.\n\nMarker gene set\nLet‚Äôs define a set of marker genes for the main cell types that we expect to see in this dataset. These were adapted from Single Cell Best Practices annotation chapter, for a more detailed overview and best practices in cell type annotation, we refer the user to it.\n\nmarker_genes = {\n    \"CD14+ Mono\": [\"FCN1\", \"CD14\"],\n    \"CD16+ Mono\": [\"TCF7L2\", \"FCGR3A\", \"LYN\"],\n    # Note: DMXL2 should be negative\n    \"cDC2\": [\"CST3\", \"COTL1\", \"LYZ\", \"DMXL2\", \"CLEC10A\", \"FCER1A\"],\n    \"Erythroblast\": [\"MKI67\", \"HBA1\", \"HBB\"],\n    # Note HBM and GYPA are negative markers\n    \"Proerythroblast\": [\"CDK6\", \"SYNGR1\", \"HBM\", \"GYPA\"],\n    \"NK\": [\"GNLY\", \"NKG7\", \"CD247\", \"FCER1G\", \"TYROBP\", \"KLRG1\", \"FCGR3A\"],\n    \"ILC\": [\"ID2\", \"PLCG2\", \"GNLY\", \"SYNE1\"],\n    \"Naive CD20+ B\": [\"MS4A1\", \"IL4R\", \"IGHD\", \"FCRL1\", \"IGHM\"],\n    # Note IGHD and IGHM are negative markers\n    \"B cells\": [\"MS4A1\", \"ITGB1\", \"COL4A4\", \"PRDM1\", \"IRF4\", \"PAX5\", \"BCL11A\", \"BLK\", \"IGHD\", \"IGHM\"],\n    \"Plasma cells\": [\"MZB1\", \"HSP90B1\", \"FNDC3B\", \"PRDM1\", \"IGKC\", \"JCHAIN\"],\n    \"Plasmablast\": [\"XBP1\", \"PRDM1\", \"PAX5\"],  # Note PAX5 is a negative marker\n    \"CD4+ T\": [\"CD4\", \"IL7R\", \"TRBC2\"],\n    \"CD8+ T\": [\"CD8A\", \"CD8B\", \"GZMK\", \"GZMA\", \"CCL5\", \"GZMB\", \"GZMH\", \"GZMA\"],\n    \"T naive\": [\"LEF1\", \"CCR7\", \"TCF7\"],\n    \"pDC\": [\"GZMB\", \"IL3RA\", \"COBLL1\", \"TCF4\"],\n}\n\n\ndef group_max(adata: sc.AnnData, groupby: str) -&gt; str:\n    import pandas as pd\n\n    agg = sc.get.aggregate(adata, by=groupby, func=\"mean\")\n    return pd.Series(agg.layers[\"mean\"].sum(1), agg.obs[groupby]).idxmax()\n\n\nsc.pl.dotplot(adata, marker_genes, groupby=\"leiden_res0_02\")\n\n\n\n\n\n\n\n\nHere, we can see that cluster {eval}group_max(adata[:, marker_genes[\"NK\"]], \"leiden_res0_02\") perhaps contains an admixture of monocytes and dendritic cells, while in cluster {eval}group_max(adata[:, marker_genes[\"B cells\"]], \"leiden_res0_02\") we have different populations of B lymphocytes. Thus, we should perhaps consider a higher clustering resolution.\n\nsc.pl.dotplot(adata, marker_genes, groupby=\"leiden_res0_5\")\n\n\n\n\n\n\n\n\nThis seems like a resolution that suitable to distinguish most of the different cell types in our data. Ideally, one would look specifically into each cluster, and attempt to subcluster those if required.\n\n\nAutomatic label prediction\nIn addition to using marker collections to annotate our labels, there exist approaches to automatically annotate scRNA-seq datasets. One such tool is CellTypist, which uses gradient-descent optimised logistic regression classifiers to predict cell type annotations.\nFirst, we need to retrive the CellTypist models that we wish to use, in this case we will use models with immune cell type and subtype populations generated using 20 tissues from 18 studies (Dom√≠nguez Conde, et al.¬†2022).\n\nif first_time:\n    ct.models.download_models(model=[\"Immune_All_Low.pkl\"], force_update=True)\n\nThen we predict the major cell type annotations. In this case we will enable majority_voting, which will assign a label to the clusters that we obtained previously.\n\nmodel = ct.models.Model.load(model=\"Immune_All_Low.pkl\")\npredictions = ct.annotate(adata, model=\"Immune_All_Low.pkl\", majority_voting=True, over_clustering=\"leiden_res0_5\")\n# convert back to anndata||\nadata = predictions.to_adata()\n\n‚ö†Ô∏è Warning: invalid expression matrix, expect ALL genes and log1p normalized expression to 10000 counts per cell. The prediction result may not be accurate\nüî¨ Input data has 16828 cells and 23427 genes\nüîó Matching reference genes in the model\nüß¨ 5852 features used for prediction\n‚öñÔ∏è Scaling input data\nüñãÔ∏è Predicting labels\n‚úÖ Prediction done!\nüó≥Ô∏è Majority voting the predictions\n‚úÖ Majority voting done!\n\n\nLet‚Äôs examine the results of automatic clustering:\n\nsc.pl.umap(adata, color=\"majority_voting\", ncols=1)\n\n\n\n\n\n\n\n\nNote that our previously ‚ÄòUnknown‚Äô cluster is now assigned as ‚ÄòPro-B cells‚Äô.\n\n\nAnnotation with enrichment analysis\nAutomatic cell type labelling with methods that require pre-trained models will not always work as smoothly, as such classifiers need to be trained on and be representitive for a given tissue and the cell types within it. So, as a more generalizable approach to annotate the cells, we can also use the marker genes from any database, for example PanglaoDB. Here we will use it with simple multi-variate linear regression, implemented in decoupler. Essentially, this will test if any collection of genes are enriched in any of the cells. Ultimately, this approach is similar to many other marker-based classifiers.\nLet‚Äôs get canonical cell markers using with decoupler which queries the OmniPath metadata-base to obtain the PanglaoDB marker gene database with cannonical cell type markers.\n\n# Query Omnipath and get PanglaoDB\nmarkers = dc.get_resource(name=\"PanglaoDB\", organism=\"human\")\n\n# Print initial information about the markers DataFrame\nprint(\"Initial markers DataFrame shape:\", markers.shape)\n\n# Convert canonical_marker to boolean - fixing the conversion\nmarkers[\"canonical_marker\"] = markers[\"canonical_marker\"].astype(str).map({'True': True, 'False': False})\n\n# Keep canonical cell type markers alone\nmarkers = markers[markers[\"canonical_marker\"] == True]\nprint(\"\\nShape after filtering canonical markers:\", markers.shape)\n\n# Remove duplicated entries\nmarkers = markers[~markers.duplicated([\"cell_type\", \"genesymbol\"])]\nprint(\"\\nShape after removing duplicates:\", markers.shape)\n\n# Convert gene symbols to uppercase in both datasets to ensure consistent matching\nmarkers[\"genesymbol\"] = markers[\"genesymbol\"].str.upper()\nadata.var_names = adata.var_names.str.upper()\n\n# Print overlap statistics\nprint(\"\\nNumber of unique genes in markers:\", len(markers[\"genesymbol\"].unique()))\nprint(\"Number of genes in adata:\", adata.n_vars)\nprint(\"Number of overlapping genes:\", len(set(markers[\"genesymbol\"].unique()) & set(adata.var_names)))\n\n# Print some example cell types and their marker counts\nprint(\"\\nNumber of markers per cell type:\")\nprint(markers.groupby(\"cell_type\")[\"genesymbol\"].count().sort_values(ascending=False).head())\n\n# Run MLM with a lower min_n value\ndc.run_mlm(mat=adata, net=markers, weight=None, source=\"cell_type\", target=\"genesymbol\", verbose=True, use_raw=False, min_n=2)\n\nInitial markers DataFrame shape: (8461, 14)\n\nShape after filtering canonical markers: (5527, 14)\n\nShape after removing duplicates: (5502, 14)\n\nNumber of unique genes in markers: 3366\nNumber of genes in adata: 23427\nNumber of overlapping genes: 2424\n\nNumber of markers per cell type:\ncell_type\nEndothelial cells    192\nMacrophages          166\nDendritic cells      147\nMast cells           144\nHepatocytes          135\nName: genesymbol, dtype: int64\nRunning mlm on mat with 16828 samples and 23427 targets for 154 sources.\n\n\n  0%|          | 0/2 [00:00&lt;?, ?it/s] 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:03&lt;00:03,  3.56s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06&lt;00:00,  2.92s/it]100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06&lt;00:00,  3.02s/it]\n\n\nThe obtained results are stored in the .obsm key, with mlm_estimate representing coefficient t-values:\n\nadata.obsm[\"mlm_estimate\"].head()\n\n\n\n\n\n\n\n\nAcinar cells\nAdipocyte progenitor cells\nAdipocytes\nAdrenergic neurons\nAirway epithelial cells\nAirway goblet cells\nAlpha cells\nAlveolar macrophages\nAnterior pituitary gland cells\nAstrocytes\n...\nT regulatory cells\nTanycytes\nTaste receptor cells\nThymocytes\nTrichocytes\nTrigeminal neurons\nTrophoblast cells\nTrophoblast progenitor cells\nTuft cells\nUrothelial cells\n\n\n\n\nAAACCCAAGGATGGCT-1\n-0.829379\n-0.192387\n-0.433855\n-0.259944\n0.105843\n-0.896782\n0.286354\n0.445317\n-0.360782\n0.482100\n...\n-1.861577\n0.653052\n-0.914405\n2.135190\n-0.409456\n-0.240998\n0.870413\n2.188858\n1.590695\n-0.726171\n\n\nAAACCCAAGGCCTAGA-1\n0.305256\n-0.409719\n0.115441\n-0.659562\n-0.503354\n-0.793676\n0.980069\n-3.680857\n-0.313018\n1.018442\n...\n-1.191339\n-0.122365\n-0.713158\n-0.302480\n-0.585186\n0.926535\n-0.557362\n-0.477797\n2.051256\n-1.071484\n\n\nAAACCCAAGTGAGTGC-1\n0.367563\n-0.238991\n0.066879\n-0.181742\n0.530131\n-0.246711\n-0.459827\n-0.713275\n-0.197698\n-1.156477\n...\n-1.127890\n-0.514065\n-0.735793\n0.894280\n-0.270854\n-0.110214\n2.406705\n-0.221142\n1.700772\n-0.384683\n\n\nAAACCCACAAGAGGCT-1\n-1.453875\n0.022096\n-1.493422\n-0.303853\n-1.110833\n-1.383801\n1.080116\n0.622730\n0.170097\n-1.103424\n...\n-1.076105\n-0.212070\n-0.546337\n0.964560\n-0.583426\n-0.543244\n0.502228\n0.681523\n3.035343\n-1.010428\n\n\nAAACCCACATCGTGGC-1\n1.160588\n-0.435748\n-1.242875\n-0.100550\n-1.379393\n-0.165989\n-0.541564\n-0.351312\n-0.015558\n0.188370\n...\n1.963929\n-0.292627\n1.647266\n-1.076357\n-0.191659\n-0.791095\n-0.017454\n-0.156497\n-0.078420\n-0.396215\n\n\n\n\n5 rows √ó 154 columns\n\n\n\nTo visualize the obtianed scores, we can re-use any of scanpy‚Äôs plotting functions. First though, we will extract them from the adata object.\n\nacts = dc.get_acts(adata=adata, obsm_key=\"mlm_estimate\")\nsc.pl.umap(\n    acts,\n    color=[\n        \"majority_voting\",\n        \"B cells\",\n        \"T cells\",\n        \"Monocytes\",\n        \"Erythroid-like and erythroid precursor cells\",\n        \"NK cells\",\n    ],\n    wspace=0.5,\n    ncols=3,\n)\n\n\n\n\n\n\n\n\nThese results further confirm the our automatic annotation with Celltypist. In addition, here we can also transfer the max over-representation score estimates to assign a label to each cluster.\n\nmean_enr = dc.summarize_acts(acts, groupby=\"leiden_res0_5\", min_std=1.0)\nannotation_dict = dc.assign_groups(mean_enr)\nadata.obs[\"dc_anno\"] = [annotation_dict[clust] for clust in adata.obs[\"leiden_res0_5\"]]\n\nLet‚Äôs compare all resulting annotations here\n\nsc.pl.umap(adata, color=[\"majority_voting\", \"dc_anno\"], ncols=1)\n\n... storing 'dc_anno' as categorical\n\n\n\n\n\n\n\n\n\nGreat. We can see that the different approaches to annotate the data are largely concordant. Though, these annotations are decent, cell type annotation is laborous and repetitive task, one which typically requires multiple rounds of sublucstering and re-annotation. Nevertheless, we now have a good basis with which we can further proceed with manually refining our annotations.\n\n\nDifferentially-expressed Genes as Markers\nFurthermore, one can also calculate marker genes per cluster and then look up whether we can link those marker genes to any known biology, such as cell types and/or states. This is typically done using simple statistical tests, such as Wilcoxon and t-test, for each cluster vs the rest.\n\n# Obtain cluster-specific differentially expressed genes\nsc.tl.rank_genes_groups(adata, groupby=\"leiden_res0_5\")\n# Filter those\nsc.tl.filter_rank_genes_groups(adata, min_fold_change=1.5)\n\nWe can then visualize the top 5 differentially-expressed genes on a dotplot.\n\nsc.pl.rank_genes_groups_dotplot(adata, groupby=\"leiden_res0_5\", standard_scale=\"var\", n_genes=5)\n\nWARNING: dendrogram data not found (using key=dendrogram_leiden_res0_5). Running `sc.tl.dendrogram` with default parameters. For fine tuning it is recommended to run `sc.tl.dendrogram` independently.\n\n\n\n\n\n\n\n\n\nWe see that LYZ, ACT8, S100A6, S100A4, and CST3 are all highly expressed in cluster 3. Let‚Äôs visualize those at the UMAP space:\n\ncluster3_genes = [\"LYZ\", \"ACTB\", \"S100A6\", \"S100A4\", \"CST3\"]\nsc.pl.umap(adata, color=[*cluster3_genes, \"leiden_res0_5\"], legend_loc=\"on data\", frameon=False, ncols=3)\n\n\n\n\n\n\n\n\nSimilarly, we can also generate a Violin plot with the distrubtions of the same genes across the clusters.\n\nsc.pl.violin(adata, keys=cluster3_genes[0:3], groupby=\"leiden_res0_5\", multi_panel=True)\n\nUsing categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\nUsing categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\nUsing categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\nUsing categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\nUsing categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\nUsing categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\nUsing categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\nUsing categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\nUsing categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\nUsing categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\nUsing categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\nUsing categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\nUsing categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\nUsing categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\nUsing categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\nUsing categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\nUsing categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting.\nUsing categorical units to plot a list of strings that are all parsable as floats or dates. If these strings should be plotted as numbers, cast to the appropriate data type before plotting."
  },
  {
    "objectID": "post/2025-05-12-unit04/index.html",
    "href": "post/2025-05-12-unit04/index.html",
    "title": "Unit 04 Single Cell Genomics",
    "section": "",
    "text": "Unit 04 Single Cell Genomics\n\n\n\n\n\n\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-05-01-unit03/homework-06.html",
    "href": "post/2025-05-01-unit03/homework-06.html",
    "title": "homework 6",
    "section": "",
    "text": "If you run the analysis locally, make sure you clone the gene46100 conda environment and install the python packages as needed. See this jupyter notebook for an example. Rendered here for your convenience.\nTotal Points: 100\n\n\n\nDownload the Enformer model and reference genome from the provided Box links (5 points)\nChoose a gene of interest and find its transcription start site (TSS) (5 points)\nModify the notebook code to predict human epigenome at this location (5 points)\nRun Enformer on the Neanderthal sequence at the same location (5 points)\nCreate scatter plots comparing human and Neanderthal predictions. Choose relevant tracks to compare (5 points)\n\n\n\n\n\nCalculate correlation coefficients between human and Neanderthal predictions for each track (10 points)\nIdentify tracks showing the highest and lowest correlations (5 points)\nWhat might explain these differences in correlation? (10 points)\n\n\n\n\n\nIdentify regions where peaks are present in human but absent in Neanderthal (or vice versa) (10 points)\nFor these regions:\n\nWhat cell types or marks show the most differences? (5 points)\nAre these differences consistent across both haplotypes? (5 points)\nWhat might be the functional significance of these differences? (10 points)\n\n\n\n\n\n\nWhy do we need to one-hot encode the sequences? (5 points)\nWhat is the purpose of the SEQUENCE_LENGTH parameter? (5 points)\nHow does the model handle the two haplotypes in the Neanderthal genome? (10 points)\n\n\n\n\nUse enformer to predict the DNA binding score from project 1. Compare the results with your original DNA binding score.\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-05-01-unit03/homework-06.html#data-setup-and-basic-analysis-25-points",
    "href": "post/2025-05-01-unit03/homework-06.html#data-setup-and-basic-analysis-25-points",
    "title": "homework 6",
    "section": "",
    "text": "Download the Enformer model and reference genome from the provided Box links (5 points)\nChoose a gene of interest and find its transcription start site (TSS) (5 points)\nModify the notebook code to predict human epigenome at this location (5 points)\nRun Enformer on the Neanderthal sequence at the same location (5 points)\nCreate scatter plots comparing human and Neanderthal predictions. Choose relevant tracks to compare (5 points)"
  },
  {
    "objectID": "post/2025-05-01-unit03/homework-06.html#comparative-analysis-25-points",
    "href": "post/2025-05-01-unit03/homework-06.html#comparative-analysis-25-points",
    "title": "homework 6",
    "section": "",
    "text": "Calculate correlation coefficients between human and Neanderthal predictions for each track (10 points)\nIdentify tracks showing the highest and lowest correlations (5 points)\nWhat might explain these differences in correlation? (10 points)"
  },
  {
    "objectID": "post/2025-05-01-unit03/homework-06.html#peak-analysis-30-points",
    "href": "post/2025-05-01-unit03/homework-06.html#peak-analysis-30-points",
    "title": "homework 6",
    "section": "",
    "text": "Identify regions where peaks are present in human but absent in Neanderthal (or vice versa) (10 points)\nFor these regions:\n\nWhat cell types or marks show the most differences? (5 points)\nAre these differences consistent across both haplotypes? (5 points)\nWhat might be the functional significance of these differences? (10 points)"
  },
  {
    "objectID": "post/2025-05-01-unit03/homework-06.html#technical-understanding-20-points",
    "href": "post/2025-05-01-unit03/homework-06.html#technical-understanding-20-points",
    "title": "homework 6",
    "section": "",
    "text": "Why do we need to one-hot encode the sequences? (5 points)\nWhat is the purpose of the SEQUENCE_LENGTH parameter? (5 points)\nHow does the model handle the two haplotypes in the Neanderthal genome? (10 points)"
  },
  {
    "objectID": "post/2025-05-01-unit03/homework-06.html#extra-credit-10-points",
    "href": "post/2025-05-01-unit03/homework-06.html#extra-credit-10-points",
    "title": "homework 6",
    "section": "",
    "text": "Use enformer to predict the DNA binding score from project 1. Compare the results with your original DNA binding score."
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer.html",
    "href": "post/2025-05-01-unit03/enformer.html",
    "title": "Enformer Architecture",
    "section": "",
    "text": "Enformer is a deep learning model designed for predicting gene expression and other functional genomics signals directly from DNA sequence. It was introduced by DeepMind and Calico, and is notable for its ability to model long-range interactions in DNA using transformer architectures.\n\n\n\n\n\nThe main model class (Enformer) inherits from snt.Module (Sonnet, a neural network library).\n\n\n\nInputs: DNA sequence (as a one-hot encoded tensor)\nOutputs: Predictions for multiple genomic tracks (e.g., gene expression, chromatin accessibility) for human and mouse\n\n\n\n\n\nchannels: Model width (number of convolutional filters)\nnum_transformer_layers: Number of transformer blocks\nnum_heads: Number of attention heads in each transformer block\npooling_type: Type of pooling (attention or max)\nname: Module name\n\n\n\n\n\nStem: Initial convolution and pooling layers to process the input\nConvolutional Tower: Series of convolutional blocks with increasing filter sizes, interleaved with pooling\nTransformer Blocks: Stack of transformer layers to capture long-range dependencies\nCrop Layer: Crops the output to a fixed target length\nFinal Pointwise Layer: Final convolution and activation\nHeads: Separate output layers for human and mouse predictions\n\n\n\n\n\n\nSequential: Custom sequential container that passes is_training flag to layers that accept it\nResidual: Residual connection wrapper (adds input to output of a submodule)\nSoftmaxPooling1D: Custom pooling layer that uses softmax-weighted pooling\nTargetLengthCrop1D: Crops the sequence to a target length (centered)\ngelu: Gaussian Error Linear Unit activation function\none_hot_encode: Utility to convert DNA sequence strings to one-hot encoded numpy arrays\n\n\n\n\nThe model processes the input sequence through: 1. The stem 2. Convolutional tower 3. Transformer stack 4. Cropping 5. Final pointwise layer\nThe resulting embedding is passed to each output head (for human and mouse), producing the final predictions.\n\n\n\n\n\nLong-range modeling: Uses transformers to capture dependencies across hundreds of thousands of base pairs\nMulti-task: Predicts thousands of genomic signals at once\nFlexible pooling: Uses attention-based pooling to better aggregate information\n\n\n\n\n\nInput: One-hot encoded DNA sequence of length 196,608 (with 4 channels for A, C, G, T)\nOutput: Dictionary with predictions for human and mouse tracks\n\n\n\n\n\n\n\nComponent\nPurpose\n\n\n\n\nStem\nInitial feature extraction\n\n\nConv Tower\nHierarchical feature extraction\n\n\nTransformer\nLong-range dependency modeling\n\n\nCrop\nFix output length\n\n\nFinal Pointwise\nFinal feature transformation\n\n\nHeads\nTask-specific outputs (human/mouse)\n\n\n\n\n\n\nThe following code shows the TensorFlow implementation of the Enformer model, downloaded from enformer.py and annotated with Gemini.\n```{python}\n# Copyright 2021 DeepMind Technologies Limited\n# Licensed under the Apache License, Version 2.0\n\n\"\"\"Tensorflow implementation of Enformer model.\n\"Effective gene expression prediction from sequence by integrating long-range interactions\"\nAuthors: ≈Ωiga Avsec1, Vikram Agarwal2,4, Daniel Visentin1,4, Joseph R. Ledsam1,3,\nAgnieszka Grabska-Barwinska1, Kyle R. Taylor1, Yannis Assael1, John Jumper1,\nPushmeet Kohli1, David R. Kelley2*\n\n1 DeepMind, London, UK\n2 Calico Life Sciences, South San Francisco, CA, USA\n3 Google, Tokyo, Japan\n4 These authors contributed equally.\n* correspondence: avsec@google.com, pushmeet@google.com, drk@calicolabs.com\n\"\"\"\n\nimport inspect\nfrom typing import Any, Callable, Dict, Optional, Text, Union, Iterable\nimport attention_module\nimport numpy as np\nimport sonnet as snt\nimport tensorflow as tf\n\n# Model configuration constants\nSEQUENCE_LENGTH = 196_608  # Input DNA sequence length\nBIN_SIZE = 128            # Output bin size\nTARGET_LENGTH = 896       # Target output length\n\nclass Enformer(snt.Module):\n    \"\"\"Main model for predicting genomic signals from DNA sequence.\"\"\"\n\n    def __init__(self,\n                 channels: int = 1536,              # Model width/dimensionality\n                 num_transformer_layers: int = 11,  # Number of transformer blocks\n                 num_heads: int = 8,                # Number of attention heads\n                 pooling_type: str = 'attention',   # Pooling type ('attention' or 'max')\n                 name: str = 'enformer'):           # Module name\n        super().__init__(name=name)\n        \n        # Output channels for different species\n        heads_channels = {'human': 5313, 'mouse': 1643}\n        dropout_rate = 0.4\n        \n        # Validate model configuration\n        assert channels % num_heads == 0, ('channels must be divisible by num_heads')\n\n        # Multi-head attention configuration\n        whole_attention_kwargs = {\n            'attention_dropout_rate': 0.05,\n            'initializer': None,\n            'key_size': 64,\n            'num_heads': num_heads,\n            'num_relative_position_features': channels // num_heads,\n            'positional_dropout_rate': 0.01,\n            'relative_position_functions': [\n                'positional_features_exponential',\n                'positional_features_central_mask',\n                'positional_features_gamma'\n            ],\n            'relative_positions': True,\n            'scaling': True,\n            'value_size': channels // num_heads,\n            'zero_initialize': True\n        }\n\n        # Build model trunk\n        with tf.name_scope('trunk'):\n            # Convolutional block helper\n            def conv_block(filters, width=1, w_init=None, name='conv_block', **kwargs):\n                return Sequential(lambda: [\n                    snt.distribute.CrossReplicaBatchNorm(\n                        create_scale=True,\n                        create_offset=True,\n                        scale_init=snt.initializers.Ones(),\n                        moving_mean=snt.ExponentialMovingAverage(0.9),\n                        moving_variance=snt.ExponentialMovingAverage(0.9)),\n                    gelu,\n                    snt.Conv1D(filters, width, w_init=w_init, **kwargs)\n                ], name=name)\n\n            # Initial stem\n            stem = Sequential(lambda: [\n                snt.Conv1D(channels // 2, 15),\n                Residual(conv_block(channels // 2, 1, name='pointwise_conv_block')),\n                pooling_module(pooling_type, pool_size=2),\n            ], name='stem')\n\n            # Convolutional tower\n            filter_list = exponential_linspace_int(\n                start=channels // 2, \n                end=channels,\n                num=6, \n                divisible_by=128\n            )\n            \n            conv_tower = Sequential(lambda: [\n                Sequential(lambda: [\n                    conv_block(num_filters, 5),\n                    Residual(conv_block(num_filters, 1, name='pointwise_conv_block')),\n                    pooling_module(pooling_type, pool_size=2),\n                ], name=f'conv_tower_block_{i}')\n                for i, num_filters in enumerate(filter_list)\n            ], name='conv_tower')\n\n            # Transformer blocks\n            def transformer_mlp():\n                return Sequential(lambda: [\n                    snt.LayerNorm(axis=-1, create_scale=True, create_offset=True),\n                    snt.Linear(channels * 2),\n                    snt.Dropout(dropout_rate),\n                    tf.nn.relu,\n                    snt.Linear(channels),\n                    snt.Dropout(dropout_rate)\n                ], name='mlp')\n\n            transformer = Sequential(lambda: [\n                Sequential(lambda: [\n                    Residual(Sequential(lambda: [\n                        snt.LayerNorm(axis=-1, create_scale=True, create_offset=True,\n                                    scale_init=snt.initializers.Ones()),\n                        attention_module.MultiheadAttention(**whole_attention_kwargs,\n                                                          name=f'attention_{i}'),\n                        snt.Dropout(dropout_rate)\n                    ], name='mha')),\n                    Residual(transformer_mlp())\n                ], name=f'transformer_block_{i}')\n                for i in range(num_transformer_layers)\n            ], name='transformer')\n\n            # Final layers\n            crop_final = TargetLengthCrop1D(TARGET_LENGTH, name='target_input')\n            final_pointwise = Sequential(lambda: [\n                conv_block(channels * 2, 1),\n                snt.Dropout(dropout_rate / 8),\n                gelu\n            ], name='final_pointwise')\n\n            # Combine trunk modules\n            self._trunk = Sequential([\n                stem,\n                conv_tower,\n                transformer,\n                crop_final,\n                final_pointwise\n            ], name='trunk')\n\n        # Output heads\n        with tf.name_scope('heads'):\n            self._heads = {\n                head: Sequential(\n                    lambda: [snt.Linear(num_channels), tf.nn.softplus],\n                    name=f'head_{head}')\n                for head, num_channels in heads_channels.items()\n            }\n\n    @property\n    def trunk(self):\n        return self._trunk\n\n    @property\n    def heads(self):\n        return self._heads\n\n    def __call__(self, inputs: tf.Tensor, is_training: bool) -&gt; Dict[str, tf.Tensor]:\n        \"\"\"Forward pass through the model.\"\"\"\n        trunk_embedding = self.trunk(inputs, is_training=is_training)\n        return {\n            head: head_module(trunk_embedding, is_training=is_training)\n            for head, head_module in self.heads.items()\n        }\n\n    @tf.function(input_signature=[\n        tf.TensorSpec([None, SEQUENCE_LENGTH, 4], tf.float32)])\n    def predict_on_batch(self, x):\n        \"\"\"Prediction method for SavedModel.\"\"\"\n        return self(x, is_training=False)\n\nclass TargetLengthCrop1D(snt.Module):\n    \"\"\"Crops sequence to target length.\"\"\"\n\n    def __init__(self, target_length: Optional[int], name: str = 'target_length_crop'):\n        super().__init__(name=name)\n        self._target_length = target_length\n\n    def __call__(self, inputs):\n        if self._target_length is None:\n            return inputs\n        trim = (inputs.shape[-2] - self._target_length) // 2\n        if trim &lt; 0:\n            raise ValueError('inputs longer than target length')\n        elif trim == 0:\n            return inputs\n        else:\n            return inputs[..., trim:-trim, :]\n\nclass Sequential(snt.Module):\n    \"\"\"Sequential container with is_training support.\"\"\"\n\n    def __init__(self,\n                 layers: Optional[Union[Callable[[], Iterable[snt.Module]],\n                                      Iterable[Callable[..., Any]]]] = None,\n                 name: Optional[Text] = None):\n        super().__init__(name=name)\n        if layers is None:\n            self._layers = []\n        else:\n            if hasattr(layers, '__call__'):\n                layers = layers()\n            self._layers = [layer for layer in layers if layer is not None]\n\n    def __call__(self, inputs: tf.Tensor, is_training: bool, **kwargs):\n        outputs = inputs\n        for mod in self._layers:\n            if accepts_is_training(mod):\n                outputs = mod(outputs, is_training=is_training, **kwargs)\n            else:\n                outputs = mod(outputs, **kwargs)\n        return outputs\n\ndef pooling_module(kind, pool_size):\n    \"\"\"Returns appropriate pooling module.\"\"\"\n    if kind == 'attention':\n        return SoftmaxPooling1D(pool_size=pool_size, per_channel=True,\n                              w_init_scale=2.0)\n    elif kind == 'max':\n        return tf.keras.layers.MaxPool1D(pool_size=pool_size, padding='same')\n    else:\n        raise ValueError(f'Invalid pooling kind: {kind}.')\n\nclass SoftmaxPooling1D(snt.Module):\n    \"\"\"Softmax-weighted pooling operation.\"\"\"\n\n    def __init__(self,\n                 pool_size: int = 2,\n                 per_channel: bool = False,\n                 w_init_scale: float = 0.0,\n                 name: str = 'softmax_pooling'):\n        super().__init__(name=name)\n        self._pool_size = pool_size\n        self._per_channel = per_channel\n        self._w_init_scale = w_init_scale\n        self._logit_linear = None\n\n    @snt.once\n    def _initialize(self, num_features):\n        self._logit_linear = snt.Linear(\n            output_size=num_features if self._per_channel else 1,\n            with_bias=False,\n            w_init=snt.initializers.Identity(self._w_init_scale))\n\n    def __call__(self, inputs):\n        _, length, num_features = inputs.shape\n        self._initialize(num_features)\n        inputs = tf.reshape(\n            inputs,\n            (-1, length // self._pool_size, self._pool_size, num_features))\n        return tf.reduce_sum(\n            inputs * tf.nn.softmax(self._logit_linear(inputs), axis=-2),\n            axis=-2)\n\nclass Residual(snt.Module):\n    \"\"\"Residual connection wrapper.\"\"\"\n\n    def __init__(self, module: snt.Module, name='residual'):\n        super().__init__(name=name)\n        self._module = module\n\n    def __call__(self, inputs: tf.Tensor, is_training: bool, *args,\n                 **kwargs) -&gt; tf.Tensor:\n        return inputs + self._module(inputs, is_training, *args, **kwargs)\n\ndef gelu(x: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"Gaussian Error Linear Unit activation function.\"\"\"\n    return tf.nn.sigmoid(1.702 * x) * x\n\ndef one_hot_encode(sequence: str,\n                   alphabet: str = 'ACGT',\n                   neutral_alphabet: str = 'N',\n                   neutral_value: Any = 0,\n                   dtype=np.float32) -&gt; np.ndarray:\n    \"\"\"One-hot encodes DNA sequence.\"\"\"\n    def to_uint8(string):\n        return np.frombuffer(string.encode('ascii'), dtype=np.uint8)\n    \n    hash_table = np.zeros((np.iinfo(np.uint8).max, len(alphabet)), dtype=dtype)\n    hash_table[to_uint8(alphabet)] = np.eye(len(alphabet), dtype=dtype)\n    hash_table[to_uint8(neutral_alphabet)] = neutral_value\n    hash_table = hash_table.astype(dtype)\n    return hash_table[to_uint8(sequence)]\n\ndef exponential_linspace_int(start, end, num, divisible_by=1):\n    \"\"\"Generates exponentially spaced integers.\"\"\"\n    def _round(x):\n        return int(np.round(x / divisible_by) * divisible_by)\n\n    base = np.exp(np.log(end / start) / (num - 1))\n    return [_round(start * base**i) for i in range(num)]\n\ndef accepts_is_training(module):\n    \"\"\"Checks if module accepts is_training parameter.\"\"\"\n    return 'is_training' in list(inspect.signature(module.__call__).parameters)\n```\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer.html#what-is-enformer",
    "href": "post/2025-05-01-unit03/enformer.html#what-is-enformer",
    "title": "Enformer Architecture",
    "section": "",
    "text": "Enformer is a deep learning model designed for predicting gene expression and other functional genomics signals directly from DNA sequence. It was introduced by DeepMind and Calico, and is notable for its ability to model long-range interactions in DNA using transformer architectures."
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer.html#key-components-in-the-code",
    "href": "post/2025-05-01-unit03/enformer.html#key-components-in-the-code",
    "title": "Enformer Architecture",
    "section": "",
    "text": "The main model class (Enformer) inherits from snt.Module (Sonnet, a neural network library).\n\n\n\nInputs: DNA sequence (as a one-hot encoded tensor)\nOutputs: Predictions for multiple genomic tracks (e.g., gene expression, chromatin accessibility) for human and mouse\n\n\n\n\n\nchannels: Model width (number of convolutional filters)\nnum_transformer_layers: Number of transformer blocks\nnum_heads: Number of attention heads in each transformer block\npooling_type: Type of pooling (attention or max)\nname: Module name\n\n\n\n\n\nStem: Initial convolution and pooling layers to process the input\nConvolutional Tower: Series of convolutional blocks with increasing filter sizes, interleaved with pooling\nTransformer Blocks: Stack of transformer layers to capture long-range dependencies\nCrop Layer: Crops the output to a fixed target length\nFinal Pointwise Layer: Final convolution and activation\nHeads: Separate output layers for human and mouse predictions\n\n\n\n\n\n\nSequential: Custom sequential container that passes is_training flag to layers that accept it\nResidual: Residual connection wrapper (adds input to output of a submodule)\nSoftmaxPooling1D: Custom pooling layer that uses softmax-weighted pooling\nTargetLengthCrop1D: Crops the sequence to a target length (centered)\ngelu: Gaussian Error Linear Unit activation function\none_hot_encode: Utility to convert DNA sequence strings to one-hot encoded numpy arrays\n\n\n\n\nThe model processes the input sequence through: 1. The stem 2. Convolutional tower 3. Transformer stack 4. Cropping 5. Final pointwise layer\nThe resulting embedding is passed to each output head (for human and mouse), producing the final predictions."
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer.html#why-is-enformer-special",
    "href": "post/2025-05-01-unit03/enformer.html#why-is-enformer-special",
    "title": "Enformer Architecture",
    "section": "",
    "text": "Long-range modeling: Uses transformers to capture dependencies across hundreds of thousands of base pairs\nMulti-task: Predicts thousands of genomic signals at once\nFlexible pooling: Uses attention-based pooling to better aggregate information"
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer.html#example-usage",
    "href": "post/2025-05-01-unit03/enformer.html#example-usage",
    "title": "Enformer Architecture",
    "section": "",
    "text": "Input: One-hot encoded DNA sequence of length 196,608 (with 4 channels for A, C, G, T)\nOutput: Dictionary with predictions for human and mouse tracks"
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer.html#summary-table",
    "href": "post/2025-05-01-unit03/enformer.html#summary-table",
    "title": "Enformer Architecture",
    "section": "",
    "text": "Component\nPurpose\n\n\n\n\nStem\nInitial feature extraction\n\n\nConv Tower\nHierarchical feature extraction\n\n\nTransformer\nLong-range dependency modeling\n\n\nCrop\nFix output length\n\n\nFinal Pointwise\nFinal feature transformation\n\n\nHeads\nTask-specific outputs (human/mouse)"
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer.html#enformer-class-implementation",
    "href": "post/2025-05-01-unit03/enformer.html#enformer-class-implementation",
    "title": "Enformer Architecture",
    "section": "",
    "text": "The following code shows the TensorFlow implementation of the Enformer model, downloaded from enformer.py and annotated with Gemini.\n```{python}\n# Copyright 2021 DeepMind Technologies Limited\n# Licensed under the Apache License, Version 2.0\n\n\"\"\"Tensorflow implementation of Enformer model.\n\"Effective gene expression prediction from sequence by integrating long-range interactions\"\nAuthors: ≈Ωiga Avsec1, Vikram Agarwal2,4, Daniel Visentin1,4, Joseph R. Ledsam1,3,\nAgnieszka Grabska-Barwinska1, Kyle R. Taylor1, Yannis Assael1, John Jumper1,\nPushmeet Kohli1, David R. Kelley2*\n\n1 DeepMind, London, UK\n2 Calico Life Sciences, South San Francisco, CA, USA\n3 Google, Tokyo, Japan\n4 These authors contributed equally.\n* correspondence: avsec@google.com, pushmeet@google.com, drk@calicolabs.com\n\"\"\"\n\nimport inspect\nfrom typing import Any, Callable, Dict, Optional, Text, Union, Iterable\nimport attention_module\nimport numpy as np\nimport sonnet as snt\nimport tensorflow as tf\n\n# Model configuration constants\nSEQUENCE_LENGTH = 196_608  # Input DNA sequence length\nBIN_SIZE = 128            # Output bin size\nTARGET_LENGTH = 896       # Target output length\n\nclass Enformer(snt.Module):\n    \"\"\"Main model for predicting genomic signals from DNA sequence.\"\"\"\n\n    def __init__(self,\n                 channels: int = 1536,              # Model width/dimensionality\n                 num_transformer_layers: int = 11,  # Number of transformer blocks\n                 num_heads: int = 8,                # Number of attention heads\n                 pooling_type: str = 'attention',   # Pooling type ('attention' or 'max')\n                 name: str = 'enformer'):           # Module name\n        super().__init__(name=name)\n        \n        # Output channels for different species\n        heads_channels = {'human': 5313, 'mouse': 1643}\n        dropout_rate = 0.4\n        \n        # Validate model configuration\n        assert channels % num_heads == 0, ('channels must be divisible by num_heads')\n\n        # Multi-head attention configuration\n        whole_attention_kwargs = {\n            'attention_dropout_rate': 0.05,\n            'initializer': None,\n            'key_size': 64,\n            'num_heads': num_heads,\n            'num_relative_position_features': channels // num_heads,\n            'positional_dropout_rate': 0.01,\n            'relative_position_functions': [\n                'positional_features_exponential',\n                'positional_features_central_mask',\n                'positional_features_gamma'\n            ],\n            'relative_positions': True,\n            'scaling': True,\n            'value_size': channels // num_heads,\n            'zero_initialize': True\n        }\n\n        # Build model trunk\n        with tf.name_scope('trunk'):\n            # Convolutional block helper\n            def conv_block(filters, width=1, w_init=None, name='conv_block', **kwargs):\n                return Sequential(lambda: [\n                    snt.distribute.CrossReplicaBatchNorm(\n                        create_scale=True,\n                        create_offset=True,\n                        scale_init=snt.initializers.Ones(),\n                        moving_mean=snt.ExponentialMovingAverage(0.9),\n                        moving_variance=snt.ExponentialMovingAverage(0.9)),\n                    gelu,\n                    snt.Conv1D(filters, width, w_init=w_init, **kwargs)\n                ], name=name)\n\n            # Initial stem\n            stem = Sequential(lambda: [\n                snt.Conv1D(channels // 2, 15),\n                Residual(conv_block(channels // 2, 1, name='pointwise_conv_block')),\n                pooling_module(pooling_type, pool_size=2),\n            ], name='stem')\n\n            # Convolutional tower\n            filter_list = exponential_linspace_int(\n                start=channels // 2, \n                end=channels,\n                num=6, \n                divisible_by=128\n            )\n            \n            conv_tower = Sequential(lambda: [\n                Sequential(lambda: [\n                    conv_block(num_filters, 5),\n                    Residual(conv_block(num_filters, 1, name='pointwise_conv_block')),\n                    pooling_module(pooling_type, pool_size=2),\n                ], name=f'conv_tower_block_{i}')\n                for i, num_filters in enumerate(filter_list)\n            ], name='conv_tower')\n\n            # Transformer blocks\n            def transformer_mlp():\n                return Sequential(lambda: [\n                    snt.LayerNorm(axis=-1, create_scale=True, create_offset=True),\n                    snt.Linear(channels * 2),\n                    snt.Dropout(dropout_rate),\n                    tf.nn.relu,\n                    snt.Linear(channels),\n                    snt.Dropout(dropout_rate)\n                ], name='mlp')\n\n            transformer = Sequential(lambda: [\n                Sequential(lambda: [\n                    Residual(Sequential(lambda: [\n                        snt.LayerNorm(axis=-1, create_scale=True, create_offset=True,\n                                    scale_init=snt.initializers.Ones()),\n                        attention_module.MultiheadAttention(**whole_attention_kwargs,\n                                                          name=f'attention_{i}'),\n                        snt.Dropout(dropout_rate)\n                    ], name='mha')),\n                    Residual(transformer_mlp())\n                ], name=f'transformer_block_{i}')\n                for i in range(num_transformer_layers)\n            ], name='transformer')\n\n            # Final layers\n            crop_final = TargetLengthCrop1D(TARGET_LENGTH, name='target_input')\n            final_pointwise = Sequential(lambda: [\n                conv_block(channels * 2, 1),\n                snt.Dropout(dropout_rate / 8),\n                gelu\n            ], name='final_pointwise')\n\n            # Combine trunk modules\n            self._trunk = Sequential([\n                stem,\n                conv_tower,\n                transformer,\n                crop_final,\n                final_pointwise\n            ], name='trunk')\n\n        # Output heads\n        with tf.name_scope('heads'):\n            self._heads = {\n                head: Sequential(\n                    lambda: [snt.Linear(num_channels), tf.nn.softplus],\n                    name=f'head_{head}')\n                for head, num_channels in heads_channels.items()\n            }\n\n    @property\n    def trunk(self):\n        return self._trunk\n\n    @property\n    def heads(self):\n        return self._heads\n\n    def __call__(self, inputs: tf.Tensor, is_training: bool) -&gt; Dict[str, tf.Tensor]:\n        \"\"\"Forward pass through the model.\"\"\"\n        trunk_embedding = self.trunk(inputs, is_training=is_training)\n        return {\n            head: head_module(trunk_embedding, is_training=is_training)\n            for head, head_module in self.heads.items()\n        }\n\n    @tf.function(input_signature=[\n        tf.TensorSpec([None, SEQUENCE_LENGTH, 4], tf.float32)])\n    def predict_on_batch(self, x):\n        \"\"\"Prediction method for SavedModel.\"\"\"\n        return self(x, is_training=False)\n\nclass TargetLengthCrop1D(snt.Module):\n    \"\"\"Crops sequence to target length.\"\"\"\n\n    def __init__(self, target_length: Optional[int], name: str = 'target_length_crop'):\n        super().__init__(name=name)\n        self._target_length = target_length\n\n    def __call__(self, inputs):\n        if self._target_length is None:\n            return inputs\n        trim = (inputs.shape[-2] - self._target_length) // 2\n        if trim &lt; 0:\n            raise ValueError('inputs longer than target length')\n        elif trim == 0:\n            return inputs\n        else:\n            return inputs[..., trim:-trim, :]\n\nclass Sequential(snt.Module):\n    \"\"\"Sequential container with is_training support.\"\"\"\n\n    def __init__(self,\n                 layers: Optional[Union[Callable[[], Iterable[snt.Module]],\n                                      Iterable[Callable[..., Any]]]] = None,\n                 name: Optional[Text] = None):\n        super().__init__(name=name)\n        if layers is None:\n            self._layers = []\n        else:\n            if hasattr(layers, '__call__'):\n                layers = layers()\n            self._layers = [layer for layer in layers if layer is not None]\n\n    def __call__(self, inputs: tf.Tensor, is_training: bool, **kwargs):\n        outputs = inputs\n        for mod in self._layers:\n            if accepts_is_training(mod):\n                outputs = mod(outputs, is_training=is_training, **kwargs)\n            else:\n                outputs = mod(outputs, **kwargs)\n        return outputs\n\ndef pooling_module(kind, pool_size):\n    \"\"\"Returns appropriate pooling module.\"\"\"\n    if kind == 'attention':\n        return SoftmaxPooling1D(pool_size=pool_size, per_channel=True,\n                              w_init_scale=2.0)\n    elif kind == 'max':\n        return tf.keras.layers.MaxPool1D(pool_size=pool_size, padding='same')\n    else:\n        raise ValueError(f'Invalid pooling kind: {kind}.')\n\nclass SoftmaxPooling1D(snt.Module):\n    \"\"\"Softmax-weighted pooling operation.\"\"\"\n\n    def __init__(self,\n                 pool_size: int = 2,\n                 per_channel: bool = False,\n                 w_init_scale: float = 0.0,\n                 name: str = 'softmax_pooling'):\n        super().__init__(name=name)\n        self._pool_size = pool_size\n        self._per_channel = per_channel\n        self._w_init_scale = w_init_scale\n        self._logit_linear = None\n\n    @snt.once\n    def _initialize(self, num_features):\n        self._logit_linear = snt.Linear(\n            output_size=num_features if self._per_channel else 1,\n            with_bias=False,\n            w_init=snt.initializers.Identity(self._w_init_scale))\n\n    def __call__(self, inputs):\n        _, length, num_features = inputs.shape\n        self._initialize(num_features)\n        inputs = tf.reshape(\n            inputs,\n            (-1, length // self._pool_size, self._pool_size, num_features))\n        return tf.reduce_sum(\n            inputs * tf.nn.softmax(self._logit_linear(inputs), axis=-2),\n            axis=-2)\n\nclass Residual(snt.Module):\n    \"\"\"Residual connection wrapper.\"\"\"\n\n    def __init__(self, module: snt.Module, name='residual'):\n        super().__init__(name=name)\n        self._module = module\n\n    def __call__(self, inputs: tf.Tensor, is_training: bool, *args,\n                 **kwargs) -&gt; tf.Tensor:\n        return inputs + self._module(inputs, is_training, *args, **kwargs)\n\ndef gelu(x: tf.Tensor) -&gt; tf.Tensor:\n    \"\"\"Gaussian Error Linear Unit activation function.\"\"\"\n    return tf.nn.sigmoid(1.702 * x) * x\n\ndef one_hot_encode(sequence: str,\n                   alphabet: str = 'ACGT',\n                   neutral_alphabet: str = 'N',\n                   neutral_value: Any = 0,\n                   dtype=np.float32) -&gt; np.ndarray:\n    \"\"\"One-hot encodes DNA sequence.\"\"\"\n    def to_uint8(string):\n        return np.frombuffer(string.encode('ascii'), dtype=np.uint8)\n    \n    hash_table = np.zeros((np.iinfo(np.uint8).max, len(alphabet)), dtype=dtype)\n    hash_table[to_uint8(alphabet)] = np.eye(len(alphabet), dtype=dtype)\n    hash_table[to_uint8(neutral_alphabet)] = neutral_value\n    hash_table = hash_table.astype(dtype)\n    return hash_table[to_uint8(sequence)]\n\ndef exponential_linspace_int(start, end, num, divisible_by=1):\n    \"\"\"Generates exponentially spaced integers.\"\"\"\n    def _round(x):\n        return int(np.round(x / divisible_by) * divisible_by)\n\n    base = np.exp(np.log(end / start) / (num - 1))\n    return [_round(start * base**i) for i in range(num)]\n\ndef accepts_is_training(module):\n    \"\"\"Checks if module accepts is_training parameter.\"\"\"\n    return 'is_training' in list(inspect.signature(module.__call__).parameters)\n```"
  },
  {
    "objectID": "post/2025-05-01-unit03/borzoi_personal_gen_prediction.html",
    "href": "post/2025-05-01-unit03/borzoi_personal_gen_prediction.html",
    "title": "Borzoi prediction from personal genome - qmd version",
    "section": "",
    "text": "This notebook demonstrates how to use Borzoi, a deep learning model for predicting genomic regulatory activity, to analyze personal genome sequences. We‚Äôll learn how to:\n\nSet up the environment and install required packages\nDownload and prepare model files and reference data\nProcess personal genome data from VCF files\nMake predictions using Borzoi\nVisualize and interpret the results\n\n\n\nFirst, we need to set up our Python environment. Borzoi requires Python 3.10 or lower due to compatibility requirements.\nconda create --name borzoi46100 python=3.10\n\n\nShow the code\nimport os\nGITHUB_DIR = '/Users/haekyungim/Github'\n\n# clone baskerville and borzoi if not already cloned\nbaskerville_path = os.path.join(GITHUB_DIR, 'baskerville')\nborzoi_path = os.path.join(GITHUB_DIR, 'borzoi')\nif not os.path.exists(baskerville_path):\n    !git clone https://github.com/calico/baskerville.git {GITHUB_DIR}/baskerville\n\nif not os.path.exists(borzoi_path):\n    !git clone https://github.com/calico/borzoi.git {GITHUB_DIR}/borzoi\n\n\nAfter loading baskerville, restart runtime, run code from here\nInstall libraries\n\n\nShow the code\ntry:\n    import baskerville\nexcept ImportError:\n    !pip install -e {GITHUB_DIR}/baskerville\n\n\n\n\nShow the code\ntry:\n    import borzoi\nexcept ImportError:\n    !pip install -e {GITHUB_DIR}/borzoi\n\n\nNOTE: install tensorflow-metal if running on a mac\n\n\nShow the code\n# import platform\n# if platform.processor() == 'arm':\n#     print(\"Apple Silicon detected, installing tensorflow-metal...\")\n#     %pip install tensorflow-metal\n# else:\n#     print(\"Not running on Apple Silicon, skipping tensorflow-metal installation\")\n\n\n\n\n\n\nShow the code\ntry:\n    import kipoiseq\nexcept ImportError:\n    %pip install kipoiseq\nfrom kipoiseq import Interval\ntry:\n    import cyvcf2\nexcept ImportError:\n    %pip install cyvcf2\nimport os\nimport time\nimport io\nimport gzip\n\n#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n\nimport h5py\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport baskerville\nfrom baskerville import seqnn\nfrom baskerville import gene as bgene\nfrom baskerville import dna\n\nimport json\n\nimport pysam\n\nimport pyfaidx\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n\n\n\n\n\n\nShow the code\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\ngpu_device = tf.config.list_physical_devices('GPU')[0]\ntf.config.experimental.set_memory_growth(gpu_device, True)\n\n\n\n\n\n\n\nShow the code\nPRE = '/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100'\nCONTENT_DIR = PRE + '/borzoi'\n# Create borzoi/data directory if it doesn't exist\nif not os.path.exists(os.path.join(CONTENT_DIR)):\n    !mkdir {CONTENT_DIR}\nif not os.path.exists(os.path.join(CONTENT_DIR, 'data')):\n    !mkdir {CONTENT_DIR}/data\n\n# Create model file structure\nsaved_models_path = os.path.join(CONTENT_DIR, 'data/saved_models')\nif not os.path.exists(saved_models_path):\n    !mkdir {saved_models_path}\nif not os.path.exists(os.path.join(saved_models_path, 'f0')):\n    !mkdir {saved_models_path}/f0\nif not os.path.exists(os.path.join(saved_models_path, 'f1')):\n    !mkdir {saved_models_path}/f1\nif not os.path.exists(os.path.join(saved_models_path, 'f2')):\n    !mkdir {saved_models_path}/f2\nif not os.path.exists(os.path.join(saved_models_path, 'f3')):\n    !mkdir {saved_models_path}/f3\n\n\n\n\nShow the code\n#Download model weights\nif not os.path.exists(os.path.join(saved_models_path, 'f0', 'model0_best.h5')):\n    !wget https://storage.googleapis.com/seqnn-share/borzoi/f0/model0_best.h5 -O {saved_models_path}/f0/model0_best.h5\nif not os.path.exists(os.path.join(saved_models_path, 'f1', 'model0_best.h5')): \n    !wget https://storage.googleapis.com/seqnn-share/borzoi/f1/model0_best.h5 -O {saved_models_path}/f1/model0_best.h5\nif not os.path.exists(os.path.join(saved_models_path, 'f2', 'model0_best.h5')):\n    !wget https://storage.googleapis.com/seqnn-share/borzoi/f2/model0_best.h5 -O {saved_models_path}/f2/model0_best.h5\nif not os.path.exists(os.path.join(saved_models_path, 'f3', 'model0_best.h5')):\n    !wget https://storage.googleapis.com/seqnn-share/borzoi/f3/model0_best.h5 -O {saved_models_path}/f3/model0_best.h5\n\n\n\n\nShow the code\n#Download and uncompress annotation files\nif not os.path.exists(os.path.join(CONTENT_DIR, 'data', 'gencode41_basic_nort.gtf')):\n    !wget  -O - https://storage.googleapis.com/seqnn-share/helper/gencode41_basic_nort.gtf.gz | gunzip -c &gt; {os.path.join(CONTENT_DIR, 'data', 'gencode41_basic_nort.gtf')}\nif not os.path.exists(os.path.join(CONTENT_DIR, 'data', 'gencode41_basic_protein_splice.csv.gz')):\n    !wget https://storage.googleapis.com/seqnn-share/helper/gencode41_basic_protein_splice.csv.gz -O {os.path.join(CONTENT_DIR, 'data', 'gencode41_basic_protein_splice.csv.gz')}\nif not os.path.exists(os.path.join(CONTENT_DIR, 'data', 'polyadb_human_v3.csv.gz')):\n    !wget https://storage.googleapis.com/seqnn-share/helper/polyadb_human_v3.csv.gz -O {os. path.join(CONTENT_DIR, 'data', 'polyadb_human_v3.csv.gz')}\n\n\n\n\n\n\n\nShow the code\nfasta_file = os.path.join(CONTENT_DIR, 'data/hg38.fa')\nif not os.path.exists(fasta_file):\n    !wget -O - http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz | gunzip -c &gt; {fasta_file}\n\n\n\n\nShow the code\n### download vcf file and index for chromosome 22\nVCF_FILE = os.path.join(CONTENT_DIR, 'data/ALL.chr22.shapeit2_integrated_SNPs_v2a_27022019.GRCh38.phased.gz')\nif not os.path.exists(VCF_FILE):\n    !wget https://uchicago.box.com/shared/static/u526pjh29w93ufn65yomlch4z8lkb1sp.gz -O {VCF_FILE}\n\n\n\n\nShow the code\n#!ls /content/data\n!ls {CONTENT_DIR}/data\n\n\n\n\nShow the code\n%cd {CONTENT_DIR}\n\n\n\n\n\n\nShow the code\n# !wget https://github.com/samtools/htslib/releases/download/1.9/htslib-1.9.tar.bz2\n# !tar -vxjf htslib-1.9.tar.bz2\n# %cd htslib-1.9\n# !autoreconf -i\n# !./configure\n# !make\n# !make install\n\n\n\n\nShow the code\n## if you Permission denied error on a mac, try\n# !brew install htslib\n\n\n\n\n\n\n\nShow the code\nif not os.path.exists(VCF_FILE + '.tbi'):\n    !tabix {VCF_FILE}\n\n\n\n\n\n\n\n\nShow the code\nmodels_path = CONTENT_DIR + '/data/saved_models/'\nparams_file = GITHUB_DIR + '/borzoi/examples/params_pred.json'\ntargets_file = GITHUB_DIR + '/borzoi/examples/targets_human.txt'\n\n\n\n\nShow the code\n# Load splice site annotation\n# splice_df = pd.read_csv(CONTENT_DIR+'/data/gencode41_basic_protein_splice.csv.gz', sep='\\t', compression='gzip')\n# print(\"len(splice_df) = \" + str(len(splice_df)))\n\n\nLoad transcriptome\n\n\nShow the code\ntranscriptome = bgene.Transcriptome(CONTENT_DIR + '/data/gencode41_basic_nort.gtf')\n\n\n\n\n\n\n\n\nFrom reference seq fasta: - Indexes ref sequence - Gets chromosome sizes into a dictionary\nExtract function: - gets target chromosome length from kipoiseq interval and chr sizes dictionary - truncates interval if it extends beyond chromosome lengths - extracts sequence\n\n\n\n\none hot encodes given sequence\n\n\n\n\n\nmakes prediction of ALL tracks given an encoded sequence\n\n\n\nShow the code\nclass FastaStringExtractor:\n\n    def __init__(self, fasta_file):\n        self.fasta = pyfaidx.Fasta(fasta_file)\n        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n    #import pd.Interval as Interval\n\n    def extract(self, interval: Interval, **kwargs) -&gt; str:\n        # Truncate interval if it extends beyond the chromosome lengths.\n        chromosome_length = self._chromosome_sizes[interval.chrom]\n        trimmed_interval = Interval(interval.chrom,\n                                    max(interval.start, 0),\n                                    min(interval.end, chromosome_length),\n                                    )\n        # pyfaidx wants a 1-based interval\n        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n                                          trimmed_interval.start + 1,\n                                          trimmed_interval.stop).seq).upper()\n        # Fill truncated values with N's.\n        pad_upstream = 'N' * max(-interval.start, 0)\n        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n        return pad_upstream + sequence + pad_downstream # returns padded sequence\n\n    def close(self):\n        return self.fasta.close()\n\n\n\n\nShow the code\n# Make one-hot coded sequence (modified from borzoi)\ndef make_seq_1hot(sequence):\n    return dna.dna_1hot(sequence).astype(\"float32\")\n\ndef predict_tracks(models, sequence_one_hot):\n\n  predicted_tracks = []\n  for fold_ix in range(len(models)):\n\n    yh = models[fold_ix](sequence_one_hot[None, ...])[:, None, ...].astype(\"float16\")\n\n    predicted_tracks.append(yh)\n\n  predicted_tracks = np.concatenate(predicted_tracks, axis=1)\n\n  return predicted_tracks\n\n\n\n\n\n\nModifies ref sequence to personal sequence given the vcf\n\n\n\nShow the code\ndef vcf_to_seq_faster(target_interval, individual, vcf_file, fasta_extractor):\n  # target_inverval is a kipoiseq interval, e.g. kipoiseq.Interval(\"chr22\", 18118779, 18145669)\n  # individual is the id of the individual in the vcf file\n\n  target_fa = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))\n  window_coords = target_interval.resize(SEQUENCE_LENGTH)\n      # two haplotypes per individual\n  haplo_1 = list(target_fa[:])\n  haplo_2 = list(target_fa[:])\n\n  # Open the VCF file\n  vcf_reader = cyvcf2.VCF(vcf_file)\n\n  # Specific genomic region\n  CHROM = window_coords.chrom\n  START = max(1,window_coords.start)\n  END = min(window_coords.end, fasta_extractor._chromosome_sizes[CHROM]-1)\n\n  count1 = 0\n  count2 = 0\n\n  # Iterate through variants in the specified region\n  for variant in vcf_reader(CHROM + ':' + str(START) + '-' + str(END)):\n      # Access the individual's genotype using index (0-based) of the sample\n      individual_index = vcf_reader.samples.index(individual)\n      genotype = variant.genotypes[individual_index]\n      ALT=variant.ALT[0]\n      REF=variant.REF\n      POS=variant.POS\n      if REF == target_fa[POS - window_coords.start - 1]:\n        if genotype[0] == 1:\n          haplo_1[POS - window_coords.start - 1] = ALT\n          count1 = count1 + 1\n        if genotype[1] == 1:\n          haplo_2[POS - window_coords.start - 1] = ALT\n          count2 = count2 + 1\n      else:\n        print(\"ERROR: REF in vcf \"+ REF + \"!= REF from the build\" + target_fa[POS - window_coords.start - 1])\n  print(\"number of changes haplo1:\")\n  print(count1)\n  print(\"number of changes haplo2:\")\n  print(count2)\n  return haplo_1, haplo_2\n\n\n\n\n\n\n\nShow the code\ndef make_prediction(gene, interval, haplo, sequence_one_hot, window = 131072): # snp_pos, alt_allele\n    with tf.device('/GPU:0'):\n        search_gene = gene # gene to predict\n\n        resized_interval = interval.resize(SEQUENCE_LENGTH)\n        start = resized_interval.start\n        end = resized_interval.end\n        center_pos = start + SEQUENCE_LENGTH//2 # figure center position\n\n        chrom = resized_interval.chr # chromosome\n\n        #Get exon bin range\n        gene_keys = [gene_key for gene_key in transcriptome.genes.keys() if search_gene in gene_key]\n\n        gene = transcriptome.genes[gene_keys[0]]\n\n        #Determine output sequence start\n        seq_out_start = start + seqnn_model.model_strides[0]*seqnn_model.target_crops[0]\n        seq_out_len = seqnn_model.model_strides[0]*seqnn_model.target_lengths[0]\n\n        #Determine output positions of gene exons\n        gene_slice = gene.output_slice(seq_out_start, seq_out_len, seqnn_model.model_strides[0], False)\n\n        #Make predictions\n        #y_wt\n        return predict_tracks(models, sequence_one_hot)\n\n\n\n\n\n\n\nShow the code\n# Get indexes of track\ndef get_track_idx(tracks):\n  track_idx = []\n  targets_df['local_index'] = np.arange(len(targets_df))\n  for track in tracks:\n    track_idx.append(targets_df.loc[targets_df['description'] == track]['local_index'].tolist())\n  return track_idx\n\n\n\n\n\n\n\nShow the code\ndef plot_tracks(prediction, interval, haplo, anno_df=None, tracks=[],\n                log = [False, False, False],\n                sqrt_scale = [False, False, False]):\n  # Gene slice not included\n  bin_size = 32\n  pad = 16\n\n  resized_interval = interval.resize(SEQUENCE_LENGTH)\n  start = resized_interval.start\n  end = resized_interval.end\n  center_pos = start + SEQUENCE_LENGTH//2 # figure center position\n  window=131072\n\n  plot_start = center_pos - window // 2\n  plot_end = center_pos + window // 2\n\n  plot_start_bin = (plot_start - start) // bin_size - pad\n  plot_end_bin = (plot_end - start) // bin_size - pad\n\n  track_idx = get_track_idx(tracks)\n  track_scales = [0.01, 0.01, 0.01]\n  track_transforms = [3./4., 3./4., 3./4.]\n  soft_clips = [384., 384., 384.]\n\n  # Get annotation positions\n  anno_poses = []\n  if anno_df is not None:\n      anno_poses = anno_df.query(\n          \"chrom == '\"\n          + interval.chr\n          + \"' and position_hg38 &gt;= \"\n          + str(plot_start)\n          + \" and position_hg38 &lt; \"\n          + str(plot_end)\n        )[\"position_hg38\"].values.tolist()\n  # BIG plot\n  fig, axes = plt.subplots(len(tracks), figsize=(20, 1.5 * len(tracks)), sharex = True)\n\n  for ax, track, track_index, track_scale, track_transform, clip_soft in zip(axes,\n        tracks, track_idx, track_scales, track_transforms, soft_clips):\n    # Plot track densities\n    y = np.array(np.copy(prediction), dtype = np.float32)\n    y = np.mean(y[..., track_index], axis=(0, 1, 3))\n    y = y[plot_start_bin:plot_end_bin]\n    signal = np.max(y)\n    if log:\n      y = np.log2(y + 1.0)\n    elif sqrt_scale:\n      y = np.sqrt(y + 1.0)\n\n    max_y = np.max(y)\n    print(\"Max signal for \"+ str(track) + \" = \" + str(round(signal, 4)))\n    print(\"Max transformed signal for \"+ str(track) + \" = \" + str(round(max_y, 4)))\n    print(\"---\")\n    ax.bar(np.arange(plot_end_bin - plot_start_bin) + plot_start_bin,\n            y,\n            width=1.0,\n            color=\"green\",\n            alpha=0.5)\n            # label=\"Prediction\",)\n    # X axis tick annotations\n    xtick_vals = []\n    for _, anno_pos in enumerate(anno_poses):\n      pas_bin = int((anno_pos - start) // 32) - 16\n      xtick_vals.append(pas_bin)\n      bin_end = pas_bin + 3 - 0.5\n      bin_start = bin_end - 5\n\n      ax.axvline(x=pas_bin,\n                color=\"cyan\",\n                linewidth=2,\n                alpha=0.5,\n                linestyle=\"-\",\n                zorder=-1,)\n\n    plt.xlim(plot_start_bin, plot_end_bin - 1)\n    plt.xticks([], [])\n    plt.yticks([], [])\n\n    ax.set(ylabel = \"Signal(log)\" if log else \"Signal\")# , fontsize = 8)\n    ax.set_title(\"Track: \" + str(track)+ \" in \"+ str(haplo), fontsize=10)\n    # plt.legend(fontsize=8)\n    plt.tight_layout()\n  ax.set_xlabel(\n            interval.chr\n            + \":\"\n            + str(plot_start)\n            + \"-\"\n            + str(plot_end)\n            + \" (\"\n            + str(window)\n            + \"bp window)\")\n            # fontsize=8,)\n  plt.show()\n\n\n\n\n\n\n\nShow the code\nfasta_extractor = FastaStringExtractor(fasta_file)\n\n\n\n\n\n\n\n\nShow the code\nSEQUENCE_LENGTH = 524288\n#n_folds = 4       #To use only one model fold, change to 'n_folds = 1'\nn_folds = 1       #To use only one model fold, change to 'n_folds = 1'\nrc = True         #Average across reverse-complement prediction\n\n#Read model parameters\n\nwith open(params_file) as params_open :\n\n    params = json.load(params_open)\n\n    params_model = params['model']\n    params_train = params['train']\n\n#Read targets\n\ntargets_df = pd.read_csv(targets_file, index_col=0, sep='\\t')\ntarget_index = targets_df.index\n\n#Create local index of strand_pair (relative to sliced targets)\nif rc :\n    strand_pair = targets_df.strand_pair\n\n    target_slice_dict = {ix : i for i, ix in enumerate(target_index.values.tolist())}\n    slice_pair = np.array([\n        target_slice_dict[ix] if ix in target_slice_dict else ix for ix in strand_pair.values.tolist()\n    ], dtype='int32')\n    \n\n#Initialize model ensemble\n\nmodels = []\nfor fold_ix in range(n_folds) :\n\n    model_file = models_path + \"f\" + str(fold_ix) + \"/model0_best.h5\"\n\n    seqnn_model = seqnn.SeqNN(params_model)\n    seqnn_model.restore(model_file, 0)\n    seqnn_model.build_slice(target_index)\n    if rc :\n        seqnn_model.strand_pair.append(slice_pair)\n    #seqnn_model.build_ensemble(rc, '0')\n    seqnn_model.build_ensemble(rc, [0])\n\n    models.append(seqnn_model)\n\n\n\n\n\n\n\nShow the code\n# read VCFs and encode haplotypes\nCHROM = 'chr22'\nvcf_file = CONTENT_DIR + \"/data/ALL.\" + CHROM + \".shapeit2_integrated_SNPs_v2a_27022019.GRCh38.phased.gz\"\ntarget_interval = kipoiseq.Interval(CHROM, 18118779, 18145669)\nhaplo1, haplo2 = vcf_to_seq_faster(target_interval, 'HG00097', vcf_file, fasta_extractor)\nhaplo0 = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH)) # ref seq\n\n# removed extra dimension as input layer is of shape=(None, 524288, 4)\nhaplo1_enc = make_seq_1hot(\"\".join(haplo1))\nhaplo2_enc = make_seq_1hot(\"\".join(haplo2))\nhaplo0_enc = make_seq_1hot(\"\".join(haplo0))\n\n\n\n\n\nENSG00000099968\n\n\nShow the code\nsequences = [haplo0_enc, haplo1_enc, haplo2_enc]\nmy_tracks = ['CAGE:B lymphoblastoid cell line: GM12878 ENCODE, biol_',\n                          'DNASE:CD14-positive monocyte female',\n                          'RNA:blood']\n\n\n\n\nShow the code\nfor h in range(0, len(sequences)):\n  haplotypes = [\"Reference\", \"Haplotype 1\", \"Haplotype 2\"]\n  prediction = make_prediction(gene = 'ENSG00000099968', interval = target_interval,\n                               haplo = haplotypes[h], sequence_one_hot=sequences[h])\n  print(haplotypes[h])\n  plot_tracks(prediction, interval = target_interval, haplo = haplotypes[h],\n              tracks = my_tracks, log = [True, True, False])\n\n\n\n\nShow the code\nprediction.shape\n\n\n\n\n\n\n\nBorzoi is a deep learning model that: - Takes DNA sequences as input (one-hot encoded) - Uses a convolutional neural network architecture and transformer layers - Predicts multiple genomic features simultaneously - Can identify regulatory elements and their activity levels\n\n\n\n\nSEQUENCE_LENGTH: 524,288 bp (512 kb) - the size of genomic windows processed\nn_folds: 4 - number of model replicates for ensemble predictions\nrc: True - whether to average predictions from forward and reverse complement sequences\n\n\n\n\n\nReference Genome (hg38):\n\nUsed as baseline for sequence comparison\nRequired for proper variant mapping\n\nVCF File:\n\nContains personal genome variants\nMust be phased for haplotype analysis\nShould be properly indexed\n\nAnnotation Files:\n\nGTF file for gene annotations\nAdditional files for specific features (CAGE, DNase, etc.)\n\n\n\n\n\n\nSequence Extraction:\n\nExtract reference sequence for target region\nModify sequence based on personal variants\nGenerate two haplotypes\n\nModel Input:\n\nConvert sequences to one-hot encoding\nProcess in 512kb windows\nHandle sequence padding and truncation\n\nPrediction:\n\nRun through model ensemble\nAverage predictions across folds\nGenerate tissue-specific predictions\n\nVisualization:\n\nPlot predictions for each haplotype\nCompare with reference sequence\nHighlight significant differences\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-05-01-unit03/borzoi_personal_gen_prediction.html#environment-setup",
    "href": "post/2025-05-01-unit03/borzoi_personal_gen_prediction.html#environment-setup",
    "title": "Borzoi prediction from personal genome - qmd version",
    "section": "",
    "text": "First, we need to set up our Python environment. Borzoi requires Python 3.10 or lower due to compatibility requirements.\nconda create --name borzoi46100 python=3.10\n\n\nShow the code\nimport os\nGITHUB_DIR = '/Users/haekyungim/Github'\n\n# clone baskerville and borzoi if not already cloned\nbaskerville_path = os.path.join(GITHUB_DIR, 'baskerville')\nborzoi_path = os.path.join(GITHUB_DIR, 'borzoi')\nif not os.path.exists(baskerville_path):\n    !git clone https://github.com/calico/baskerville.git {GITHUB_DIR}/baskerville\n\nif not os.path.exists(borzoi_path):\n    !git clone https://github.com/calico/borzoi.git {GITHUB_DIR}/borzoi\n\n\nAfter loading baskerville, restart runtime, run code from here\nInstall libraries\n\n\nShow the code\ntry:\n    import baskerville\nexcept ImportError:\n    !pip install -e {GITHUB_DIR}/baskerville\n\n\n\n\nShow the code\ntry:\n    import borzoi\nexcept ImportError:\n    !pip install -e {GITHUB_DIR}/borzoi\n\n\nNOTE: install tensorflow-metal if running on a mac\n\n\nShow the code\n# import platform\n# if platform.processor() == 'arm':\n#     print(\"Apple Silicon detected, installing tensorflow-metal...\")\n#     %pip install tensorflow-metal\n# else:\n#     print(\"Not running on Apple Silicon, skipping tensorflow-metal installation\")\n\n\n\n\n\n\nShow the code\ntry:\n    import kipoiseq\nexcept ImportError:\n    %pip install kipoiseq\nfrom kipoiseq import Interval\ntry:\n    import cyvcf2\nexcept ImportError:\n    %pip install cyvcf2\nimport os\nimport time\nimport io\nimport gzip\n\n#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n\nimport h5py\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nimport baskerville\nfrom baskerville import seqnn\nfrom baskerville import gene as bgene\nfrom baskerville import dna\n\nimport json\n\nimport pysam\n\nimport pyfaidx\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\n\n\n\n\n\n\nShow the code\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\ngpu_device = tf.config.list_physical_devices('GPU')[0]\ntf.config.experimental.set_memory_growth(gpu_device, True)\n\n\n\n\n\n\n\nShow the code\nPRE = '/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100'\nCONTENT_DIR = PRE + '/borzoi'\n# Create borzoi/data directory if it doesn't exist\nif not os.path.exists(os.path.join(CONTENT_DIR)):\n    !mkdir {CONTENT_DIR}\nif not os.path.exists(os.path.join(CONTENT_DIR, 'data')):\n    !mkdir {CONTENT_DIR}/data\n\n# Create model file structure\nsaved_models_path = os.path.join(CONTENT_DIR, 'data/saved_models')\nif not os.path.exists(saved_models_path):\n    !mkdir {saved_models_path}\nif not os.path.exists(os.path.join(saved_models_path, 'f0')):\n    !mkdir {saved_models_path}/f0\nif not os.path.exists(os.path.join(saved_models_path, 'f1')):\n    !mkdir {saved_models_path}/f1\nif not os.path.exists(os.path.join(saved_models_path, 'f2')):\n    !mkdir {saved_models_path}/f2\nif not os.path.exists(os.path.join(saved_models_path, 'f3')):\n    !mkdir {saved_models_path}/f3\n\n\n\n\nShow the code\n#Download model weights\nif not os.path.exists(os.path.join(saved_models_path, 'f0', 'model0_best.h5')):\n    !wget https://storage.googleapis.com/seqnn-share/borzoi/f0/model0_best.h5 -O {saved_models_path}/f0/model0_best.h5\nif not os.path.exists(os.path.join(saved_models_path, 'f1', 'model0_best.h5')): \n    !wget https://storage.googleapis.com/seqnn-share/borzoi/f1/model0_best.h5 -O {saved_models_path}/f1/model0_best.h5\nif not os.path.exists(os.path.join(saved_models_path, 'f2', 'model0_best.h5')):\n    !wget https://storage.googleapis.com/seqnn-share/borzoi/f2/model0_best.h5 -O {saved_models_path}/f2/model0_best.h5\nif not os.path.exists(os.path.join(saved_models_path, 'f3', 'model0_best.h5')):\n    !wget https://storage.googleapis.com/seqnn-share/borzoi/f3/model0_best.h5 -O {saved_models_path}/f3/model0_best.h5\n\n\n\n\nShow the code\n#Download and uncompress annotation files\nif not os.path.exists(os.path.join(CONTENT_DIR, 'data', 'gencode41_basic_nort.gtf')):\n    !wget  -O - https://storage.googleapis.com/seqnn-share/helper/gencode41_basic_nort.gtf.gz | gunzip -c &gt; {os.path.join(CONTENT_DIR, 'data', 'gencode41_basic_nort.gtf')}\nif not os.path.exists(os.path.join(CONTENT_DIR, 'data', 'gencode41_basic_protein_splice.csv.gz')):\n    !wget https://storage.googleapis.com/seqnn-share/helper/gencode41_basic_protein_splice.csv.gz -O {os.path.join(CONTENT_DIR, 'data', 'gencode41_basic_protein_splice.csv.gz')}\nif not os.path.exists(os.path.join(CONTENT_DIR, 'data', 'polyadb_human_v3.csv.gz')):\n    !wget https://storage.googleapis.com/seqnn-share/helper/polyadb_human_v3.csv.gz -O {os. path.join(CONTENT_DIR, 'data', 'polyadb_human_v3.csv.gz')}\n\n\n\n\n\n\n\nShow the code\nfasta_file = os.path.join(CONTENT_DIR, 'data/hg38.fa')\nif not os.path.exists(fasta_file):\n    !wget -O - http://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz | gunzip -c &gt; {fasta_file}\n\n\n\n\nShow the code\n### download vcf file and index for chromosome 22\nVCF_FILE = os.path.join(CONTENT_DIR, 'data/ALL.chr22.shapeit2_integrated_SNPs_v2a_27022019.GRCh38.phased.gz')\nif not os.path.exists(VCF_FILE):\n    !wget https://uchicago.box.com/shared/static/u526pjh29w93ufn65yomlch4z8lkb1sp.gz -O {VCF_FILE}\n\n\n\n\nShow the code\n#!ls /content/data\n!ls {CONTENT_DIR}/data\n\n\n\n\nShow the code\n%cd {CONTENT_DIR}\n\n\n\n\n\n\nShow the code\n# !wget https://github.com/samtools/htslib/releases/download/1.9/htslib-1.9.tar.bz2\n# !tar -vxjf htslib-1.9.tar.bz2\n# %cd htslib-1.9\n# !autoreconf -i\n# !./configure\n# !make\n# !make install\n\n\n\n\nShow the code\n## if you Permission denied error on a mac, try\n# !brew install htslib\n\n\n\n\n\n\n\nShow the code\nif not os.path.exists(VCF_FILE + '.tbi'):\n    !tabix {VCF_FILE}\n\n\n\n\n\n\n\n\nShow the code\nmodels_path = CONTENT_DIR + '/data/saved_models/'\nparams_file = GITHUB_DIR + '/borzoi/examples/params_pred.json'\ntargets_file = GITHUB_DIR + '/borzoi/examples/targets_human.txt'\n\n\n\n\nShow the code\n# Load splice site annotation\n# splice_df = pd.read_csv(CONTENT_DIR+'/data/gencode41_basic_protein_splice.csv.gz', sep='\\t', compression='gzip')\n# print(\"len(splice_df) = \" + str(len(splice_df)))\n\n\nLoad transcriptome\n\n\nShow the code\ntranscriptome = bgene.Transcriptome(CONTENT_DIR + '/data/gencode41_basic_nort.gtf')"
  },
  {
    "objectID": "post/2025-05-01-unit03/borzoi_personal_gen_prediction.html#define-functions",
    "href": "post/2025-05-01-unit03/borzoi_personal_gen_prediction.html#define-functions",
    "title": "Borzoi prediction from personal genome - qmd version",
    "section": "",
    "text": "From reference seq fasta: - Indexes ref sequence - Gets chromosome sizes into a dictionary\nExtract function: - gets target chromosome length from kipoiseq interval and chr sizes dictionary - truncates interval if it extends beyond chromosome lengths - extracts sequence\n\n\n\n\none hot encodes given sequence\n\n\n\n\n\nmakes prediction of ALL tracks given an encoded sequence\n\n\n\nShow the code\nclass FastaStringExtractor:\n\n    def __init__(self, fasta_file):\n        self.fasta = pyfaidx.Fasta(fasta_file)\n        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n    #import pd.Interval as Interval\n\n    def extract(self, interval: Interval, **kwargs) -&gt; str:\n        # Truncate interval if it extends beyond the chromosome lengths.\n        chromosome_length = self._chromosome_sizes[interval.chrom]\n        trimmed_interval = Interval(interval.chrom,\n                                    max(interval.start, 0),\n                                    min(interval.end, chromosome_length),\n                                    )\n        # pyfaidx wants a 1-based interval\n        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n                                          trimmed_interval.start + 1,\n                                          trimmed_interval.stop).seq).upper()\n        # Fill truncated values with N's.\n        pad_upstream = 'N' * max(-interval.start, 0)\n        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n        return pad_upstream + sequence + pad_downstream # returns padded sequence\n\n    def close(self):\n        return self.fasta.close()\n\n\n\n\nShow the code\n# Make one-hot coded sequence (modified from borzoi)\ndef make_seq_1hot(sequence):\n    return dna.dna_1hot(sequence).astype(\"float32\")\n\ndef predict_tracks(models, sequence_one_hot):\n\n  predicted_tracks = []\n  for fold_ix in range(len(models)):\n\n    yh = models[fold_ix](sequence_one_hot[None, ...])[:, None, ...].astype(\"float16\")\n\n    predicted_tracks.append(yh)\n\n  predicted_tracks = np.concatenate(predicted_tracks, axis=1)\n\n  return predicted_tracks\n\n\n\n\n\n\nModifies ref sequence to personal sequence given the vcf\n\n\n\nShow the code\ndef vcf_to_seq_faster(target_interval, individual, vcf_file, fasta_extractor):\n  # target_inverval is a kipoiseq interval, e.g. kipoiseq.Interval(\"chr22\", 18118779, 18145669)\n  # individual is the id of the individual in the vcf file\n\n  target_fa = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))\n  window_coords = target_interval.resize(SEQUENCE_LENGTH)\n      # two haplotypes per individual\n  haplo_1 = list(target_fa[:])\n  haplo_2 = list(target_fa[:])\n\n  # Open the VCF file\n  vcf_reader = cyvcf2.VCF(vcf_file)\n\n  # Specific genomic region\n  CHROM = window_coords.chrom\n  START = max(1,window_coords.start)\n  END = min(window_coords.end, fasta_extractor._chromosome_sizes[CHROM]-1)\n\n  count1 = 0\n  count2 = 0\n\n  # Iterate through variants in the specified region\n  for variant in vcf_reader(CHROM + ':' + str(START) + '-' + str(END)):\n      # Access the individual's genotype using index (0-based) of the sample\n      individual_index = vcf_reader.samples.index(individual)\n      genotype = variant.genotypes[individual_index]\n      ALT=variant.ALT[0]\n      REF=variant.REF\n      POS=variant.POS\n      if REF == target_fa[POS - window_coords.start - 1]:\n        if genotype[0] == 1:\n          haplo_1[POS - window_coords.start - 1] = ALT\n          count1 = count1 + 1\n        if genotype[1] == 1:\n          haplo_2[POS - window_coords.start - 1] = ALT\n          count2 = count2 + 1\n      else:\n        print(\"ERROR: REF in vcf \"+ REF + \"!= REF from the build\" + target_fa[POS - window_coords.start - 1])\n  print(\"number of changes haplo1:\")\n  print(count1)\n  print(\"number of changes haplo2:\")\n  print(count2)\n  return haplo_1, haplo_2\n\n\n\n\n\n\n\nShow the code\ndef make_prediction(gene, interval, haplo, sequence_one_hot, window = 131072): # snp_pos, alt_allele\n    with tf.device('/GPU:0'):\n        search_gene = gene # gene to predict\n\n        resized_interval = interval.resize(SEQUENCE_LENGTH)\n        start = resized_interval.start\n        end = resized_interval.end\n        center_pos = start + SEQUENCE_LENGTH//2 # figure center position\n\n        chrom = resized_interval.chr # chromosome\n\n        #Get exon bin range\n        gene_keys = [gene_key for gene_key in transcriptome.genes.keys() if search_gene in gene_key]\n\n        gene = transcriptome.genes[gene_keys[0]]\n\n        #Determine output sequence start\n        seq_out_start = start + seqnn_model.model_strides[0]*seqnn_model.target_crops[0]\n        seq_out_len = seqnn_model.model_strides[0]*seqnn_model.target_lengths[0]\n\n        #Determine output positions of gene exons\n        gene_slice = gene.output_slice(seq_out_start, seq_out_len, seqnn_model.model_strides[0], False)\n\n        #Make predictions\n        #y_wt\n        return predict_tracks(models, sequence_one_hot)\n\n\n\n\n\n\n\nShow the code\n# Get indexes of track\ndef get_track_idx(tracks):\n  track_idx = []\n  targets_df['local_index'] = np.arange(len(targets_df))\n  for track in tracks:\n    track_idx.append(targets_df.loc[targets_df['description'] == track]['local_index'].tolist())\n  return track_idx\n\n\n\n\n\n\n\nShow the code\ndef plot_tracks(prediction, interval, haplo, anno_df=None, tracks=[],\n                log = [False, False, False],\n                sqrt_scale = [False, False, False]):\n  # Gene slice not included\n  bin_size = 32\n  pad = 16\n\n  resized_interval = interval.resize(SEQUENCE_LENGTH)\n  start = resized_interval.start\n  end = resized_interval.end\n  center_pos = start + SEQUENCE_LENGTH//2 # figure center position\n  window=131072\n\n  plot_start = center_pos - window // 2\n  plot_end = center_pos + window // 2\n\n  plot_start_bin = (plot_start - start) // bin_size - pad\n  plot_end_bin = (plot_end - start) // bin_size - pad\n\n  track_idx = get_track_idx(tracks)\n  track_scales = [0.01, 0.01, 0.01]\n  track_transforms = [3./4., 3./4., 3./4.]\n  soft_clips = [384., 384., 384.]\n\n  # Get annotation positions\n  anno_poses = []\n  if anno_df is not None:\n      anno_poses = anno_df.query(\n          \"chrom == '\"\n          + interval.chr\n          + \"' and position_hg38 &gt;= \"\n          + str(plot_start)\n          + \" and position_hg38 &lt; \"\n          + str(plot_end)\n        )[\"position_hg38\"].values.tolist()\n  # BIG plot\n  fig, axes = plt.subplots(len(tracks), figsize=(20, 1.5 * len(tracks)), sharex = True)\n\n  for ax, track, track_index, track_scale, track_transform, clip_soft in zip(axes,\n        tracks, track_idx, track_scales, track_transforms, soft_clips):\n    # Plot track densities\n    y = np.array(np.copy(prediction), dtype = np.float32)\n    y = np.mean(y[..., track_index], axis=(0, 1, 3))\n    y = y[plot_start_bin:plot_end_bin]\n    signal = np.max(y)\n    if log:\n      y = np.log2(y + 1.0)\n    elif sqrt_scale:\n      y = np.sqrt(y + 1.0)\n\n    max_y = np.max(y)\n    print(\"Max signal for \"+ str(track) + \" = \" + str(round(signal, 4)))\n    print(\"Max transformed signal for \"+ str(track) + \" = \" + str(round(max_y, 4)))\n    print(\"---\")\n    ax.bar(np.arange(plot_end_bin - plot_start_bin) + plot_start_bin,\n            y,\n            width=1.0,\n            color=\"green\",\n            alpha=0.5)\n            # label=\"Prediction\",)\n    # X axis tick annotations\n    xtick_vals = []\n    for _, anno_pos in enumerate(anno_poses):\n      pas_bin = int((anno_pos - start) // 32) - 16\n      xtick_vals.append(pas_bin)\n      bin_end = pas_bin + 3 - 0.5\n      bin_start = bin_end - 5\n\n      ax.axvline(x=pas_bin,\n                color=\"cyan\",\n                linewidth=2,\n                alpha=0.5,\n                linestyle=\"-\",\n                zorder=-1,)\n\n    plt.xlim(plot_start_bin, plot_end_bin - 1)\n    plt.xticks([], [])\n    plt.yticks([], [])\n\n    ax.set(ylabel = \"Signal(log)\" if log else \"Signal\")# , fontsize = 8)\n    ax.set_title(\"Track: \" + str(track)+ \" in \"+ str(haplo), fontsize=10)\n    # plt.legend(fontsize=8)\n    plt.tight_layout()\n  ax.set_xlabel(\n            interval.chr\n            + \":\"\n            + str(plot_start)\n            + \"-\"\n            + str(plot_end)\n            + \" (\"\n            + str(window)\n            + \"bp window)\")\n            # fontsize=8,)\n  plt.show()\n\n\n\n\n\n\n\nShow the code\nfasta_extractor = FastaStringExtractor(fasta_file)"
  },
  {
    "objectID": "post/2025-05-01-unit03/borzoi_personal_gen_prediction.html#model-configuration",
    "href": "post/2025-05-01-unit03/borzoi_personal_gen_prediction.html#model-configuration",
    "title": "Borzoi prediction from personal genome - qmd version",
    "section": "",
    "text": "Show the code\nSEQUENCE_LENGTH = 524288\n#n_folds = 4       #To use only one model fold, change to 'n_folds = 1'\nn_folds = 1       #To use only one model fold, change to 'n_folds = 1'\nrc = True         #Average across reverse-complement prediction\n\n#Read model parameters\n\nwith open(params_file) as params_open :\n\n    params = json.load(params_open)\n\n    params_model = params['model']\n    params_train = params['train']\n\n#Read targets\n\ntargets_df = pd.read_csv(targets_file, index_col=0, sep='\\t')\ntarget_index = targets_df.index\n\n#Create local index of strand_pair (relative to sliced targets)\nif rc :\n    strand_pair = targets_df.strand_pair\n\n    target_slice_dict = {ix : i for i, ix in enumerate(target_index.values.tolist())}\n    slice_pair = np.array([\n        target_slice_dict[ix] if ix in target_slice_dict else ix for ix in strand_pair.values.tolist()\n    ], dtype='int32')\n    \n\n#Initialize model ensemble\n\nmodels = []\nfor fold_ix in range(n_folds) :\n\n    model_file = models_path + \"f\" + str(fold_ix) + \"/model0_best.h5\"\n\n    seqnn_model = seqnn.SeqNN(params_model)\n    seqnn_model.restore(model_file, 0)\n    seqnn_model.build_slice(target_index)\n    if rc :\n        seqnn_model.strand_pair.append(slice_pair)\n    #seqnn_model.build_ensemble(rc, '0')\n    seqnn_model.build_ensemble(rc, [0])\n\n    models.append(seqnn_model)"
  },
  {
    "objectID": "post/2025-05-01-unit03/borzoi_personal_gen_prediction.html#build-sequence",
    "href": "post/2025-05-01-unit03/borzoi_personal_gen_prediction.html#build-sequence",
    "title": "Borzoi prediction from personal genome - qmd version",
    "section": "",
    "text": "Show the code\n# read VCFs and encode haplotypes\nCHROM = 'chr22'\nvcf_file = CONTENT_DIR + \"/data/ALL.\" + CHROM + \".shapeit2_integrated_SNPs_v2a_27022019.GRCh38.phased.gz\"\ntarget_interval = kipoiseq.Interval(CHROM, 18118779, 18145669)\nhaplo1, haplo2 = vcf_to_seq_faster(target_interval, 'HG00097', vcf_file, fasta_extractor)\nhaplo0 = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH)) # ref seq\n\n# removed extra dimension as input layer is of shape=(None, 524288, 4)\nhaplo1_enc = make_seq_1hot(\"\".join(haplo1))\nhaplo2_enc = make_seq_1hot(\"\".join(haplo2))\nhaplo0_enc = make_seq_1hot(\"\".join(haplo0))"
  },
  {
    "objectID": "post/2025-05-01-unit03/borzoi_personal_gen_prediction.html#predict",
    "href": "post/2025-05-01-unit03/borzoi_personal_gen_prediction.html#predict",
    "title": "Borzoi prediction from personal genome - qmd version",
    "section": "",
    "text": "ENSG00000099968\n\n\nShow the code\nsequences = [haplo0_enc, haplo1_enc, haplo2_enc]\nmy_tracks = ['CAGE:B lymphoblastoid cell line: GM12878 ENCODE, biol_',\n                          'DNASE:CD14-positive monocyte female',\n                          'RNA:blood']\n\n\n\n\nShow the code\nfor h in range(0, len(sequences)):\n  haplotypes = [\"Reference\", \"Haplotype 1\", \"Haplotype 2\"]\n  prediction = make_prediction(gene = 'ENSG00000099968', interval = target_interval,\n                               haplo = haplotypes[h], sequence_one_hot=sequences[h])\n  print(haplotypes[h])\n  plot_tracks(prediction, interval = target_interval, haplo = haplotypes[h],\n              tracks = my_tracks, log = [True, True, False])\n\n\n\n\nShow the code\nprediction.shape"
  },
  {
    "objectID": "post/2025-05-01-unit03/borzoi_personal_gen_prediction.html#technical-details",
    "href": "post/2025-05-01-unit03/borzoi_personal_gen_prediction.html#technical-details",
    "title": "Borzoi prediction from personal genome - qmd version",
    "section": "",
    "text": "Borzoi is a deep learning model that: - Takes DNA sequences as input (one-hot encoded) - Uses a convolutional neural network architecture and transformer layers - Predicts multiple genomic features simultaneously - Can identify regulatory elements and their activity levels\n\n\n\n\nSEQUENCE_LENGTH: 524,288 bp (512 kb) - the size of genomic windows processed\nn_folds: 4 - number of model replicates for ensemble predictions\nrc: True - whether to average predictions from forward and reverse complement sequences\n\n\n\n\n\nReference Genome (hg38):\n\nUsed as baseline for sequence comparison\nRequired for proper variant mapping\n\nVCF File:\n\nContains personal genome variants\nMust be phased for haplotype analysis\nShould be properly indexed\n\nAnnotation Files:\n\nGTF file for gene annotations\nAdditional files for specific features (CAGE, DNase, etc.)\n\n\n\n\n\n\nSequence Extraction:\n\nExtract reference sequence for target region\nModify sequence based on personal variants\nGenerate two haplotypes\n\nModel Input:\n\nConvert sequences to one-hot encoding\nProcess in 512kb windows\nHandle sequence padding and truncation\n\nPrediction:\n\nRun through model ensemble\nAverage predictions across folds\nGenerate tissue-specific predictions\n\nVisualization:\n\nPlot predictions for each haplotype\nCompare with reference sequence\nHighlight significant differences"
  },
  {
    "objectID": "post/2025-04-15-unit01/homework-04.html",
    "href": "post/2025-04-15-unit01/homework-04.html",
    "title": "homework 4",
    "section": "",
    "text": "Homework 4\nWatch Karpathy‚Äôs nanoGPT tutorial on youtube link and answer the questions in the form\nlist of questions in text format is here\n\n\n\n\n\n\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html",
    "href": "post/2025-03-25-unit00/index.html",
    "title": "preparing environment for unit 00",
    "section": "",
    "text": "```{bash}\n\n## download miniconda from the command line\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\n\n## or manually from https://www.anaconda.com/docs/getting-started/miniconda/main\n\n## install miniconda\nbash Miniconda3-latest-Linux-x86_64.sh\n\n```\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#install-conda",
    "href": "post/2025-03-25-unit00/index.html#install-conda",
    "title": "preparing environment for unit 00",
    "section": "",
    "text": "```{bash}\n\n## download miniconda from the command line\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\n\n## or manually from https://www.anaconda.com/docs/getting-started/miniconda/main\n\n## install miniconda\nbash Miniconda3-latest-Linux-x86_64.sh\n\n```"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#create-conda-environment",
    "href": "post/2025-03-25-unit00/index.html#create-conda-environment",
    "title": "preparing environment for unit 00",
    "section": "create conda environment",
    "text": "create conda environment\n```{bash}\nconda create -n gene46100 python=3.12\nconda activate gene46100\n\n```"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#do-not-install-packages-with-conda",
    "href": "post/2025-03-25-unit00/index.html#do-not-install-packages-with-conda",
    "title": "preparing environment for unit 00",
    "section": "do not install packages with conda",
    "text": "do not install packages with conda\n(I ran into lots of issues with torch version incompatibility with torchvision and torchmetrics)\n```{bash}\n# DONT USE CONDA TO INSTALL PYTORCH at least for now\n# conda install scikit-learn plotnine pytorch \n\n## installing torchvision and torchmetrics forced downgrading torch to 2.3.1 which was not compatible with mps\n\n```"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#install-all-packages-within-jupyter-notebook-with-pip",
    "href": "post/2025-03-25-unit00/index.html#install-all-packages-within-jupyter-notebook-with-pip",
    "title": "preparing environment for unit 00",
    "section": "install all packages within jupyter notebook with %pip",
    "text": "install all packages within jupyter notebook with %pip\nas of 2025-03-24\nusing %pip will make sure that the packages are accessible by the kernel you are using for the jupyter notebook\n```{bash}\n%pip install scikit-learn plotnine tqdm pandas \n%pip install torch\n%pip install torchvision\n%pip install torchmetrics\n```"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#install-cursor-or-vscode-to-run-the-jupyter-notebook",
    "href": "post/2025-03-25-unit00/index.html#install-cursor-or-vscode-to-run-the-jupyter-notebook",
    "title": "preparing environment for unit 00",
    "section": "install cursor or vscode to run the jupyter notebook",
    "text": "install cursor or vscode to run the jupyter notebook\n\ninstall the python extension for cursor or vscode.\nselect the python interpreter to be the one in the conda environment you created (gene46100)"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#save-the-environment-for-reproducibility",
    "href": "post/2025-03-25-unit00/index.html#save-the-environment-for-reproducibility",
    "title": "preparing environment for unit 00",
    "section": "Save the environment for reproducibility",
    "text": "Save the environment for reproducibility\n\nto reproduce the environment exactly, save the environment\n```{bash}\n# Save conda packages with their sources\nconda env export --from-history &gt; environment-gene46100.yml\n\n# Save pip-installed packages separately\npip list --format=freeze &gt; requirements-gene46100.txt\n\n# Save full environment state (for reference)\nconda env export &gt; environment_full-gene46100.yml\n```\n\n\nto clone the environment\n```{bash}\n# create a new environment named newenv46100 with the same packages as the original environment test46100\nconda create --name newenv46100 --clone test46100\n```\n\n\nto remove the environment\n```{bash}\nconda env remove --name newenv46100\n```"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#do-not-run-the-following-unless-need-to-reinstall-the-environment-for-some-reason",
    "href": "post/2025-03-25-unit00/index.html#do-not-run-the-following-unless-need-to-reinstall-the-environment-for-some-reason",
    "title": "preparing environment for unit 00",
    "section": "DO NOT RUN THE FOLLOWING UNLESS NEED TO REINSTALL THE ENVIRONMENT FOR SOME REASON",
    "text": "DO NOT RUN THE FOLLOWING UNLESS NEED TO REINSTALL THE ENVIRONMENT FOR SOME REASON\n\nreinstall the environment\n```{bash}\n# Create environment from conda packages\nconda env create -f environment-gene46100.yml\n\n# Activate the environment\nconda activate gene46100\n\n# Install pip packages\npip install -r requirements-gene46100.txt\n\n```"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#configuration-to-run-python-in-qmd-files",
    "href": "post/2025-03-25-unit00/index.html#configuration-to-run-python-in-qmd-files",
    "title": "preparing environment for unit 00",
    "section": "configuration to run python in qmd files",
    "text": "configuration to run python in qmd files\nFollow instructions in https://thedatasavvycorner.com/blogs/08-quarto-conda-en copied to /DRAFTS/2025-02-20-testin/08-quarto-conda-env.qmd\nBelow is a shortened version of the instructions when you already have a conda environment.\n```{bash}\nconda activate gene46100\npip install nbclient nbformat\n\n# Use conda to install the nb_conda_kernels package. This is used to manage python jupyter kernels for notebooks.\n\nconda install nb_conda_kernels\n## in new install I needed conda install nb_conda_kernels jupyter_server\n\n# Copy the path returned from the below command\njupyter --config-dir\n\n# in my case it was ~/.jupyter\n\n# Create a jupyter_config.json in the jupyter config directory:\ntouch ~/.jupyter/jupyter_config.json\necho '{\n  \"CondaKernelSpecManager\": {\n    \"kernelspec_path\": \"--user\"\n  }\n}' &gt;&gt; ~/.jupyter/jupyter_config.json\n\n# Run the below command to return a list of available kernels:\npython -m nb_conda_kernels list\n\n# Copy the name (not the path) for the environment that you created with the format `conda-env-&lt;YOUR_ENV_NAME&gt;-py`.\n```\nadd the following to the yaml header in the qmd file with python blocks\n```{yaml}\njupyter: \n  kernelspec:\n    name: \"conda-env-&lt;YOUR_ENV_NAME&gt;-py\"\n    language: \"python\"\n    display_name: \"&lt;YOUR_ENV_NAME&gt;\"\n```\nmy environment was conda-env-gene46100-py you can run quarto render and it will be rendered with the conda environment you specified when running interactively, you can choose the kernel, similarly to a jupyter notebook\n```{yaml}\njupyter: \n  kernelspec:\n    name: \"conda-env-gene46100-py\"\n    language: \"python\"\n    display_name: \"gene46100\"\nfreeze: true\n## you may want to choose freeze true if you don't want to run the code every time you render the qmd file\n\n```"
  },
  {
    "objectID": "post/2025-03-25-unit00/homework-02.html",
    "href": "post/2025-03-25-unit00/homework-02.html",
    "title": "homework 2",
    "section": "",
    "text": "Homework 2\n\nrun all the code in the DNA scoring tutorial notebook. You should also be able to find the notebook in your cloned course repository. (20 points)\ncomplete the exercises in the notebook (30 points)\nsubmit the jupyter notebook with the results to canvas assignment\n\n\n\n\n\n\n\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning in Genomics Course Material",
    "section": "",
    "text": "This page contains material for the GENE46100 Deep learning in genomics course.\nFind the 2025 syllabus here.\nEdit github source here\n\nSee all notes here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscgpt quickstart - qmd version\n\n\n\n\n\n\nnotebook\n\n\ngene46100\n\n\n\n\n\n\n\n\n\nMay 13, 2025\n\n\nHaky Im based on cziscience page\n\n\n\n\n\n\n\n\n\n\n\n\nscgpt quickstart - jupyter notebook\n\n\n\n\n\n\nnotebook\n\n\ngene46100\n\n\n\n\n\n\n\n\n\nMay 13, 2025\n\n\nHaky Im based on cziscience page\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 04 Single Cell Genomics\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nscanpy tutorial - qmd\n\n\n\n\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nMay 12, 2025\n\n\nscanpy authors\n\n\n\n\n\n\n\n\n\n\n\n\nSeurat - Guided Clustering Tutorial\n\n\n\n\n\n\ngene46100\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nMay 12, 2025\n\n\nSatija lab\n\n\n\n\n\n\n\n\n\n\n\n\nBorzoi prediction from personal genome - qmd version\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\nUsing Borzoi for predicting on a personal genome, derived from VCF\n\n\n\n\n\nMay 5, 2025\n\n\nSofia Salazar\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting alternative polyadenylation with Borzoi\n\n\n\n\n\n\nnotebook\n\n\ngene46100\n\n\n\n\n\n\n\n\n\nMay 5, 2025\n\n\nSofia Salazar\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 6\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMay 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nEnformer Architecture\n\n\n\n\n\n\ngene46100\n\n\n\n\n\n\n\n\n\nMay 1, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nunit 03\n\n\n\n\n\n\ngene46100\n\n\n\n\n\n\n\n\n\nMay 1, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nEnformer training\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\ntrain enformer on human and mouse\n\n\n\n\n\nApr 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nEnformer usage neanderthal - jupyter notebook version\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\npredict neanderthal epigenome with enformer\n\n\n\n\n\nApr 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 5\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nApr 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 4\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nApr 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a GPT - companion notebook annotated\n\n\n\n\n\nCompanion notebook from Karpathys video on building a minimal GPT, annotated by cursors LLM with summary from gemini.\n\n\n\n\n\nApr 15, 2025\n\n\nAndrey Karpathy\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 3\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nApr 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCalibrating hyperparameters with weights and biases\n\n\n\n\n\n\ngene46100\n\n\nproject\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nApr 10, 2025\n\n\nSofia Salazar\n\n\n\n\n\n\n\n\n\n\n\n\nTF binding prediction challenge\n\n\n\n\n\n\ngene46100\n\n\nproject\n\n\n\nCompetition details for TF binding prediction challenge\n\n\n\n\n\nApr 8, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 2\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nApr 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nUpdated - DNA score prediction with Pytorch\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nApr 4, 2025\n\n\nErin Wilson\n\n\n\n\n\n\n\n\n\n\n\n\nTF Binding prediction project\n\n\n\n\n\n\ngene46100\n\n\nproject\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nApr 3, 2025\n\n\nSofia Salazar\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 1\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMar 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nQuick introduction to deep learning\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nMar 25, 2025\n\n\nBoxiang Liu, modified by Haky Im\n\n\n\n\n\n\n\n\n\n\n\n\npreparing environment for unit 00\n\n\n\n\n\n\ngene46100\n\n\nhow-to\n\n\n\nsetting up conda environement and installing packages\n\n\n\n\n\nMar 24, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a GPT From Scratch summary\n\n\n\n\n\n\ngene46100\n\n\nslides\n\n\ngpt\n\n\n\n\n\n\n\n\n\nMar 24, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nBorzoi prediction from personal genome - jupyter notebook\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\nUsing Borzoi for predicting on a personal genome, derived from VCF\n\n\n\n\n\nSep 24, 2023\n\n\nSofia Salazar\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "how-to/01-how-to-install-local-llm.html",
    "href": "how-to/01-how-to-install-local-llm.html",
    "title": "how to install local llm",
    "section": "",
    "text": "#how-to install ollama open webui local llm on a macbook adapted using conda instead of pyenv from https://medium.com/@hautel.alex2000/build-your-local-ai-from-zero-to-a-custom-chatgpt-interface-with-ollama-open-webui-6bee2c5abba3\n\ninstall homebrew if not already installed on your mac\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\ninstall and start ollama service\n\nbrew install ollama\nbrew services start ollama\n\nrun ollama with deepseek-r1-14b or other models from https://ollama.com/library\n\nollama run deepseek-r1:14b\n\ninstall miniconda if not already installed\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh -O ~/miniconda.sh\nbash ~/miniconda.sh -b -p $HOME/miniconda\n\ncreate and activate conda environment\n\nconda create -n open-webui python=3.11\nconda activate open-webui\n\ninstall openwebui\n\npip install open-webui\n\nstart open-webui\n\nopen-webui serve\n\nopen browser and go to http://localhost:8080\n\n\n\n\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "DRAFTS/embeding-analysis.html",
    "href": "DRAFTS/embeding-analysis.html",
    "title": "Embeddings Analysis",
    "section": "",
    "text": "** embeddings provided by Henry from his DNA GPT model**\n\nsuppressMessages(library(tidyverse))\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\nsuppressMessages(library(glue))\n\nWarning: package 'glue' was built under R version 4.3.3\n\nsuppressMessages(library(ggrepel))\n\nWarning: package 'ggrepel' was built under R version 4.3.3\n\n# suppressMessages(library(qvalue))\n# suppressMessages(library(devtools))\n# suppressMessages(source_gist(\"38431b74c6c0bf90c12f\")) ## get qqunif\n# suppressMessages(library(googlesheets4))\n\n\nWEBDATA = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\n\nDATA = glue(\"{WEBDATA}/web-GENE-46100\")\n\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\n##system(glue(\"open {DATA}\")) ## this will open the folder \n\n\nembeddings = read_csv(glue(\"{DATA}/henry-dna-gpt-embed_df.csv.gz\"))\n\nNew names:\nRows: 40 Columns: 385\n‚îÄ‚îÄ Column specification\n‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Delimiter: \",\" chr\n(1): ...1 dbl (384): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,\n17, 18,...\n‚Ñπ Use `spec()` to retrieve the full column specification for this data. ‚Ñπ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n‚Ä¢ `` -&gt; `...1`\n\nnames(embeddings)[1] = \"token\"\n\nunique(embeddings$token)\n\n [1] \"\\n\" \"0\"  \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"&gt;\"  \"A\"  \"B\"  \"C\" \n[16] \"G\"  \"H\"  \"I\"  \"J\"  \"K\"  \"L\"  \"M\"  \"N\"  \"T\"  \"U\"  \"X\"  \"Y\"  \"_\"  \"a\"  \"c\" \n[31] \"d\"  \"g\"  \"h\"  \"l\"  \"m\"  \"n\"  \"o\"  \"r\"  \"t\"  \"v\" \n\nsvdfit = svd(embeddings[,-1])\n\n\nACGT cluster togeter, acgt also cluster together.\n\n\nN is closer to ACGT and acgt, probably because of N appears more frequently than the remainig tokens.\n\n\npng2file = FALSE\n\n# if(png2file) png(glue(\"{DATA}/svd-plot.png\"), width=1000, height=1000)\n# plot(svdfit$u[,1], svdfit$u[,2], type=\"n\", xlab=\"\", ylab=\"\", main=\"SVD of DNA sequence embeddings\")\n# text(svdfit$u[,1], svdfit$u[,2], embeddings$token, cex=1)\n# if(png2file) dev.off()\n\n# plot(svdfit$v[,1],svdfit$v[,2],type=\"n\",xlab=\"\",ylab=\"\",main=\"SVD of DNA sequence embeddings\")\n# text(svdfit$v[,1],svdfit$v[,2],names(embeddings)[-1],cex=2)\n\n\n# Create a data frame for plotting\nplot_data &lt;- data.frame(\n  x = svdfit$u[,1],\n  y = svdfit$u[,2],\n  label = embeddings$token\n)\n\n# Create the plot\nggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0.5) +\n  geom_label_repel(\n    size = 4,\n    max.overlaps = 20,\n    box.padding = 0.5,\n    point.padding = 0.5,\n    segment.color = 'grey50',\n    segment.alpha = 0.5,\n    fill = \"white\",\n    alpha = 0.8,\n    label.size = 0.2\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"SVD of DNA Sequence Embeddings\",\n    x = \"First Principal Component\",\n    y = \"Second Principal Component\"\n  ) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    panel.grid = element_line(color = \"grey90\")\n  )\n\n\n\n\n\n\n\n# Save to file if needed\nif(png2file) {\n  ggsave(\n    glue(\"{DATA}/svd-plot-ggplot.png\"),\n    width = 10,\n    height = 10,\n    dpi = 300\n  )\n}\n\n\n\n\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "DRAFTS/08-quarto-conda-env.html",
    "href": "DRAFTS/08-quarto-conda-env.html",
    "title": "Conda Environments for Quarto Documents",
    "section": "",
    "text": "borrowed from https://thedatasavvycorner.com/blogs/08-quarto-conda-env modified for GENE-46100\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "DRAFTS/08-quarto-conda-env.html#introduction",
    "href": "DRAFTS/08-quarto-conda-env.html#introduction",
    "title": "Conda Environments for Quarto Documents",
    "section": "Introduction",
    "text": "Introduction\nThis article sets out minimal instructions on rendering quarto documents that rely on specified conda virtual environments. This article collates information from:\n\nQuarto Documentation: Using Conda\nQuarto Documentation: Using Python\nnb_conda_kernels readme\n\nIt is presumed that you will be working within a git repository at times. If this is not the case, ignoring steps specifying git instructions should not affect your ability to successfully render the quarto documents.\n\n\n\n\n\n\nA Note on the Purpose\n\n\n\n\n\nThe purpose of this article is not to explore reasons for using conda enviroments or to compare the pros and cons of the many different options for managing python virtual environments. It aims to help the reader configure quarto documents to run with a specified conda environment while remaining minimal, opting to link to sources of further information where discussion may complement the content.\n\n\n\n\nIntended Audience\nPython practitioners familiar with conda environment management @CondaDocsEnvMgmt who are less familiar working with quarto documents @HelloQuarto.\n\n\nThe Scenario\nYou are writing a quarto document that contains python code. You would like to use conda to manage your python dependencies. You are encountering problems in having quarto select the appropriate conda environment.\n\n\n\n\n\n\nNamed Environments\n\n\n\n\n\nThis article covers having quarto execute with ‚Äúprefix‚Äù conda environments. This setup may be useful specifically for a website where different site pages have different dependencies.\nHowever, many readers may wish for a simpler solution. It is possible to have quarto websites and documents execute with a named environment instead. If you have an environment created like below:\nconda create -n my-awesome-env python -y\nThen including the following statement within either the _quarto.yml or quarto document‚Äôs YAML header should be enough to guarantee that the target environment is picked up when rendering:\njupyter: my-awesome-env\nAdditionally, if you would just prefer the site or document to render with whatever version of python is available in the currently active environment, then use:\njupyter: python3\nMany thanks to Ethan for this tip.\n\n\n\n\n\nWhat You‚Äôll Need:\n\nConda or miniconda\nQuarto\nText editor eg VS Code\nPython package manager (eg pip)\nAccess to a command line interface (CLI) such as terminal / Bash.\nrequirements.txt:\n\n\n\nrequirements.txt\n\nnbclient\nnbformat\npalmerpenguins\n\n\ngit (optional)"
  },
  {
    "objectID": "DRAFTS/08-quarto-conda-env.html#configuring-quarto-with-conda-env",
    "href": "DRAFTS/08-quarto-conda-env.html#configuring-quarto-with-conda-env",
    "title": "Conda Environments for Quarto Documents",
    "section": "Configuring Quarto with Conda Env",
    "text": "Configuring Quarto with Conda Env\n\nProject Structure\n\nCreate a new project folder. Open a terminal and change directory to the new project.\nSave the requirements to a requirements.txt file.\nCreate a new quarto document in the project root. In VS Code, use\nFile  New File...  Quarto Document.\nWrite the following content to a python code chunk in the quarto file and save as penguins.qmd\n\n\n\npenguins.qmd\n\ndf = penguins.load_penguins().dropna()\ndf.head()\n\n\n\nConfigure the Environment\n\nCreate a new conda environment with the -p flag and give it a suitably descriptive name 1. Ensure that the environment is built with python 3.11 2.\n\n\n\nCLI\n\nconda create -p ~/Github/web-GENE-46100/.venv python=3.12 -y\n\n\nActivate the environment.\n\n\n\nCLI\n\nconda activate ~/Github/web-GENE-46100/.venv\n\n\nInstall the requirements file.\n\n\n\nCLI\n\npip install -r ~/Github/web-GENE-46100/requirements-qmd46100.txt\n\n\nAdd a .gitignore file and include the name of the local environment directory created in step 4.\n\n\n\n.gitignore\n\nSOME_ENV_NAME/\n\n\n\nConfigure the Quarto Project\n\nCreate a _quarto.yml configuration file in the project root. In this file, we will specify that the quarto render command should render any qmd files and ignore any files found within your local environment. Add the following content:\n\n\n\n_quarto.yaml\n\nproject:\n  type: website\n  output-dir: docs\n  render:\n    - \"*.qmd\"\n    - \"!/./SOME_ENV_NAME/\"\n\n\nUse conda to install the nb_conda_kernels package. This is used to manage python jupyter kernels for notebooks.\n\n\n\nCLI\n\nconda install nb_conda_kernels\n\n\nCopy the path returned from the below command\n\n\n\nCLI\n\njupyter --config-dir\n\n/Users/haekyungim/.jupyter\n\nCreate a jupyter_config.json in the jupyter config directory:\n\n\n\nCLI\n\ntouch &lt;INSERT_YOUR_CONFIG_DIR&gt;/jupyter_config.json\n\n\nWrite the below content to this file and save.\n\n\n\nCLI\n\necho -e '{\\n  \"CondaKernelSpecManager\": {\\n    \"kernelspec_path\": \"--user\"\\n  }\\n}' &gt;&gt; &lt;INSERT_YOUR_CONFIG_DIR&gt;/jupyter_config.json\n\n\nRun the below command to return a list of available kernels:\n\n\n\nCLI\n\npython -m nb_conda_kernels list\n\n\nCopy the name (not the path) for the environment that you created with the format conda-env-&lt;YOUR_ENV_NAME&gt;-py.\nOpen penguins.qmd. Adjust the YAML header so that it contains the following:\n\n\n\npenguins.qmd\n\njupyter: \n  kernelspec:\n    name: \"conda-env-&lt;YOUR_ENV_NAME&gt;-py\"\n    language: \"python\"\n    display_name: \"&lt;YOUR_ENV_NAME&gt;\"\n\n\nYou should now be able to render the quarto project, confirming that the target environment was activated in the CLI output. eg:\n\n\n\nCLI\n\nquarto render\n\nStarting &lt;YOUR_ENV_NAME&gt; kernel...Done\n\n\nTips\n\nWhen encountering issues with quarto render, it can be informative to examine the output of quarto check or quarto check jupyter in the CLI.\nAs there are many steps to configuring conda, it may be a good idea to create a dedicated conda environment for all of your quarto projects. Quarto attempts to select an appropriate kernel based upon the content of the first executable python code chunk in your quarto document. Usually, this chunk would contain the import statements. However, over time this would likely result in package conflicts over time.\nThe approach set out in this how-to would be a good fit for a website built with quarto, where the configuration steps can be performed only once in a parent website environment, and then specific, minimal environments created for each article requiring a python environment.\nAlternatively, consider using venv or poetry to manage python environments @DO4DSPKGLayer for quarto projects."
  },
  {
    "objectID": "DRAFTS/08-quarto-conda-env.html#troubleshooting",
    "href": "DRAFTS/08-quarto-conda-env.html#troubleshooting",
    "title": "Conda Environments for Quarto Documents",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nYou‚Äôve created a new environment and it is not discovered when running python -m nb_conda_kernels list:\n\nActivate your new environment\npip install ipykernel\nRun:\n\npython -m ipykernel install --user --name &lt;INSERT_ENV_NAME&gt; --display-name \"&lt;INSERT_DISPLAY_NAME&gt;\"\n\nDeactivate the new environment.\nRun python -m nb_conda_kernels list once more and the new env should appear.\nTaken from this SO thread"
  },
  {
    "objectID": "DRAFTS/08-quarto-conda-env.html#conclusion",
    "href": "DRAFTS/08-quarto-conda-env.html#conclusion",
    "title": "Conda Environments for Quarto Documents",
    "section": "Conclusion",
    "text": "Conclusion\nThis article has walked the reader through setting up a basic quarto project, creating a conda environment, and configuring a quarto document to render with a specified environment. For more help with quarto, consult the quarto-cli GitHub repository @QuartoCLI and the RStudio Community @PositCommunity (soon to rebrand to the Posit Community).\n\nfin!"
  },
  {
    "objectID": "DRAFTS/08-quarto-conda-env.html#footnotes",
    "href": "DRAFTS/08-quarto-conda-env.html#footnotes",
    "title": "Conda Environments for Quarto Documents",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhen creating conda environments, the use of generic names such as env will result in conda prepending the environment name with numbers to avoid conflicts. Use descriptive environment names in order to avoid this, eg penguins-env.‚Ü©Ô∏é\nnb_conda_kernels (a package required in a later step) does not currently work with python 3.12 or newer.‚Ü©Ô∏é"
  },
  {
    "objectID": "DRAFTS/2020-01-01-TODO/index.html",
    "href": "DRAFTS/2020-01-01-TODO/index.html",
    "title": "TODO",
    "section": "",
    "text": "TODO\n\n\n\n\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "DRAFTS/slides-00.html",
    "href": "DRAFTS/slides-00.html",
    "title": "Slides unit 00",
    "section": "",
    "text": "An MLP is a fully connected feedforward neural network ‚Äî the classic deep learning architecture.\n\n\n\nInput Layer ‚Üí Hidden Layer(s) ‚Üí Output Layer\n\n\n\nMLP with 2 features in the input layer, one hidden layer, one dimentional output layer\n\n\nEach layer performs:\noutput = activation(Wx + b)\nWhere:\nW = weight matrix\n\nx = input vector\n\nb = bias\n\nactivation = non-linear function like ReLU\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "DRAFTS/slides-00.html#core-idea",
    "href": "DRAFTS/slides-00.html#core-idea",
    "title": "Slides unit 00",
    "section": "",
    "text": "An MLP is a fully connected feedforward neural network ‚Äî the classic deep learning architecture."
  },
  {
    "objectID": "DRAFTS/slides-00.html#architecture",
    "href": "DRAFTS/slides-00.html#architecture",
    "title": "Slides unit 00",
    "section": "",
    "text": "Input Layer ‚Üí Hidden Layer(s) ‚Üí Output Layer\n\n\n\nMLP with 2 features in the input layer, one hidden layer, one dimentional output layer\n\n\nEach layer performs:\noutput = activation(Wx + b)\nWhere:\nW = weight matrix\n\nx = input vector\n\nb = bias\n\nactivation = non-linear function like ReLU"
  },
  {
    "objectID": "DRAFTS/slides-00.html#data---model-input",
    "href": "DRAFTS/slides-00.html#data---model-input",
    "title": "Slides unit 00",
    "section": "1. Data -> Model Input",
    "text": "1. Data -&gt; Model Input\nDNA/RNA/protein sequences ‚Üí One-hot encoded or embedded tensors\n\nWrapped in custom Dataset + DataLoader for batching"
  },
  {
    "objectID": "DRAFTS/slides-00.html#training-loop",
    "href": "DRAFTS/slides-00.html#training-loop",
    "title": "Slides unit 00",
    "section": "2. Training Loop",
    "text": "2. Training Loop\nfor xb, yb in train_loader: optimizer.zero_grad() # Step 1: Clear old gradients preds = model(xb) # Step 2: Forward pass loss = loss_fn(preds, yb) # Step 3: Compute loss loss.backward() # Step 4: Backprop: compute gradients optimizer.step() # Step 5: Update weights"
  },
  {
    "objectID": "DRAFTS/slides-00.html#model-components",
    "href": "DRAFTS/slides-00.html#model-components",
    "title": "Slides unit 00",
    "section": "3. Model Components",
    "text": "3. Model Components\nnn.Module: Defines layers and forward pass\n\nnn.Conv1d, nn.Linear, nn.ReLU, etc.\n\nOutputs: regression (e.g. expression levels) or classification (e.g. binding sites)"
  },
  {
    "objectID": "DRAFTS/slides-00.html#loss-function",
    "href": "DRAFTS/slides-00.html#loss-function",
    "title": "Slides unit 00",
    "section": "4. Loss Function",
    "text": "4. Loss Function\nnn.MSELoss() for expression prediction\n\nnn.CrossEntropyLoss() for classification\n\nnn.BCELoss() for binary classification, this is just -log likelihood for a bernoulli distribution"
  },
  {
    "objectID": "DRAFTS/slides-00.html#optimizer",
    "href": "DRAFTS/slides-00.html#optimizer",
    "title": "Slides unit 00",
    "section": "5. Optimizer",
    "text": "5. Optimizer\nCommon: torch.optim.Adam, SGD, etc.\n\nControls learning rate and parameter updates"
  },
  {
    "objectID": "DRAFTS/slides-00.html#evaluation",
    "href": "DRAFTS/slides-00.html#evaluation",
    "title": "Slides unit 00",
    "section": "6. Evaluation",
    "text": "6. Evaluation\nUse model.eval() + torch.no_grad() during validation\n\nTrack metrics like loss, R¬≤, accuracy, AUC"
  },
  {
    "objectID": "index-all.html",
    "href": "index-all.html",
    "title": "Deep Learning in Genomics Course Material - ALL",
    "section": "",
    "text": "This page contains ALL material for the GENE46100 Deep learning in genomics course, including test notes.\nFind the 2025 syllabus here.\nEdit github source here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nscgpt quickstart - qmd version\n\n\n\n\n\n\nnotebook\n\n\ngene46100\n\n\n\n\n\n\n\n\n\nMay 13, 2025\n\n\nHaky Im based on cziscience page\n\n\n\n\n\n\n\n\n\n\n\n\nscgpt quickstart - jupyter notebook\n\n\n\n\n\n\nnotebook\n\n\ngene46100\n\n\n\n\n\n\n\n\n\nMay 13, 2025\n\n\nHaky Im based on cziscience page\n\n\n\n\n\n\n\n\n\n\n\n\nUnit 04 Single Cell Genomics\n\n\n\n\n\n\n\n\n\n\n\nMay 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nscanpy tutorial - qmd\n\n\n\n\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nMay 12, 2025\n\n\nscanpy authors\n\n\n\n\n\n\n\n\n\n\n\n\nSeurat - Guided Clustering Tutorial\n\n\n\n\n\n\ngene46100\n\n\ntutorial\n\n\n\n\n\n\n\n\n\nMay 12, 2025\n\n\nSatija lab\n\n\n\n\n\n\n\n\n\n\n\n\nBorzoi prediction from personal genome - qmd version\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\nUsing Borzoi for predicting on a personal genome, derived from VCF\n\n\n\n\n\nMay 5, 2025\n\n\nSofia Salazar\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting alternative polyadenylation with Borzoi\n\n\n\n\n\n\nnotebook\n\n\ngene46100\n\n\n\n\n\n\n\n\n\nMay 5, 2025\n\n\nSofia Salazar\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 6\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMay 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nEnformer Architecture\n\n\n\n\n\n\ngene46100\n\n\n\n\n\n\n\n\n\nMay 1, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nunit 03\n\n\n\n\n\n\ngene46100\n\n\n\n\n\n\n\n\n\nMay 1, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nEnformer training\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\ntrain enformer on human and mouse\n\n\n\n\n\nApr 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nEnformer usage neanderthal - jupyter notebook version\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\npredict neanderthal epigenome with enformer\n\n\n\n\n\nApr 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 5\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nApr 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nEmbeddings Analysis\n\n\n\n\n\n\n\n\n\n\n\nApr 23, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 4\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nApr 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a GPT - companion notebook annotated\n\n\n\n\n\nCompanion notebook from Karpathys video on building a minimal GPT, annotated by cursors LLM with summary from gemini.\n\n\n\n\n\nApr 15, 2025\n\n\nAndrey Karpathy\n\n\n\n\n\n\n\n\n\n\n\n\nhow to install local llm\n\n\n\n\n\n\nhow-to\n\n\nllm\n\n\n\n\n\n\n\n\n\nApr 14, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 3\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nApr 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCalibrating hyperparameters with weights and biases\n\n\n\n\n\n\ngene46100\n\n\nproject\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nApr 10, 2025\n\n\nSofia Salazar\n\n\n\n\n\n\n\n\n\n\n\n\nTF binding prediction challenge\n\n\n\n\n\n\ngene46100\n\n\nproject\n\n\n\nCompetition details for TF binding prediction challenge\n\n\n\n\n\nApr 8, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 2\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nApr 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nUpdated - DNA score prediction with Pytorch\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nApr 4, 2025\n\n\nErin Wilson\n\n\n\n\n\n\n\n\n\n\n\n\nTF Binding prediction project\n\n\n\n\n\n\ngene46100\n\n\nproject\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nApr 3, 2025\n\n\nSofia Salazar\n\n\n\n\n\n\n\n\n\n\n\n\nFit linear mode to linear data\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring information in DNA sequence\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 1\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMar 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nQuick introduction to deep learning\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nMar 25, 2025\n\n\nBoxiang Liu, modified by Haky Im\n\n\n\n\n\n\n\n\n\n\n\n\npreparing environment for unit 00\n\n\n\n\n\n\ngene46100\n\n\nhow-to\n\n\n\nsetting up conda environement and installing packages\n\n\n\n\n\nMar 24, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a GPT From Scratch summary\n\n\n\n\n\n\ngene46100\n\n\nslides\n\n\ngpt\n\n\n\n\n\n\n\n\n\nMar 24, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nGradient descent illustration\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nConda Environments for Quarto Documents\n\n\n\n\n\n\nhow-to\n\n\n\nHow to specify a specific Conda environment when rendering quarto documents.\n\n\n\n\n\nJan 6, 2024\n\n\nRich Leyshon\n\n\n\n\n\n\n\n\n\n\n\n\nBorzoi prediction from personal genome - jupyter notebook\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\nUsing Borzoi for predicting on a personal genome, derived from VCF\n\n\n\n\n\nSep 24, 2023\n\n\nSofia Salazar\n\n\n\n\n\n\n\n\n\n\n\n\nSlides unit 00\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTODO\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2020\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/homework-01.html",
    "href": "post/2025-03-25-unit00/homework-01.html",
    "title": "homework 1",
    "section": "",
    "text": "Homework 1\n\nset up the environment on macos as described in the page (10 points)\n\nUpload the environment-gene46100.yml and requirements-gene46100.txt with the canvas assignment\nIf you have any issues, please let me know.\n\ngit clone the course repository to your local machine (10 points)\n\nUpload the screenshot of the terminal command you used to clone the repository with the canvas assignment\n\nrun all the code in the notebook. You should also be able to find the notebook in your cloned course repository. (20 points)\ncomplete the exercises in the notebook (30 points)\nsubmit the jupyter notebook with the results to canvas assignment\n\n\n\n\n\n\n\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/homework-03.html",
    "href": "post/2025-03-25-unit00/homework-03.html",
    "title": "homework 3",
    "section": "",
    "text": "Implement hyperparameter tuning using Weights & Biases (wandb)\nChoose one of the following problems to apply this to:\n\nTF binding prediction\nSimple DNA scoring\nCubic function with MLP\n\nReference example: TF Binding with wandb\nDocument the hyperparameters you tuned and their impact on model performance\n\n\n\n\n\nUpload your complete Jupyter notebook containing:\n\nModel architecture\nTraining code\nEvaluation metrics\nPrediction code\n\nInclude a separate, self-contained prediction script that:\n\nLoads all required packages\nLoads the trained model architecture and weights\nMakes predictions on the hold-out test set\nCan be run independently without executing the entire notebook\n\n\n\n\n\n\nSubmit your Jupyter notebook with all code and documentation\nInclude a separate prediction script as described above\nDocument your hyperparameter tuning process and results\nProvide clear instructions for running the prediction script\n\n\n\n\n\nProper implementation of hyperparameter tuning\nModel performance on test set\nCode organization and documentation\nReproducibility of results\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/homework-03.html#part-1-hyperparameter-calibration-with-weights-biases",
    "href": "post/2025-03-25-unit00/homework-03.html#part-1-hyperparameter-calibration-with-weights-biases",
    "title": "homework 3",
    "section": "",
    "text": "Implement hyperparameter tuning using Weights & Biases (wandb)\nChoose one of the following problems to apply this to:\n\nTF binding prediction\nSimple DNA scoring\nCubic function with MLP\n\nReference example: TF Binding with wandb\nDocument the hyperparameters you tuned and their impact on model performance"
  },
  {
    "objectID": "post/2025-03-25-unit00/homework-03.html#part-2-model-deployment-and-testing",
    "href": "post/2025-03-25-unit00/homework-03.html#part-2-model-deployment-and-testing",
    "title": "homework 3",
    "section": "",
    "text": "Upload your complete Jupyter notebook containing:\n\nModel architecture\nTraining code\nEvaluation metrics\nPrediction code\n\nInclude a separate, self-contained prediction script that:\n\nLoads all required packages\nLoads the trained model architecture and weights\nMakes predictions on the hold-out test set\nCan be run independently without executing the entire notebook"
  },
  {
    "objectID": "post/2025-03-25-unit00/homework-03.html#submission-requirements",
    "href": "post/2025-03-25-unit00/homework-03.html#submission-requirements",
    "title": "homework 3",
    "section": "",
    "text": "Submit your Jupyter notebook with all code and documentation\nInclude a separate prediction script as described above\nDocument your hyperparameter tuning process and results\nProvide clear instructions for running the prediction script"
  },
  {
    "objectID": "post/2025-03-25-unit00/homework-03.html#evaluation-criteria",
    "href": "post/2025-03-25-unit00/homework-03.html#evaluation-criteria",
    "title": "homework 3",
    "section": "",
    "text": "Proper implementation of hyperparameter tuning\nModel performance on test set\nCode organization and documentation\nReproducibility of results"
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-challenge.html",
    "href": "post/2025-03-25-unit00/tf-binding-challenge.html",
    "title": "TF binding prediction challenge",
    "section": "",
    "text": "Goal: Predict transcription factor (TF) binding scores in DNA sequences\nInput: 300bp human DNA sequences\nTarget: Binding scores for transcription factors\n\n\n\n\nThe challenge uses real genomic data to predict TF binding scores:\n\nSequence Data: chr#_sequences.txt.gz files containing 300bp DNA sequences\n\nEach sequence has a unique identifier in format chr#_start_end\n\nTarget Data: chr#_scores.txt.gz files containing binding scores\n\nEach sequence has a corresponding 300-long vector of binding scores\nScores are predicted using Homer, a widely used motif discovery tool\nEach position in the vector represents the binding score at that position in the sequence\n\n\nData prepared by Sofia Salazar.\n\n\n\n\nData download link\nStarter notebook\nExample implementation\n\n\n\n\n\nTraining Sessions:\n\nTuesday, April 8: Sofia will review implementation of using the code in the basic DNA scoring model. Students will continue working on the project. Charles, Sofia, and Ran will be available to help.\nThursday, April 10: Sofia will explain how to use weights and biases to calibrate hyperparameters of the model (learning rate, number of filters, kernel size, etc). Charles, Sofia, Ran, and Haky will be available to help.\n\nPresentation Day (Thursday, April 10): Students will present:\n\nModel architecture\nModel performance\nFilter interpretation (time permitting)\nLessons learned\n\nSubmission Deadline: April 18\n\nSubmit best model to Canvas\nTA will test on held-out data\nLeaderboard will be created\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-challenge.html#overview",
    "href": "post/2025-03-25-unit00/tf-binding-challenge.html#overview",
    "title": "TF binding prediction challenge",
    "section": "",
    "text": "Goal: Predict transcription factor (TF) binding scores in DNA sequences\nInput: 300bp human DNA sequences\nTarget: Binding scores for transcription factors"
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-challenge.html#data-description",
    "href": "post/2025-03-25-unit00/tf-binding-challenge.html#data-description",
    "title": "TF binding prediction challenge",
    "section": "",
    "text": "The challenge uses real genomic data to predict TF binding scores:\n\nSequence Data: chr#_sequences.txt.gz files containing 300bp DNA sequences\n\nEach sequence has a unique identifier in format chr#_start_end\n\nTarget Data: chr#_scores.txt.gz files containing binding scores\n\nEach sequence has a corresponding 300-long vector of binding scores\nScores are predicted using Homer, a widely used motif discovery tool\nEach position in the vector represents the binding score at that position in the sequence\n\n\nData prepared by Sofia Salazar."
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-challenge.html#getting-started",
    "href": "post/2025-03-25-unit00/tf-binding-challenge.html#getting-started",
    "title": "TF binding prediction challenge",
    "section": "",
    "text": "Data download link\nStarter notebook\nExample implementation"
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-challenge.html#timeline",
    "href": "post/2025-03-25-unit00/tf-binding-challenge.html#timeline",
    "title": "TF binding prediction challenge",
    "section": "",
    "text": "Training Sessions:\n\nTuesday, April 8: Sofia will review implementation of using the code in the basic DNA scoring model. Students will continue working on the project. Charles, Sofia, and Ran will be available to help.\nThursday, April 10: Sofia will explain how to use weights and biases to calibrate hyperparameters of the model (learning rate, number of filters, kernel size, etc). Charles, Sofia, Ran, and Haky will be available to help.\n\nPresentation Day (Thursday, April 10): Students will present:\n\nModel architecture\nModel performance\nFilter interpretation (time permitting)\nLessons learned\n\nSubmission Deadline: April 18\n\nSubmit best model to Canvas\nTA will test on held-out data\nLeaderboard will be created"
  },
  {
    "objectID": "post/2025-04-15-unit01/homework-05.html",
    "href": "post/2025-04-15-unit01/homework-05.html",
    "title": "homework 5",
    "section": "",
    "text": "Homework 5\n\nCalculate expected cross entropy loss for an untrained classification model, i.e.¬†random guess, as a function of the number of classes\nReplicate Henry‚Äôs training of a DNA language model using nanoGPT\n\n\nChoose at least one of the following\nImprove Henry‚Äôs model to predict promoters by tweaking the model, or freezing the language model and adding more layers after the last layer of the model, etc\nPredict DNA binding score using the data from project 1\n\n\n(extra credit) Re-train DNA based nano GPT keeping only ACGT characters. how does removing the lower case characters change performance, loss function values? how much of the training set is in lower case?\n\n\n\n\n\n\n\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-04-15-unit01/nano-gpt/annotated-gpt_dev.html",
    "href": "post/2025-04-15-unit01/nano-gpt/annotated-gpt_dev.html",
    "title": "Building a GPT - companion notebook annotated",
    "section": "",
    "text": "Companion notebook to the Zero To Hero video on GPT. Downloaded from here\n(https://github.com/karpathy/nanoGPT)\n\n\n\n\nShow the code\n# Download the tiny shakespeare dataset\n#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n\n\n\nShow the code\n# read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n\n\n\nShow the code\n# print the length of the dataset\nprint(\"length of dataset in characters: \", len(text))\n\n\nlength of dataset in characters:  1115394\n\n\n\n\nShow the code\n# let's look at the first 1000 characters\nprint(text[:1000])\n\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n\n\n\n\nShow the code\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n\n\n\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n\n\n\n\n\n\n\nShow the code\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n\n\n[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n\n\n\n\n\n\n\nShow the code\n# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch # we use PyTorch: [https://pytorch.org](https://pytorch.org)\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this\n\n\ntorch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n\n\n\n\n\n\n\nShow the code\n# split up the data into train and validation sets\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n\n\n\n\n\n\nShow the code\nblock_size = 8\ntrain_data[:block_size+1]\n\n\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n\n\n\n\n\n\n\nShow the code\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n\n\nwhen input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n\n\n\n\n\n\n\nShow the code\ntorch.manual_seed(1337)\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n\n\ninputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\nwhen input is [24] the target: 43\nwhen input is [24, 43] the target: 58\nwhen input is [24, 43, 58] the target: 5\nwhen input is [24, 43, 58, 5] the target: 57\nwhen input is [24, 43, 58, 5, 57] the target: 1\nwhen input is [24, 43, 58, 5, 57, 1] the target: 46\nwhen input is [24, 43, 58, 5, 57, 1, 46] the target: 43\nwhen input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\nwhen input is [44] the target: 53\nwhen input is [44, 53] the target: 56\nwhen input is [44, 53, 56] the target: 1\nwhen input is [44, 53, 56, 1] the target: 58\nwhen input is [44, 53, 56, 1, 58] the target: 46\nwhen input is [44, 53, 56, 1, 58, 46] the target: 39\nwhen input is [44, 53, 56, 1, 58, 46, 39] the target: 58\nwhen input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52] the target: 58\nwhen input is [52, 58] the target: 1\nwhen input is [52, 58, 1] the target: 58\nwhen input is [52, 58, 1, 58] the target: 46\nwhen input is [52, 58, 1, 58, 46] the target: 39\nwhen input is [52, 58, 1, 58, 46, 39] the target: 58\nwhen input is [52, 58, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\nwhen input is [25] the target: 17\nwhen input is [25, 17] the target: 27\nwhen input is [25, 17, 27] the target: 10\nwhen input is [25, 17, 27, 10] the target: 0\nwhen input is [25, 17, 27, 10, 0] the target: 21\nwhen input is [25, 17, 27, 10, 0, 21] the target: 1\nwhen input is [25, 17, 27, 10, 0, 21, 1] the target: 54\nwhen input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n\n\n\n\n\n\n\nShow the code\n# define the bigram language model\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\n\n\n\n\nLoss = -\\sum_{i}(y_i * \\log(p_i))x\nwhere:\ny_i = actual probability (0 or 1 for the i-th class) p_i = predicted probability for the i-th class \\sum = sum over all classes (characters)\nThis is the loss for a single token prediction. The total loss reported by F.cross_entropy is the average loss across all B*T tokens in the batch, where:\nB = batch_size T = block_size (sequence length)\nBefore training, we would expect the model to predict the next character from a uniform distribution (random guessing). The probability for the correct character would be 1 / \\text{vocab_size}.\nExpected initial loss \\approx - \\log(1 / \\text{vocab_size}) = \\log(\\text{vocab_size}) \\log(65) \\approx 4.1744\n\n\n\n\n\nShow the code\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb) # xb/yb are from the previous cell (B=4, T=8)\nprint(logits.shape) # Expected: (B, T, C) = (4, 8, 65)\nprint(loss) # Expected: Around 4.17\n\n\ntorch.Size([32, 65])\ntensor(4.8786, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\n\n\n\nShow the code\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\n\n\nSKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\nwnYWmnxKWWev-tDqXErVKLgJ\n\n\n\n\n\n\n\nShow the code\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\n\n\n\n\n\n\nShow the code\nbatch_size = 32 # Redefine batch size for training\nfor steps in range(100): # # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n\n\n4.65630578994751\n\n\n\n\n\n\n\nShow the code\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n\n\n\noTo.JUZ!!zqe!\nxBP qbs$Gy'AcOmrLwwt\np$x;Seh-onQbfM?OjKbn'NwUAW -Np3fkz$FVwAUEa-wzWC -wQo-R!v -Mj?,SPiTyZ;o-opr$mOiPJEYD-CfigkzD3p3?zvS;ADz;.y?o,ivCuC'zqHxcVT cHA\nrT'Fd,SBMZyOslg!NXeF$sBe,juUzLq?w-wzP-h\nERjjxlgJzPbHxf$ q,q,KCDCU fqBOQT\nSV&CW:xSVwZv'DG'NSPypDhKStKzC -$hslxIVzoivnp ,ethA:NCCGoi\ntN!ljjP3fwJMwNelgUzzPGJlgihJ!d?q.d\npSPYgCuCJrIFtb\njQXg\npA.P LP,SPJi\nDBcuBM:CixjJ$Jzkq,OLf3KLQLMGph$O 3DfiPHnXKuHMlyjxEiyZib3FaHV-oJa!zoc'XSP :CKGUhd?lgCOF$;;DTHZMlvvcmZAm;:iv'MMgO&Ywbc;BLCUd&vZINLIzkuTGZa\nD.?\n\n\n\n\n\ntoy example illustrating how matrix multiplication can be used for a ‚Äúweighted aggregation‚Äù\n\n\nShow the code\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3)) # Lower triangular matrix of 1s\na = a / torch.sum(a, 1, keepdim=True) # Normalize rows to sum to 1\nb = torch.randint(0,10,(3,2)).float() # Some data\nc = a @ b # Matrix multiply\nprint('a=')\nprint(a)\nprint('--')\nprint('b=')\nprint(b)\nprint('--')\nprint('c=')\nprint(c)\n\n\na=\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n--\nb=\ntensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n--\nc=\ntensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n\n\n\n\nShow the code\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2 # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\n\ntorch.Size([4, 8, 2])\n\n\n\n\n\n\n\nShow the code\n# We want x[b,t] = mean_{i&lt;=t} x[b,i]\nxbow = torch.zeros((B,T,C)) # x bag-of-words (running average)\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # Select vectors from start up to time t: shape (t+1, C)\n        xbow[b,t] = torch.mean(xprev, 0) # Compute mean along the time dimension (dim 0)\n\n\n\n\n\n\n\nShow the code\n# Create the averaging weight matrix\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True) # Normalize rows to sum to 1\n# Perform batched matrix multiplication\nxbow2 = wei @ x # (T, T) @ (B, T, C) -&gt; (B, T, C) via broadcasting\ntorch.allclose(xbow, xbow2) # Check if results are identical\n\n\nTrue\n\n\n\n\n\n\n\nShow the code\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\n# Mask out future positions by setting them to -infinity before softmax\nwei = wei.masked_fill(tril == 0, float('-inf'))\n# Apply softmax to get row-wise probability distributions (weights)\nwei = F.softmax(wei, dim=-1)\n# Perform weighted aggregation\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3) # Check if results are identical\n\n\nTrue\n\n\n\n\n\nsoftmax(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n\n\n\n\n\nShow the code\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels (embedding dimension)\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)   # (B, T, head_size)\nq = query(x) # (B, T, head_size)\n# Compute attention scores (\"affinities\")\nwei = q @ k.transpose(-2, -1) # (B, T, hs) @ (B, hs, T) ---&gt; (B, T, T)\n\n# Scale the scores\n# Note: Karpathy uses C**-0.5 here (sqrt(embedding_dim)). Standard Transformer uses sqrt(head_size).\nwei = wei * (C**-0.5)\n\n# Apply causal mask\ntril = torch.tril(torch.ones(T, T))\n#wei = torch.zeros((T,T)) # This line is commented out in original, was from softmax demo\nwei = wei.masked_fill(tril == 0, float('-inf')) # Mask future tokens\n\n# Apply softmax to get attention weights\nwei = F.softmax(wei, dim=-1) # (B, T, T)\n\n# Perform weighted aggregation of Values\nv = value(x) # (B, T, head_size)\nout = wei @ v # (B, T, T) @ (B, T, hs) ---&gt; (B, T, hs)\n#out = wei @ x # This would aggregate original x, not the projected values 'v'\n\nout.shape # Expected: (B, T, head_size) = (4, 8, 16)\n\n\ntorch.Size([4, 8, 16])\n\n\n\n\nShow the code\nwei[0] # Show attention weights for the first sequence in the batch\n\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.4264, 0.5736, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3151, 0.3022, 0.3827, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3007, 0.2272, 0.2467, 0.2253, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1635, 0.2048, 0.1776, 0.1616, 0.2926, 0.0000, 0.0000, 0.0000],\n        [0.1403, 0.2272, 0.1454, 0.1244, 0.2678, 0.0949, 0.0000, 0.0000],\n        [0.1554, 0.1815, 0.1224, 0.1213, 0.1428, 0.1603, 0.1164, 0.0000],\n        [0.0952, 0.1217, 0.1130, 0.1453, 0.1137, 0.1180, 0.1467, 0.1464]],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\n\n\n\n\nnC = 64\nX = matrix(rnorm(4*64), nrow=4, ncol=nC)\n## make it so that the third token is similar to the last one\nX[2,] = X[4,]*0.5 + X[2,]*0.5\n## normalize X\nX = t(scale(t(X)))\n\nq = X\nk = X\nv = X\n\nqkt = q %*% t(k)/(nC-1)\nxcor = cor(t(q),t(k))\ndim(xcor)\ndim(qkt)\ncat(\"xcor\\n\")\nxcor\ncat(\"---\\n qkt\\n\")\nqkt\n\ncat(\"are xcor and qkt equal?\")\nall.equal(xcor, qkt)\n\npar(mar=c(5, 6, 4, 2) + 0.1)  # increase left margin to avoid cutting of the y label\npar(pty=\"s\")  # Set plot type to \"square\"\nplot(c(xcor), c(qkt),cex=3,cex.lab=3,cex.axis=2,cex.main=2,cex.sub=2); abline(0,1)\npar(pty=\"m\")  # Reset to default plot type\npar(mar=c(5, 4, 4, 2) + 0.1)  # Reset to default margins\n\n\n\n\nAttention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n\nThere is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens. example: ‚Äúthe cat sat on the mat‚Äù should be different from ‚Äúthe mat sat on the cat‚Äù\nEach example across batch dimension is of course processed completely independently and never ‚Äútalk‚Äù to each other.\nIn an ‚Äúencoder‚Äù attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a ‚Äúdecoder‚Äù attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n‚Äúself-attention‚Äù just means that the keys and values are produced from the same source as queries (all come from x). In ‚Äúcross-attention‚Äù, the queries still get produced from x, but the keys and values come from some other, external source (e.g.¬†an encoder module)\n\n\n\n\n‚ÄúScaled‚Äù attention additionaly divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n\n\nShow the code\n# Demonstrate variance without scaling\nk_unscaled = torch.randn(B,T,head_size)\nq_unscaled = torch.randn(B,T,head_size)\nwei_unscaled = q_unscaled @ k_unscaled.transpose(-2, -1)\nprint(f\"k var: {k_unscaled.var():.4f}, q var: {q_unscaled.var():.4f}, wei (unscaled) var: {wei_unscaled.var():.4f}\")\n\n# Demonstrate variance *with* scaling (using head_size for illustration)\nk = torch.randn(B,T,head_size)\nq = torch.randn(B,T,head_size)\nwei = q @ k.transpose(-2, -1) * head_size**-0.5 # Scale by sqrt(head_size)\nprint(f\"k var: {k.var():.4f}, q var: {q.var():.4f}, wei (scaled) var: {wei.var():.4f}\") # Variance should be closer to 1\n\n\nk var: 1.0449, q var: 1.0700, wei (unscaled) var: 17.4690\nk var: 0.9006, q var: 1.0037, wei (scaled) var: 0.9957\n\n\n\n\nShow the code\nk.var() # Should be close to 1\n\n\ntensor(0.9006)\n\n\n\n\nShow the code\nq.var() # Should be close to 1\n\n\ntensor(1.0037)\n\n\n\n\nShow the code\nwei.var() # With scaling, should be closer to 1 than head_size (16)\n\n\ntensor(0.9957)\n\n\n\n\nShow the code\n# Softmax with small inputs (diffuse distribution)\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n\n\ntensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n\n\n\n\nShow the code\n# Softmax with large inputs (simulating unscaled attention scores) -&gt; peaks\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot\n\n\ntensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n\n\n\n\n\n\n\nShow the code\nclass LayerNorm1d: # (used to be BatchNorm1d)\n    def __init__(self, dim, eps=1e-5, momentum=0.1): # Momentum is not used in typical LayerNorm\n        self.eps = eps\n        # Learnable scale and shift parameters, initialized to 1 and 0\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        # Calculate mean over the *last* dimension (features/embedding)\n        xmean = x.mean(1, keepdim=True) # batch mean (shape B, 1, C if input B, T, C) --&gt; Needs adjustment for (B,C) input shape here. Assumes input is (B, dim)\n        # Correction: x is (32, 100). dim=1 is correct for features. Shape (32, 1)\n        xvar = x.var(1, keepdim=True) # batch variance (shape 32, 1)\n        # Normalize each feature vector independently\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n        # Apply scale and shift\n        self.out = self.gamma * xhat + self.beta\n        return self.out\n\n    def parameters(self):\n        # Expose gamma and beta as learnable parameters\n        return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = LayerNorm1d(100) # Create LayerNorm for 100 features\nx = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\nx = module(x)\nx.shape # Should be (32, 100)\n\n\ntorch.Size([32, 100])\n\n\nExplanation of layernorm\nInput shape: (B, T, C) where: B = batch size T = sequence length (number of tokens) C = embedding dimension (features of each token) For each token in the sequence (each position T), LayerNorm: Takes its embedding vector of size C Calculates the mean and standard deviation of just that vector Normalizes that vector by subtracting its mean and dividing by its standard deviation Applies the learnable scale (gamma) and shift (beta) parameters So if you have a sequence like ‚ÄúThe cat sat‚Äù, and each word is represented by a 64-dimensional embedding vector, LayerNorm would: Take ‚ÄúThe‚Äù‚Äôs 64-dimensional vector and normalize it Take ‚Äúcat‚Äù‚Äôs 64-dimensional vector and normalize it Take ‚Äúsat‚Äù‚Äôs 64-dimensional vector and normalize it Each token‚Äôs vector is normalized independently of the others. This is different from BatchNorm, which would normalize across the batch dimension (i.e., looking at the same position across different examples in the batch). This per-token normalization helps maintain stable gradients during training and is particularly important in Transformers where the attention mechanism needs to work with normalized vectors to compute meaningful attention scores.\n\n\nShow the code\n# Mean and std of the first feature *across the batch*. Not expected to be 0 and 1.\nx[:,0].mean(), x[:,0].std()\n\n\n(tensor(0.1469), tensor(0.8803))\n\n\n\n\nShow the code\n# Mean and std *across features* for the first item in the batch. Expected to be ~0 and ~1.\nx[0,:].mean(), x[0,:].std()\n\n\n(tensor(2.3842e-09), tensor(1.0000))\n\n\n\n\n\n\n\nShow the code\n# &lt;--------- ENCODE ------------------&gt;&lt;--------------- DECODE -----------------&gt;\n# les r√©seaux de neurones sont g√©niaux! &lt;START&gt; neural networks are awesome!&lt;END&gt;\n\n\n\n\n\n\n\nShow the code\n# Import necessary PyTorch modules\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# ===== HYPERPARAMETERS =====\nbatch_size = 16       # Number of sequences per batch (Smaller than Bigram training)\nblock_size = 32       # Context length (Larger than Bigram demo)\nmax_iters = 5000      # Total training iterations (More substantial training) TODO change to 5000 later\neval_interval = 100   # How often to check validation loss\nlearning_rate = 1e-3  # Optimizer learning rate\neval_iters = 200      # Number of batches to average for validation loss estimate\nn_embd = 64           # Embedding dimension (Size of token vectors)\nn_head = 4            # Number of attention heads\nn_layer = 4           # Number of Transformer blocks (layers)\ndropout = 0.0         # Dropout probability (0.0 means no dropout here)\n# ==========================\n\n# Device selection: MPS (Apple Silicon) &gt; CUDA &gt; CPU\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")   # Apple Silicon GPU\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")  # NVIDIA GPU\nelse:\n    device = torch.device(\"cpu\")   # CPU fallback\nprint(f\"Using device: {device}\")\n\n# Set random seed for reproducibility\ntorch.manual_seed(1337)\nif device.type == 'cuda':\n    torch.cuda.manual_seed(1337)\nelif device.type == 'mps':\n    torch.mps.manual_seed(1337)\n\n# Load and read the training text (assuming input.txt is available)\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# ===== DATA PREPROCESSING =====\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nstoi = { ch:i for i,ch in enumerate(chars) }   # string to index\nitos = { i:ch for i,ch in enumerate(chars) }   # index to string\nencode = lambda s: [stoi[c] for c in s]   # convert string to list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])   # convert list of integers to string\n\n# Split data into training and validation sets\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data))   # first 90% for training\ntrain_data = data[:n]\nval_data = data[n:]\n# =============================\n\n# ===== DATA LOADING FUNCTION =====\ndef get_batch(split):\n    \"\"\"Generate a batch of data for training or validation.\"\"\"\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device) # Move data to the target device\n    return x, y\n# ================================\n\n# ===== LOSS ESTIMATION FUNCTION =====\n@torch.no_grad()   # Disable gradient calculation for efficiency\ndef estimate_loss():\n    \"\"\"Estimate the loss on training and validation sets.\"\"\"\n    out = {}\n    model.eval()   # Set model to evaluation mode\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()  # Set model back to training mode\n    return out\n# ===================================\n\n# ===== ATTENTION HEAD IMPLEMENTATION =====\nclass Head(nn.Module):\n    \"\"\"Single head of self-attention.\"\"\"\n    \n    def __init__(self, head_size):\n        super().__init__()\n        # Linear projections for Key, Query, Value\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        # Causal mask (tril). 'register_buffer' makes it part of the model state but not a parameter to be trained.\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        # Dropout layer (applied after softmax)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape # C here is n_embd\n        # Project input to K, Q, V\n        k = self.key(x)   # (B,T,head_size)\n        q = self.query(x) # (B,T,head_size)\n        # Compute attention scores, scale, mask, softmax\n        # Note the scaling by C**-0.5 (sqrt(n_embd)) as discussed before\n        wei = q @ k.transpose(-2,-1) * C**-0.5   # (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))   # Use dynamic slicing [:T, :T] for flexibility if T &lt; block_size\n        wei = F.softmax(wei, dim=-1)   # (B, T, T)\n        wei = self.dropout(wei) # Apply dropout to attention weights\n        # Weighted aggregation of values\n        v = self.value(x) # (B,T,head_size)\n        out = wei @ v # (B, T, T) @ (B, T, head_size) -&gt; (B, T, head_size)\n        return out\n# ========================================\n\n# ===== MULTI-HEAD ATTENTION =====\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        # Linear layer after concatenating heads\n        self.proj = nn.Linear(n_embd, n_embd) # Projects back to n_embd dimension\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # Compute attention for each head and concatenate results\n        out = torch.cat([h(x) for h in self.heads], dim=-1) # Shape (B, T, num_heads * head_size) = (B, T, n_embd)\n        # Apply final projection and dropout\n        out = self.dropout(self.proj(out))\n        return out\n# ===============================\n\n# ===== FEED-FORWARD NETWORK =====\nclass FeedFoward(nn.Module):\n    \"\"\"Simple position-wise feed-forward network with one hidden layer.\"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),   # Expand dimension (common practice)\n            nn.ReLU(),                      # Non-linearity\n            nn.Linear(4 * n_embd, n_embd),   # Project back to original dimension\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n# ==============================\n\n# ===== TRANSFORMER BLOCK =====\nclass Block(nn.Module):\n    \"\"\"Transformer block: communication (attention) followed by computation (FFN).\"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head   # Calculate size for each head\n        self.sa = MultiHeadAttention(n_head, head_size) # Self-Attention layer\n        self.ffwd = FeedFoward(n_embd) # Feed-Forward layer\n        self.ln1 = nn.LayerNorm(n_embd) # LayerNorm for Attention input\n        self.ln2 = nn.LayerNorm(n_embd) # LayerNorm for FFN input\n\n    def forward(self, x):\n        # Pre-Normalization variant: Norm -&gt; Sublayer -&gt; Residual\n        x = x + self.sa(self.ln1(x))  # Attention block\n        x = x + self.ffwd(self.ln2(x)) # Feed-forward block\n        return x\n# ============================\n\n# ===== LANGUAGE MODEL =====\nclass BigramLanguageModel(nn.Module):\n    \"\"\"GPT-like language model using Transformer blocks.\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        # Token Embedding Table: Maps character index to embedding vector. (vocab_size, n_embd)\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        # Position Embedding Table: Maps position index (0 to block_size-1) to embedding vector. (block_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        # Sequence of Transformer Blocks\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        # Final Layer Normalization (applied after blocks)\n        self.ln_f = nn.LayerNorm(n_embd)   # Final layer norm\n        # Linear Head: Maps final embedding back to vocabulary size to get logits. (n_embd, vocab_size)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        \n        # Get token embeddings from indices: (B, T) -&gt; (B, T, n_embd)\n        tok_emb = self.token_embedding_table(idx)\n        # Get position embeddings: Create indices 0..T-1, look up embeddings -&gt; (T, n_embd)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n        # Combine token and position embeddings by addition: (B, T, n_embd). Broadcasting handles the addition.\n        x = tok_emb + pos_emb   # (B,T,C)\n        # Pass through Transformer blocks: (B, T, n_embd) -&gt; (B, T, n_embd)\n        x = self.blocks(x)\n        # Apply final LayerNorm\n        x = self.ln_f(x)\n        # Map to vocabulary logits: (B, T, n_embd) -&gt; (B, T, vocab_size)\n        logits = self.lm_head(x)\n\n        # Calculate loss if targets are provided (same as before)\n        if targets is None:\n            loss = None\n        else:\n            # Reshape for cross_entropy: (B*T, vocab_size) and (B*T)\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        \"\"\"Generate new text given a starting sequence.\"\"\"\n        for _ in range(max_new_tokens):\n            # Crop context `idx` to the last `block_size` tokens. Important as position embeddings only go up to block_size.\n            idx_cond = idx[:, -block_size:]\n            # Get predictions (logits) from the model\n            logits, loss = self(idx_cond)\n            # Focus on the logits for the *last* time step: (B, C)\n            logits = logits[:, -1, :]\n            # Convert logits to probabilities via softmax\n            probs = F.softmax(logits, dim=-1)   # (B, C)\n            # Sample next token index from the probability distribution\n            idx_next = torch.multinomial(probs, num_samples=1)   # (B, 1)\n            # Append the sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1)   # (B, T+1)\n        return idx\n# =========================\n\n# ===== MODEL INITIALIZATION AND TRAINING =====\n# Create model instance and move it to the selected device\nmodel = BigramLanguageModel()\nm = model.to(device)\n# Print number of parameters (useful for understanding model size)\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters') # Calculate and print M parameters\n\n# Create optimizer (AdamW again)\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor iter in range(max_iters):\n    # Evaluate loss periodically\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss() # Get train/val loss using the helper function\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\") # Print losses\n\n    # Sample a batch of data\n    xb, yb = get_batch('train')\n\n    # Forward pass: Evaluate loss\n    logits, loss = model(xb, yb)\n    # Backward pass: Calculate gradients\n    optimizer.zero_grad(set_to_none=True) # Zero gradients\n    loss.backward() # Backpropagation\n    # Update parameters\n    optimizer.step() # Optimizer step\n\n# Generate text from the trained model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device) # Starting context: [[0]]\nprint(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n# ============================================\n\n\nUsing device: mps\n0.209729 M parameters\nstep 0: train loss 4.4116, val loss 4.4022\nstep 100: train loss 2.6568, val loss 2.6670\nstep 200: train loss 2.5091, val loss 2.5059\nstep 300: train loss 2.4194, val loss 2.4336\nstep 400: train loss 2.3499, val loss 2.3563\nstep 500: train loss 2.2963, val loss 2.3126\nstep 600: train loss 2.2411, val loss 2.2501\nstep 700: train loss 2.2053, val loss 2.2188\nstep 800: train loss 2.1645, val loss 2.1882\nstep 900: train loss 2.1238, val loss 2.1498\nstep 1000: train loss 2.1027, val loss 2.1297\nstep 1100: train loss 2.0699, val loss 2.1186\nstep 1200: train loss 2.0394, val loss 2.0806\nstep 1300: train loss 2.0255, val loss 2.0644\nstep 1400: train loss 1.9924, val loss 2.0376\nstep 1500: train loss 1.9697, val loss 2.0303\nstep 1600: train loss 1.9644, val loss 2.0482\nstep 1700: train loss 1.9413, val loss 2.0122\nstep 1800: train loss 1.9087, val loss 1.9949\nstep 1900: train loss 1.9106, val loss 1.9898\nstep 2000: train loss 1.8858, val loss 1.9993\nstep 2100: train loss 1.8722, val loss 1.9762\nstep 2200: train loss 1.8602, val loss 1.9636\nstep 2300: train loss 1.8577, val loss 1.9551\nstep 2400: train loss 1.8442, val loss 1.9467\nstep 2500: train loss 1.8153, val loss 1.9439\nstep 2600: train loss 1.8224, val loss 1.9363\nstep 2700: train loss 1.8125, val loss 1.9370\nstep 2800: train loss 1.8054, val loss 1.9250\nstep 2900: train loss 1.8045, val loss 1.9336\nstep 3000: train loss 1.7950, val loss 1.9202\nstep 3100: train loss 1.7707, val loss 1.9197\nstep 3200: train loss 1.7545, val loss 1.9107\nstep 3300: train loss 1.7569, val loss 1.9075\nstep 3400: train loss 1.7533, val loss 1.8942\nstep 3500: train loss 1.7374, val loss 1.8960\nstep 3600: train loss 1.7268, val loss 1.8909\nstep 3700: train loss 1.7277, val loss 1.8814\nstep 3800: train loss 1.7188, val loss 1.8889\nstep 3900: train loss 1.7194, val loss 1.8714\nstep 4000: train loss 1.7127, val loss 1.8636\nstep 4100: train loss 1.7073, val loss 1.8710\nstep 4200: train loss 1.7022, val loss 1.8597\nstep 4300: train loss 1.6994, val loss 1.8488\nstep 4400: train loss 1.7048, val loss 1.8664\nstep 4500: train loss 1.6860, val loss 1.8461\nstep 4600: train loss 1.6854, val loss 1.8304\nstep 4700: train loss 1.6841, val loss 1.8469\nstep 4800: train loss 1.6655, val loss 1.8454\nstep 4900: train loss 1.6713, val loss 1.8387\nstep 4999: train loss 1.6656, val loss 1.8277\n\nFoast.\n\nMENENIUS:\nPraviely your niews? I cank, CORiced aggele;\nOr heave worth sunt bone Ammiod, Lord,\nWho is make thy batted oub! servilings\nToke as lihtch you basw to see swife,\nIs letsts lown'd us; to lace and though mistrair took\nAnd the proply enstriaghte for a shien.\nWhy, they foul tlead,\nup is later and\nbehoy cried men as thou beatt his you.\n\nHERRY VI:\nThere, you weaks mirre and all was imper, Then death, doth those I will read;\nWeas sul't is King me, I what lady so not this dire.\n\nROMEO:\nO, upon to death! him not this bornorow-prove.\n\nMUCIOND:\nWhy leave ye no you?\n\nDUCUCHESTEH:\nBut one thyies, if will the save your blages wore I mong father you hast;\nAlaitle not arm thither crown tow doth.\n\nFROM WTARDit't me reven.\n\nWARWICK:\nOr, as extress womb voishmas!\nGood me you; and incaes up done! make,\nOr I serigh to emmequerel, to speak, herse to supomet?\n\nLUCIO:\nThe like, But twast on was theirs\npoor of thou do\nAs hath lay but so bredaint, forweet of For which his lictless me,\nThat while fumseriands thy unclity,\nWheree I wam my broth? am the too to virsant, whould enterfuly,\nAll there, ontreman one his him;\nWhen whom to Luvinge one the rews,\nWarwixt kill himfined me the bights the with and\nThost will in him,\nMor Sonme man, make to men, Must took.\n\nServer:\nIs aid the underer you: if\nThe I holseld at most lost! Comioli his but a bedrip thy lord,\nAnd then you pringent, and what you kingle is a gestreface is ears.\nBut take me. Tis basdeh,--\nCendom to nie,\nYou lordone turn to mine hath dels in woo forth.\nPoy devisecity, Ineed and encont\nOnking, pleasiness, here's me?\nWhat the have of the doet.\n\nClaytAM:\nNow tweett, cour is plose,\nOstate, and you raint this made untu\nWith ould to Warwith that me bone;\nWill him drown the have wesest: doth,\nAre goody gent yours the pot opings, time same, that BI thirself have gative. I' cown love this mind,\nNether if thou her fortune they have fight my ftlair aggainst for him burry.\n\nBRUTUS:\nWhoth lost for for leth\nAnd, being eyes\nAnd if for\n\n\n\n\n\n\n\n\nNote\n\n\n\nWith 5000 iterations, the model is able to generate text that is similar to the training text.\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-04-15-unit01/nano-gpt/annotated-gpt_dev.html#building-a-gpt",
    "href": "post/2025-04-15-unit01/nano-gpt/annotated-gpt_dev.html#building-a-gpt",
    "title": "Building a GPT - companion notebook annotated",
    "section": "",
    "text": "Companion notebook to the Zero To Hero video on GPT. Downloaded from here\n(https://github.com/karpathy/nanoGPT)\n\n\n\n\nShow the code\n# Download the tiny shakespeare dataset\n#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n\n\n\nShow the code\n# read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n\n\n\nShow the code\n# print the length of the dataset\nprint(\"length of dataset in characters: \", len(text))\n\n\nlength of dataset in characters:  1115394\n\n\n\n\nShow the code\n# let's look at the first 1000 characters\nprint(text[:1000])\n\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n\n\n\n\nShow the code\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n\n\n\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n\n\n\n\n\n\n\nShow the code\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n\n\n[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n\n\n\n\n\n\n\nShow the code\n# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch # we use PyTorch: [https://pytorch.org](https://pytorch.org)\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this\n\n\ntorch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n\n\n\n\n\n\n\nShow the code\n# split up the data into train and validation sets\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n\n\n\n\n\n\nShow the code\nblock_size = 8\ntrain_data[:block_size+1]\n\n\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n\n\n\n\n\n\n\nShow the code\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n\n\nwhen input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n\n\n\n\n\n\n\nShow the code\ntorch.manual_seed(1337)\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n\n\ninputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\nwhen input is [24] the target: 43\nwhen input is [24, 43] the target: 58\nwhen input is [24, 43, 58] the target: 5\nwhen input is [24, 43, 58, 5] the target: 57\nwhen input is [24, 43, 58, 5, 57] the target: 1\nwhen input is [24, 43, 58, 5, 57, 1] the target: 46\nwhen input is [24, 43, 58, 5, 57, 1, 46] the target: 43\nwhen input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\nwhen input is [44] the target: 53\nwhen input is [44, 53] the target: 56\nwhen input is [44, 53, 56] the target: 1\nwhen input is [44, 53, 56, 1] the target: 58\nwhen input is [44, 53, 56, 1, 58] the target: 46\nwhen input is [44, 53, 56, 1, 58, 46] the target: 39\nwhen input is [44, 53, 56, 1, 58, 46, 39] the target: 58\nwhen input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52] the target: 58\nwhen input is [52, 58] the target: 1\nwhen input is [52, 58, 1] the target: 58\nwhen input is [52, 58, 1, 58] the target: 46\nwhen input is [52, 58, 1, 58, 46] the target: 39\nwhen input is [52, 58, 1, 58, 46, 39] the target: 58\nwhen input is [52, 58, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\nwhen input is [25] the target: 17\nwhen input is [25, 17] the target: 27\nwhen input is [25, 17, 27] the target: 10\nwhen input is [25, 17, 27, 10] the target: 0\nwhen input is [25, 17, 27, 10, 0] the target: 21\nwhen input is [25, 17, 27, 10, 0, 21] the target: 1\nwhen input is [25, 17, 27, 10, 0, 21, 1] the target: 54\nwhen input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n\n\n\n\n\n\n\nShow the code\n# define the bigram language model\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\n\n\n\n\nLoss = -\\sum_{i}(y_i * \\log(p_i))x\nwhere:\ny_i = actual probability (0 or 1 for the i-th class) p_i = predicted probability for the i-th class \\sum = sum over all classes (characters)\nThis is the loss for a single token prediction. The total loss reported by F.cross_entropy is the average loss across all B*T tokens in the batch, where:\nB = batch_size T = block_size (sequence length)\nBefore training, we would expect the model to predict the next character from a uniform distribution (random guessing). The probability for the correct character would be 1 / \\text{vocab_size}.\nExpected initial loss \\approx - \\log(1 / \\text{vocab_size}) = \\log(\\text{vocab_size}) \\log(65) \\approx 4.1744\n\n\n\n\n\nShow the code\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb) # xb/yb are from the previous cell (B=4, T=8)\nprint(logits.shape) # Expected: (B, T, C) = (4, 8, 65)\nprint(loss) # Expected: Around 4.17\n\n\ntorch.Size([32, 65])\ntensor(4.8786, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\n\n\n\nShow the code\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\n\n\nSKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\nwnYWmnxKWWev-tDqXErVKLgJ\n\n\n\n\n\n\n\nShow the code\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\n\n\n\n\n\n\nShow the code\nbatch_size = 32 # Redefine batch size for training\nfor steps in range(100): # # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n\n\n4.65630578994751\n\n\n\n\n\n\n\nShow the code\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n\n\n\noTo.JUZ!!zqe!\nxBP qbs$Gy'AcOmrLwwt\np$x;Seh-onQbfM?OjKbn'NwUAW -Np3fkz$FVwAUEa-wzWC -wQo-R!v -Mj?,SPiTyZ;o-opr$mOiPJEYD-CfigkzD3p3?zvS;ADz;.y?o,ivCuC'zqHxcVT cHA\nrT'Fd,SBMZyOslg!NXeF$sBe,juUzLq?w-wzP-h\nERjjxlgJzPbHxf$ q,q,KCDCU fqBOQT\nSV&CW:xSVwZv'DG'NSPypDhKStKzC -$hslxIVzoivnp ,ethA:NCCGoi\ntN!ljjP3fwJMwNelgUzzPGJlgihJ!d?q.d\npSPYgCuCJrIFtb\njQXg\npA.P LP,SPJi\nDBcuBM:CixjJ$Jzkq,OLf3KLQLMGph$O 3DfiPHnXKuHMlyjxEiyZib3FaHV-oJa!zoc'XSP :CKGUhd?lgCOF$;;DTHZMlvvcmZAm;:iv'MMgO&Ywbc;BLCUd&vZINLIzkuTGZa\nD.?\n\n\n\n\n\ntoy example illustrating how matrix multiplication can be used for a ‚Äúweighted aggregation‚Äù\n\n\nShow the code\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3)) # Lower triangular matrix of 1s\na = a / torch.sum(a, 1, keepdim=True) # Normalize rows to sum to 1\nb = torch.randint(0,10,(3,2)).float() # Some data\nc = a @ b # Matrix multiply\nprint('a=')\nprint(a)\nprint('--')\nprint('b=')\nprint(b)\nprint('--')\nprint('c=')\nprint(c)\n\n\na=\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n--\nb=\ntensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n--\nc=\ntensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n\n\n\n\nShow the code\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2 # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\n\ntorch.Size([4, 8, 2])\n\n\n\n\n\n\n\nShow the code\n# We want x[b,t] = mean_{i&lt;=t} x[b,i]\nxbow = torch.zeros((B,T,C)) # x bag-of-words (running average)\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # Select vectors from start up to time t: shape (t+1, C)\n        xbow[b,t] = torch.mean(xprev, 0) # Compute mean along the time dimension (dim 0)\n\n\n\n\n\n\n\nShow the code\n# Create the averaging weight matrix\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True) # Normalize rows to sum to 1\n# Perform batched matrix multiplication\nxbow2 = wei @ x # (T, T) @ (B, T, C) -&gt; (B, T, C) via broadcasting\ntorch.allclose(xbow, xbow2) # Check if results are identical\n\n\nTrue\n\n\n\n\n\n\n\nShow the code\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\n# Mask out future positions by setting them to -infinity before softmax\nwei = wei.masked_fill(tril == 0, float('-inf'))\n# Apply softmax to get row-wise probability distributions (weights)\nwei = F.softmax(wei, dim=-1)\n# Perform weighted aggregation\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3) # Check if results are identical\n\n\nTrue\n\n\n\n\n\nsoftmax(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n\n\n\n\n\nShow the code\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels (embedding dimension)\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)   # (B, T, head_size)\nq = query(x) # (B, T, head_size)\n# Compute attention scores (\"affinities\")\nwei = q @ k.transpose(-2, -1) # (B, T, hs) @ (B, hs, T) ---&gt; (B, T, T)\n\n# Scale the scores\n# Note: Karpathy uses C**-0.5 here (sqrt(embedding_dim)). Standard Transformer uses sqrt(head_size).\nwei = wei * (C**-0.5)\n\n# Apply causal mask\ntril = torch.tril(torch.ones(T, T))\n#wei = torch.zeros((T,T)) # This line is commented out in original, was from softmax demo\nwei = wei.masked_fill(tril == 0, float('-inf')) # Mask future tokens\n\n# Apply softmax to get attention weights\nwei = F.softmax(wei, dim=-1) # (B, T, T)\n\n# Perform weighted aggregation of Values\nv = value(x) # (B, T, head_size)\nout = wei @ v # (B, T, T) @ (B, T, hs) ---&gt; (B, T, hs)\n#out = wei @ x # This would aggregate original x, not the projected values 'v'\n\nout.shape # Expected: (B, T, head_size) = (4, 8, 16)\n\n\ntorch.Size([4, 8, 16])\n\n\n\n\nShow the code\nwei[0] # Show attention weights for the first sequence in the batch\n\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.4264, 0.5736, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3151, 0.3022, 0.3827, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3007, 0.2272, 0.2467, 0.2253, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1635, 0.2048, 0.1776, 0.1616, 0.2926, 0.0000, 0.0000, 0.0000],\n        [0.1403, 0.2272, 0.1454, 0.1244, 0.2678, 0.0949, 0.0000, 0.0000],\n        [0.1554, 0.1815, 0.1224, 0.1213, 0.1428, 0.1603, 0.1164, 0.0000],\n        [0.0952, 0.1217, 0.1130, 0.1453, 0.1137, 0.1180, 0.1467, 0.1464]],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\n\n\n\n\nnC = 64\nX = matrix(rnorm(4*64), nrow=4, ncol=nC)\n## make it so that the third token is similar to the last one\nX[2,] = X[4,]*0.5 + X[2,]*0.5\n## normalize X\nX = t(scale(t(X)))\n\nq = X\nk = X\nv = X\n\nqkt = q %*% t(k)/(nC-1)\nxcor = cor(t(q),t(k))\ndim(xcor)\ndim(qkt)\ncat(\"xcor\\n\")\nxcor\ncat(\"---\\n qkt\\n\")\nqkt\n\ncat(\"are xcor and qkt equal?\")\nall.equal(xcor, qkt)\n\npar(mar=c(5, 6, 4, 2) + 0.1)  # increase left margin to avoid cutting of the y label\npar(pty=\"s\")  # Set plot type to \"square\"\nplot(c(xcor), c(qkt),cex=3,cex.lab=3,cex.axis=2,cex.main=2,cex.sub=2); abline(0,1)\npar(pty=\"m\")  # Reset to default plot type\npar(mar=c(5, 4, 4, 2) + 0.1)  # Reset to default margins\n\n\n\n\nAttention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n\nThere is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens. example: ‚Äúthe cat sat on the mat‚Äù should be different from ‚Äúthe mat sat on the cat‚Äù\nEach example across batch dimension is of course processed completely independently and never ‚Äútalk‚Äù to each other.\nIn an ‚Äúencoder‚Äù attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a ‚Äúdecoder‚Äù attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n‚Äúself-attention‚Äù just means that the keys and values are produced from the same source as queries (all come from x). In ‚Äúcross-attention‚Äù, the queries still get produced from x, but the keys and values come from some other, external source (e.g.¬†an encoder module)\n\n\n\n\n‚ÄúScaled‚Äù attention additionaly divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n\n\nShow the code\n# Demonstrate variance without scaling\nk_unscaled = torch.randn(B,T,head_size)\nq_unscaled = torch.randn(B,T,head_size)\nwei_unscaled = q_unscaled @ k_unscaled.transpose(-2, -1)\nprint(f\"k var: {k_unscaled.var():.4f}, q var: {q_unscaled.var():.4f}, wei (unscaled) var: {wei_unscaled.var():.4f}\")\n\n# Demonstrate variance *with* scaling (using head_size for illustration)\nk = torch.randn(B,T,head_size)\nq = torch.randn(B,T,head_size)\nwei = q @ k.transpose(-2, -1) * head_size**-0.5 # Scale by sqrt(head_size)\nprint(f\"k var: {k.var():.4f}, q var: {q.var():.4f}, wei (scaled) var: {wei.var():.4f}\") # Variance should be closer to 1\n\n\nk var: 1.0449, q var: 1.0700, wei (unscaled) var: 17.4690\nk var: 0.9006, q var: 1.0037, wei (scaled) var: 0.9957\n\n\n\n\nShow the code\nk.var() # Should be close to 1\n\n\ntensor(0.9006)\n\n\n\n\nShow the code\nq.var() # Should be close to 1\n\n\ntensor(1.0037)\n\n\n\n\nShow the code\nwei.var() # With scaling, should be closer to 1 than head_size (16)\n\n\ntensor(0.9957)\n\n\n\n\nShow the code\n# Softmax with small inputs (diffuse distribution)\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n\n\ntensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n\n\n\n\nShow the code\n# Softmax with large inputs (simulating unscaled attention scores) -&gt; peaks\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot\n\n\ntensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n\n\n\n\n\n\n\nShow the code\nclass LayerNorm1d: # (used to be BatchNorm1d)\n    def __init__(self, dim, eps=1e-5, momentum=0.1): # Momentum is not used in typical LayerNorm\n        self.eps = eps\n        # Learnable scale and shift parameters, initialized to 1 and 0\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        # Calculate mean over the *last* dimension (features/embedding)\n        xmean = x.mean(1, keepdim=True) # batch mean (shape B, 1, C if input B, T, C) --&gt; Needs adjustment for (B,C) input shape here. Assumes input is (B, dim)\n        # Correction: x is (32, 100). dim=1 is correct for features. Shape (32, 1)\n        xvar = x.var(1, keepdim=True) # batch variance (shape 32, 1)\n        # Normalize each feature vector independently\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n        # Apply scale and shift\n        self.out = self.gamma * xhat + self.beta\n        return self.out\n\n    def parameters(self):\n        # Expose gamma and beta as learnable parameters\n        return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = LayerNorm1d(100) # Create LayerNorm for 100 features\nx = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\nx = module(x)\nx.shape # Should be (32, 100)\n\n\ntorch.Size([32, 100])\n\n\nExplanation of layernorm\nInput shape: (B, T, C) where: B = batch size T = sequence length (number of tokens) C = embedding dimension (features of each token) For each token in the sequence (each position T), LayerNorm: Takes its embedding vector of size C Calculates the mean and standard deviation of just that vector Normalizes that vector by subtracting its mean and dividing by its standard deviation Applies the learnable scale (gamma) and shift (beta) parameters So if you have a sequence like ‚ÄúThe cat sat‚Äù, and each word is represented by a 64-dimensional embedding vector, LayerNorm would: Take ‚ÄúThe‚Äù‚Äôs 64-dimensional vector and normalize it Take ‚Äúcat‚Äù‚Äôs 64-dimensional vector and normalize it Take ‚Äúsat‚Äù‚Äôs 64-dimensional vector and normalize it Each token‚Äôs vector is normalized independently of the others. This is different from BatchNorm, which would normalize across the batch dimension (i.e., looking at the same position across different examples in the batch). This per-token normalization helps maintain stable gradients during training and is particularly important in Transformers where the attention mechanism needs to work with normalized vectors to compute meaningful attention scores.\n\n\nShow the code\n# Mean and std of the first feature *across the batch*. Not expected to be 0 and 1.\nx[:,0].mean(), x[:,0].std()\n\n\n(tensor(0.1469), tensor(0.8803))\n\n\n\n\nShow the code\n# Mean and std *across features* for the first item in the batch. Expected to be ~0 and ~1.\nx[0,:].mean(), x[0,:].std()\n\n\n(tensor(2.3842e-09), tensor(1.0000))\n\n\n\n\n\n\n\nShow the code\n# &lt;--------- ENCODE ------------------&gt;&lt;--------------- DECODE -----------------&gt;\n# les r√©seaux de neurones sont g√©niaux! &lt;START&gt; neural networks are awesome!&lt;END&gt;\n\n\n\n\n\n\n\nShow the code\n# Import necessary PyTorch modules\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# ===== HYPERPARAMETERS =====\nbatch_size = 16       # Number of sequences per batch (Smaller than Bigram training)\nblock_size = 32       # Context length (Larger than Bigram demo)\nmax_iters = 5000      # Total training iterations (More substantial training) TODO change to 5000 later\neval_interval = 100   # How often to check validation loss\nlearning_rate = 1e-3  # Optimizer learning rate\neval_iters = 200      # Number of batches to average for validation loss estimate\nn_embd = 64           # Embedding dimension (Size of token vectors)\nn_head = 4            # Number of attention heads\nn_layer = 4           # Number of Transformer blocks (layers)\ndropout = 0.0         # Dropout probability (0.0 means no dropout here)\n# ==========================\n\n# Device selection: MPS (Apple Silicon) &gt; CUDA &gt; CPU\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")   # Apple Silicon GPU\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")  # NVIDIA GPU\nelse:\n    device = torch.device(\"cpu\")   # CPU fallback\nprint(f\"Using device: {device}\")\n\n# Set random seed for reproducibility\ntorch.manual_seed(1337)\nif device.type == 'cuda':\n    torch.cuda.manual_seed(1337)\nelif device.type == 'mps':\n    torch.mps.manual_seed(1337)\n\n# Load and read the training text (assuming input.txt is available)\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# ===== DATA PREPROCESSING =====\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nstoi = { ch:i for i,ch in enumerate(chars) }   # string to index\nitos = { i:ch for i,ch in enumerate(chars) }   # index to string\nencode = lambda s: [stoi[c] for c in s]   # convert string to list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])   # convert list of integers to string\n\n# Split data into training and validation sets\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data))   # first 90% for training\ntrain_data = data[:n]\nval_data = data[n:]\n# =============================\n\n# ===== DATA LOADING FUNCTION =====\ndef get_batch(split):\n    \"\"\"Generate a batch of data for training or validation.\"\"\"\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device) # Move data to the target device\n    return x, y\n# ================================\n\n# ===== LOSS ESTIMATION FUNCTION =====\n@torch.no_grad()   # Disable gradient calculation for efficiency\ndef estimate_loss():\n    \"\"\"Estimate the loss on training and validation sets.\"\"\"\n    out = {}\n    model.eval()   # Set model to evaluation mode\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()  # Set model back to training mode\n    return out\n# ===================================\n\n# ===== ATTENTION HEAD IMPLEMENTATION =====\nclass Head(nn.Module):\n    \"\"\"Single head of self-attention.\"\"\"\n    \n    def __init__(self, head_size):\n        super().__init__()\n        # Linear projections for Key, Query, Value\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        # Causal mask (tril). 'register_buffer' makes it part of the model state but not a parameter to be trained.\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        # Dropout layer (applied after softmax)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape # C here is n_embd\n        # Project input to K, Q, V\n        k = self.key(x)   # (B,T,head_size)\n        q = self.query(x) # (B,T,head_size)\n        # Compute attention scores, scale, mask, softmax\n        # Note the scaling by C**-0.5 (sqrt(n_embd)) as discussed before\n        wei = q @ k.transpose(-2,-1) * C**-0.5   # (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))   # Use dynamic slicing [:T, :T] for flexibility if T &lt; block_size\n        wei = F.softmax(wei, dim=-1)   # (B, T, T)\n        wei = self.dropout(wei) # Apply dropout to attention weights\n        # Weighted aggregation of values\n        v = self.value(x) # (B,T,head_size)\n        out = wei @ v # (B, T, T) @ (B, T, head_size) -&gt; (B, T, head_size)\n        return out\n# ========================================\n\n# ===== MULTI-HEAD ATTENTION =====\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        # Linear layer after concatenating heads\n        self.proj = nn.Linear(n_embd, n_embd) # Projects back to n_embd dimension\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # Compute attention for each head and concatenate results\n        out = torch.cat([h(x) for h in self.heads], dim=-1) # Shape (B, T, num_heads * head_size) = (B, T, n_embd)\n        # Apply final projection and dropout\n        out = self.dropout(self.proj(out))\n        return out\n# ===============================\n\n# ===== FEED-FORWARD NETWORK =====\nclass FeedFoward(nn.Module):\n    \"\"\"Simple position-wise feed-forward network with one hidden layer.\"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),   # Expand dimension (common practice)\n            nn.ReLU(),                      # Non-linearity\n            nn.Linear(4 * n_embd, n_embd),   # Project back to original dimension\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n# ==============================\n\n# ===== TRANSFORMER BLOCK =====\nclass Block(nn.Module):\n    \"\"\"Transformer block: communication (attention) followed by computation (FFN).\"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head   # Calculate size for each head\n        self.sa = MultiHeadAttention(n_head, head_size) # Self-Attention layer\n        self.ffwd = FeedFoward(n_embd) # Feed-Forward layer\n        self.ln1 = nn.LayerNorm(n_embd) # LayerNorm for Attention input\n        self.ln2 = nn.LayerNorm(n_embd) # LayerNorm for FFN input\n\n    def forward(self, x):\n        # Pre-Normalization variant: Norm -&gt; Sublayer -&gt; Residual\n        x = x + self.sa(self.ln1(x))  # Attention block\n        x = x + self.ffwd(self.ln2(x)) # Feed-forward block\n        return x\n# ============================\n\n# ===== LANGUAGE MODEL =====\nclass BigramLanguageModel(nn.Module):\n    \"\"\"GPT-like language model using Transformer blocks.\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        # Token Embedding Table: Maps character index to embedding vector. (vocab_size, n_embd)\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        # Position Embedding Table: Maps position index (0 to block_size-1) to embedding vector. (block_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        # Sequence of Transformer Blocks\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        # Final Layer Normalization (applied after blocks)\n        self.ln_f = nn.LayerNorm(n_embd)   # Final layer norm\n        # Linear Head: Maps final embedding back to vocabulary size to get logits. (n_embd, vocab_size)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        \n        # Get token embeddings from indices: (B, T) -&gt; (B, T, n_embd)\n        tok_emb = self.token_embedding_table(idx)\n        # Get position embeddings: Create indices 0..T-1, look up embeddings -&gt; (T, n_embd)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n        # Combine token and position embeddings by addition: (B, T, n_embd). Broadcasting handles the addition.\n        x = tok_emb + pos_emb   # (B,T,C)\n        # Pass through Transformer blocks: (B, T, n_embd) -&gt; (B, T, n_embd)\n        x = self.blocks(x)\n        # Apply final LayerNorm\n        x = self.ln_f(x)\n        # Map to vocabulary logits: (B, T, n_embd) -&gt; (B, T, vocab_size)\n        logits = self.lm_head(x)\n\n        # Calculate loss if targets are provided (same as before)\n        if targets is None:\n            loss = None\n        else:\n            # Reshape for cross_entropy: (B*T, vocab_size) and (B*T)\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        \"\"\"Generate new text given a starting sequence.\"\"\"\n        for _ in range(max_new_tokens):\n            # Crop context `idx` to the last `block_size` tokens. Important as position embeddings only go up to block_size.\n            idx_cond = idx[:, -block_size:]\n            # Get predictions (logits) from the model\n            logits, loss = self(idx_cond)\n            # Focus on the logits for the *last* time step: (B, C)\n            logits = logits[:, -1, :]\n            # Convert logits to probabilities via softmax\n            probs = F.softmax(logits, dim=-1)   # (B, C)\n            # Sample next token index from the probability distribution\n            idx_next = torch.multinomial(probs, num_samples=1)   # (B, 1)\n            # Append the sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1)   # (B, T+1)\n        return idx\n# =========================\n\n# ===== MODEL INITIALIZATION AND TRAINING =====\n# Create model instance and move it to the selected device\nmodel = BigramLanguageModel()\nm = model.to(device)\n# Print number of parameters (useful for understanding model size)\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters') # Calculate and print M parameters\n\n# Create optimizer (AdamW again)\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor iter in range(max_iters):\n    # Evaluate loss periodically\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss() # Get train/val loss using the helper function\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\") # Print losses\n\n    # Sample a batch of data\n    xb, yb = get_batch('train')\n\n    # Forward pass: Evaluate loss\n    logits, loss = model(xb, yb)\n    # Backward pass: Calculate gradients\n    optimizer.zero_grad(set_to_none=True) # Zero gradients\n    loss.backward() # Backpropagation\n    # Update parameters\n    optimizer.step() # Optimizer step\n\n# Generate text from the trained model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device) # Starting context: [[0]]\nprint(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n# ============================================\n\n\nUsing device: mps\n0.209729 M parameters\nstep 0: train loss 4.4116, val loss 4.4022\nstep 100: train loss 2.6568, val loss 2.6670\nstep 200: train loss 2.5091, val loss 2.5059\nstep 300: train loss 2.4194, val loss 2.4336\nstep 400: train loss 2.3499, val loss 2.3563\nstep 500: train loss 2.2963, val loss 2.3126\nstep 600: train loss 2.2411, val loss 2.2501\nstep 700: train loss 2.2053, val loss 2.2188\nstep 800: train loss 2.1645, val loss 2.1882\nstep 900: train loss 2.1238, val loss 2.1498\nstep 1000: train loss 2.1027, val loss 2.1297\nstep 1100: train loss 2.0699, val loss 2.1186\nstep 1200: train loss 2.0394, val loss 2.0806\nstep 1300: train loss 2.0255, val loss 2.0644\nstep 1400: train loss 1.9924, val loss 2.0376\nstep 1500: train loss 1.9697, val loss 2.0303\nstep 1600: train loss 1.9644, val loss 2.0482\nstep 1700: train loss 1.9413, val loss 2.0122\nstep 1800: train loss 1.9087, val loss 1.9949\nstep 1900: train loss 1.9106, val loss 1.9898\nstep 2000: train loss 1.8858, val loss 1.9993\nstep 2100: train loss 1.8722, val loss 1.9762\nstep 2200: train loss 1.8602, val loss 1.9636\nstep 2300: train loss 1.8577, val loss 1.9551\nstep 2400: train loss 1.8442, val loss 1.9467\nstep 2500: train loss 1.8153, val loss 1.9439\nstep 2600: train loss 1.8224, val loss 1.9363\nstep 2700: train loss 1.8125, val loss 1.9370\nstep 2800: train loss 1.8054, val loss 1.9250\nstep 2900: train loss 1.8045, val loss 1.9336\nstep 3000: train loss 1.7950, val loss 1.9202\nstep 3100: train loss 1.7707, val loss 1.9197\nstep 3200: train loss 1.7545, val loss 1.9107\nstep 3300: train loss 1.7569, val loss 1.9075\nstep 3400: train loss 1.7533, val loss 1.8942\nstep 3500: train loss 1.7374, val loss 1.8960\nstep 3600: train loss 1.7268, val loss 1.8909\nstep 3700: train loss 1.7277, val loss 1.8814\nstep 3800: train loss 1.7188, val loss 1.8889\nstep 3900: train loss 1.7194, val loss 1.8714\nstep 4000: train loss 1.7127, val loss 1.8636\nstep 4100: train loss 1.7073, val loss 1.8710\nstep 4200: train loss 1.7022, val loss 1.8597\nstep 4300: train loss 1.6994, val loss 1.8488\nstep 4400: train loss 1.7048, val loss 1.8664\nstep 4500: train loss 1.6860, val loss 1.8461\nstep 4600: train loss 1.6854, val loss 1.8304\nstep 4700: train loss 1.6841, val loss 1.8469\nstep 4800: train loss 1.6655, val loss 1.8454\nstep 4900: train loss 1.6713, val loss 1.8387\nstep 4999: train loss 1.6656, val loss 1.8277\n\nFoast.\n\nMENENIUS:\nPraviely your niews? I cank, CORiced aggele;\nOr heave worth sunt bone Ammiod, Lord,\nWho is make thy batted oub! servilings\nToke as lihtch you basw to see swife,\nIs letsts lown'd us; to lace and though mistrair took\nAnd the proply enstriaghte for a shien.\nWhy, they foul tlead,\nup is later and\nbehoy cried men as thou beatt his you.\n\nHERRY VI:\nThere, you weaks mirre and all was imper, Then death, doth those I will read;\nWeas sul't is King me, I what lady so not this dire.\n\nROMEO:\nO, upon to death! him not this bornorow-prove.\n\nMUCIOND:\nWhy leave ye no you?\n\nDUCUCHESTEH:\nBut one thyies, if will the save your blages wore I mong father you hast;\nAlaitle not arm thither crown tow doth.\n\nFROM WTARDit't me reven.\n\nWARWICK:\nOr, as extress womb voishmas!\nGood me you; and incaes up done! make,\nOr I serigh to emmequerel, to speak, herse to supomet?\n\nLUCIO:\nThe like, But twast on was theirs\npoor of thou do\nAs hath lay but so bredaint, forweet of For which his lictless me,\nThat while fumseriands thy unclity,\nWheree I wam my broth? am the too to virsant, whould enterfuly,\nAll there, ontreman one his him;\nWhen whom to Luvinge one the rews,\nWarwixt kill himfined me the bights the with and\nThost will in him,\nMor Sonme man, make to men, Must took.\n\nServer:\nIs aid the underer you: if\nThe I holseld at most lost! Comioli his but a bedrip thy lord,\nAnd then you pringent, and what you kingle is a gestreface is ears.\nBut take me. Tis basdeh,--\nCendom to nie,\nYou lordone turn to mine hath dels in woo forth.\nPoy devisecity, Ineed and encont\nOnking, pleasiness, here's me?\nWhat the have of the doet.\n\nClaytAM:\nNow tweett, cour is plose,\nOstate, and you raint this made untu\nWith ould to Warwith that me bone;\nWill him drown the have wesest: doth,\nAre goody gent yours the pot opings, time same, that BI thirself have gative. I' cown love this mind,\nNether if thou her fortune they have fight my ftlair aggainst for him burry.\n\nBRUTUS:\nWhoth lost for for leth\nAnd, being eyes\nAnd if for\n\n\n\n\n\n\n\n\nNote\n\n\n\nWith 5000 iterations, the model is able to generate text that is similar to the training text."
  },
  {
    "objectID": "post/2025-04-15-unit01/nano-gpt/slides-gpt.html",
    "href": "post/2025-04-15-unit01/nano-gpt/slides-gpt.html",
    "title": "Building a GPT From Scratch summary",
    "section": "",
    "text": "Karpathy‚Äôs gpt_dev.ipynb summarized by gemini, reviewed by me.\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-04-15-unit01/nano-gpt/slides-gpt.html#building-a-gpt-from-scratch-a-character-level-approach",
    "href": "post/2025-04-15-unit01/nano-gpt/slides-gpt.html#building-a-gpt-from-scratch-a-character-level-approach",
    "title": "Building a GPT From Scratch summary",
    "section": "Building a GPT From Scratch: A Character-Level Approach",
    "text": "Building a GPT From Scratch: A Character-Level Approach\n\nUnderstanding the Core Concepts (Based on A. Karpathy‚Äôs NanoGPT)"
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer_training.html",
    "href": "post/2025-05-01-unit03/enformer_training.html",
    "title": "Enformer training",
    "section": "",
    "text": "not yet tested locally\n\n\n\nneeds data for training\nCopyright 2021 DeepMind Technologies Limited\nLicensed under the Apache License, Version 2.0 (the ‚ÄúLicense‚Äù); you may not use this file except in compliance with the License. You may obtain a copy of the License at\nUnless required by applicable law or agreed to in writing, software distributed under the License is distributed on an ‚ÄúAS IS‚Äù BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\nThis colab showcases training of the Enformer model published in\n‚ÄúEffective gene expression prediction from sequence by integrating long-range interactions‚Äù\n≈Ωiga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R. Ledsam, Agnieszka Grabska-Barwinska, Kyle R. Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, David R. Kelley\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer_training.html#steps",
    "href": "post/2025-05-01-unit03/enformer_training.html#steps",
    "title": "Enformer training",
    "section": "Steps",
    "text": "Steps\n\nSetup tf.data.Dataset by directly accessing the Basenji2 data on GCS: gs://basenji_barnyard/data\nTrain the model for a few steps, alternating training on human and mouse data batches\nEvaluate the model on human and mouse genomes"
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer_training.html#setup",
    "href": "post/2025-05-01-unit03/enformer_training.html#setup",
    "title": "Enformer training",
    "section": "Setup",
    "text": "Setup\nStart the colab kernel with GPU: Runtime -&gt; Change runtime type -&gt; GPU\n\nInstall dependencies\n\n# %pip install dm-sonnet tqdm\n\n\n# Get enformer source code\n# !wget -q https://raw.githubusercontent.com/deepmind/deepmind-research/master/enformer/attention_module.py\n# !wget -q https://raw.githubusercontent.com/deepmind/deepmind-research/master/enformer/enformer.py\n\n\n\nImport\n\nimport tensorflow as tf\n# Make sure the GPU is enabled\nassert tf.config.list_physical_devices('GPU'), 'Start the colab kernel with GPU: Runtime -&gt; Change runtime type -&gt; GPU'\n\n# Easier debugging of OOM\n%env TF_ENABLE_GPU_GARBAGE_COLLECTION=false\n\n\nimport sonnet as snt\nfrom tqdm import tqdm\nfrom IPython.display import clear_output\nimport numpy as np\nimport pandas as pd\nimport time\nimport os\n\n\nassert snt.__version__.startswith('2.0')\n\n\ntf.__version__\n\n\n# this doesn't work on mac os\n# !nvidia-smi\nprint(tf.config.list_physical_devices('GPU'))\n# There is no direct command-line equivalent to nvidia-smi for Apple Silicon. Use Activity Monitor for a graphical view, or check GPU availability in TensorFlow with Python code.\n\n\n\nCode\n\nimport enformer\n\n\n# @title `get_targets(organism)`\ndef get_targets(organism):\n  targets_txt = f'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_{organism}.txt'\n  return pd.read_csv(targets_txt, sep='\\t')\n\n\n# @title `get_dataset(organism, subset, num_threads=8)`\nimport glob\nimport json\nimport functools\n\n\ndef organism_path(organism):\n  return os.path.join('gs://basenji_barnyard/data', organism)\n\n\ndef get_dataset(organism, subset, num_threads=8):\n  metadata = get_metadata(organism)\n  dataset = tf.data.TFRecordDataset(tfrecord_files(organism, subset),\n                                    compression_type='ZLIB',\n                                    num_parallel_reads=num_threads)\n  dataset = dataset.map(functools.partial(deserialize, metadata=metadata),\n                        num_parallel_calls=num_threads)\n  return dataset\n\n\ndef get_metadata(organism):\n  # Keys:\n  # num_targets, train_seqs, valid_seqs, test_seqs, seq_length,\n  # pool_width, crop_bp, target_length\n  path = os.path.join(organism_path(organism), 'statistics.json')\n  with tf.io.gfile.GFile(path, 'r') as f:\n    return json.load(f)\n\n\ndef tfrecord_files(organism, subset):\n  # Sort the values by int(*).\n  return sorted(tf.io.gfile.glob(os.path.join(\n      organism_path(organism), 'tfrecords', f'{subset}-*.tfr'\n  )), key=lambda x: int(x.split('-')[-1].split('.')[0]))\n\n\ndef deserialize(serialized_example, metadata):\n  \"\"\"Deserialize bytes stored in TFRecordFile.\"\"\"\n  feature_map = {\n      'sequence': tf.io.FixedLenFeature([], tf.string),\n      'target': tf.io.FixedLenFeature([], tf.string),\n  }\n  example = tf.io.parse_example(serialized_example, feature_map)\n  sequence = tf.io.decode_raw(example['sequence'], tf.bool)\n  sequence = tf.reshape(sequence, (metadata['seq_length'], 4))\n  sequence = tf.cast(sequence, tf.float32)\n\n  target = tf.io.decode_raw(example['target'], tf.float16)\n  target = tf.reshape(target,\n                      (metadata['target_length'], metadata['num_targets']))\n  target = tf.cast(target, tf.float32)\n\n  return {'sequence': sequence,\n          'target': target}"
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer_training.html#load-dataset",
    "href": "post/2025-05-01-unit03/enformer_training.html#load-dataset",
    "title": "Enformer training",
    "section": "Load dataset",
    "text": "Load dataset\n\ndf_targets_human = get_targets('human')\ndf_targets_human.head()\n\n\nhuman_dataset = get_dataset('human', 'train').batch(1).repeat()\nmouse_dataset = get_dataset('mouse', 'train').batch(1).repeat()\nhuman_mouse_dataset = tf.data.Dataset.zip((human_dataset, mouse_dataset)).prefetch(2)\n\n\nit = iter(mouse_dataset)\nexample = next(it)\n\n\n# Example input\nit = iter(human_mouse_dataset)\nexample = next(it)\nfor i in range(len(example)):\n  print(['human', 'mouse'][i])\n  print({k: (v.shape, v.dtype) for k,v in example[i].items()})"
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer_training.html#model-training",
    "href": "post/2025-05-01-unit03/enformer_training.html#model-training",
    "title": "Enformer training",
    "section": "Model training",
    "text": "Model training\n\ndef create_step_function(model, optimizer):\n\n  @tf.function\n  def train_step(batch, head, optimizer_clip_norm_global=0.2):\n    with tf.GradientTape() as tape:\n      outputs = model(batch['sequence'], is_training=True)[head]\n      loss = tf.reduce_mean(\n          tf.keras.losses.poisson(batch['target'], outputs))\n\n    gradients = tape.gradient(loss, model.trainable_variables)\n    optimizer.apply(gradients, model.trainable_variables)\n\n    return loss\n  return train_step\n\n\nlearning_rate = tf.Variable(0., trainable=False, name='learning_rate')\noptimizer = snt.optimizers.Adam(learning_rate=learning_rate)\nnum_warmup_steps = 5000\ntarget_learning_rate = 0.0005\n\nmodel = enformer.Enformer(channels=1536 // 4,  # Use 4x fewer channels to train faster.\n                          num_heads=8,\n                          num_transformer_layers=11,\n                          pooling_type='max')\n\ntrain_step = create_step_function(model, optimizer)\n\n\n# Train the model\nsteps_per_epoch = 20\nnum_epochs = 5\n\ndata_it = iter(human_mouse_dataset)\nglobal_step = 0\nfor epoch_i in range(num_epochs):\n  for i in tqdm(range(steps_per_epoch)):\n    global_step += 1\n\n    if global_step &gt; 1:\n      learning_rate_frac = tf.math.minimum(\n          1.0, global_step / tf.math.maximum(1.0, num_warmup_steps))\n      learning_rate.assign(target_learning_rate * learning_rate_frac)\n\n    batch_human, batch_mouse = next(data_it)\n\n    loss_human = train_step(batch=batch_human, head='human')\n    loss_mouse = train_step(batch=batch_mouse, head='mouse')\n\n  # End of epoch.\n  print('')\n  print('loss_human', loss_human.numpy(),\n        'loss_mouse', loss_mouse.numpy(),\n        'learning_rate', optimizer.learning_rate.numpy()\n        )"
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer_training.html#evaluate",
    "href": "post/2025-05-01-unit03/enformer_training.html#evaluate",
    "title": "Enformer training",
    "section": "Evaluate",
    "text": "Evaluate\n\n# @title `PearsonR` and `R2` metrics\n\ndef _reduced_shape(shape, axis):\n  if axis is None:\n    return tf.TensorShape([])\n  return tf.TensorShape([d for i, d in enumerate(shape) if i not in axis])\n\n\nclass CorrelationStats(tf.keras.metrics.Metric):\n  \"\"\"Contains shared code for PearsonR and R2.\"\"\"\n\n  def __init__(self, reduce_axis=None, name='pearsonr'):\n    \"\"\"Pearson correlation coefficient.\n\n    Args:\n      reduce_axis: Specifies over which axis to compute the correlation (say\n        (0, 1). If not specified, it will compute the correlation across the\n        whole tensor.\n      name: Metric name.\n    \"\"\"\n    super(CorrelationStats, self).__init__(name=name)\n    self._reduce_axis = reduce_axis\n    self._shape = None  # Specified in _initialize.\n\n  def _initialize(self, input_shape):\n    # Remaining dimensions after reducing over self._reduce_axis.\n    self._shape = _reduced_shape(input_shape, self._reduce_axis)\n\n    weight_kwargs = dict(shape=self._shape, initializer='zeros')\n    self._count = self.add_weight(name='count', **weight_kwargs)\n    self._product_sum = self.add_weight(name='product_sum', **weight_kwargs)\n    self._true_sum = self.add_weight(name='true_sum', **weight_kwargs)\n    self._true_squared_sum = self.add_weight(name='true_squared_sum',\n                                             **weight_kwargs)\n    self._pred_sum = self.add_weight(name='pred_sum', **weight_kwargs)\n    self._pred_squared_sum = self.add_weight(name='pred_squared_sum',\n                                             **weight_kwargs)\n\n  def update_state(self, y_true, y_pred, sample_weight=None):\n    \"\"\"Update the metric state.\n\n    Args:\n      y_true: Multi-dimensional float tensor [batch, ...] containing the ground\n        truth values.\n      y_pred: float tensor with the same shape as y_true containing predicted\n        values.\n      sample_weight: 1D tensor aligned with y_true batch dimension specifying\n        the weight of individual observations.\n    \"\"\"\n    if self._shape is None:\n      # Explicit initialization check.\n      self._initialize(y_true.shape)\n    y_true.shape.assert_is_compatible_with(y_pred.shape)\n    y_true = tf.cast(y_true, 'float32')\n    y_pred = tf.cast(y_pred, 'float32')\n\n    self._product_sum.assign_add(\n        tf.reduce_sum(y_true * y_pred, axis=self._reduce_axis))\n\n    self._true_sum.assign_add(\n        tf.reduce_sum(y_true, axis=self._reduce_axis))\n\n    self._true_squared_sum.assign_add(\n        tf.reduce_sum(tf.math.square(y_true), axis=self._reduce_axis))\n\n    self._pred_sum.assign_add(\n        tf.reduce_sum(y_pred, axis=self._reduce_axis))\n\n    self._pred_squared_sum.assign_add(\n        tf.reduce_sum(tf.math.square(y_pred), axis=self._reduce_axis))\n\n    self._count.assign_add(\n        tf.reduce_sum(tf.ones_like(y_true), axis=self._reduce_axis))\n\n  def result(self):\n    raise NotImplementedError('Must be implemented in subclasses.')\n\n  def reset_states(self):\n    if self._shape is not None:\n      tf.keras.backend.batch_set_value([(v, np.zeros(self._shape))\n                                        for v in self.variables])\n\n\nclass PearsonR(CorrelationStats):\n  \"\"\"Pearson correlation coefficient.\n\n  Computed as:\n  ((x - x_avg) * (y - y_avg) / sqrt(Var[x] * Var[y])\n  \"\"\"\n\n  def __init__(self, reduce_axis=(0,), name='pearsonr'):\n    \"\"\"Pearson correlation coefficient.\n\n    Args:\n      reduce_axis: Specifies over which axis to compute the correlation.\n      name: Metric name.\n    \"\"\"\n    super(PearsonR, self).__init__(reduce_axis=reduce_axis,\n                                   name=name)\n\n  def result(self):\n    true_mean = self._true_sum / self._count\n    pred_mean = self._pred_sum / self._count\n\n    covariance = (self._product_sum\n                  - true_mean * self._pred_sum\n                  - pred_mean * self._true_sum\n                  + self._count * true_mean * pred_mean)\n\n    true_var = self._true_squared_sum - self._count * tf.math.square(true_mean)\n    pred_var = self._pred_squared_sum - self._count * tf.math.square(pred_mean)\n    tp_var = tf.math.sqrt(true_var) * tf.math.sqrt(pred_var)\n    correlation = covariance / tp_var\n\n    return correlation\n\n\nclass R2(CorrelationStats):\n  \"\"\"R-squared  (fraction of explained variance).\"\"\"\n\n  def __init__(self, reduce_axis=None, name='R2'):\n    \"\"\"R-squared metric.\n\n    Args:\n      reduce_axis: Specifies over which axis to compute the correlation.\n      name: Metric name.\n    \"\"\"\n    super(R2, self).__init__(reduce_axis=reduce_axis,\n                             name=name)\n\n  def result(self):\n    true_mean = self._true_sum / self._count\n    total = self._true_squared_sum - self._count * tf.math.square(true_mean)\n    residuals = (self._pred_squared_sum - 2 * self._product_sum\n                 + self._true_squared_sum)\n\n    return tf.ones_like(residuals) - residuals / total\n\n\nclass MetricDict:\n  def __init__(self, metrics):\n    self._metrics = metrics\n\n  def update_state(self, y_true, y_pred):\n    for k, metric in self._metrics.items():\n      metric.update_state(y_true, y_pred)\n\n  def result(self):\n    return {k: metric.result() for k, metric in self._metrics.items()}\n\n\ndef evaluate_model(model, dataset, head, max_steps=None):\n  metric = MetricDict({'PearsonR': PearsonR(reduce_axis=(0,1))})\n  @tf.function\n  def predict(x):\n    return model(x, is_training=False)[head]\n\n  for i, batch in tqdm(enumerate(dataset)):\n    if max_steps is not None and i &gt; max_steps:\n      break\n    metric.update_state(batch['target'], predict(batch['sequence']))\n\n  return metric.result()\n\n\nmetrics_human = evaluate_model(model,\n                               dataset=get_dataset('human', 'valid').batch(1).prefetch(2),\n                               head='human',\n                               max_steps=100)\nprint('')\nprint({k: v.numpy().mean() for k, v in metrics_human.items()})\n\n\nmetrics_mouse = evaluate_model(model,\n                               dataset=get_dataset('mouse', 'valid').batch(1).prefetch(2),\n                               head='mouse',\n                               max_steps=100)\nprint('')\nprint({k: v.numpy().mean() for k, v in metrics_mouse.items()})"
  },
  {
    "objectID": "post/2025-05-01-unit03/index.html",
    "href": "post/2025-05-01-unit03/index.html",
    "title": "unit 03",
    "section": "",
    "text": "Slides for Unit 3 - Enformer, Borzoi, and PrediXcan link\nIn this unit, we will explore the Enformer model, learn to use it to predict gene expression from DNA sequences, and see how it can be used to understand the genetic basis of complex traits. We will compare to Borzoi, a recent model that also predicts gene expression from DNA sequences.\n\n\n\n\n\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-05-12-unit04/scGPT/scgpt-quickstart.html",
    "href": "post/2025-05-12-unit04/scGPT/scgpt-quickstart.html",
    "title": "scgpt quickstart - qmd version",
    "section": "",
    "text": "Downloaded from https://virtualcellmodels.cziscience.com/quickstart/scgpt-quickstart\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-05-12-unit04/scGPT/scgpt-quickstart.html#quick-start-scgpt",
    "href": "post/2025-05-12-unit04/scGPT/scgpt-quickstart.html#quick-start-scgpt",
    "title": "scgpt quickstart - qmd version",
    "section": "Quick Start: scGPT",
    "text": "Quick Start: scGPT\nThis quick start will guide you through using the scGPT model, trained on 33 million cells (including data from the CZ CELLxGENE Census), to generate embeddings for single-cell transcriptomic data analysis.\n\nLearning Goals\nBy the end of this tutorial, you will understand how to:\n\nAccess and prepare the scGPT model for use\nGenerate embeddings to analyze and compare your dataset against the CZ CELLxGENE Census\nVisualize the results using a UMAP, colored by cell type\n\n\n\nPre-requisites and Requirements\nBefore starting, ensure you are familiar with:\n\nPython and AnnData\nSingle-cell data analysis (see this tutorial for a primer on the subject)\nYou can run this tutorial locally (tested on an M3 MacBook with 32 GiB memory) or in Google Colab using a T4 instance. Environment setup will be covered in a later section.\n\n\n\n\nAnnData schema\n\n\n\n\nAnnData Structure\nThe AnnData object is a Python-based data structure used by scanpy, scGPT, and other single-cell analysis tools. Here are its key components:\n\nMain Attributes\n\n.X: Main data matrix (cells √ó genes)\n.obs: Cell annotations (pandas dataframe with metadata for each cell)\n.var: Gene annotations (pandas dataframe with metadata for each gene)\n\n\n\nEmbedding Storage\n\n.obsm: Cell embeddings (e.g., PCA, UMAP coordinates)\n.varm: Gene embeddings (e.g., gene loadings)\n\n\n\n\nOverview\nThis notebook provides a step-by-step guide to:\n\nSetting up your environment\nDownloading the necessary model checkpoints and h5ad dataset\nPerforming model inference to create embeddings\nVisualizing the results with UMAP\n\n\n\nSetup\nLet‚Äôs start by setting up dependencies. The released version of scGPT requires PyTorch 2.1.2, so we will remove the existing PyTorch installation and replace it with the required one. If you want to run this on another environment, this step might not be necessary.\n** but then I got error about torch version. So I installed torch 2.3.0 and torchvision 0.16.2. and then reinstalled torch 2.1.2.**\n```{bash}\nconda create -n scgpt python=3.9\nconda activate scgpt\n```\n\nfirst_time = False\nif first_time:\n    # First uninstall the conflicting packages\n    %pip uninstall -y -q torch torchvision\n    %pip uninstall -y numpy pandas scipy scikit-learn anndata cell-gears datasets dcor\n    #%pip install -q torchvision==0.16.2 torch==2.1.2\n    %pip install -q torch==2.3.0 torchvision==0.16.2\n    %pip install -q scgpt scanpy gdown\n\n\n    # Then install them in the correct order with specific versions\n    %pip install numpy==1.23.5\n    %pip install pandas==1.5.3  # This version is compatible with anndata 0.10.9\n    %pip install scipy==1.10.1  # This version is &gt;1.8 as required by anndata\n    %pip install scikit-learn==1.2.2\n    %pip install anndata==0.10.9\n    %pip install cell-gears==0.0.2\n    %pip install dcor==0.6\n\n    %pip install datasets==2.3.0\n\n\n    # First uninstall both packages\n    %pip uninstall -y torch torchtext\n\n    # Then install compatible versions\n    %pip install torch==2.1.2 torchtext==0.16.2\n\nWe can install the rest of our dependencies and import the relevant libraries.\n\n# Import libraries\n\n# Import required packages\nimport os\nimport multiprocessing\n\n# Set MPS fallback for unimplemented operations\nos.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n\n# Monkey-patch os.sched_getaffinity for macOS\nif not hasattr(os, 'sched_getaffinity'):\n    def sched_getaffinity(pid):\n        return set(range(multiprocessing.cpu_count()))\n    os.sched_getaffinity = sched_getaffinity\n\nimport warnings\nimport urllib.request\nfrom pathlib import Path\n\nimport scgpt as scg\nimport scanpy as sc\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Check for MPS availability\ndevice = (\n    torch.device(\"mps\")\n    if torch.backends.mps.is_available()\n    else torch.device(\"cpu\")\n)\nprint(f\"Using device: {device}\")\nprint(\"Note: Some operations may fall back to CPU due to MPS limitations\")\n\nwarnings.filterwarnings(\"ignore\")\n\n\n# Define the base working directory\nWORKDIR = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100/scgpt\"\n# Convert to Path objects for better path handling\nWORKDIR = Path(WORKDIR)\nDATA_DIR = WORKDIR / \"data\"\nMODEL_DIR = WORKDIR / \"model\" \n\nDownload Model Checkpoints and Data\nLet‚Äôs download the checkpoints from the scGPT repository.\n\nwarnings.simplefilter(\"ignore\", ResourceWarning)\nwarnings.filterwarnings(\"ignore\", category=ImportWarning)\n\n# Use gdown with the recursive flag to download the folder\n# Replace the folder ID with the ID of your folder\nfolder_id = '1oWh_-ZRdhtoGQ2Fw24HP41FgLoomVo-y'\n\n# Check if model files already exist\nif not (MODEL_DIR / \"args.json\").exists():\n    print(\"Downloading model checkpoint...\")\n    !gdown --folder {folder_id} -O {MODEL_DIR}\nelse:\n    print(\"Model files already exist in\", MODEL_DIR)\n\nWe will now download an H5AD dataset from CZ CELLxGENE. To reduce memory utilization, we will also perform a reduction to the top 3000 highly variable genes using scanpy‚Äôs highly_variable_genes function.\n\nuri = \"https://datasets.cellxgene.cziscience.com/f50deffa-43ae-4f12-85ed-33e45040a1fa.h5ad\"\nsource_path = DATA_DIR / \"source.h5ad\"\n\n# Check if file exists before downloading\nif not source_path.exists():\n    print(f\"Downloading dataset to {source_path}...\")\n    urllib.request.urlretrieve(uri, filename=str(source_path))\nelse:\n    print(f\"Dataset already exists at {source_path}\")\n\n# Read the data\nadata = sc.read_h5ad(source_path)\n\nbatch_key = \"sample\"\nN_HVG = 3000\n\nsc.pp.highly_variable_genes(adata, n_top_genes=N_HVG, flavor='seurat_v3')\nadata_hvg = adata[:, adata.var['highly_variable']]\n\nWe can now use embed_data to generate the embeddings. Note that gene_col needs to point to the column where the gene names (not symbols!) are defined. For CZ CELLxGENE datasets, they are stored in the feature_name column.\n\n# Monkey patch get_batch_cell_embeddings to force single processor\nimport types\nfrom scgpt.tasks.cell_emb import get_batch_cell_embeddings as original_get_batch_cell_embeddings\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, SequentialSampler\nfrom scgpt.data_collator import DataCollator\nimport numpy as np\nfrom tqdm import tqdm\n\n# Define Dataset class at module level\nclass CellEmbeddingDataset(Dataset):\n    def __init__(self, count_matrix, gene_ids, batch_ids=None, vocab=None, model_configs=None):\n        self.count_matrix = count_matrix\n        self.gene_ids = gene_ids\n        self.batch_ids = batch_ids\n        self.vocab = vocab\n        self.model_configs = model_configs\n\n    def __len__(self):\n        return len(self.count_matrix)\n\n    def __getitem__(self, idx):\n        row = self.count_matrix[idx]\n        nonzero_idx = np.nonzero(row)[0]\n        values = row[nonzero_idx]\n        genes = self.gene_ids[nonzero_idx]\n        # append &lt;cls&gt; token at the beginning\n        genes = np.insert(genes, 0, self.vocab[\"&lt;cls&gt;\"])\n        values = np.insert(values, 0, self.model_configs[\"pad_value\"])\n        genes = torch.from_numpy(genes).long()\n        values = torch.from_numpy(values).float()\n        output = {\n            \"id\": idx,\n            \"genes\": genes,\n            \"expressions\": values,\n        }\n        if self.batch_ids is not None:\n            output[\"batch_labels\"] = self.batch_ids[idx]\n        return output\n\ndef patched_get_batch_cell_embeddings(\n    adata,\n    cell_embedding_mode: str = \"cls\",\n    model=None,\n    vocab=None,\n    max_length=1200,\n    batch_size=64,\n    model_configs=None,\n    gene_ids=None,\n    use_batch_labels=False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Patched version of get_batch_cell_embeddings that uses the module-level Dataset class\n    and forces num_workers=0.\n    \"\"\"\n    count_matrix = adata.X\n    count_matrix = (\n        count_matrix if isinstance(count_matrix, np.ndarray) else count_matrix.toarray()\n    )\n\n    # gene vocabulary ids\n    if gene_ids is None:\n        gene_ids = np.array(adata.var[\"id_in_vocab\"])\n        assert np.all(gene_ids &gt;= 0)\n\n    if use_batch_labels:\n        batch_ids = np.array(adata.obs[\"batch_id\"].tolist())\n\n    if cell_embedding_mode == \"cls\":\n        dataset = CellEmbeddingDataset(\n            count_matrix, \n            gene_ids, \n            batch_ids if use_batch_labels else None,\n            vocab=vocab,\n            model_configs=model_configs\n        )\n        collator = DataCollator(\n            do_padding=True,\n            pad_token_id=vocab[model_configs[\"pad_token\"]],\n            pad_value=model_configs[\"pad_value\"],\n            do_mlm=False,\n            do_binning=True,\n            max_length=max_length,\n            sampling=True,\n            keep_first_n_tokens=1,\n        )\n        data_loader = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            sampler=SequentialSampler(dataset),\n            collate_fn=collator,\n            drop_last=False,\n            num_workers=0,  # Force single worker\n            pin_memory=True,\n        )\n\n        # Use the global device variable instead of getting it from model\n        cell_embeddings = np.zeros(\n            (len(dataset), model_configs[\"embsize\"]), dtype=np.float32\n        )\n        with torch.no_grad():\n            # Disable autocast for MPS as it's not supported\n            count = 0\n            for data_dict in tqdm(data_loader, desc=\"Embedding cells\"):\n                input_gene_ids = data_dict[\"gene\"].to(device)\n                src_key_padding_mask = input_gene_ids.eq(\n                    vocab[model_configs[\"pad_token\"]]\n                )\n                embeddings = model._encode(\n                    input_gene_ids,\n                    data_dict[\"expr\"].to(device),\n                    src_key_padding_mask=src_key_padding_mask,\n                    batch_labels=data_dict[\"batch_labels\"].to(device)\n                    if use_batch_labels\n                    else None,\n                )\n\n                embeddings = embeddings[:, 0, :]  # get the &lt;cls&gt; position embedding\n                embeddings = embeddings.cpu().numpy()\n                cell_embeddings[count : count + len(embeddings)] = embeddings\n                count += len(embeddings)\n        cell_embeddings = cell_embeddings / np.linalg.norm(\n            cell_embeddings, axis=1, keepdims=True\n        )\n    else:\n        raise ValueError(f\"Unknown cell embedding mode: {cell_embedding_mode}\")\n    return cell_embeddings\n\n# Replace the original function with our patched version\nimport scgpt.tasks.cell_emb\nscgpt.tasks.cell_emb.get_batch_cell_embeddings = patched_get_batch_cell_embeddings\n\nos.environ['PYTHONWARNINGS'] = 'ignore'\n\n\nmodel_dir = MODEL_DIR #/ \"scGPT_human\"\ngene_col = \"feature_name\"\ncell_type_key = \"cell_type\"\n\nembedding_file = DATA_DIR / \"ref_embed_adata.h5ad\"\n\nif embedding_file.exists():\n    print(f\"Loading existing embeddings from {embedding_file}\")\n    ref_embed_adata = sc.read_h5ad(str(embedding_file))\nelse:\n    print(\"Computing new embeddings...\")\n    ref_embed_adata = scg.tasks.embed_data(\n        adata_hvg,\n        model_dir,\n        gene_col=gene_col,\n        obs_to_save=cell_type_key,\n        batch_size=64,\n        return_new_adata=True,\n        device=device,  # Pass the device to embed_data\n    )\n    print(f\"Saving embeddings to {embedding_file}\")\n    ref_embed_adata.write(str(embedding_file))\n\nOur scGPT embeddings are stored in the .X attribute of the returned AnnData object and have a dimensionality of 512.\n\nref_embed_adata.X.shape\n\nWe can now calculate neighbors based on scGPT embeddings.\n\nsc.pp.neighbors(ref_embed_adata, use_rep=\"X\")\nsc.tl.umap(ref_embed_adata)\n\nWe will put our calculated UMAP and embeddings in our original adata object with our original annotations.\n\nadata.obsm[\"X_scgpt\"] = ref_embed_adata.X\nadata.obsm[\"X_umap\"] = ref_embed_adata.obsm[\"X_umap\"]\n\nWe can also switch our .var index which is currently set to Ensembl ID‚Äôs, to be gene symbols, allowing us to plot gene expression more easily.\n\n# Add the current index ('ensembl_id') as a new column\nadata.var['ensembl_id'] = adata.var.index\n\n# Set the new index to the 'feature_name' column\nadata.var.set_index('feature_name', inplace=True)\n\n\n# Add a copy of the gene symbols back to the var dataframe\nadata.var['gene_symbol'] = adata.var.index\n\nWe can now plot a UMAP, coloring it by cell type to visualize our embeddings. Below, we color by both the standard cell type labels provided by CZ CELLxGENE and the original cell type annotations from the authors. The embeddings generated by scGPT effectively capture the structure of the data, closely aligning with the original author annotations.\n\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\")\n    #sc.pp.neighbors(ref_embed_adata, use_rep=\"X\")\n    #sc.tl.umap(ref_embed_adata)\n    sc.pl.umap(adata, color=[\"cell_type\", \"annotation_res0.34_new2\"], wspace = 0.6)\n\nWe can also take a look at some markers of the major cell types represented in the dataset.\n\nsc.pl.umap(adata, color=['cell_type', 'MKI67', 'LYZ', 'RBP2', 'MUC2', 'CHGA', 'TAGLN', 'ELAVL3'], frameon=False, use_raw=False, legend_fontsize =\"xx-small\", legend_loc=\"none\")\n\nReferences\nPlease refer to the following papers for information about:\nscGPT: Toward building a foundation model for single-cell multi-omics using generative AI\nCui, H., Wang, C., Maan, H. et al.¬†scGPT: toward building a foundation model for single-cell multi-omics using generative AI. Nat Methods 21, 1470‚Äì1480 (2024). https://doi.org/10.1038/s41592-024-02201-0\nThe dataset used in this tutorial\nMoerkens, R., Mooiweer, J., Ram√≠rez-S√°nchez, A. D., Oelen, R., Franke, L., Wijmenga, C., Barrett, R. J., Jonkers, I. H., & Withoff, S. (2024). An iPSC-derived small intestine-on-chip with self-organizing epithelial, mesenchymal, and neural cells. Cell Reports, 43(7). https://doi.org/10.1016/j.celrep.2024.114247\nCZ CELLxGENE Discover and Census\nCZ CELLxGENE Discover: A single-cell data platform for scalable exploration, analysis and modeling of aggregated data CZI Single-Cell Biology, et al.¬†bioRxiv 2023.10.30; doi: https://doi.org/10.1101/2023.10.30.563174"
  },
  {
    "objectID": "post/2025-05-12-unit04/seurat5_pbmc3k_tutorial.html",
    "href": "post/2025-05-12-unit04/seurat5_pbmc3k_tutorial.html",
    "title": "Seurat - Guided Clustering Tutorial",
    "section": "",
    "text": "suppressMessages(library(tidyverse))\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\nsuppressMessages(library(glue))\n\nWarning: package 'glue' was built under R version 4.3.3\n\nWEBDATA = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\n# SLUG=\"seurat-pbmc3k-tutorial\" ## copy the slug from the header\n# bDATE='2025-05-12' ## copy the date from the blog's header here\nDATA = glue(\"{WEBDATA}/web-GENE-46100/seurat/\")\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\nWORK=DATA\n\n## move data to DATA\n#tempodata=(\"~/Downloads/tempo/gwas_catalog_v1.0.2-associations_e105_r2022-04-07.tsv\")\n#system(glue::glue(\"cp {tempodata} {DATA}/\"))\n\n#system(glue(\"open {DATA}\")) ## this will open the folder\nall_times &lt;- list()  # store the time for each chunk\nknitr::knit_hooks$set(time_it = local({\n  now &lt;- NULL\n  function(before, options) {\n    if (before) {\n      now &lt;&lt;- Sys.time()\n    } else {\n      res &lt;- difftime(Sys.time(), now, units = \"secs\")\n      all_times[[options$label]] &lt;&lt;- res\n    }\n  }\n}))\nknitr::opts_chunk$set(\n  tidy = TRUE,\n  tidy.opts = list(width.cutoff = 95),\n  message = FALSE,\n  warning = FALSE,\n  time_it = TRUE,\n  error = TRUE\n)\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-05-12-unit04/seurat5_pbmc3k_tutorial.html#what-does-data-in-a-count-matrix-look-like",
    "href": "post/2025-05-12-unit04/seurat5_pbmc3k_tutorial.html#what-does-data-in-a-count-matrix-look-like",
    "title": "Seurat - Guided Clustering Tutorial",
    "section": "What does data in a count matrix look like?",
    "text": "What does data in a count matrix look like?\n\n# Lets examine a few genes in the first thirty cells\npbmc.data[c(\"CD3D\", \"TCL1A\", \"MS4A1\"), 1:30]\n\n3 x 30 sparse Matrix of class \"dgCMatrix\"\n\n\n                                                                   \nCD3D  4 . 10 . . 1 2 3 1 . . 2 7 1 . . 1 3 . 2  3 . . . . . 3 4 1 5\nTCL1A . .  . . . . . . 1 . . . . . . . . . . .  . 1 . . . . . . . .\nMS4A1 . 6  . . . . . . 1 1 1 . . . . . . . . . 36 1 2 . . 2 . . . .\n\n\nThe . values in the matrix represent 0s (no molecules detected). Since most values in an scRNA-seq matrix are 0, Seurat uses a sparse-matrix representation whenever possible. This results in significant memory and speed savings for Drop-seq/inDrop/10x data.\n\ndense.size &lt;- object.size(as.matrix(pbmc.data))\ndense.size\n\n709591472 bytes\n\nsparse.size &lt;- object.size(pbmc.data)\nsparse.size\n\n29905192 bytes\n\ndense.size/sparse.size\n\n23.7 bytes"
  },
  {
    "objectID": "post/2025-05-12-unit04/seurat5_pbmc3k_tutorial.html#qc-and-selecting-cells-for-further-analysis",
    "href": "post/2025-05-12-unit04/seurat5_pbmc3k_tutorial.html#qc-and-selecting-cells-for-further-analysis",
    "title": "Seurat - Guided Clustering Tutorial",
    "section": "QC and selecting cells for further analysis",
    "text": "QC and selecting cells for further analysis\nSeurat allows you to easily explore QC metrics and filter cells based on any user-defined criteria. A few QC metrics commonly used by the community include\n\nThe number of unique genes detected in each cell.\n\nLow-quality cells or empty droplets will often have very few genes\nCell doublets or multiplets may exhibit an aberrantly high gene count\n\nSimilarly, the total number of molecules detected within a cell (correlates strongly with unique genes)\nThe percentage of reads that map to the mitochondrial genome\n\nLow-quality / dying cells often exhibit extensive mitochondrial contamination\nWe calculate mitochondrial QC metrics with the PercentageFeatureSet() function, which calculates the percentage of counts originating from a set of features\nWe use the set of all genes starting with MT- as a set of mitochondrial genes\n\n\n\n# The [[ operator can add columns to object metadata. This is a great place to stash QC stats\npbmc[[\"percent.mt\"]] &lt;- PercentageFeatureSet(pbmc, pattern = \"^MT-\")\n\n\n\nWhere are QC metrics stored in Seurat?\n\n\nThe number of unique genes and total molecules are automatically calculated during CreateSeuratObject()\n\nYou can find them stored in the object meta data\n\n\n\n# Show QC metrics for the first 5 cells\nhead(pbmc@meta.data, 5)\n\n                 orig.ident nCount_RNA nFeature_RNA percent.mt\nAAACATACAACCAC-1     pbmc3k       2419          779  3.0177759\nAAACATTGAGCTAC-1     pbmc3k       4903         1352  3.7935958\nAAACATTGATCAGC-1     pbmc3k       3147         1129  0.8897363\nAAACCGTGCTTCCG-1     pbmc3k       2639          960  1.7430845\nAAACCGTGTATGCG-1     pbmc3k        980          521  1.2244898\n\n\n\n¬†\nIn the example below, we visualize QC metrics, and use these to filter cells.\n\nWe filter cells that have unique feature counts over 2,500 or less than 200\nWe filter cells that have &gt;5% mitochondrial counts\n\n\n# Visualize QC metrics as a violin plot\nVlnPlot(pbmc, features = c(\"nFeature_RNA\", \"nCount_RNA\", \"percent.mt\"), ncol = 3)\n\n\n\n\n\n\n\n# FeatureScatter is typically used to visualize feature-feature relationships, but can be used\n# for anything calculated by the object, i.e. columns in object metadata, PC scores etc.\n\nplot1 &lt;- FeatureScatter(pbmc, feature1 = \"nCount_RNA\", feature2 = \"percent.mt\")\nplot2 &lt;- FeatureScatter(pbmc, feature1 = \"nCount_RNA\", feature2 = \"nFeature_RNA\")\nplot1 + plot2\n\n\n\n\n\n\n\npbmc &lt;- subset(pbmc, subset = nFeature_RNA &gt; 200 & nFeature_RNA &lt; 2500 & percent.mt &lt; 5)"
  },
  {
    "objectID": "DRAFTS/2025-02-20-testing/gradient-descent-illustration.html",
    "href": "DRAFTS/2025-02-20-testing/gradient-descent-illustration.html",
    "title": "Gradient descent illustration",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom plotnine import *\n\n\n# Loss function and its gradient\ndef loss(beta):\n    return beta ** 2\n\ndef grad(beta):\n    return 2 * beta\n\n# Simulate gradient descent\nbeta = 3.5  # random initial value\nlearning_rate = 0.2\nsteps = []\n\nfor i in range(10):\n    current_loss = loss(beta)\n    steps.append({'step': i, 'beta': beta, 'loss': current_loss})\n    beta -= learning_rate * grad(beta)\n\ndf = pd.DataFrame(steps)\n\n# Create full curve for loss function\nbeta_vals = np.linspace(-4, 4, 200)\nloss_vals = loss(beta_vals)\ndf_curve = pd.DataFrame({'beta': beta_vals, 'loss': loss_vals})\n\n# Create arrows for learning steps\ndf_arrows = df[:-1].copy()\ndf_arrows['beta_end'] = df['beta'][1:].values\ndf_arrows['loss_end'] = df['loss'][1:].values\n\n# Plot\np = (\n    ggplot(df_curve, aes('beta', 'loss')) +\n    geom_line(size=1.5, color=\"gray\") +\n    geom_point(df, aes('beta', 'loss'), color='purple', size=3) +\n    geom_point(df.tail(1), aes('beta', 'loss'), color='yellow', size=4) +\n    geom_segment(df_arrows,\n                 aes(x='beta', y='loss', xend='beta_end', yend='loss_end'),\n                 arrow=arrow(length=0.15, type='closed'),\n                 color='gray') +\n    annotate('text', x=3.5, y=12, label='Random\\ninitial value', ha='left') +\n#    annotate('text', x=1.5, y=5, label='Learning step', ha='left') +\n#    annotate('text', x=0, y=0.5, label='Minimum', ha='left') +\n    geom_vline(xintercept=0, linetype='dashed', color='gray') +\n    labs(x=r'$\\beta$', y='Loss') +\n    theme_minimal() +\n    theme(\n        figure_size=(7, 4),\n        axis_title=element_text(size=12),\n        axis_text=element_text(size=10)\n    )\n)\n\np\n\n\n\n\n\n\n\n\n\n\n\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "notebooks/05-train-gpt.html",
    "href": "notebooks/05-train-gpt.html",
    "title": "Building a GPT - from colab",
    "section": "",
    "text": "Companion notebook to the Zero To Hero video on GPT. Downloaded from https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing\n\n# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n--2023-01-17 01:39:27--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‚Äòinput.txt‚Äô\n\ninput.txt           100%[===================&gt;]   1.06M  --.-KB/s    in 0.04s   \n\n2023-01-17 01:39:28 (29.0 MB/s) - ‚Äòinput.txt‚Äô saved [1115394/1115394]\n\n\n\n\n# read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n\nprint(\"length of dataset in characters: \", len(text))\n\nlength of dataset in characters:  1115394\n\n\n\n# let's look at the first 1000 characters\nprint(text[:1000])\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n\n\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n\n\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n\n\n\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n\n[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n\n\n\n# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch # we use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this\n\ntorch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n\n\n\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n\nblock_size = 8\ntrain_data[:block_size+1]\n\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n\n\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n\nwhen input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n\n\n\ntorch.manual_seed(1337)\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n\ninputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\nwhen input is [24] the target: 43\nwhen input is [24, 43] the target: 58\nwhen input is [24, 43, 58] the target: 5\nwhen input is [24, 43, 58, 5] the target: 57\nwhen input is [24, 43, 58, 5, 57] the target: 1\nwhen input is [24, 43, 58, 5, 57, 1] the target: 46\nwhen input is [24, 43, 58, 5, 57, 1, 46] the target: 43\nwhen input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\nwhen input is [44] the target: 53\nwhen input is [44, 53] the target: 56\nwhen input is [44, 53, 56] the target: 1\nwhen input is [44, 53, 56, 1] the target: 58\nwhen input is [44, 53, 56, 1, 58] the target: 46\nwhen input is [44, 53, 56, 1, 58, 46] the target: 39\nwhen input is [44, 53, 56, 1, 58, 46, 39] the target: 58\nwhen input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52] the target: 58\nwhen input is [52, 58] the target: 1\nwhen input is [52, 58, 1] the target: 58\nwhen input is [52, 58, 1, 58] the target: 46\nwhen input is [52, 58, 1, 58, 46] the target: 39\nwhen input is [52, 58, 1, 58, 46, 39] the target: 58\nwhen input is [52, 58, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\nwhen input is [25] the target: 17\nwhen input is [25, 17] the target: 27\nwhen input is [25, 17, 27] the target: 10\nwhen input is [25, 17, 27, 10] the target: 0\nwhen input is [25, 17, 27, 10, 0] the target: 21\nwhen input is [25, 17, 27, 10, 0, 21] the target: 1\nwhen input is [25, 17, 27, 10, 0, 21, 1] the target: 54\nwhen input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n\n\n\nprint(xb) # our input to the transformer\n\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\ntorch.Size([32, 65])\ntensor(4.8786, grad_fn=&lt;NllLossBackward0&gt;)\n\nSKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\nwnYWmnxKWWev-tDqXErVKLgJ\n\n\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\n\nbatch_size = 32\nfor steps in range(100): # increase number of steps for good results...\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n\n4.65630578994751\n\n\n\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n\n\noTo.JUZ!!zqe!\nxBP qbs$Gy'AcOmrLwwt\np$x;Seh-onQbfM?OjKbn'NwUAW -Np3fkz$FVwAUEa-wzWC -wQo-R!v -Mj?,SPiTyZ;o-opr$mOiPJEYD-CfigkzD3p3?zvS;ADz;.y?o,ivCuC'zqHxcVT cHA\nrT'Fd,SBMZyOslg!NXeF$sBe,juUzLq?w-wzP-h\nERjjxlgJzPbHxf$ q,q,KCDCU fqBOQT\nSV&CW:xSVwZv'DG'NSPypDhKStKzC -$hslxIVzoivnp ,ethA:NCCGoi\ntN!ljjP3fwJMwNelgUzzPGJlgihJ!d?q.d\npSPYgCuCJrIFtb\njQXg\npA.P LP,SPJi\nDBcuBM:CixjJ$Jzkq,OLf3KLQLMGph$O 3DfiPHnXKuHMlyjxEiyZib3FaHV-oJa!zoc'XSP :CKGUhd?lgCOF$;;DTHZMlvvcmZAm;:iv'MMgO&Ywbc;BLCUd&vZINLIzkuTGZa\nD.?\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "notebooks/05-train-gpt.html#building-a-gpt",
    "href": "notebooks/05-train-gpt.html#building-a-gpt",
    "title": "Building a GPT - from colab",
    "section": "",
    "text": "Companion notebook to the Zero To Hero video on GPT. Downloaded from https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing\n\n# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n--2023-01-17 01:39:27--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‚Äòinput.txt‚Äô\n\ninput.txt           100%[===================&gt;]   1.06M  --.-KB/s    in 0.04s   \n\n2023-01-17 01:39:28 (29.0 MB/s) - ‚Äòinput.txt‚Äô saved [1115394/1115394]\n\n\n\n\n# read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n\nprint(\"length of dataset in characters: \", len(text))\n\nlength of dataset in characters:  1115394\n\n\n\n# let's look at the first 1000 characters\nprint(text[:1000])\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n\n\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n\n\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n\n\n\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n\n[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n\n\n\n# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch # we use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this\n\ntorch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n\n\n\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n\nblock_size = 8\ntrain_data[:block_size+1]\n\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n\n\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n\nwhen input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n\n\n\ntorch.manual_seed(1337)\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n\ninputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\nwhen input is [24] the target: 43\nwhen input is [24, 43] the target: 58\nwhen input is [24, 43, 58] the target: 5\nwhen input is [24, 43, 58, 5] the target: 57\nwhen input is [24, 43, 58, 5, 57] the target: 1\nwhen input is [24, 43, 58, 5, 57, 1] the target: 46\nwhen input is [24, 43, 58, 5, 57, 1, 46] the target: 43\nwhen input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\nwhen input is [44] the target: 53\nwhen input is [44, 53] the target: 56\nwhen input is [44, 53, 56] the target: 1\nwhen input is [44, 53, 56, 1] the target: 58\nwhen input is [44, 53, 56, 1, 58] the target: 46\nwhen input is [44, 53, 56, 1, 58, 46] the target: 39\nwhen input is [44, 53, 56, 1, 58, 46, 39] the target: 58\nwhen input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52] the target: 58\nwhen input is [52, 58] the target: 1\nwhen input is [52, 58, 1] the target: 58\nwhen input is [52, 58, 1, 58] the target: 46\nwhen input is [52, 58, 1, 58, 46] the target: 39\nwhen input is [52, 58, 1, 58, 46, 39] the target: 58\nwhen input is [52, 58, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\nwhen input is [25] the target: 17\nwhen input is [25, 17] the target: 27\nwhen input is [25, 17, 27] the target: 10\nwhen input is [25, 17, 27, 10] the target: 0\nwhen input is [25, 17, 27, 10, 0] the target: 21\nwhen input is [25, 17, 27, 10, 0, 21] the target: 1\nwhen input is [25, 17, 27, 10, 0, 21, 1] the target: 54\nwhen input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n\n\n\nprint(xb) # our input to the transformer\n\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\ntorch.Size([32, 65])\ntensor(4.8786, grad_fn=&lt;NllLossBackward0&gt;)\n\nSKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\nwnYWmnxKWWev-tDqXErVKLgJ\n\n\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\n\nbatch_size = 32\nfor steps in range(100): # increase number of steps for good results...\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n\n4.65630578994751\n\n\n\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n\n\noTo.JUZ!!zqe!\nxBP qbs$Gy'AcOmrLwwt\np$x;Seh-onQbfM?OjKbn'NwUAW -Np3fkz$FVwAUEa-wzWC -wQo-R!v -Mj?,SPiTyZ;o-opr$mOiPJEYD-CfigkzD3p3?zvS;ADz;.y?o,ivCuC'zqHxcVT cHA\nrT'Fd,SBMZyOslg!NXeF$sBe,juUzLq?w-wzP-h\nERjjxlgJzPbHxf$ q,q,KCDCU fqBOQT\nSV&CW:xSVwZv'DG'NSPypDhKStKzC -$hslxIVzoivnp ,ethA:NCCGoi\ntN!ljjP3fwJMwNelgUzzPGJlgihJ!d?q.d\npSPYgCuCJrIFtb\njQXg\npA.P LP,SPJi\nDBcuBM:CixjJ$Jzkq,OLf3KLQLMGph$O 3DfiPHnXKuHMlyjxEiyZib3FaHV-oJa!zoc'XSP :CKGUhd?lgCOF$;;DTHZMlvvcmZAm;:iv'MMgO&Ywbc;BLCUd&vZINLIzkuTGZa\nD.?"
  },
  {
    "objectID": "notebooks/05-train-gpt.html#the-mathematical-trick-in-self-attention",
    "href": "notebooks/05-train-gpt.html#the-mathematical-trick-in-self-attention",
    "title": "Building a GPT - from colab",
    "section": "The mathematical trick in self-attention",
    "text": "The mathematical trick in self-attention\n\n# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0,10,(3,2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('--')\nprint('b=')\nprint(b)\nprint('--')\nprint('c=')\nprint(c)\n\na=\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n--\nb=\ntensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n--\nc=\ntensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n\n\n\n# consider the following toy example:\n\ntorch.manual_seed(1337)\nB,T,C = 4,8,2 # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n\n\n# We want x[b,t] = mean_{i&lt;=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n\n\n# version 2: using matrix multiply for a weighted aggregation\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x # (B, T, T) @ (B, T, C) ----&gt; (B, T, C)\ntorch.allclose(xbow, xbow2)\n\nTrue\n\n\n\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\nTrue\n\n\n\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)   # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\n#wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n#out = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 16])\n\n\n\nwei[0]\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\nNotes: - Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights. - There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens. - Each example across batch dimension is of course processed completely independently and never ‚Äútalk‚Äù to each other - In an ‚Äúencoder‚Äù attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a ‚Äúdecoder‚Äù attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling. - ‚Äúself-attention‚Äù just means that the keys and values are produced from the same source as queries. In ‚Äúcross-attention‚Äù, the queries still get produced from x, but the keys and values come from some other, external source (e.g.¬†an encoder module) - ‚ÄúScaled‚Äù attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n\nk = torch.randn(B,T,head_size)\nq = torch.randn(B,T,head_size)\nwei = q @ k.transpose(-2, -1) * head_size**-0.5\n\n\nk.var()\n\ntensor(1.0449)\n\n\n\nq.var()\n\ntensor(1.0700)\n\n\n\nwei.var()\n\ntensor(1.0918)\n\n\n\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n\ntensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n\n\n\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot\n\ntensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n\n\n\nclass LayerNorm1d: # (used to be BatchNorm1d)\n\n  def __init__(self, dim, eps=1e-5, momentum=0.1):\n    self.eps = eps\n    self.gamma = torch.ones(dim)\n    self.beta = torch.zeros(dim)\n\n  def __call__(self, x):\n    # calculate the forward pass\n    xmean = x.mean(1, keepdim=True) # batch mean\n    xvar = x.var(1, keepdim=True) # batch variance\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n    return self.out\n\n  def parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = LayerNorm1d(100)\nx = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\nx = module(x)\nx.shape\n\ntorch.Size([32, 100])\n\n\n\nx[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs\n\n(tensor(0.1469), tensor(0.8803))\n\n\n\nx[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features\n\n(tensor(-9.5367e-09), tensor(1.0000))\n\n\n\n# French to English translation example:\n\n# &lt;--------- ENCODE ------------------&gt;&lt;--------------- DECODE -----------------&gt;\n# les r√©seaux de neurones sont g√©niaux! &lt;START&gt; neural networks are awesome!&lt;END&gt;\n\n\n\nFull finished code, for reference\nYou may want to refer directly to the git repo instead though.\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 16 # how many independent sequences will we process in parallel?\nblock_size = 32 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 64\nn_head = 4\nn_layer = 4\ndropout = 0.0\n# ------------\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -&gt; (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,C)\n        out = wei @ v # (B, T, T) @ (B, T, C) -&gt; (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = BigramLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n\n0.209729 M parameters\nstep 0: train loss 4.4116, val loss 4.4022\nstep 100: train loss 2.6568, val loss 2.6670\nstep 200: train loss 2.5090, val loss 2.5058\nstep 300: train loss 2.4198, val loss 2.4340\nstep 400: train loss 2.3503, val loss 2.3567\nstep 500: train loss 2.2970, val loss 2.3136\nstep 600: train loss 2.2410, val loss 2.2506\nstep 700: train loss 2.2062, val loss 2.2198\nstep 800: train loss 2.1638, val loss 2.1871\nstep 900: train loss 2.1232, val loss 2.1494\nstep 1000: train loss 2.1020, val loss 2.1293\nstep 1100: train loss 2.0704, val loss 2.1196\nstep 1200: train loss 2.0382, val loss 2.0798\nstep 1300: train loss 2.0249, val loss 2.0640\nstep 1400: train loss 1.9922, val loss 2.0354\nstep 1500: train loss 1.9707, val loss 2.0308\nstep 1600: train loss 1.9614, val loss 2.0474\nstep 1700: train loss 1.9393, val loss 2.0130\nstep 1800: train loss 1.9070, val loss 1.9943\nstep 1900: train loss 1.9057, val loss 1.9871\nstep 2000: train loss 1.8834, val loss 1.9954\nstep 2100: train loss 1.8719, val loss 1.9758\nstep 2200: train loss 1.8582, val loss 1.9623\nstep 2300: train loss 1.8546, val loss 1.9517\nstep 2400: train loss 1.8410, val loss 1.9476\nstep 2500: train loss 1.8167, val loss 1.9455\nstep 2600: train loss 1.8263, val loss 1.9401\nstep 2700: train loss 1.8108, val loss 1.9340\nstep 2800: train loss 1.8040, val loss 1.9247\nstep 2900: train loss 1.8044, val loss 1.9304\nstep 3000: train loss 1.7963, val loss 1.9242\nstep 3100: train loss 1.7687, val loss 1.9147\nstep 3200: train loss 1.7547, val loss 1.9102\nstep 3300: train loss 1.7557, val loss 1.9037\nstep 3400: train loss 1.7547, val loss 1.8946\nstep 3500: train loss 1.7385, val loss 1.8968\nstep 3600: train loss 1.7260, val loss 1.8914\nstep 3700: train loss 1.7257, val loss 1.8808\nstep 3800: train loss 1.7204, val loss 1.8919\nstep 3900: train loss 1.7215, val loss 1.8788\nstep 4000: train loss 1.7146, val loss 1.8639\nstep 4100: train loss 1.7095, val loss 1.8724\nstep 4200: train loss 1.7079, val loss 1.8707\nstep 4300: train loss 1.7035, val loss 1.8502\nstep 4400: train loss 1.7043, val loss 1.8693\nstep 4500: train loss 1.6914, val loss 1.8522\nstep 4600: train loss 1.6853, val loss 1.8357\nstep 4700: train loss 1.6862, val loss 1.8483\nstep 4800: train loss 1.6671, val loss 1.8434\nstep 4900: train loss 1.6736, val loss 1.8415\nstep 4999: train loss 1.6635, val loss 1.8226\n\nFlY BOLINGLO:\nThem thrumply towiter arts the\nmuscue rike begatt the sea it\nWhat satell in rowers that some than othis Marrity.\n\nLUCENTVO:\nBut userman these that, where can is not diesty rege;\nWhat and see to not. But's eyes. What?\n\nJOHN MARGARET:\nThan up I wark, what out, I ever of and love,\none these do sponce, vois I me;\nBut my pray sape to ries all to the not erralied in may.\n\nBENVOLIO:\nTo spits as stold's bewear I would and say mesby all\non sworn make he anough\nAs cousins the solle, whose be my conforeful may lie them yet\nnobe allimely untraled to be thre I say be,\nNotham a brotes theme an make come,\nAnd that his reach to the duke ento\nthe grmeants bell! and now there king-liff-or grief?\n\nGLOUCESTER:\nAll the bettle dreene, for To his like thou thron!\n\nMENENIUS:\nThen, if I knom her all.\nMy lord, but terruly friend\nRish of the ploceiness and wilt tends sure?\nIs you knows a fasir wead\nThat with him my spaut,\nI shall not tas where's not, becomity; my coulds sting,\nthen the wit be dong to tyget our hereefore,\nWho strop me, mend here, if agains, bitten, thy lack.\nThe but these it were is tus. For the her skeep the fasting. joy tweet Bumner:-\nHow the enclady: It you and how,\nI am in him, And ladderle:\nTheir hand whose wife, it my hithre,\nRoman and where sposs gives'd you.\n\nTROMIOLANUS:\nBut livants you great, I shom mistrot come, for to she to lot\nfor smy to men ventry mehus. Gazise;\nFull't were some the cause, and stouch set,\nOr promises, which a kingsasted to your gove them; and sterrer,\nAnd that wae love him.\n\nBRUTUS:\nYou shape with these sweet.\n\nCORTENGONO:\nLo, where 'twon elmes, 'morth young agres;\nSir, azavoust to striel accurded we missery sets crave.\n\nANGOLUM:\nFor is Henry to have gleise the dreason\nThat I ant shorfold wefth their servy in enscy.\n\nISABELLA:\nO, I better you eyse such formfetrews.\n\nBUCKINGHARENT:\nQead my lightle this righanneds flase them\nWam which an take was our some pleasurs,\nLovisoname to me, then fult me?--have it?\n\nHENRY BOLINGBROY:\nThat wha"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html",
    "title": "Quick introduction to deep learning",
    "section": "",
    "text": "Understand and implement gradient descent to fit a linear model\nBuild and train a multi-layer perceptron (MLP) using PyTorch\nLearn the basics of PyTorch, including model definition, forward pass, and optimization\n\n\n\nWe‚Äôll start by making sure we have all the required Python packages installed and ready to go. PyTorch is the main library we‚Äôll use for deep learning.\n\n## install packages if needed\nif False:\n    %pip install scikit-learn plotnine tqdm pandas\n\n\nfrom sklearn.datasets import make_regression\nimport numpy as np\nfrom numpy.linalg import inv\nfrom plotnine import qplot, ggplot, geom_point, geom_line, aes, geom_abline\nfrom plotnine.themes import theme_bw\nfrom plotnine.geoms import annotate\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#simulate-linear-data",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#simulate-linear-data",
    "title": "Quick introduction to deep learning",
    "section": "Simulate linear data",
    "text": "Simulate linear data\nWe generate 1000 samples, each with 2 features. We‚Äôll define the ‚Äútrue‚Äù coefficients and add a bit of noise to make it realistic.\n\nx: predictors/features (shape: 1000√ó2)\ny: response variable (shape: 1000√ó1)\ncoef = [\\beta_1 \\beta_2]: the true coefficients used to generate y\n\n\nnp.random.seed(42)\nbias = 0\nnoise = 10\nx, y, coef = make_regression(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_targets=1,\n    bias=bias,\n    noise=noise,\n    coef=True\n    )\n\n\nshow simulated x, y, and coef= [\\beta_1 \\beta_2]\n\nprint('x.shape:', x.shape)\nprint('y.shape:', y.shape)\nprint('coef.shape:', coef.shape)\n\nx.shape: (1000, 2)\ny.shape: (1000,)\ncoef.shape: (2,)\n\n\n\nprint('x:\\n',x,'\\n')\nprint('y[0:10]:\\n',y[0:10],'\\n')\nprint('coef:\\n',coef,'\\n')\n\nx:\n [[-0.16711808  0.14671369]\n [-0.02090159  0.11732738]\n [ 0.15041891  0.364961  ]\n ...\n [ 0.30263547 -0.75427585]\n [ 0.38193545  0.43004165]\n [ 0.07736831 -0.8612842 ]] \n\ny[0:10]:\n [-14.99694989 -12.67808888  17.77545452   6.66146467 -14.19552996\n -25.24484815 -39.23162627 -52.01803821   5.76368853 -50.11860295] \n\ncoef:\n [40.71064891  6.60098441]"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#predict-y-with-ground-truth-parameters",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#predict-y-with-ground-truth-parameters",
    "title": "Quick introduction to deep learning",
    "section": "Predict y with ground truth parameters",
    "text": "Predict y with ground truth parameters\nSince we know the true coefficients, we can directly compute the predicted y and visualize how well they fit.\n\ny_hat = x.dot(coef) + bias\n\n\ncompare \\hat{y} and y\n\n(qplot(x=y, y=y_hat, geom=\"point\", xlab=\"y\", ylab=\"y_hat\") + geom_abline(intercept=0, slope=1, color='gray', linetype='dashed'))"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#compute-analytical-solution",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#compute-analytical-solution",
    "title": "Quick introduction to deep learning",
    "section": "Compute analytical solution",
    "text": "Compute analytical solution\nFor linear regression, there‚Äôs a closed-form solution known as the normal equation:\n\\hat{\\beta} = (X^T X)^{-1} X^T y\nThis gives us the optimal coefficients that minimize mean square errors.\nLet‚Äôs get \\hat{\\beta} and compare it with the ground truth.\n\nExercise\nDerive the normal equation from the model, using matrix algebra\n\nvar = x.transpose().dot(x)\ncov = x.transpose().dot(y)\nb_hat = inv(var).dot(cov)\n\n\nprint(\"Estimated\")\nprint(b_hat)\n\nprint(\"Ground truth\")\nprint(coef)\n\nEstimated\n[41.06972678  6.79965716]\nGround truth\n[40.71064891  6.60098441]\n\n\n\n\nResult Comparison\nEstimated coefficients using the normal equation are close to the true ones, despite noise. That‚Äôs a good sanity check.\n\n\nExercise\n\nPlot the prediction with the analystical estimates vs the target y"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#define-stochastic-gradient-descent-sgd",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#define-stochastic-gradient-descent-sgd",
    "title": "Quick introduction to deep learning",
    "section": "Define Stochastic Gradient Descent (SGD)",
    "text": "Define Stochastic Gradient Descent (SGD)\nWhen analytical solutions aren‚Äôt available (which is often), we rely on numerical optimization like gradient descent.\nAlthough linear regression has analytical solutions, this is unfortunately not the case for many other models. We may need to resort to numerical approximations to find the optimal parameters. One of the most popular numerical optimizers is called stochastic gradient descent."
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-a-gradient",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-a-gradient",
    "title": "Quick introduction to deep learning",
    "section": "What is a Gradient?",
    "text": "What is a Gradient?\nA gradient is the derivative of a function, and it tells us how to adjust parameters to reduce the error.\n$f‚Äô() = _{‚Üí 0} = $"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-gradient-descent",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-gradient-descent",
    "title": "Quick introduction to deep learning",
    "section": "What is Gradient Descent?",
    "text": "What is Gradient Descent?\nAn optimization algorithm that iteratively updates parameters in the direction of steepest descent (negative gradient) to minimize a loss function.\n\nLearning rate (\\alpha): step size. It is a hyperparameters that determines how fast we move in the direction of the gradient.\nLoss surface: the landscape we are optimizing over\n\nAt a particular Œ≤_i, we find the gradient f'(\\beta), and take a step along the direction of the gradient to find the next point \\beta_{i+1}.\n\\beta_{i+1} = \\beta_i - \\alpha f'(\\beta_i)\nThe \\alpha is called the learning rate.\n\n\n\ngradient descent"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-stochastic-gradient-descent",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-stochastic-gradient-descent",
    "title": "Quick introduction to deep learning",
    "section": "What is stochastic gradient descent?",
    "text": "What is stochastic gradient descent?\nIn real-world applications, the size of our dataset is so large that it is impossible to calculate the gradient using all data points. Therefore, we take a small chunk (called a ‚Äúbatch‚Äù) of the dataset to calcalate the gradient. This approximates the full-data gradients, thus the word stochastic."
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#three-components-of-machine-learning",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#three-components-of-machine-learning",
    "title": "Quick introduction to deep learning",
    "section": "Three Components of Machine Learning",
    "text": "Three Components of Machine Learning\nTo build a machine learning model, we need:\n\nModel: Defines the hypothesis space (e.g., linear model)\nLoss Function: Measures how well the model fits\nOptimizer: Updates parameters to reduce the loss (e.g., SGD)"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#define-the-loss-function",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#define-the-loss-function",
    "title": "Quick introduction to deep learning",
    "section": "Define the Loss Function",
    "text": "Define the Loss Function\nCommon choice for regression: Mean Squared Error (MSE)\n\\ell(\\beta) = \\frac{1}{2}\\sum\\limits_{i=1}^m (f(\\beta)^{i} - y^{i})^2 / m"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#compute-the-gradient",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#compute-the-gradient",
    "title": "Quick introduction to deep learning",
    "section": "Compute the gradient",
    "text": "Compute the gradient\n\\frac{\\partial}{\\partial \\beta_j}\\ell(\\beta) = \\frac{\\partial}{\\partial \\beta_j} \\frac{1}{2} \\sum_i (f(\\beta)^i - y^i)^2 =  \\sum_i (f(\\beta)^i - y^i) x_j\nRecall that in our example $ f() = = _1 x_1 + _2 x_2$. Here \\beta_j is either \\beta_1 or \\beta_2.\n\nExercise\nShow that the derivative of the loss function is as stated above."
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#optimize-estimate-parameters-with-gradient-descent",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#optimize-estimate-parameters-with-gradient-descent",
    "title": "Quick introduction to deep learning",
    "section": "Optimize: estimate parameters with gradient descent",
    "text": "Optimize: estimate parameters with gradient descent\n\nlr = 0.1 # learning rate\nb = [0.0, 0.0] # initialize all betas to 0\nn_examples = len(y)\ntrajectory = [b]\n\nfor _ in range(50): # 50 steps\n  diff = x.dot(b) - y\n  grad = diff.dot(x) / n_examples\n  b -= lr*grad\n  trajectory.append(b.copy())\n\ntrajectory = np.stack(trajectory, axis=1)\n\n\nqplot(x=trajectory[0], y=trajectory[1], xlab=\"beta_1\", ylab=\"beta_2\", geom=[\"point\", \"line\"]) + theme_bw() + annotate(geom=\"point\", x=coef[0], y=coef[1], size=8, color=\"blue\",shape='+',stroke=1)\n\n\n\n\n\n\n\n\n\nExercise\n\nAdd to the plot the normal equation estimates (traditional linear regression) of the coefficients in a different green\n\n\n\nExercise\n\nSimulate y with larger noise, estimate the regression coefficients using the normal equation and the gradient descent method, and plot the trajectory, the ground truth, and the two estimates for comparison."
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#simulate-non-linear-data",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#simulate-non-linear-data",
    "title": "Quick introduction to deep learning",
    "section": "Simulate non linear data",
    "text": "Simulate non linear data\nLet‚Äôs start by simulating data according to the generative model:\n$ y = x_1^3$\n\nx = np.random.normal(size=1000)\ny = x ** 3"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#predict-with-simple-linear-model",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#predict-with-simple-linear-model",
    "title": "Quick introduction to deep learning",
    "section": "Predict with simple linear model",
    "text": "Predict with simple linear model\nLet‚Äôs try to predict with a simple linear regression model\n$ y = X $\n\nQuestion\nhow many parameters do we need to estimate for this simple linear model?\n\nlr = 0.1 # learning rate\nb = 0.0 # initialize all betas to 0\nn_examples = len(y)\n\nfor _ in range(50): # 50 steps\n  diff = x.dot(b) - y\n  grad = diff.dot(x) / n_examples\n  b -= lr*grad\n\n\ny_hat = x.dot(b)\n( qplot(x = y, y=y_hat, geom=\"point\", xlab=\"y\", ylab=\"y_hat\") +\ngeom_abline(intercept=0, slope=1, color='gray', linetype='dashed') )\n\n\n\n\n\n\n\n\n\n\nWe would like to be closer to the dashed gray, identity line! Let‚Äôs try MLP."
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#install-packages-if-needed",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#install-packages-if-needed",
    "title": "Quick introduction to deep learning",
    "section": "Install packages if needed",
    "text": "Install packages if needed\n\n## install packages if needed\nif False:  # Change to True to run the commands\n    %pip install tqdm\n    %pip install torch\n    %pip install torchvision torchmetrics\n\n\nimport torch\nprint(torch.__version__)\n## if on a mac check if mps is available so you use gpu\n## print(torch.backends.mps.is_available())\n\n2.6.0\n\n\n\nDefine a non linear MLP\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n#device = torch.device(\"cuda\")\ndevice = torch.device(\"mps\") ## use this line instead of the one above withb cuda\n\n\nclass MLP(nn.Module):\n  def __init__(self, input_dim, hid_dim, output_dim):\n    super().__init__()\n    self.fc1 = nn.Linear(input_dim, hid_dim)\n    self.fc2 = nn.Linear(hid_dim, output_dim)\n\n  def forward(self, x):\n    x = F.relu(self.fc1(x))\n    y = self.fc2(x)\n\n    return y.squeeze(1)\n\nmlp = MLP(input_dim=1, hid_dim=1024, output_dim=1).to(device)"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#train-mlp-model-using-pytorch",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#train-mlp-model-using-pytorch",
    "title": "Quick introduction to deep learning",
    "section": "train MLP model using pytorch",
    "text": "train MLP model using pytorch\nWe are now ready to train our model. To understand how our model is doing, we record the loss vs step, which is called the learning curve in ML literature.\n\nx_tensor = torch.Tensor(x).unsqueeze(1).to(device)\ny_tensor = torch.Tensor(y).to(device)\nlearning_curve = []\n\noptimizer = torch.optim.SGD(mlp.parameters(), lr=0.001)\nloss_fn = nn.MSELoss()\n\nfor epoch in range(10000):\n  optimizer.zero_grad()\n  y_hat = mlp(x_tensor)\n  loss = loss_fn(y_hat, y_tensor)\n  learning_curve.append(loss.item())\n  loss.backward()\n  optimizer.step()\n\n\nqplot(x=range(10000), y=learning_curve, xlab=\"epoch\", ylab=\"loss\")\n\n\n\n\n\n\n\n\n\ny_hat = mlp(x_tensor)\ny_hat = y_hat.detach().cpu().numpy()\n#qplot(x=y, y=y_hat, geom=\"point\", xlab=\"y\", ylab=\"y_hat\")\nqplot(x=y, y=y_hat, geom=[\"point\", \"abline\"],\n      xlab=\"y\", ylab=\"y_hat\",\n      abline=dict(slope=1, intercept=0, color='red', linetype='dashed'))\n\n\n\n\n\n\n\n\nNow this looks much better!"
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-wandb.html",
    "href": "post/2025-03-25-unit00/tf-binding-wandb.html",
    "title": "Calibrating hyperparameters with weights and biases",
    "section": "",
    "text": "Goal: Learn how to use weights and biases to calibrate the hyperparameters of a DL model.\n\nWeights and biases is a platform used for AI developers to track, visualize and manage their ML models and experiments. The coolest part is that W&B allows you to log various performance metrics during training, like training and validation loss, test set correlations, etc. Additionally, it allows you to compare between different experiments or versions of your models. Making it easier to identify the best performing models and see which hyperparameter configuration is the optimal.\nIn this notebook, we will focus on using W&B as a tool to help callibrate the hyperparameters of the TF binding prediction model to find an optimal solution. However, we encourage you to explore other applications that W&B offers.\n\n\n\nFirst install w&b in your environment with the following command, it should take only a couple of seconds\n\n%pip install wandb onnx -Uq\n%pip install nbformat\n\nLoad all libraries\n\nimport torch\nfrom torch import nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport os\nimport pandas as pd\nimport wandb\nfrom scipy.stats import pearsonr\n\n\n\n\nIf you don‚Äôt already have a w&b account, sign up here. Then, run the following command which will prompt you to insert your API key\n\nwandb.login()\n\nwandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\nwandb: Currently logged in as: ssalazar_02 to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n\n\nTrue\n\n\n\n\n\nThen, define your functions, this will remain unchanged\n\ndef get_device():\n  \"\"\"\n  Determines the device to use for PyTorch computations.\n\n  Prioritizes Metal Performance Shaders (MPS), then CUDA, then CPU.\n\n  Returns:\n    torch.device: The selected device.\n  \"\"\"\n  if torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    print(\"Using MPS device.\")\n  elif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"Using CUDA device.\")\n  else:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU device.\")\n  return device\n\n# Example usage:\ndevice = get_device()\n\nUsing MPS device.\n\n\n\ndef one_hot_encode(seq):\n    \"\"\"\n    Given a DNA sequence, return its one-hot encoding\n    \"\"\"\n    # Make sure seq has only allowed bases\n    allowed = set(\"ACTGN\")\n    if not set(seq).issubset(allowed):\n        invalid = set(seq) - allowed\n        print(seq)\n        raise ValueError(f\"Sequence contains chars not in allowed DNA alphabet (ACGTN): {invalid}\")\n\n    # Dictionary returning one-hot encoding for each nucleotide\n    nuc_d = {'A':[1.0,0.0,0.0,0.0],\n             'C':[0.0,1.0,0.0,0.0],\n             'G':[0.0,0.0,1.0,0.0],\n             'T':[0.0,0.0,0.0,1.0],\n             'N':[0.0,0.0,0.0,0.0]}\n\n    # Create array from nucleotide sequence\n    vec=np.array([nuc_d[x] for x in seq],dtype='float32')\n\n    return vec\n\n\ndef quick_split(df, split_frac=0.8, verbose=False):\n    '''\n    Given a df of samples, randomly split indices between\n    train and test at the desired fraction\n    '''\n    cols = df.columns # original columns, use to clean up reindexed cols\n    df = df.reset_index()\n\n    # shuffle indices\n    idxs = list(range(df.shape[0]))\n    random.shuffle(idxs)\n\n    # split shuffled index list by split_frac\n    split = int(len(idxs)*split_frac)\n    train_idxs = idxs[:split]\n    test_idxs = idxs[split:]\n\n    # split dfs and return\n    train_df = df[df.index.isin(train_idxs)]\n    test_df = df[df.index.isin(test_idxs)]\n\n    return train_df[cols], test_df[cols]\n\n\ndef split_sequences(sequences_df):\n    full_train_sequences, test_sequences = quick_split(sequences_df)\n    train_sequences, val_sequences = quick_split(full_train_sequences)\n    print(\"Train:\", train_sequences.shape)\n    print(\"Val:\", val_sequences.shape)\n    print(\"Test:\", test_sequences.shape)\n    return train_sequences, val_sequences, test_sequences\n\n\ndef get_data_tensors(scores_df, sequences_df):\n    # split sequences in train, validation and test sets\n    train_sequences, val_sequences, test_sequences = split_sequences(sequences_df)\n    # get scores for each set of sequences\n    train_scores = scores_df[train_sequences['window_name'].to_list()].transpose().values.astype('float32') # shape is (num_sequences, 300)\n    val_scores = scores_df[val_sequences['window_name'].to_list()].transpose().values.astype('float32')\n    test_scores = scores_df[test_sequences['window_name'].to_list()].transpose().values.astype('float32')\n\n    train_scores = torch.tensor(train_scores, dtype=torch.float32).to(device)\n    val_scores = torch.tensor(val_scores, dtype=torch.float32).to(device)\n    test_scores = torch.tensor(test_scores, dtype=torch.float32).to(device)\n\n    # get one hot encoded sequences for each set\n    train_one_hot = [one_hot_encode(seq) for seq in train_sequences['sequence'].to_list()]\n    train_sequences_tensor = torch.tensor(np.stack(train_one_hot))\n\n    val_one_hot = [one_hot_encode(seq) for seq in val_sequences['sequence'].to_list()]\n    val_sequences_tensor = torch.tensor(np.stack(val_one_hot))\n\n    test_one_hot = [one_hot_encode(seq) for seq in test_sequences['sequence'].to_list()]\n    test_sequences_tensor = torch.tensor(np.stack(test_one_hot))\n\n    return train_scores, train_sequences_tensor, val_scores, val_sequences_tensor, test_scores, test_sequences_tensor\n\n\n\ndef create_dataloader(predictors, targets, batch_size, is_train = True):\n    '''\n    features: one hot encoded sequences\n    targets: sequence scores\n    batch_size\n    is_train: if True, data is reshuffled at every epoch\n    '''\n\n    dataset = torch.utils.data.TensorDataset(predictors, targets)\n    return torch.utils.data.DataLoader(dataset, batch_size, shuffle = is_train)\n\n\nclass DNA_CNN(nn.Module):\n    def __init__(self,\n                 seq_len,\n                 num_filters=16,\n                 kernel_size=10,\n                 add_sigmoid=False):\n        super().__init__()\n        self.seq_len = seq_len\n        self.add_sigmoid = add_sigmoid\n        # Define layers individually\n        self.conv = nn.Conv1d(in_channels = 4, out_channels = num_filters, kernel_size=kernel_size)\n        self.relu = nn.ReLU(inplace=True)\n        self.linear = nn.Linear(num_filters*(seq_len-kernel_size+1), 300)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, xb):\n        # reshape view to batch_size x 4channel x seq_len\n        # permute to put channel in correct order\n        xb = xb.permute(0,2,1) # (batch_size, 300, 4) to (batch_size, 4, 300)\n\n        # Apply layers step by step\n        x = self.conv(xb)\n        x = self.relu(x)\n        x = x.flatten(1)  # flatten all dimensions except batch\n        out = self.linear(x)\n        \n        if self.add_sigmoid:\n            out = self.sigmoid(out)\n        return out\n\n\ndef process_batch(model, loss_func, x_batch, y_batch, opt=None):\n    xb_out = model(x_batch.to(torch.float32))\n\n    loss = loss_func(xb_out, y_batch)\n\n    if opt is not None: # backpropagate if train step (optimizer given)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    return loss.item(), len(x_batch)\n\n\ndef train_epoch(model, train_dl, loss_func, device, opt):\n\n    model.train()\n    tl = [] # train losses\n    ns = [] # batch sizes, n\n\n    # loop through batches\n    for x_batch, y_batch in train_dl:\n\n        x_batch, y_batch = x_batch.to(device),y_batch.to(device)\n\n        t, n = process_batch(model, loss_func, x_batch, y_batch, opt=opt)\n\n        # collect train loss and batch sizes\n        tl.append(t)\n        ns.append(n)\n\n    # average the losses over all batches\n    train_loss = np.sum(np.multiply(tl, ns)) / np.sum(ns)\n\n    return train_loss\n\n\ndef val_epoch(model, val_dl, loss_func, device):\n\n    # Set model to Evaluation mode\n    model.eval()\n    with torch.no_grad():\n        vl = [] # val losses\n        ns = [] # batch sizes, n\n\n        # loop through validation DataLoader\n        for x_batch, y_batch in val_dl:\n\n            x_batch, y_batch = x_batch.to(device),y_batch.to(device)\n\n            v, n = process_batch(model, loss_func, x_batch, y_batch)\n\n            # collect val loss and batch sizes\n            vl.append(v)\n            ns.append(n)\n\n    # average the losses over all batches\n    val_loss = np.sum(np.multiply(vl, ns)) / np.sum(ns)\n\n    return val_loss\n\n\n\n\nThe only function that we need to change is the train_loop function, because here is where we are recovering the parameters that we want to track with w&b. We will use wandb.log and create a dictionary with the parmeters that we want to track.\n\ndef train_loop(epochs, model, loss_func, opt, train_dl, val_dl, device):\n\n    # keep track of losses\n    train_losses = []\n    val_losses = []\n\n    # loop through epochs\n    for epoch in range(epochs):\n        # take a training step\n        train_loss = train_epoch(model,train_dl,loss_func,device,opt)\n        train_losses.append(train_loss)\n\n        # take a validation step\n        val_loss = val_epoch(model,val_dl,loss_func,device)\n        val_losses.append(val_loss)\n\n        print(f\"Epoch {epoch + 1} | train loss: {train_loss:.3f} | val loss: {val_loss:.3f}\")\n        wandb.log({\"epoch\": epoch + 1,\n                   \"train_loss\": train_loss,\n                   \"val_loss\": val_loss})\n\n    return train_losses, val_losses\n\nDefining the train_model function. We omit the plot_curves function since all performance metrics will be tracked on w&b.\n\ndef train_model(train_dl,val_dl,model,device, lr=0.01, epochs=50, lossf=None,opt=None):\n\n    # define optimizer\n    if opt:\n        optimizer = opt(model.parameters(), lr=lr)\n    else: # if no opt provided, just use SGD\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n\n    # define loss function\n    if lossf:\n        loss_func = lossf\n    else: # if no loss function provided, just use MSE\n        loss_func = torch.nn.MSELoss()\n\n    # run the training loop\n    train_losses, val_losses = train_loop(\n                                epochs,\n                                model,\n                                loss_func,\n                                optimizer,\n                                train_dl,\n                                val_dl,\n                                device)\n\n\n\n\n\nAs a way to evaluate each model, let‚Äôs modify the test_model() function so that wand also keeps track of the performance metrics. In this case the metrics are pearson_per_sample, test_pearson_r and best_test.\n\ndef test_model(model, test_features, test_targets):\n  model.eval()\n  predictions = model(test_features.to(torch.float32).to(device)).detach().cpu().numpy()\n  observations = test_targets.cpu().numpy()\n  pearson_per_sample = np.array([pearsonr(predictions[i], observations[i])[0] for i in range(300)])\n  test_pearsonr = pearson_per_sample.mean()\n  best_test = pearson_per_sample.max()\n  wandb.log({'test_avg_pearsonr': test_pearsonr,\n             'beast_pearsonr': best_test})\n\n\n\n\nA sweep is the training and testing of a single model with a given configuration of hyperparameters. with wandb.sweep we define the set of hyperparameters to test, which will be then combined in different configurations each sweep.\n\nsweep_config = {\n    'method': 'random',\n    'metric': {'name': 'test_avg_pearsonr', 'goal': 'maximize'},\n    'parameters': {\n        'num_filters': {'values': [4, 16]},\n        'kernel_size': {'values': [5, 10]},\n        'add_sigmoid': {'values': [True, False],},\n        'learning_rate':{'values':[0.1, 0.05]},\n        'batch_size': {'values':[16, 32, 64]},\n        'optimizer': {'values': ['SGD','Adam']}\n    }\n}\n\nCreate a project ID for your model, all your tests will be saved in this project\n\nsweep_id = wandb.sweep(sweep_config, project=\"DNA_model\")\n\nCreate sweep with ID: yfwcc62j\nSweep URL: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n\n\n\n\nDIR = '/Users/sofiasalazar/Library/CloudStorage/Box-Box/imlab-data/Courses/AI-in-Genomics-2025/data/'\nsequences = pd.read_csv(os.path.join(DIR, 'chr22_sequences.txt.gz'), sep=\"\\t\", compression='gzip')\nscores = pd.read_csv(os.path.join(DIR, 'chr22_scores.txt.gz'), sep=\"\\t\", compression='gzip',dtype='float32')\n\n\ntrain_scores, train_sequences_tensor, val_scores, val_sequences_tensor, test_scores, test_sequences_tensor = get_data_tensors(scores, sequences)\n\nTrain: (14808, 2)\nVal: (3703, 2)\nTest: (4628, 2)\n\n\n\n\n\nWith wandb.init we initialize one sweep and what we want to do in each. This consists on\n\nLoading a configuration of hyperparameters with wandb.config\nLoading the model\nTelling wandb to track the training with wandb.watch\nCreate the dataloaders\nTrain and test the model\n\n\ndef train_sweep():\n    with wandb.init(project = \"DNA_model\"):\n        config = wandb.config\n        model = DNA_CNN(seq_len=300, num_filters=config.num_filters, kernel_size=config.kernel_size, add_sigmoid=config.add_sigmoid).to(device)\n        wandb.watch(model, log=\"all\", log_freq=10) # log all: logs all gradients and parameters, every log_freq number of training steps (batches) \n        train_loader = create_dataloader(train_sequences_tensor, train_scores, batch_size=config.batch_size)\n        val_loader = create_dataloader(val_sequences_tensor, val_scores, batch_size=config.batch_size, is_train=False)\n        if config.optimizer == 'SGD':\n            opt = torch.optim.SGD\n        else: opt = torch.optim.Adam \n\n        train_model(train_loader, val_loader, model, device, epochs=30, lr = config.learning_rate, opt=opt)\n        test_model(model, test_sequences_tensor, test_scores)\n\nFinally, we train with wandb.agent, the argument count is the number of combinations of hyperparameters I want to try. The maximum in my case is 240 combinations\n\n# wandb.agent(sweep_id, train_sweep, count=240)\nwandb.agent(sweep_id, train_sweep, count=6)\n\nwandb: Agent Starting Run: 0un6by39 with config:\nwandb:  add_sigmoid: True\nwandb:  batch_size: 16\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.1\nwandb:  num_filters: 4\nwandb:  optimizer: SGD\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151021-0un6by39\n\n\nSyncing run stellar-sweep-1 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/0un6by39\n\n\nEpoch 1 | train loss: 2.423 | val loss: 2.452\nEpoch 2 | train loss: 2.417 | val loss: 2.447\nEpoch 3 | train loss: 2.402 | val loss: 2.417\nEpoch 4 | train loss: 2.352 | val loss: 2.363\nEpoch 5 | train loss: 2.314 | val loss: 2.344\nEpoch 6 | train loss: 2.297 | val loss: 2.334\nEpoch 7 | train loss: 2.287 | val loss: 2.328\nEpoch 8 | train loss: 2.279 | val loss: 2.324\nEpoch 9 | train loss: 2.273 | val loss: 2.321\nEpoch 10 | train loss: 2.267 | val loss: 2.318\nEpoch 11 | train loss: 2.262 | val loss: 2.317\nEpoch 12 | train loss: 2.257 | val loss: 2.313\nEpoch 13 | train loss: 2.252 | val loss: 2.311\nEpoch 14 | train loss: 2.246 | val loss: 2.311\nEpoch 15 | train loss: 2.240 | val loss: 2.308\nEpoch 16 | train loss: 2.235 | val loss: 2.305\nEpoch 17 | train loss: 2.229 | val loss: 2.303\nEpoch 18 | train loss: 2.223 | val loss: 2.301\nEpoch 19 | train loss: 2.217 | val loss: 2.301\nEpoch 20 | train loss: 2.211 | val loss: 2.299\nEpoch 21 | train loss: 2.205 | val loss: 2.298\nEpoch 22 | train loss: 2.199 | val loss: 2.296\nEpoch 23 | train loss: 2.193 | val loss: 2.295\nEpoch 24 | train loss: 2.187 | val loss: 2.295\nEpoch 25 | train loss: 2.181 | val loss: 2.294\nEpoch 26 | train loss: 2.176 | val loss: 2.294\nEpoch 27 | train loss: 2.170 | val loss: 2.293\nEpoch 28 | train loss: 2.165 | val loss: 2.293\nEpoch 29 | train loss: 2.160 | val loss: 2.292\nEpoch 30 | train loss: 2.155 | val loss: 2.292\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n‚ñÅ\n\n\nepoch\n‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n\n\ntest_avg_pearsonr\n‚ñÅ\n\n\ntrain_loss\n‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nval_loss\n‚ñà‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\n\n\nbeast_pearsonr\n0.68177\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n0.23191\n\n\ntrain_loss\n2.15465\n\n\nval_loss\n2.29175\n\n\n\n\n\n\n View run stellar-sweep-1 at: https://wandb.ai/ssalazar_02/DNA_model/runs/0un6by39 View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_151021-0un6by39/logs\n\n\nwandb: Agent Starting Run: bt4sqsaj with config:\nwandb:  add_sigmoid: False\nwandb:  batch_size: 16\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.1\nwandb:  num_filters: 4\nwandb:  optimizer: SGD\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151218-bt4sqsaj\n\n\nSyncing run vivid-sweep-2 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/bt4sqsaj\n\n\nEpoch 1 | train loss: 2.387 | val loss: 2.354\nEpoch 2 | train loss: 2.291 | val loss: 2.311\nEpoch 3 | train loss: 2.259 | val loss: 2.314\nEpoch 4 | train loss: 2.244 | val loss: 2.289\nEpoch 5 | train loss: 2.229 | val loss: 2.259\nEpoch 6 | train loss: 2.128 | val loss: 2.137\nEpoch 7 | train loss: 2.045 | val loss: 2.091\nEpoch 8 | train loss: 1.995 | val loss: 2.044\nEpoch 9 | train loss: 1.934 | val loss: 1.987\nEpoch 10 | train loss: 1.873 | val loss: 1.942\nEpoch 11 | train loss: 1.815 | val loss: 1.886\nEpoch 12 | train loss: 1.757 | val loss: 1.846\nEpoch 13 | train loss: 1.708 | val loss: 1.807\nEpoch 14 | train loss: 1.666 | val loss: 1.774\nEpoch 15 | train loss: 1.632 | val loss: 1.746\nEpoch 16 | train loss: 1.602 | val loss: 1.724\nEpoch 17 | train loss: 1.576 | val loss: 1.702\nEpoch 18 | train loss: 1.552 | val loss: 1.684\nEpoch 19 | train loss: 1.530 | val loss: 1.667\nEpoch 20 | train loss: 1.509 | val loss: 1.650\nEpoch 21 | train loss: 1.489 | val loss: 1.637\nEpoch 22 | train loss: 1.473 | val loss: 1.635\nEpoch 23 | train loss: 1.458 | val loss: 1.622\nEpoch 24 | train loss: 1.446 | val loss: 1.608\nEpoch 25 | train loss: 1.435 | val loss: 1.603\nEpoch 26 | train loss: 1.425 | val loss: 1.599\nEpoch 27 | train loss: 1.416 | val loss: 1.593\nEpoch 28 | train loss: 1.407 | val loss: 1.586\nEpoch 29 | train loss: 1.397 | val loss: 1.584\nEpoch 30 | train loss: 1.388 | val loss: 1.580\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n‚ñÅ\n\n\nepoch\n‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n\n\ntest_avg_pearsonr\n‚ñÅ\n\n\ntrain_loss\n‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nval_loss\n‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\n\n\nbeast_pearsonr\n0.92833\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n0.57389\n\n\ntrain_loss\n1.38804\n\n\nval_loss\n1.57982\n\n\n\n\n\n\n View run vivid-sweep-2 at: https://wandb.ai/ssalazar_02/DNA_model/runs/bt4sqsaj View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_151218-bt4sqsaj/logs\n\n\nwandb: Agent Starting Run: 7ca57ikz with config:\nwandb:  add_sigmoid: True\nwandb:  batch_size: 16\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.05\nwandb:  num_filters: 32\nwandb:  optimizer: SGD\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151414-7ca57ikz\n\n\nSyncing run unique-sweep-3 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/7ca57ikz\n\n\nEpoch 1 | train loss: 2.423 | val loss: 2.451\nEpoch 2 | train loss: 2.415 | val loss: 2.447\nEpoch 3 | train loss: 2.408 | val loss: 2.436\nEpoch 4 | train loss: 2.385 | val loss: 2.404\nEpoch 5 | train loss: 2.345 | val loss: 2.370\nEpoch 6 | train loss: 2.316 | val loss: 2.350\nEpoch 7 | train loss: 2.300 | val loss: 2.340\nEpoch 8 | train loss: 2.289 | val loss: 2.336\nEpoch 9 | train loss: 2.281 | val loss: 2.328\nEpoch 10 | train loss: 2.274 | val loss: 2.325\nEpoch 11 | train loss: 2.268 | val loss: 2.323\nEpoch 12 | train loss: 2.262 | val loss: 2.321\nEpoch 13 | train loss: 2.257 | val loss: 2.318\nEpoch 14 | train loss: 2.251 | val loss: 2.318\nEpoch 15 | train loss: 2.247 | val loss: 2.316\nEpoch 16 | train loss: 2.242 | val loss: 2.316\nEpoch 17 | train loss: 2.237 | val loss: 2.315\nEpoch 18 | train loss: 2.232 | val loss: 2.315\nEpoch 19 | train loss: 2.227 | val loss: 2.313\nEpoch 20 | train loss: 2.222 | val loss: 2.314\nEpoch 21 | train loss: 2.217 | val loss: 2.314\nEpoch 22 | train loss: 2.212 | val loss: 2.314\nEpoch 23 | train loss: 2.206 | val loss: 2.313\nEpoch 24 | train loss: 2.201 | val loss: 2.314\nEpoch 25 | train loss: 2.196 | val loss: 2.315\nEpoch 26 | train loss: 2.191 | val loss: 2.315\nEpoch 27 | train loss: 2.186 | val loss: 2.315\nEpoch 28 | train loss: 2.181 | val loss: 2.315\nEpoch 29 | train loss: 2.176 | val loss: 2.315\nEpoch 30 | train loss: 2.171 | val loss: 2.316\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n‚ñÅ\n\n\nepoch\n‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n\n\ntest_avg_pearsonr\n‚ñÅ\n\n\ntrain_loss\n‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nval_loss\n‚ñà‚ñà‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\n\n\nbeast_pearsonr\n0.6957\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n0.21384\n\n\ntrain_loss\n2.17106\n\n\nval_loss\n2.31633\n\n\n\n\n\n\n View run unique-sweep-3 at: https://wandb.ai/ssalazar_02/DNA_model/runs/7ca57ikz View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_151414-7ca57ikz/logs\n\n\nwandb: Agent Starting Run: 2pk9jgsh with config:\nwandb:  add_sigmoid: False\nwandb:  batch_size: 32\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.1\nwandb:  num_filters: 32\nwandb:  optimizer: Adam\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151710-2pk9jgsh\n\n\nSyncing run breezy-sweep-4 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/2pk9jgsh\n\n\nEpoch 1 | train loss: 8.430 | val loss: 2.453\nEpoch 2 | train loss: 2.422 | val loss: 2.457\nEpoch 3 | train loss: 2.423 | val loss: 2.455\nEpoch 4 | train loss: 2.424 | val loss: 2.457\nEpoch 5 | train loss: 2.425 | val loss: 2.457\nEpoch 6 | train loss: 2.425 | val loss: 2.459\nEpoch 7 | train loss: 2.427 | val loss: 2.464\nEpoch 8 | train loss: 2.428 | val loss: 2.463\nEpoch 9 | train loss: 2.429 | val loss: 2.461\nEpoch 10 | train loss: 2.430 | val loss: 2.463\nEpoch 11 | train loss: 2.429 | val loss: 2.459\nEpoch 12 | train loss: 2.430 | val loss: 2.465\nEpoch 13 | train loss: 2.430 | val loss: 2.461\nEpoch 14 | train loss: 2.431 | val loss: 2.458\nEpoch 15 | train loss: 2.430 | val loss: 2.462\nEpoch 16 | train loss: 2.431 | val loss: 2.464\nEpoch 17 | train loss: 2.431 | val loss: 2.463\nEpoch 18 | train loss: 2.430 | val loss: 2.461\nEpoch 19 | train loss: 2.431 | val loss: 2.458\nEpoch 20 | train loss: 2.430 | val loss: 2.466\nEpoch 21 | train loss: 2.431 | val loss: 2.466\nEpoch 22 | train loss: 2.431 | val loss: 2.463\nEpoch 23 | train loss: 2.431 | val loss: 2.463\nEpoch 24 | train loss: 2.430 | val loss: 2.470\nEpoch 25 | train loss: 2.431 | val loss: 2.460\nEpoch 26 | train loss: 2.431 | val loss: 2.459\nEpoch 27 | train loss: 2.431 | val loss: 2.463\nEpoch 28 | train loss: 2.431 | val loss: 2.462\nEpoch 29 | train loss: 2.431 | val loss: 2.462\nEpoch 30 | train loss: 2.431 | val loss: 2.461\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n‚ñÅ\n\n\nepoch\n‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n\n\ntest_avg_pearsonr\n‚ñÅ\n\n\ntrain_loss\n‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nval_loss\n‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÑ\n\n\n\n\n\n\n\nbeast_pearsonr\n0.35183\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n-0.00231\n\n\ntrain_loss\n2.43134\n\n\nval_loss\n2.46144\n\n\n\n\n\n\n View run breezy-sweep-4 at: https://wandb.ai/ssalazar_02/DNA_model/runs/2pk9jgsh View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_151710-2pk9jgsh/logs\n\n\nwandb: Agent Starting Run: ohia2jz9 with config:\nwandb:  add_sigmoid: True\nwandb:  batch_size: 32\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.05\nwandb:  num_filters: 32\nwandb:  optimizer: Adam\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151916-ohia2jz9\n\n\nSyncing run vibrant-sweep-5 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/ohia2jz9\n\n\nEpoch 1 | train loss: 2.520 | val loss: 2.554\nEpoch 2 | train loss: 2.520 | val loss: 2.556\nEpoch 3 | train loss: 2.520 | val loss: 2.556\nEpoch 4 | train loss: 2.520 | val loss: 2.556\nEpoch 5 | train loss: 2.520 | val loss: 2.556\nEpoch 6 | train loss: 2.520 | val loss: 2.556\nEpoch 7 | train loss: 2.520 | val loss: 2.556\nEpoch 8 | train loss: 2.520 | val loss: 2.556\nEpoch 9 | train loss: 2.520 | val loss: 2.556\nEpoch 10 | train loss: 2.520 | val loss: 2.556\nEpoch 11 | train loss: 2.520 | val loss: 2.556\nEpoch 12 | train loss: 2.520 | val loss: 2.556\nEpoch 13 | train loss: 2.520 | val loss: 2.556\nEpoch 14 | train loss: 2.520 | val loss: 2.556\nEpoch 15 | train loss: 2.520 | val loss: 2.556\nEpoch 16 | train loss: 2.520 | val loss: 2.556\nEpoch 17 | train loss: 2.520 | val loss: 2.556\nEpoch 18 | train loss: 2.520 | val loss: 2.556\nEpoch 19 | train loss: 2.520 | val loss: 2.556\nEpoch 20 | train loss: 2.520 | val loss: 2.556\nEpoch 21 | train loss: 2.520 | val loss: 2.556\nEpoch 22 | train loss: 2.520 | val loss: 2.556\nEpoch 23 | train loss: 2.520 | val loss: 2.556\nEpoch 24 | train loss: 2.520 | val loss: 2.556\nEpoch 25 | train loss: 2.520 | val loss: 2.556\nEpoch 26 | train loss: 2.520 | val loss: 2.556\nEpoch 27 | train loss: 2.520 | val loss: 2.556\nEpoch 28 | train loss: 2.520 | val loss: 2.556\nEpoch 29 | train loss: 2.520 | val loss: 2.556\nEpoch 30 | train loss: 2.520 | val loss: 2.556\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n‚ñÅ\n\n\nepoch\n‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n\n\ntest_avg_pearsonr\n‚ñÅ\n\n\ntrain_loss\n‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\n\nval_loss\n‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\n\n\n\n\n\n\nbeast_pearsonr\n0.20144\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n-0.00629\n\n\ntrain_loss\n2.52015\n\n\nval_loss\n2.5559\n\n\n\n\n\n\n View run vibrant-sweep-5 at: https://wandb.ai/ssalazar_02/DNA_model/runs/ohia2jz9 View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_151916-ohia2jz9/logs\n\n\nwandb: Agent Starting Run: gmg0gdbw with config:\nwandb:  add_sigmoid: False\nwandb:  batch_size: 64\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.1\nwandb:  num_filters: 16\nwandb:  optimizer: SGD\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_152128-gmg0gdbw\n\n\nSyncing run upbeat-sweep-6 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/gmg0gdbw\n\n\nEpoch 1 | train loss: 2.424 | val loss: 2.440\nEpoch 2 | train loss: 2.370 | val loss: 2.371\nEpoch 3 | train loss: 2.307 | val loss: 2.336\nEpoch 4 | train loss: 2.278 | val loss: 2.320\nEpoch 5 | train loss: 2.260 | val loss: 2.312\nEpoch 6 | train loss: 2.245 | val loss: 2.304\nEpoch 7 | train loss: 2.231 | val loss: 2.297\nEpoch 8 | train loss: 2.216 | val loss: 2.294\nEpoch 9 | train loss: 2.202 | val loss: 2.290\nEpoch 10 | train loss: 2.185 | val loss: 2.285\nEpoch 11 | train loss: 2.168 | val loss: 2.281\nEpoch 12 | train loss: 2.148 | val loss: 2.271\nEpoch 13 | train loss: 2.119 | val loss: 2.255\nEpoch 14 | train loss: 2.080 | val loss: 2.228\nEpoch 15 | train loss: 2.039 | val loss: 2.208\nEpoch 16 | train loss: 2.001 | val loss: 2.185\nEpoch 17 | train loss: 1.966 | val loss: 2.169\nEpoch 18 | train loss: 1.933 | val loss: 2.157\nEpoch 19 | train loss: 1.903 | val loss: 2.140\nEpoch 20 | train loss: 1.875 | val loss: 2.126\nEpoch 21 | train loss: 1.848 | val loss: 2.113\nEpoch 22 | train loss: 1.822 | val loss: 2.107\nEpoch 23 | train loss: 1.799 | val loss: 2.098\nEpoch 24 | train loss: 1.776 | val loss: 2.092\nEpoch 25 | train loss: 1.755 | val loss: 2.086\nEpoch 26 | train loss: 1.736 | val loss: 2.081\nEpoch 27 | train loss: 1.717 | val loss: 2.075\nEpoch 28 | train loss: 1.699 | val loss: 2.075\nEpoch 29 | train loss: 1.682 | val loss: 2.069\nEpoch 30 | train loss: 1.666 | val loss: 2.068\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n‚ñÅ\n\n\nepoch\n‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n\n\ntest_avg_pearsonr\n‚ñÅ\n\n\ntrain_loss\n‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nval_loss\n‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\n\n\nbeast_pearsonr\n0.80236\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n0.38552\n\n\ntrain_loss\n1.66555\n\n\nval_loss\n2.06831\n\n\n\n\n\n\n View run upbeat-sweep-6 at: https://wandb.ai/ssalazar_02/DNA_model/runs/gmg0gdbw View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_152128-gmg0gdbw/logs\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-wandb.html#calibrating-hyperparameters-with-weights-and-biases",
    "href": "post/2025-03-25-unit00/tf-binding-wandb.html#calibrating-hyperparameters-with-weights-and-biases",
    "title": "Calibrating hyperparameters with weights and biases",
    "section": "",
    "text": "Goal: Learn how to use weights and biases to calibrate the hyperparameters of a DL model.\n\nWeights and biases is a platform used for AI developers to track, visualize and manage their ML models and experiments. The coolest part is that W&B allows you to log various performance metrics during training, like training and validation loss, test set correlations, etc. Additionally, it allows you to compare between different experiments or versions of your models. Making it easier to identify the best performing models and see which hyperparameter configuration is the optimal.\nIn this notebook, we will focus on using W&B as a tool to help callibrate the hyperparameters of the TF binding prediction model to find an optimal solution. However, we encourage you to explore other applications that W&B offers.\n\n\n\nFirst install w&b in your environment with the following command, it should take only a couple of seconds\n\n%pip install wandb onnx -Uq\n%pip install nbformat\n\nLoad all libraries\n\nimport torch\nfrom torch import nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport os\nimport pandas as pd\nimport wandb\nfrom scipy.stats import pearsonr\n\n\n\n\nIf you don‚Äôt already have a w&b account, sign up here. Then, run the following command which will prompt you to insert your API key\n\nwandb.login()\n\nwandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\nwandb: Currently logged in as: ssalazar_02 to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n\n\nTrue\n\n\n\n\n\nThen, define your functions, this will remain unchanged\n\ndef get_device():\n  \"\"\"\n  Determines the device to use for PyTorch computations.\n\n  Prioritizes Metal Performance Shaders (MPS), then CUDA, then CPU.\n\n  Returns:\n    torch.device: The selected device.\n  \"\"\"\n  if torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    print(\"Using MPS device.\")\n  elif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"Using CUDA device.\")\n  else:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU device.\")\n  return device\n\n# Example usage:\ndevice = get_device()\n\nUsing MPS device.\n\n\n\ndef one_hot_encode(seq):\n    \"\"\"\n    Given a DNA sequence, return its one-hot encoding\n    \"\"\"\n    # Make sure seq has only allowed bases\n    allowed = set(\"ACTGN\")\n    if not set(seq).issubset(allowed):\n        invalid = set(seq) - allowed\n        print(seq)\n        raise ValueError(f\"Sequence contains chars not in allowed DNA alphabet (ACGTN): {invalid}\")\n\n    # Dictionary returning one-hot encoding for each nucleotide\n    nuc_d = {'A':[1.0,0.0,0.0,0.0],\n             'C':[0.0,1.0,0.0,0.0],\n             'G':[0.0,0.0,1.0,0.0],\n             'T':[0.0,0.0,0.0,1.0],\n             'N':[0.0,0.0,0.0,0.0]}\n\n    # Create array from nucleotide sequence\n    vec=np.array([nuc_d[x] for x in seq],dtype='float32')\n\n    return vec\n\n\ndef quick_split(df, split_frac=0.8, verbose=False):\n    '''\n    Given a df of samples, randomly split indices between\n    train and test at the desired fraction\n    '''\n    cols = df.columns # original columns, use to clean up reindexed cols\n    df = df.reset_index()\n\n    # shuffle indices\n    idxs = list(range(df.shape[0]))\n    random.shuffle(idxs)\n\n    # split shuffled index list by split_frac\n    split = int(len(idxs)*split_frac)\n    train_idxs = idxs[:split]\n    test_idxs = idxs[split:]\n\n    # split dfs and return\n    train_df = df[df.index.isin(train_idxs)]\n    test_df = df[df.index.isin(test_idxs)]\n\n    return train_df[cols], test_df[cols]\n\n\ndef split_sequences(sequences_df):\n    full_train_sequences, test_sequences = quick_split(sequences_df)\n    train_sequences, val_sequences = quick_split(full_train_sequences)\n    print(\"Train:\", train_sequences.shape)\n    print(\"Val:\", val_sequences.shape)\n    print(\"Test:\", test_sequences.shape)\n    return train_sequences, val_sequences, test_sequences\n\n\ndef get_data_tensors(scores_df, sequences_df):\n    # split sequences in train, validation and test sets\n    train_sequences, val_sequences, test_sequences = split_sequences(sequences_df)\n    # get scores for each set of sequences\n    train_scores = scores_df[train_sequences['window_name'].to_list()].transpose().values.astype('float32') # shape is (num_sequences, 300)\n    val_scores = scores_df[val_sequences['window_name'].to_list()].transpose().values.astype('float32')\n    test_scores = scores_df[test_sequences['window_name'].to_list()].transpose().values.astype('float32')\n\n    train_scores = torch.tensor(train_scores, dtype=torch.float32).to(device)\n    val_scores = torch.tensor(val_scores, dtype=torch.float32).to(device)\n    test_scores = torch.tensor(test_scores, dtype=torch.float32).to(device)\n\n    # get one hot encoded sequences for each set\n    train_one_hot = [one_hot_encode(seq) for seq in train_sequences['sequence'].to_list()]\n    train_sequences_tensor = torch.tensor(np.stack(train_one_hot))\n\n    val_one_hot = [one_hot_encode(seq) for seq in val_sequences['sequence'].to_list()]\n    val_sequences_tensor = torch.tensor(np.stack(val_one_hot))\n\n    test_one_hot = [one_hot_encode(seq) for seq in test_sequences['sequence'].to_list()]\n    test_sequences_tensor = torch.tensor(np.stack(test_one_hot))\n\n    return train_scores, train_sequences_tensor, val_scores, val_sequences_tensor, test_scores, test_sequences_tensor\n\n\n\ndef create_dataloader(predictors, targets, batch_size, is_train = True):\n    '''\n    features: one hot encoded sequences\n    targets: sequence scores\n    batch_size\n    is_train: if True, data is reshuffled at every epoch\n    '''\n\n    dataset = torch.utils.data.TensorDataset(predictors, targets)\n    return torch.utils.data.DataLoader(dataset, batch_size, shuffle = is_train)\n\n\nclass DNA_CNN(nn.Module):\n    def __init__(self,\n                 seq_len,\n                 num_filters=16,\n                 kernel_size=10,\n                 add_sigmoid=False):\n        super().__init__()\n        self.seq_len = seq_len\n        self.add_sigmoid = add_sigmoid\n        # Define layers individually\n        self.conv = nn.Conv1d(in_channels = 4, out_channels = num_filters, kernel_size=kernel_size)\n        self.relu = nn.ReLU(inplace=True)\n        self.linear = nn.Linear(num_filters*(seq_len-kernel_size+1), 300)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, xb):\n        # reshape view to batch_size x 4channel x seq_len\n        # permute to put channel in correct order\n        xb = xb.permute(0,2,1) # (batch_size, 300, 4) to (batch_size, 4, 300)\n\n        # Apply layers step by step\n        x = self.conv(xb)\n        x = self.relu(x)\n        x = x.flatten(1)  # flatten all dimensions except batch\n        out = self.linear(x)\n        \n        if self.add_sigmoid:\n            out = self.sigmoid(out)\n        return out\n\n\ndef process_batch(model, loss_func, x_batch, y_batch, opt=None):\n    xb_out = model(x_batch.to(torch.float32))\n\n    loss = loss_func(xb_out, y_batch)\n\n    if opt is not None: # backpropagate if train step (optimizer given)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    return loss.item(), len(x_batch)\n\n\ndef train_epoch(model, train_dl, loss_func, device, opt):\n\n    model.train()\n    tl = [] # train losses\n    ns = [] # batch sizes, n\n\n    # loop through batches\n    for x_batch, y_batch in train_dl:\n\n        x_batch, y_batch = x_batch.to(device),y_batch.to(device)\n\n        t, n = process_batch(model, loss_func, x_batch, y_batch, opt=opt)\n\n        # collect train loss and batch sizes\n        tl.append(t)\n        ns.append(n)\n\n    # average the losses over all batches\n    train_loss = np.sum(np.multiply(tl, ns)) / np.sum(ns)\n\n    return train_loss\n\n\ndef val_epoch(model, val_dl, loss_func, device):\n\n    # Set model to Evaluation mode\n    model.eval()\n    with torch.no_grad():\n        vl = [] # val losses\n        ns = [] # batch sizes, n\n\n        # loop through validation DataLoader\n        for x_batch, y_batch in val_dl:\n\n            x_batch, y_batch = x_batch.to(device),y_batch.to(device)\n\n            v, n = process_batch(model, loss_func, x_batch, y_batch)\n\n            # collect val loss and batch sizes\n            vl.append(v)\n            ns.append(n)\n\n    # average the losses over all batches\n    val_loss = np.sum(np.multiply(vl, ns)) / np.sum(ns)\n\n    return val_loss\n\n\n\n\nThe only function that we need to change is the train_loop function, because here is where we are recovering the parameters that we want to track with w&b. We will use wandb.log and create a dictionary with the parmeters that we want to track.\n\ndef train_loop(epochs, model, loss_func, opt, train_dl, val_dl, device):\n\n    # keep track of losses\n    train_losses = []\n    val_losses = []\n\n    # loop through epochs\n    for epoch in range(epochs):\n        # take a training step\n        train_loss = train_epoch(model,train_dl,loss_func,device,opt)\n        train_losses.append(train_loss)\n\n        # take a validation step\n        val_loss = val_epoch(model,val_dl,loss_func,device)\n        val_losses.append(val_loss)\n\n        print(f\"Epoch {epoch + 1} | train loss: {train_loss:.3f} | val loss: {val_loss:.3f}\")\n        wandb.log({\"epoch\": epoch + 1,\n                   \"train_loss\": train_loss,\n                   \"val_loss\": val_loss})\n\n    return train_losses, val_losses\n\nDefining the train_model function. We omit the plot_curves function since all performance metrics will be tracked on w&b.\n\ndef train_model(train_dl,val_dl,model,device, lr=0.01, epochs=50, lossf=None,opt=None):\n\n    # define optimizer\n    if opt:\n        optimizer = opt(model.parameters(), lr=lr)\n    else: # if no opt provided, just use SGD\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n\n    # define loss function\n    if lossf:\n        loss_func = lossf\n    else: # if no loss function provided, just use MSE\n        loss_func = torch.nn.MSELoss()\n\n    # run the training loop\n    train_losses, val_losses = train_loop(\n                                epochs,\n                                model,\n                                loss_func,\n                                optimizer,\n                                train_dl,\n                                val_dl,\n                                device)\n\n\n\n\n\nAs a way to evaluate each model, let‚Äôs modify the test_model() function so that wand also keeps track of the performance metrics. In this case the metrics are pearson_per_sample, test_pearson_r and best_test.\n\ndef test_model(model, test_features, test_targets):\n  model.eval()\n  predictions = model(test_features.to(torch.float32).to(device)).detach().cpu().numpy()\n  observations = test_targets.cpu().numpy()\n  pearson_per_sample = np.array([pearsonr(predictions[i], observations[i])[0] for i in range(300)])\n  test_pearsonr = pearson_per_sample.mean()\n  best_test = pearson_per_sample.max()\n  wandb.log({'test_avg_pearsonr': test_pearsonr,\n             'beast_pearsonr': best_test})\n\n\n\n\nA sweep is the training and testing of a single model with a given configuration of hyperparameters. with wandb.sweep we define the set of hyperparameters to test, which will be then combined in different configurations each sweep.\n\nsweep_config = {\n    'method': 'random',\n    'metric': {'name': 'test_avg_pearsonr', 'goal': 'maximize'},\n    'parameters': {\n        'num_filters': {'values': [4, 16]},\n        'kernel_size': {'values': [5, 10]},\n        'add_sigmoid': {'values': [True, False],},\n        'learning_rate':{'values':[0.1, 0.05]},\n        'batch_size': {'values':[16, 32, 64]},\n        'optimizer': {'values': ['SGD','Adam']}\n    }\n}\n\nCreate a project ID for your model, all your tests will be saved in this project\n\nsweep_id = wandb.sweep(sweep_config, project=\"DNA_model\")\n\nCreate sweep with ID: yfwcc62j\nSweep URL: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n\n\n\n\nDIR = '/Users/sofiasalazar/Library/CloudStorage/Box-Box/imlab-data/Courses/AI-in-Genomics-2025/data/'\nsequences = pd.read_csv(os.path.join(DIR, 'chr22_sequences.txt.gz'), sep=\"\\t\", compression='gzip')\nscores = pd.read_csv(os.path.join(DIR, 'chr22_scores.txt.gz'), sep=\"\\t\", compression='gzip',dtype='float32')\n\n\ntrain_scores, train_sequences_tensor, val_scores, val_sequences_tensor, test_scores, test_sequences_tensor = get_data_tensors(scores, sequences)\n\nTrain: (14808, 2)\nVal: (3703, 2)\nTest: (4628, 2)\n\n\n\n\n\nWith wandb.init we initialize one sweep and what we want to do in each. This consists on\n\nLoading a configuration of hyperparameters with wandb.config\nLoading the model\nTelling wandb to track the training with wandb.watch\nCreate the dataloaders\nTrain and test the model\n\n\ndef train_sweep():\n    with wandb.init(project = \"DNA_model\"):\n        config = wandb.config\n        model = DNA_CNN(seq_len=300, num_filters=config.num_filters, kernel_size=config.kernel_size, add_sigmoid=config.add_sigmoid).to(device)\n        wandb.watch(model, log=\"all\", log_freq=10) # log all: logs all gradients and parameters, every log_freq number of training steps (batches) \n        train_loader = create_dataloader(train_sequences_tensor, train_scores, batch_size=config.batch_size)\n        val_loader = create_dataloader(val_sequences_tensor, val_scores, batch_size=config.batch_size, is_train=False)\n        if config.optimizer == 'SGD':\n            opt = torch.optim.SGD\n        else: opt = torch.optim.Adam \n\n        train_model(train_loader, val_loader, model, device, epochs=30, lr = config.learning_rate, opt=opt)\n        test_model(model, test_sequences_tensor, test_scores)\n\nFinally, we train with wandb.agent, the argument count is the number of combinations of hyperparameters I want to try. The maximum in my case is 240 combinations\n\n# wandb.agent(sweep_id, train_sweep, count=240)\nwandb.agent(sweep_id, train_sweep, count=6)\n\nwandb: Agent Starting Run: 0un6by39 with config:\nwandb:  add_sigmoid: True\nwandb:  batch_size: 16\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.1\nwandb:  num_filters: 4\nwandb:  optimizer: SGD\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151021-0un6by39\n\n\nSyncing run stellar-sweep-1 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/0un6by39\n\n\nEpoch 1 | train loss: 2.423 | val loss: 2.452\nEpoch 2 | train loss: 2.417 | val loss: 2.447\nEpoch 3 | train loss: 2.402 | val loss: 2.417\nEpoch 4 | train loss: 2.352 | val loss: 2.363\nEpoch 5 | train loss: 2.314 | val loss: 2.344\nEpoch 6 | train loss: 2.297 | val loss: 2.334\nEpoch 7 | train loss: 2.287 | val loss: 2.328\nEpoch 8 | train loss: 2.279 | val loss: 2.324\nEpoch 9 | train loss: 2.273 | val loss: 2.321\nEpoch 10 | train loss: 2.267 | val loss: 2.318\nEpoch 11 | train loss: 2.262 | val loss: 2.317\nEpoch 12 | train loss: 2.257 | val loss: 2.313\nEpoch 13 | train loss: 2.252 | val loss: 2.311\nEpoch 14 | train loss: 2.246 | val loss: 2.311\nEpoch 15 | train loss: 2.240 | val loss: 2.308\nEpoch 16 | train loss: 2.235 | val loss: 2.305\nEpoch 17 | train loss: 2.229 | val loss: 2.303\nEpoch 18 | train loss: 2.223 | val loss: 2.301\nEpoch 19 | train loss: 2.217 | val loss: 2.301\nEpoch 20 | train loss: 2.211 | val loss: 2.299\nEpoch 21 | train loss: 2.205 | val loss: 2.298\nEpoch 22 | train loss: 2.199 | val loss: 2.296\nEpoch 23 | train loss: 2.193 | val loss: 2.295\nEpoch 24 | train loss: 2.187 | val loss: 2.295\nEpoch 25 | train loss: 2.181 | val loss: 2.294\nEpoch 26 | train loss: 2.176 | val loss: 2.294\nEpoch 27 | train loss: 2.170 | val loss: 2.293\nEpoch 28 | train loss: 2.165 | val loss: 2.293\nEpoch 29 | train loss: 2.160 | val loss: 2.292\nEpoch 30 | train loss: 2.155 | val loss: 2.292\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n‚ñÅ\n\n\nepoch\n‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n\n\ntest_avg_pearsonr\n‚ñÅ\n\n\ntrain_loss\n‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nval_loss\n‚ñà‚ñà‚ñÜ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\n\n\nbeast_pearsonr\n0.68177\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n0.23191\n\n\ntrain_loss\n2.15465\n\n\nval_loss\n2.29175\n\n\n\n\n\n\n View run stellar-sweep-1 at: https://wandb.ai/ssalazar_02/DNA_model/runs/0un6by39 View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_151021-0un6by39/logs\n\n\nwandb: Agent Starting Run: bt4sqsaj with config:\nwandb:  add_sigmoid: False\nwandb:  batch_size: 16\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.1\nwandb:  num_filters: 4\nwandb:  optimizer: SGD\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151218-bt4sqsaj\n\n\nSyncing run vivid-sweep-2 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/bt4sqsaj\n\n\nEpoch 1 | train loss: 2.387 | val loss: 2.354\nEpoch 2 | train loss: 2.291 | val loss: 2.311\nEpoch 3 | train loss: 2.259 | val loss: 2.314\nEpoch 4 | train loss: 2.244 | val loss: 2.289\nEpoch 5 | train loss: 2.229 | val loss: 2.259\nEpoch 6 | train loss: 2.128 | val loss: 2.137\nEpoch 7 | train loss: 2.045 | val loss: 2.091\nEpoch 8 | train loss: 1.995 | val loss: 2.044\nEpoch 9 | train loss: 1.934 | val loss: 1.987\nEpoch 10 | train loss: 1.873 | val loss: 1.942\nEpoch 11 | train loss: 1.815 | val loss: 1.886\nEpoch 12 | train loss: 1.757 | val loss: 1.846\nEpoch 13 | train loss: 1.708 | val loss: 1.807\nEpoch 14 | train loss: 1.666 | val loss: 1.774\nEpoch 15 | train loss: 1.632 | val loss: 1.746\nEpoch 16 | train loss: 1.602 | val loss: 1.724\nEpoch 17 | train loss: 1.576 | val loss: 1.702\nEpoch 18 | train loss: 1.552 | val loss: 1.684\nEpoch 19 | train loss: 1.530 | val loss: 1.667\nEpoch 20 | train loss: 1.509 | val loss: 1.650\nEpoch 21 | train loss: 1.489 | val loss: 1.637\nEpoch 22 | train loss: 1.473 | val loss: 1.635\nEpoch 23 | train loss: 1.458 | val loss: 1.622\nEpoch 24 | train loss: 1.446 | val loss: 1.608\nEpoch 25 | train loss: 1.435 | val loss: 1.603\nEpoch 26 | train loss: 1.425 | val loss: 1.599\nEpoch 27 | train loss: 1.416 | val loss: 1.593\nEpoch 28 | train loss: 1.407 | val loss: 1.586\nEpoch 29 | train loss: 1.397 | val loss: 1.584\nEpoch 30 | train loss: 1.388 | val loss: 1.580\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n‚ñÅ\n\n\nepoch\n‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n\n\ntest_avg_pearsonr\n‚ñÅ\n\n\ntrain_loss\n‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nval_loss\n‚ñà‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\n\n\nbeast_pearsonr\n0.92833\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n0.57389\n\n\ntrain_loss\n1.38804\n\n\nval_loss\n1.57982\n\n\n\n\n\n\n View run vivid-sweep-2 at: https://wandb.ai/ssalazar_02/DNA_model/runs/bt4sqsaj View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_151218-bt4sqsaj/logs\n\n\nwandb: Agent Starting Run: 7ca57ikz with config:\nwandb:  add_sigmoid: True\nwandb:  batch_size: 16\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.05\nwandb:  num_filters: 32\nwandb:  optimizer: SGD\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151414-7ca57ikz\n\n\nSyncing run unique-sweep-3 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/7ca57ikz\n\n\nEpoch 1 | train loss: 2.423 | val loss: 2.451\nEpoch 2 | train loss: 2.415 | val loss: 2.447\nEpoch 3 | train loss: 2.408 | val loss: 2.436\nEpoch 4 | train loss: 2.385 | val loss: 2.404\nEpoch 5 | train loss: 2.345 | val loss: 2.370\nEpoch 6 | train loss: 2.316 | val loss: 2.350\nEpoch 7 | train loss: 2.300 | val loss: 2.340\nEpoch 8 | train loss: 2.289 | val loss: 2.336\nEpoch 9 | train loss: 2.281 | val loss: 2.328\nEpoch 10 | train loss: 2.274 | val loss: 2.325\nEpoch 11 | train loss: 2.268 | val loss: 2.323\nEpoch 12 | train loss: 2.262 | val loss: 2.321\nEpoch 13 | train loss: 2.257 | val loss: 2.318\nEpoch 14 | train loss: 2.251 | val loss: 2.318\nEpoch 15 | train loss: 2.247 | val loss: 2.316\nEpoch 16 | train loss: 2.242 | val loss: 2.316\nEpoch 17 | train loss: 2.237 | val loss: 2.315\nEpoch 18 | train loss: 2.232 | val loss: 2.315\nEpoch 19 | train loss: 2.227 | val loss: 2.313\nEpoch 20 | train loss: 2.222 | val loss: 2.314\nEpoch 21 | train loss: 2.217 | val loss: 2.314\nEpoch 22 | train loss: 2.212 | val loss: 2.314\nEpoch 23 | train loss: 2.206 | val loss: 2.313\nEpoch 24 | train loss: 2.201 | val loss: 2.314\nEpoch 25 | train loss: 2.196 | val loss: 2.315\nEpoch 26 | train loss: 2.191 | val loss: 2.315\nEpoch 27 | train loss: 2.186 | val loss: 2.315\nEpoch 28 | train loss: 2.181 | val loss: 2.315\nEpoch 29 | train loss: 2.176 | val loss: 2.315\nEpoch 30 | train loss: 2.171 | val loss: 2.316\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n‚ñÅ\n\n\nepoch\n‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n\n\ntest_avg_pearsonr\n‚ñÅ\n\n\ntrain_loss\n‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nval_loss\n‚ñà‚ñà‚ñá‚ñÜ‚ñÑ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\n\n\nbeast_pearsonr\n0.6957\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n0.21384\n\n\ntrain_loss\n2.17106\n\n\nval_loss\n2.31633\n\n\n\n\n\n\n View run unique-sweep-3 at: https://wandb.ai/ssalazar_02/DNA_model/runs/7ca57ikz View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_151414-7ca57ikz/logs\n\n\nwandb: Agent Starting Run: 2pk9jgsh with config:\nwandb:  add_sigmoid: False\nwandb:  batch_size: 32\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.1\nwandb:  num_filters: 32\nwandb:  optimizer: Adam\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151710-2pk9jgsh\n\n\nSyncing run breezy-sweep-4 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/2pk9jgsh\n\n\nEpoch 1 | train loss: 8.430 | val loss: 2.453\nEpoch 2 | train loss: 2.422 | val loss: 2.457\nEpoch 3 | train loss: 2.423 | val loss: 2.455\nEpoch 4 | train loss: 2.424 | val loss: 2.457\nEpoch 5 | train loss: 2.425 | val loss: 2.457\nEpoch 6 | train loss: 2.425 | val loss: 2.459\nEpoch 7 | train loss: 2.427 | val loss: 2.464\nEpoch 8 | train loss: 2.428 | val loss: 2.463\nEpoch 9 | train loss: 2.429 | val loss: 2.461\nEpoch 10 | train loss: 2.430 | val loss: 2.463\nEpoch 11 | train loss: 2.429 | val loss: 2.459\nEpoch 12 | train loss: 2.430 | val loss: 2.465\nEpoch 13 | train loss: 2.430 | val loss: 2.461\nEpoch 14 | train loss: 2.431 | val loss: 2.458\nEpoch 15 | train loss: 2.430 | val loss: 2.462\nEpoch 16 | train loss: 2.431 | val loss: 2.464\nEpoch 17 | train loss: 2.431 | val loss: 2.463\nEpoch 18 | train loss: 2.430 | val loss: 2.461\nEpoch 19 | train loss: 2.431 | val loss: 2.458\nEpoch 20 | train loss: 2.430 | val loss: 2.466\nEpoch 21 | train loss: 2.431 | val loss: 2.466\nEpoch 22 | train loss: 2.431 | val loss: 2.463\nEpoch 23 | train loss: 2.431 | val loss: 2.463\nEpoch 24 | train loss: 2.430 | val loss: 2.470\nEpoch 25 | train loss: 2.431 | val loss: 2.460\nEpoch 26 | train loss: 2.431 | val loss: 2.459\nEpoch 27 | train loss: 2.431 | val loss: 2.463\nEpoch 28 | train loss: 2.431 | val loss: 2.462\nEpoch 29 | train loss: 2.431 | val loss: 2.462\nEpoch 30 | train loss: 2.431 | val loss: 2.461\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n‚ñÅ\n\n\nepoch\n‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n\n\ntest_avg_pearsonr\n‚ñÅ\n\n\ntrain_loss\n‚ñà‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nval_loss\n‚ñÅ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÑ‚ñÜ‚ñÖ‚ñÑ‚ñÖ‚ñÑ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÉ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñà‚ñÑ‚ñÉ‚ñÖ‚ñÖ‚ñÖ‚ñÑ\n\n\n\n\n\n\n\nbeast_pearsonr\n0.35183\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n-0.00231\n\n\ntrain_loss\n2.43134\n\n\nval_loss\n2.46144\n\n\n\n\n\n\n View run breezy-sweep-4 at: https://wandb.ai/ssalazar_02/DNA_model/runs/2pk9jgsh View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_151710-2pk9jgsh/logs\n\n\nwandb: Agent Starting Run: ohia2jz9 with config:\nwandb:  add_sigmoid: True\nwandb:  batch_size: 32\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.05\nwandb:  num_filters: 32\nwandb:  optimizer: Adam\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151916-ohia2jz9\n\n\nSyncing run vibrant-sweep-5 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/ohia2jz9\n\n\nEpoch 1 | train loss: 2.520 | val loss: 2.554\nEpoch 2 | train loss: 2.520 | val loss: 2.556\nEpoch 3 | train loss: 2.520 | val loss: 2.556\nEpoch 4 | train loss: 2.520 | val loss: 2.556\nEpoch 5 | train loss: 2.520 | val loss: 2.556\nEpoch 6 | train loss: 2.520 | val loss: 2.556\nEpoch 7 | train loss: 2.520 | val loss: 2.556\nEpoch 8 | train loss: 2.520 | val loss: 2.556\nEpoch 9 | train loss: 2.520 | val loss: 2.556\nEpoch 10 | train loss: 2.520 | val loss: 2.556\nEpoch 11 | train loss: 2.520 | val loss: 2.556\nEpoch 12 | train loss: 2.520 | val loss: 2.556\nEpoch 13 | train loss: 2.520 | val loss: 2.556\nEpoch 14 | train loss: 2.520 | val loss: 2.556\nEpoch 15 | train loss: 2.520 | val loss: 2.556\nEpoch 16 | train loss: 2.520 | val loss: 2.556\nEpoch 17 | train loss: 2.520 | val loss: 2.556\nEpoch 18 | train loss: 2.520 | val loss: 2.556\nEpoch 19 | train loss: 2.520 | val loss: 2.556\nEpoch 20 | train loss: 2.520 | val loss: 2.556\nEpoch 21 | train loss: 2.520 | val loss: 2.556\nEpoch 22 | train loss: 2.520 | val loss: 2.556\nEpoch 23 | train loss: 2.520 | val loss: 2.556\nEpoch 24 | train loss: 2.520 | val loss: 2.556\nEpoch 25 | train loss: 2.520 | val loss: 2.556\nEpoch 26 | train loss: 2.520 | val loss: 2.556\nEpoch 27 | train loss: 2.520 | val loss: 2.556\nEpoch 28 | train loss: 2.520 | val loss: 2.556\nEpoch 29 | train loss: 2.520 | val loss: 2.556\nEpoch 30 | train loss: 2.520 | val loss: 2.556\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n‚ñÅ\n\n\nepoch\n‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n\n\ntest_avg_pearsonr\n‚ñÅ\n\n\ntrain_loss\n‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\n\nval_loss\n‚ñÅ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n\n\n\n\n\n\n\nbeast_pearsonr\n0.20144\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n-0.00629\n\n\ntrain_loss\n2.52015\n\n\nval_loss\n2.5559\n\n\n\n\n\n\n View run vibrant-sweep-5 at: https://wandb.ai/ssalazar_02/DNA_model/runs/ohia2jz9 View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_151916-ohia2jz9/logs\n\n\nwandb: Agent Starting Run: gmg0gdbw with config:\nwandb:  add_sigmoid: False\nwandb:  batch_size: 64\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.1\nwandb:  num_filters: 16\nwandb:  optimizer: SGD\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_152128-gmg0gdbw\n\n\nSyncing run upbeat-sweep-6 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/gmg0gdbw\n\n\nEpoch 1 | train loss: 2.424 | val loss: 2.440\nEpoch 2 | train loss: 2.370 | val loss: 2.371\nEpoch 3 | train loss: 2.307 | val loss: 2.336\nEpoch 4 | train loss: 2.278 | val loss: 2.320\nEpoch 5 | train loss: 2.260 | val loss: 2.312\nEpoch 6 | train loss: 2.245 | val loss: 2.304\nEpoch 7 | train loss: 2.231 | val loss: 2.297\nEpoch 8 | train loss: 2.216 | val loss: 2.294\nEpoch 9 | train loss: 2.202 | val loss: 2.290\nEpoch 10 | train loss: 2.185 | val loss: 2.285\nEpoch 11 | train loss: 2.168 | val loss: 2.281\nEpoch 12 | train loss: 2.148 | val loss: 2.271\nEpoch 13 | train loss: 2.119 | val loss: 2.255\nEpoch 14 | train loss: 2.080 | val loss: 2.228\nEpoch 15 | train loss: 2.039 | val loss: 2.208\nEpoch 16 | train loss: 2.001 | val loss: 2.185\nEpoch 17 | train loss: 1.966 | val loss: 2.169\nEpoch 18 | train loss: 1.933 | val loss: 2.157\nEpoch 19 | train loss: 1.903 | val loss: 2.140\nEpoch 20 | train loss: 1.875 | val loss: 2.126\nEpoch 21 | train loss: 1.848 | val loss: 2.113\nEpoch 22 | train loss: 1.822 | val loss: 2.107\nEpoch 23 | train loss: 1.799 | val loss: 2.098\nEpoch 24 | train loss: 1.776 | val loss: 2.092\nEpoch 25 | train loss: 1.755 | val loss: 2.086\nEpoch 26 | train loss: 1.736 | val loss: 2.081\nEpoch 27 | train loss: 1.717 | val loss: 2.075\nEpoch 28 | train loss: 1.699 | val loss: 2.075\nEpoch 29 | train loss: 1.682 | val loss: 2.069\nEpoch 30 | train loss: 1.666 | val loss: 2.068\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n‚ñÅ\n\n\nepoch\n‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n\n\ntest_avg_pearsonr\n‚ñÅ\n\n\ntrain_loss\n‚ñà‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nval_loss\n‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\n\n\nbeast_pearsonr\n0.80236\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n0.38552\n\n\ntrain_loss\n1.66555\n\n\nval_loss\n2.06831\n\n\n\n\n\n\n View run upbeat-sweep-6 at: https://wandb.ai/ssalazar_02/DNA_model/runs/gmg0gdbw View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_152128-gmg0gdbw/logs"
  },
  {
    "objectID": "post/2025-05-01-unit03/borzoi_apa_tutorial.html",
    "href": "post/2025-05-01-unit03/borzoi_apa_tutorial.html",
    "title": "Predicting alternative polyadenylation with Borzoi",
    "section": "",
    "text": "Borzoi is a deep learning model that, similarily to Enformer:\n\nTakes DNA sequences as input (one-hot encoded)\nUses a convolutional neural network architecture and transformer layers\nPredicts multiple genomic features simultaneously\n\nHowever, Borzoi was rained to predict RNA-seq coverage as well, which makes it useful for the following aplication: predicting 3‚ÄôUTR alternative polyadenylation (APA).\n3‚ÄôUTR APA is a post-transcriptional regulatory mechanism that allows transcripts from a gene to have different lengths due to differential polyA signal usage leading to different lengths of 3‚ÄôUTR regions. Variants on the 3‚ÄôUTR sequence that alter the length of the region are refered to 3‚ÄôaQTLs read more here.\n\n\n\nIn this notebook we will learn how to use borzoi to predict RNA-seq coverage around one 3‚ÄôaQTL by predicting on both the reference and the alternative sequence. We will plot the differences on RNA-seq prediction at the 3‚ÄôUTR for the whole blood borzoi tracks.\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-05-01-unit03/borzoi_apa_tutorial.html#predicting-alternative-polyadenylation-with-borzoi",
    "href": "post/2025-05-01-unit03/borzoi_apa_tutorial.html#predicting-alternative-polyadenylation-with-borzoi",
    "title": "Predicting alternative polyadenylation with Borzoi",
    "section": "",
    "text": "Borzoi is a deep learning model that, similarily to Enformer:\n\nTakes DNA sequences as input (one-hot encoded)\nUses a convolutional neural network architecture and transformer layers\nPredicts multiple genomic features simultaneously\n\nHowever, Borzoi was rained to predict RNA-seq coverage as well, which makes it useful for the following aplication: predicting 3‚ÄôUTR alternative polyadenylation (APA).\n3‚ÄôUTR APA is a post-transcriptional regulatory mechanism that allows transcripts from a gene to have different lengths due to differential polyA signal usage leading to different lengths of 3‚ÄôUTR regions. Variants on the 3‚ÄôUTR sequence that alter the length of the region are refered to 3‚ÄôaQTLs read more here.\n\n\n\nIn this notebook we will learn how to use borzoi to predict RNA-seq coverage around one 3‚ÄôaQTL by predicting on both the reference and the alternative sequence. We will plot the differences on RNA-seq prediction at the 3‚ÄôUTR for the whole blood borzoi tracks."
  },
  {
    "objectID": "post/2025-05-01-unit03/borzoi_apa_tutorial.html#set-up-a-borzoi-adequate-environment",
    "href": "post/2025-05-01-unit03/borzoi_apa_tutorial.html#set-up-a-borzoi-adequate-environment",
    "title": "Predicting alternative polyadenylation with Borzoi",
    "section": "1. Set up a borzoi-adequate environment",
    "text": "1. Set up a borzoi-adequate environment\nThe requirements for running borzoi are outlined here. The most important dependencies are python ==3.10 and tensorflow==2.15.x.\nI had to install python 3.10 and found that working with a venv worked as of may 5th 2025.\nI followed the instructions to use the GPU of my macbook here\nbrew install python@3.10\npython3.10 -m venv ~/borzoiTools\nsource ~/borzoiTools/bin/activate\npython -m pip install -U pip\npython -m pip install ipykernel\npython3 -m ipykernel install --user --name=borzoiTools\npython3 -m pip install tensorflow==2.15\npython3 -m pip install tensorflow-metal\nTo verify python and tensforflow versions, run the following chunks:\n\nimport tensorflow as tf\n\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\ngpus\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\nif gpus:\n    try:\n        # Currently, memory growth needs to be the same across GPUs\n        for gpu in gpus:\n            print(gpu)\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(\"TensorFlow is using the GPU.\")\n    except RuntimeError as e:\n        print(e)\nelse:\n    print(\"TensorFlow is not using the GPU. Check your TensorFlow installation.\")\n\nPhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\nTensorFlow is using the GPU.\n\n\n\nimport sys\nprint(sys.version)\n\n3.10.17 (main, Apr  8 2025, 12:10:59) [Clang 16.0.0 (clang-1600.0.26.6)]\n\n\n\nprint(tf.__version__)\n\n2.15.0"
  },
  {
    "objectID": "post/2025-05-01-unit03/borzoi_apa_tutorial.html#install-borzoi-and-dependencies",
    "href": "post/2025-05-01-unit03/borzoi_apa_tutorial.html#install-borzoi-and-dependencies",
    "title": "Predicting alternative polyadenylation with Borzoi",
    "section": "2. Install borzoi and dependencies",
    "text": "2. Install borzoi and dependencies\nBorzoi depends on the baskerville software, run the following chunks to install both on your environment:\n\n%cd /Users/sofiasalazar/Desktop/IM_lab/borzoi_folder\n\n/Users/sofiasalazar/Desktop/IM_lab/borzoi_folder\n\n\n/Users/sofiasalazar/borzoiTools/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n\n\n\n!git clone https://github.com/calico/baskerville.git\n\n\n%cd baskerville\n%pip install -e .\n\n\n%cd ..\n!git clone https://github.com/calico/borzoi.git\n\n\n%cd borzoi\n%pip install -e .\n\nThen I installed other libraries that we will be using as well\n\n%pip install kipoiseq\n%pip install h5py\n\nHere I had to restart the kernel to use baskerville"
  },
  {
    "objectID": "post/2025-05-01-unit03/borzoi_apa_tutorial.html#download-models",
    "href": "post/2025-05-01-unit03/borzoi_apa_tutorial.html#download-models",
    "title": "Predicting alternative polyadenylation with Borzoi",
    "section": "3. Download models",
    "text": "3. Download models\nThe borzoi models are each ~1GB (~4GB in total as there are 4 models, each correspond to a cross validation fold from the training process). Further down you‚Äôll see that we can choose to predict using the 4 models or only a couple of them.\n\n%cd /Users/sofiasalazar/Desktop/IM_lab/borzoi_folder/borzoi\n! ./download_models.sh"
  },
  {
    "objectID": "post/2025-05-01-unit03/borzoi_apa_tutorial.html#set-up-borzoi-and-helper-functions",
    "href": "post/2025-05-01-unit03/borzoi_apa_tutorial.html#set-up-borzoi-and-helper-functions",
    "title": "Predicting alternative polyadenylation with Borzoi",
    "section": "4. Set up borzoi and helper functions",
    "text": "4. Set up borzoi and helper functions\nThe previous steps are run only once, afterwards, we can set up the borzoi functions as well as other utilities to extract the reference sequence and mutate it.\n\nimport os\nimport h5py\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport baskerville\nfrom baskerville import seqnn\nfrom baskerville import gene as bgene\nfrom baskerville import dna\nimport json\nimport kipoiseq\nimport pyfaidx\n\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\ngpus\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\nDEVICE = \"/GPU:0\"\n\nChange the following paths to your personal paths\n\nBORZOI_PATH = \"/Users/sofiasalazar/Desktop/IM_lab/borzoi_folder/borzoi\"\nmodels_path = os.path.join(BORZOI_PATH, \"examples/saved_models\" )\nfasta_file = \"/Users/sofiasalazar/Library/CloudStorage/Box-Box/imlab-data/Reference-Data/ref_sequences/hg38/Homo_sapiens_assembly38.fasta\"\nparams_file = os.path.join(BORZOI_PATH, \"examples/params_pred.json\")\ntargets_file = os.path.join(BORZOI_PATH, \"examples/targets_human.txt\")\n\nThe following classes will help us extract the reference genome sequence, mutate it and one hot encode it.\n\nclass FastaStringExtractor:\n\n    def __init__(self, fasta_file):\n        self.fasta = pyfaidx.Fasta(fasta_file)\n        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n    #import pd.Interval as Interval\n    def extract(self, interval: kipoiseq.Interval, snp_position = None, snp_base=None, **kwargs) -&gt; str:\n        # Truncate interval if it extends beyond the chromosome lengths.\n        chromosome_length = self._chromosome_sizes[interval.chrom]\n        trimmed_interval = kipoiseq.Interval(interval.chrom,\n                                    max(interval.start, 0),\n                                    min(interval.end, chromosome_length),\n                                    )\n        \n        # pyfaidx wants a 1-based interval\n        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n                                          trimmed_interval.start + 1,\n                                          trimmed_interval.stop).seq).upper()\n        \n        # Apply SNP modification if specified\n        if snp_position and snp_base:\n            pos = snp_position - trimmed_interval.start - 1\n            if 0 &lt;= pos &lt; len(sequence):\n                print(sequence[pos-1:pos+2])\n                sequence = sequence[:pos] + snp_base + sequence[pos + 1:]\n                print(sequence[pos-1:pos+2])\n                \n        # Fill truncated values with N's.\n        pad_upstream = 'N' * max(-interval.start, 0)\n        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n        return pad_upstream + sequence + pad_downstream\n\n    def close(self):\n        return self.fasta.close()\n    \ndef one_hot_encode(sequence):\n  return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)\n\nBorzoi prediction functions\nHere we define how many model folds we want to use for the prediction\n\nn_reps = 1\n\n\nseq_len = 524288\nrc = True         #Average across reverse-complement prediction\n\nwith open(params_file) as params_open :\n    \n    params = json.load(params_open)\n    \n    params_model = params['model']\n    params_train = params['train']\n\nparams_model['trunk'][-2]['cropping'] = 0\n\n\ntargets_df = pd.read_csv(targets_file, index_col=0, sep='\\t')\ntarget_index = targets_df.index \n\n#Create local index of strand_pair (relative to sliced targets)\nif rc :\n    strand_pair = targets_df.strand_pair\n    \n    target_slice_dict = {ix : i for i, ix in enumerate(target_index.values.tolist())}\n    slice_pair = np.array([\n        target_slice_dict[ix] if ix in target_slice_dict else ix for ix in strand_pair.values.tolist()\n    ], dtype='int32')\n\nmodels = []\nfor rep_ix in range(n_reps) :\n    model_file = os.path.join(BORZOI_PATH, f\"examples/saved_models/f3c{rep_ix}/train/model0_best.h5\")\n    seqnn_model = seqnn.SeqNN(params_model)\n    seqnn_model.restore(model_file, 0)\n    seqnn_model.build_slice(target_index)\n    if rc :\n        seqnn_model.strand_pair.append(slice_pair)\n    seqnn_model.build_ensemble(rc, [0])\n    \n    models.append(seqnn_model)\n\ndef predict_tracks(models, sequence_one_hot):\n\n    predicted_tracks = []\n    for fold_ix in range(len(models)):\n\n        yh = models[fold_ix](sequence_one_hot[None, ...])[:, None, ...].astype(\n            \"float16\"\n        )\n\n        predicted_tracks.append(yh)\n\n    predicted_tracks = np.concatenate(predicted_tracks, axis=1)\n\n    return predicted_tracks\n\ndef make_prediction(sequence):\n    with tf.device(DEVICE):\n        y_wt = predict_tracks(models, sequence)\n        return y_wt\n\n2025-05-05 13:09:41.154901: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n2025-05-05 13:09:41.154939: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n2025-05-05 13:09:41.154952: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\n2025-05-05 13:09:41.155004: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n2025-05-05 13:09:41.155040: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -&gt; physical PluggableDevice (device: 0, name: METAL, pci bus id: &lt;undefined&gt;)"
  },
  {
    "objectID": "post/2025-05-01-unit03/borzoi_apa_tutorial.html#predict-for-a-3aqtl",
    "href": "post/2025-05-01-unit03/borzoi_apa_tutorial.html#predict-for-a-3aqtl",
    "title": "Predicting alternative polyadenylation with Borzoi",
    "section": "5. Predict for a 3‚ÄôaQTL",
    "text": "5. Predict for a 3‚ÄôaQTL\nIn this paper, 3‚Äô alternative polyadenylation QTLs (1‚ÄôaQTLs) where identified using gtex individuals. They were stored here.\nFor the purpose of this example, we will focus on the variant rs10954213: chr7_128949373_G_A (hg38), which is one of the whole blood 3‚ÄôaQTLs that was reported on the paper (Figure 1b)\n\nfasta_extractor = FastaStringExtractor(fasta_file)\n\n\nsequence_interval = kipoiseq.Interval(\"chr7\", 128949373, 128949374)\n\n\nreference_one_hot = one_hot_encode(fasta_extractor.extract(sequence_interval.resize(seq_len)))\n\nHere the snp_position argument of the extract function refers to the position in the sequence where we want to introduce the mutation. snp_base is the nucleotide which will replace the reference nucleotide.\nNote that in the output, the reference and mutated nucleotides are printed in the middle of the upstream and downstream nucleotides.\n\nmutated_one_hot = one_hot_encode(fasta_extractor.extract(sequence_interval.resize(seq_len), snp_position=128949373, snp_base='A'))\n\nTGA\nTAA\n\n\n\nreference_one_hot = reference_one_hot.astype(\"float32\")\nmutated_one_hot = mutated_one_hot.astype(\"float32\")\n\n\npred_reference = make_prediction(reference_one_hot)\n\n\npred_reference.shape\n\n(1, 1, 16384, 7611)\n\n\nWe remove the 0 dimension and average across the 1 dimension (folds)\n\npred_reference = pred_reference.squeeze(0).mean(axis = 0)\npred_reference.shape\n\n(16384, 7611)\n\n\nThen we keep only the whole blood RNA-seq prediction see track description here\n\npred_reference = pred_reference[:, 7531:7534]\npred_reference.shape\n\n(16384, 3)\n\n\nAverage across the RNA blood tracks\n\npred_reference = pred_reference.mean(axis = 1 )\n\n\npred_reference.shape\n\n(16384,)\n\n\nWe do the same for the mutated sequence\n\npred_mutated = make_prediction(mutated_one_hot)\npred_mutated = pred_mutated.squeeze(0).mean(axis = 0)\npred_mutated = pred_mutated[:, 7531:7534]\npred_mutated = pred_mutated.mean(axis = 1 )\n\n\npred_mutated.shape\n\n(16384,)\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\ndef plot_tracks_nested(nested_tracks, interval, highlight_position = None, alpha = 0.4, height=1.5):\n    \"\"\"\n    nested_tracks: dict of dicts\n        Outer keys: data sources (e.g., replicates or conditions)\n        Inner keys: track names to plot (these become the rows)\n    interval: a kipoiseq interval with .start and .end\n    \"\"\"\n    all_inner_keys = set()\n    for subdict in nested_tracks.values():\n        all_inner_keys.update(subdict.keys())\n    all_inner_keys = sorted(all_inner_keys)\n\n    n_tracks = len(all_inner_keys)\n    fig, axes = plt.subplots(n_tracks, 1, figsize=(20, height * n_tracks), sharex=True)\n\n    if n_tracks == 1:\n        axes = [axes]\n\n    for ax, inner_key in zip(axes, all_inner_keys):\n        for outer_key, subdict in nested_tracks.items():\n            if inner_key in subdict:\n                y = subdict[inner_key]\n                x = np.linspace(interval.start, interval.end, num=len(y))\n                ax.fill_between(x, y, alpha=alpha, label=outer_key)\n        ax.set_title(inner_key, fontsize=11)\n        ax.legend(loc='upper right', fontsize=8, frameon=False)\n        sns.despine(ax=ax, top=True, right=True, bottom=True)\n        if highlight_position is not None and interval.start &lt;= highlight_position &lt;= interval.end:\n            ax.axvline(x=highlight_position, color='black', linestyle='--', linewidth=1)\n\n    axes[-1].set_xlabel(str(interval))\n    plt.tight_layout()\n    plt.show()\n\nSince we are only interested in the 3‚ÄôUTR region, we will center our plot on the 64 middle bins (for borzoi, each bin is 32 base pair-long). Remember that the prediction is centered at the 3‚ÄôaQTL (dashed line on the plot). Indeed the coverage difference between the reference and alternative sequences is evident.\n\npred_reference[8160:8224, ].shape\n\n(64,)\n\n\n\ntracks = {'Reference': {'Whole Blood RNA-seq': pred_reference[8160:8224, ]},\n          'Mutated': {'Whole Blood RNA-seq': pred_mutated[8160:8224, ]}}\nplot_tracks_nested(tracks, sequence_interval.resize(64*32), highlight_position=sequence_interval.start)"
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer-minimal-neanderthal.html",
    "href": "post/2025-05-01-unit03/enformer-minimal-neanderthal.html",
    "title": "Enformer usage neanderthal - jupyter notebook version",
    "section": "",
    "text": "modified from Enformer usage notebook in https://github.com/google-deepmind/deepmind-research/blob/master/enformer/enformer-usage.ipynb\n¬© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer-minimal-neanderthal.html#set-up-and-function-definitions",
    "href": "post/2025-05-01-unit03/enformer-minimal-neanderthal.html#set-up-and-function-definitions",
    "title": "Enformer usage neanderthal - jupyter notebook version",
    "section": "1. set up and function definitions",
    "text": "1. set up and function definitions\nThis is Sabrina‚Äôs EnformerVCF.py file with functions necessary to run vcf modified enformer, based on functions from Temi and Sai, in turn based on Avsec et al‚Äôs code\n\nInstall Required Python Packages (First Run Only)\n\n# Install Required Python Packages (First Run Only)\nfirst_time = False  # Set to False after first run\n\nif first_time:\n    %pip install tensorflow\n    ## this requires  numpy&lt;2.2.0,&gt;=1.26.0 (from tensorflow)\n    ## it will uninstall numpy not compatible with tensorflow\n\n    import platform\n    if platform.processor() == 'arm':\n        print(\"Apple Silicon detected, installing tensorflow-metal...\")\n        %pip install tensorflow-metal\n    else:\n        print(\"Not running on Apple Silicon, skipping tensorflow-metal installation\")\n\n    %pip install tensorflow_hub\n    # %pip install joblib # already installed\n    %pip install kipoiseq\n    # %pip install pyfaidx # already installed\n    # %pip install pandas # already installed\n    # %pip install numpy # already installed\n    # %pip install matplotlib # already installed\n    # %pip install seaborn # already installed\n    %pip install cyvcf2\n\n    %pip install Bio\n\n\n\nImport Libraries and Define Utility Functions\n\n# Import Libraries and Define Utility Functions\nimport tensorflow as tf\n\n# Make sure the GPU is enabled \nassert tf.config.list_physical_devices('GPU'), 'Start the colab kernel with GPU: Runtime -&gt; Change runtime type -&gt; GPU'\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n\nimport tensorflow_hub as hub # for interacting with saved models and tensorflow hub\nimport joblib\nimport gzip # for manipulating compressed files\nimport kipoiseq # for manipulating fasta files\nfrom kipoiseq import Interval # same as above, really\nimport pyfaidx # to index our reference genome file\nimport pandas as pd # for manipulating dataframes\nimport numpy as np # for numerical computations\nimport matplotlib.pyplot as plt # for plotting\nimport matplotlib as mpl # for plotting\nimport seaborn as sns # for plotting\nimport pickle # for saving large objects\nimport os, sys # functions for interacting with the operating system\nimport cyvcf2\n\nimport Bio\nfrom Bio.Seq import Seq\ndef create_rev_complement(dna_string):\n    return(str(Seq(dna_string).reverse_complement()))\n\nimport io\nimport os\nimport gzip\n\nNum GPUs Available:  1\n\n\n\n\nDefine Enformer Model Classes and Sequence Extraction\n\n# Define Enformer Model Classes and Sequence Extraction\n\nSEQUENCE_LENGTH = 393216\n\n\nclass Enformer:\n\n  def __init__(self, tfhub_url):\n    self._model = hub.load(tfhub_url).model\n\n  def predict_on_batch(self, inputs):\n    predictions = self._model.predict_on_batch(inputs)\n    return {k: v.numpy() for k, v in predictions.items()}\n\n  @tf.function\n  def contribution_input_grad(self, input_sequence,\n                              target_mask, output_head='human'):\n    input_sequence = input_sequence[tf.newaxis]\n\n    target_mask_mass = tf.reduce_sum(target_mask)\n    with tf.GradientTape() as tape:\n      tape.watch(input_sequence)\n      prediction = tf.reduce_sum(\n          target_mask[tf.newaxis] *\n          self._model.predict_on_batch(input_sequence)[output_head]) / target_mask_mass\n\n    input_grad = tape.gradient(prediction, input_sequence) * input_sequence\n    input_grad = tf.squeeze(input_grad, axis=0)\n    return tf.reduce_sum(input_grad, axis=-1)\n  \nclass EnformerScoreVariantsRaw:\n\n  def __init__(self, tfhub_url, organism='human'):\n    self._model = Enformer(tfhub_url)\n    self._organism = organism\n\n  def predict_on_batch(self, inputs):\n    ref_prediction = self._model.predict_on_batch(inputs['ref'])[self._organism]\n    alt_prediction = self._model.predict_on_batch(inputs['alt'])[self._organism]\n\n    return alt_prediction.mean(axis=1) - ref_prediction.mean(axis=1)\n\nclass EnformerScoreVariantsNormalized:\n\n  def __init__(self, tfhub_url, transform_pkl_path,\n               organism='human'):\n    assert organism == 'human', 'Transforms only compatible with organism=human'\n    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n      transform_pipeline = joblib.load(f)\n    self._transform = transform_pipeline.steps[0][1]  # StandardScaler.\n\n  def predict_on_batch(self, inputs):\n    scores = self._model.predict_on_batch(inputs)\n    return self._transform.transform(scores)\n\nclass EnformerScoreVariantsPCANormalized:\n\n  def __init__(self, tfhub_url, transform_pkl_path,\n               organism='human', num_top_features=500):\n    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n      self._transform = joblib.load(f)\n    self._num_top_features = num_top_features\n\n  def predict_on_batch(self, inputs):\n    scores = self._model.predict_on_batch(inputs)\n    return self._transform.transform(scores)[:, :self._num_top_features]\n\n# @title `variant_centered_sequences`\n\nclass FastaStringExtractor:\n\n    def __init__(self, fasta_file):\n        self.fasta = pyfaidx.Fasta(fasta_file)\n        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n    #import pd.Interval as Interval\n    def extract(self, interval: Interval, **kwargs) -&gt; str:\n        # Truncate interval if it extends beyond the chromosome lengths.\n        chromosome_length = self._chromosome_sizes[interval.chrom]\n        trimmed_interval = Interval(interval.chrom,\n                                    max(interval.start, 0),\n                                    min(interval.end, chromosome_length),\n                                    )\n        # pyfaidx wants a 1-based interval\n        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n                                          trimmed_interval.start + 1,\n                                          trimmed_interval.stop).seq).upper()\n        # Fill truncated values with N's.\n        pad_upstream = 'N' * max(-interval.start, 0)\n        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n        return pad_upstream + sequence + pad_downstream\n\n    def close(self):\n        return self.fasta.close()\n\n\n\nData Processing and Visualization Functions\n\n# Data Processing and Visualization Functions\ndef one_hot_encode(sequence):\n  return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)\n\n# @title `plot_tracks`\n\ndef plot_tracks(tracks, interval, height=1.5):\n  fig, axes = plt.subplots(len(tracks), 1, figsize=(20, height * len(tracks)), sharex=True)\n  for ax, (title, y) in zip(axes, tracks.items()):\n    ax.fill_between(np.linspace(interval.start, interval.end, num=len(y)), y)\n    ax.set_title(title)\n    sns.despine(top=True, right=True, bottom=True)\n  ax.set_xlabel(str(interval))\n  plt.tight_layout()\n\ndef read_vcf(path):\n    with gzip.open(path, 'rt') as f:\n        lines = [l for l in f if not l.startswith('##')]\n    return pd.read_csv(\n        io.StringIO(''.join(lines)),\n        dtype={'#CHROM': str, 'POS': int, 'ID': str, 'REF': str, 'ALT': str,\n               'QUAL': str, 'FILTER': str, 'INFO': str},\n        sep='\\t'\n    ).rename(columns={'#CHROM': 'CHROM'})\n\n# def vcf_to_seq(target_interval, individual, vcf, fasta_extractor):\n#   ## should be replaced with vcf_to_seq_faster\n#   target_fa = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))\n#   window_coords = target_interval.resize(SEQUENCE_LENGTH)\n#       # two haplotypes per individual\n#   haplo_1 = list(target_fa[:])\n#   haplo_2 = list(target_fa[:])\n\n#   ref_mismatch_count = 0\n#   for i,row in vcf.iterrows():\n#     geno = row[individual].split(\"|\")\n#     if (row[\"POS\"]-window_coords.start-1) &gt;= len(haplo_2):\n#       continue\n#     if (row[\"POS\"]-window_coords.start-1) &lt; 0:\n#       continue\n#     if geno[0] == \"1\":\n#       haplo_1[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n#     if geno[1] == \"1\":\n#       haplo_2[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n#   return haplo_1, haplo_2\n\ndef vcf_to_seq_faster(target_interval, individual, vcf_file, fasta_extractor):\n  # target_inverval is a kipoiseq interval, e.g. kipoiseq.Interval(\"chr22\", 18118779, 18145669)\n  # individual is the id of the individual in the vcf file\n  \n  target_fa = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))\n  window_coords = target_interval.resize(SEQUENCE_LENGTH)\n      # two haplotypes per individual\n  haplo_1 = list(target_fa[:])\n  haplo_2 = list(target_fa[:])\n\n  # Open the VCF file\n  vcf_reader = cyvcf2.VCF(vcf_file)\n\n  # Specific genomic region\n  CHROM = window_coords.chrom\n  START = max(1,window_coords.start)\n  END = min(window_coords.end, fasta_extractor._chromosome_sizes[CHROM]-1)\n\n  count1 = 0\n  count2 = 0\n\n  # Iterate through variants in the specified region\n  for variant in vcf_reader(CHROM + ':' + str(START) + '-' + str(END)):\n      # Access the individual's genotype using index (0-based) of the sample\n      individual_index = vcf_reader.samples.index(individual)\n      genotype = variant.genotypes[individual_index]\n      ALT=variant.ALT[0]\n      REF=variant.REF\n      POS=variant.POS\n      if REF == target_fa[POS - window_coords.start - 1]:\n        if genotype[0] == 1:\n          haplo_1[POS - window_coords.start - 1] = ALT\n          count1 = count1 + 1\n        if genotype[1] == 1:\n          haplo_2[POS - window_coords.start - 1] = ALT  \n          count2 = count2 + 1\n      else:\n        print(\"ERROR: REF in vcf \"+ REF + \"!= REF from the build\" + target_fa[POS - window_coords.start - 1])\n  print(\"number of changes haplo1:\")\n  print(count1)\n  print(\"number of changes haplo2:\")\n  print(count2)\n  return haplo_1, haplo_2\n\n\n\nDefine comparison functions\n\n# Comparison and Summary Statistics Functions\n\n\ndef get_diffmat(mat1, mat2):\n    \n    diffmat = mat1 - mat2\n    abs_diffmat = np.abs(diffmat)\n\n    colwise_maxes1 = np.max(mat1, axis=0)\n    colwise_maxes2 = np.max(mat2, axis=0)\n\n    colwise_maxes_maxes = np.maximum(colwise_maxes1, colwise_maxes2)\n\n    relmax3_diffmat = diffmat / colwise_maxes_maxes\n    relmax3_diffmat = np.abs(relmax3_diffmat)\n\n    return relmax3_diffmat\n\n\ndef get_summary(arr):\n    summary = {\n        \"mean\": np.mean(arr),\n        \"median\": np.median(arr),\n        \"minimum\": np.min(arr),\n        \"maximum\": np.max(arr),\n        \"q1\": np.percentile(arr, 25),\n        \"q3\": np.percentile(arr, 75),\n    }\n    return summary\n\n\ndef plot_hist(arr, bin_num, xlab='Value', ylab='Frequency', title='Histogram'):\n    plt.hist(arr, bins=bin_num)\n    plt.title(title)\n    plt.xlabel(xlab)\n    plt.ylabel(ylab)\n    plt.show()\n\ndef column_correlations(mat1, mat2):\n    if mat1.shape != mat2.shape:\n        raise ValueError(\"Input matrices must have the same shape\")\n\n    num_columns = mat1.shape[1]\n    correlations = np.empty(num_columns)\n\n    for col in range(num_columns):\n        correlation = np.corrcoef(mat1[:, col], mat2[:, col])[0, 1]\n        correlations[col] = correlation\n\n    return correlations"
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer-minimal-neanderthal.html#set-file-paths-and-load-enformer-model",
    "href": "post/2025-05-01-unit03/enformer-minimal-neanderthal.html#set-file-paths-and-load-enformer-model",
    "title": "Enformer usage neanderthal - jupyter notebook version",
    "section": "2. Set File Paths and Load Enformer Model",
    "text": "2. Set File Paths and Load Enformer Model\nIf first time, download the model from here https://uchicago.box.com/s/ppzn9lqqsnr3i9jqcgc52zf668sllkjx\nand the hg19 fasta file from here https://uchicago.box.com/s/0rh4q4syucn66ne1d8n2aw9g3yyst9a0\nIf needed, you can also download the hg38 fasta file from here (but I believe neanderthal vcf is based on hg19) https://uchicago.box.com/s/wl50ji7jms2c8alyqxyk4q6uru37nnt9\n\n# Set File Paths and Load Enformer Model\n\n## edit this path to the location of the files on your computer\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/Reference-Data/\"\n\nmodel_path = PRE + \"models/enformer/raw\"\nfasta_file = PRE + \"ref_sequences/hg19/raw/genome.fa\"\n## check whether specific reference fasta used for the calling of the neanderthal vcf should be used\n#fasta_file = PRE + \"ref_sequences/hg38/Homo_sapiens_assembly38.fasta\"\n\nmodel = Enformer(model_path) # here we load the model architecture.\nfasta_extractor = FastaStringExtractor(fasta_file)\n\n2025-05-01 18:57:44.433668: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M2 Pro\n2025-05-01 18:57:44.433703: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 32.00 GB\n2025-05-01 18:57:44.433706: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 10.67 GB\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1746143864.434317  634286 pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\nI0000 00:00:1746143864.434366  634286 pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -&gt; physical PluggableDevice (device: 0, name: METAL, pci bus id: &lt;undefined&gt;)\n\n\n\nLoad Target Annotation Data\n\n## Load Target Annotation Data\ntargets_txt = 'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_human.txt'\n# df_targets = pd.read_csv(targets_txt, sep='\\t')\ntargets_slim_file = PRE + \"models/enformer/targets_slims.csv\"\ntargets_slim_df = pd.read_csv(targets_slim_file)"
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer-minimal-neanderthal.html#run-enformer-on-neanderthal-genomes",
    "href": "post/2025-05-01-unit03/enformer-minimal-neanderthal.html#run-enformer-on-neanderthal-genomes",
    "title": "Enformer usage neanderthal - jupyter notebook version",
    "section": "3. Run Enformer on Neanderthal genomes",
    "text": "3. Run Enformer on Neanderthal genomes\n\nShell Script: Preprocess Neanderthal VCF Files\nDownload Altai ch5 filtered vcf brew install htslib bgzip AltaiNea.hg19_1000g.5.vcf tabix -p vcf AltaiNea.hg19_1000g.5.vcf.gz\ncreate file filter-add-chr.sh with the following content chmod u+x filter-add-chr.sh to make it executable\n```{bash}\n#!/bin/bash\n\nfor NUM in {1..22}; do\n    # Filter missing genotypes and non-variant sites\n    bcftools view -e '(GT=\"./.\") || (GT=\"0/0\") || (ALT=\".\")' AltaiNea.hg19_1000g.${NUM}.vcf.gz &gt; AltaiNea.hg19_1000g.${NUM}.nomiss.vcf\n\n    # Compress the resulting VCF\n    bgzip AltaiNea.hg19_1000g.${NUM}.nomiss.vcf\n    \n    # Add \"chr\" prefix to all non-header lines and compress\n    # zcat &lt; ... is used on a mac terminal; in linux, it should be without &lt;,i.e., zcat AltaiNea...\n    zcat &lt; AltaiNea.hg19_1000g.${NUM}.nomiss.vcf.gz | awk 'BEGIN{OFS=FS=\"\\t\"} /^#/ {print; next} {print \"chr\"$0}' | bgzip &gt; AltaiNea.hg19_1000g.chr${NUM}.nomiss.vcf.gz\n    \n    # Filter to retain only SNPs\n    bcftools view -i 'strlen(REF) = 1 && strlen(ALT) = 1' AltaiNea.hg19_1000g.chr${NUM}.nomiss.vcf.gz &gt; AltaiNea.hg19_1000g.chr${NUM}.nomiss.snps_only.vcf\ndone\n```\n\n\nLoad Neanderthal vcf and predict epigenome\n\n# Load Neanderthal vcf and predict epigenome\n\n# download the vcf file from here https://uchicago.box.com/s/f682q1c6tl3cdnqwbvga0z72u5e203zs\n# and put it in your data folder\n\n# read VCFs and encode haplotypes\nCHROM='chr5'\nvcf_file = PRE + \"neanderthal/AltaiNea.hg19_1000g.\" + CHROM + \".nomiss.snps_only.vcf.gz\"\n\ntarget_interval = kipoiseq.Interval(CHROM,96875939 , 96919716)\nhaplo1, haplo2 = vcf_to_seq_faster(target_interval, 'AltaiNea', vcf_file, fasta_extractor)\nhaplo0 = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))\n\nhaplo1_enc = one_hot_encode(\"\".join(haplo1))[np.newaxis]\nhaplo2_enc = one_hot_encode(\"\".join(haplo2))[np.newaxis]\nhaplo0_enc = one_hot_encode(\"\".join(haplo0))[np.newaxis]\n\nprint(\"number of changes\");print(np.sum(haplo2_enc != haplo0_enc))\n\npred_human = model.predict_on_batch(haplo0_enc)['human'][0]\npred_altai = model.predict_on_batch((haplo1_enc + haplo2_enc)/2)['human'][0]\n\n[W::bcf_hrec_check] Invalid tag name: \"1000gALT\"\n[W::vcf_parse_filter] FILTER 'LowQual' is not defined in the header\n\n\nnumber of changes haplo1:\n538\nnumber of changes haplo2:\n650\nnumber of changes\n1300\n\n\n2025-05-01 19:00:48.346036: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n\n\n\n\nPlot Human reference epigenome\n\n#  Plot Human epigenomes\n\npredictions = pred_human\ntracks = {'DNASE:CD14-positive monocyte female': predictions[:, 41],\n          'DNASE:keratinocyte female': predictions[:, 42],\n          'CHIP:H3K27ac:keratinocyte female': predictions[:, 706],\n          'CAGE:LCL': np.log10(1 + predictions[:, 5110])}\nplot_tracks(tracks, target_interval)\n\n\n\n\n\n\n\n\n\n\nPlot Neanderthal epigenome\n\npredictions = pred_altai\ntracks = {'DNASE:CD14-positive monocyte female': predictions[:, 41],\n          'DNASE:keratinocyte female': predictions[:, 42],\n          'CHIP:H3K27ac:keratinocyte female': predictions[:, 706],\n          'CAGE:LCL': np.log10(1 + predictions[:, 5110])}\nplot_tracks(tracks, target_interval)\n\n\n\n\n\n\n\n\n\nget_summary(get_diffmat(pred_human,pred_altai))\nget_summary(column_correlations(pred_human,pred_altai))\n\n{'mean': np.float64(0.9963786626060424),\n 'median': np.float64(0.9985426207070724),\n 'minimum': np.float64(0.8450667250216665),\n 'maximum': np.float64(0.9999900682433824),\n 'q1': np.float64(0.9971584662782969),\n 'q3': np.float64(0.9990126346399585)}\n\n\n\n\nCreate scatter plots comparing human and Neanderthal predictions\n\ndef plot_prediction_scatters(pred_human, pred_altai, tracks):\n    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n    axes = axes.flatten()\n    \n    for idx, (track_name, track_idx) in enumerate(tracks.items()):\n        ax = axes[idx]\n        \n        # Get predictions for this track\n        human_pred = pred_human[:, track_idx]\n        altai_pred = pred_altai[:, track_idx]\n        \n        # Create scatter plot\n        ax.scatter(human_pred, altai_pred, alpha=0.5, s=10)\n        \n        # Add diagonal line\n        min_val = min(human_pred.min(), altai_pred.min())\n        max_val = max(human_pred.max(), altai_pred.max())\n        ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.5)\n        \n        # Calculate correlation\n        corr = np.corrcoef(human_pred, altai_pred)[0,1]\n        \n        # Add labels and title\n        ax.set_xlabel('Human Prediction')\n        ax.set_ylabel('Neanderthal Prediction')\n        ax.set_title(f'{track_name}\\nCorrelation: {corr:.3f}')\n        \n        # Make plot square\n        ax.set_aspect('equal')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Define tracks\ntracks = {\n    'DNASE:CD14-positive monocyte female': 41,\n    'DNASE:keratinocyte female': 42,\n    'CHIP:H3K27ac:keratinocyte female': 706,\n    'CAGE:LCL': 5110\n}\n\n# Create the plots\nplot_prediction_scatters(pred_human, pred_altai, tracks)"
  }
]