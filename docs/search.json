[
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "",
    "text": "created by Erin Wilson. Downloaded from here.\nSome edits by Haky Im and Ran Blekhman for the deep learning in genomics gene46100 course.\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#import-necessary-modules",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#import-necessary-modules",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "import necessary modules",
    "text": "import necessary modules\n\nfrom collections import defaultdict\nfrom itertools import product\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport random\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nif torch.backends.mps.is_available():\n    torch.set_default_dtype(torch.float32)\n    print(\"Set default to float32 for MPS compatibility\")\n\nSet default to float32 for MPS compatibility"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#set-seeds-for-reproduciblity-across-runs",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#set-seeds-for-reproduciblity-across-runs",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "set seeds for reproduciblity across runs",
    "text": "set seeds for reproduciblity across runs\n\n# Set a random seed in a bunch of different places\ndef set_seed(seed: int = 42) -&gt; None:\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    \n    if torch.backends.mps.is_available():\n        # For MacBooks with Apple Silicon\n        torch.mps.manual_seed(seed)\n    elif torch.cuda.is_available():\n        # For CUDA GPUs\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    print(f\"Random seed set as {seed}\")\n    \nset_seed(17)\n\nRandom seed set as 17"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#define-gpu-device",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#define-gpu-device",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "define GPU device",
    "text": "define GPU device\nAre you working on a GPU? If so, you can put your data/models on DEVICE (and have to do so explicity)! If not, you can probably remove all instances of foo.to(DEVICE) and it should still work fine on a CPU.\n\nDEVICE = torch.device('mps' if torch.backends.mps.is_available() \n                     else 'cuda' if torch.cuda.is_available() \n                     else 'cpu')\nDEVICE\n\ndevice(type='mps')\n\n\n ## 1. Generate synthetic DNA data\nUsually scientists might be interested in predicting something like a binding score, an expression strength, or classifying a TF binding event. But here, we are going to keep it simple: the goal in this tutorial is to observe if a deep learning model can learn to detect a very small, simple pattern in a DNA sequence and score it appropriately (again, just a practice task to convince ourselves that we have actually set up the Pytorch pieces correctly such that it can learn from input that looks like a DNA sequence).\nSo arbitrarily, let’s say that given an 8-mer DNA sequence, we will score it based on the following rules: * A = +20 points * C = +17 points * G = +14 points * T = +11 points\nFor every 8-mer, let’s sum up its total points based on the nucleotides in its sequence, then take the average. For example,\n\nAAAAAAAA would score 20.0\n\n(mean(20 + 20 + 20 + 20 + 20 + 20 + 20 + 20) = 20.0)\n\nACAAAAAA would score 19.625\n\n(mean(20 + 17 + 20 + 20 + 20 + 20 + 20 + 20) = 19.625)\n\n\nThese values for the nucleotides are arbitrary - there’s no real biology here! It’s just a way to assign sequences a score for the purposes of our Pytorch practice.\nHowever, since many recent papers use methods like CNNs to automatically detect “motifs,” or short patterns in the DNA that can activate or repress a biological response, let’s add one more piece to our scoring system. To simulate something like motifs influencing gene expression, let’s say a given sequence gets a +10 bump if TAT appears anywhere in the 8-mer, and a -10 bump if it has a GCG in it. Again, these motifs don’t mean anything in real life, they are just a mechanism for simulating a really simple activation or repression effect.\n\nSo let’s implement this basic scoring function!\n\n# define function for generating all k-mers of length k\ndef kmers(k):\n    '''Generate a list of all k-mers for a given k'''\n    \n    return [''.join(x) for x in product(['A','C','G','T'], repeat=k)]\n\n\ngenerate all 8-mers\n\n# generate all 8-mers\nseqs8 = kmers(8)\nprint('Total 8mers:',len(seqs8))\n\nTotal 8mers: 65536\n\n\n\n\ndefine scoring function for the DNA sequences\n\n# define score_dict\nscore_dict = {\n    'A':20,\n    'C':17,\n    'G':14,\n    'T':11\n}\n# define function for scoring sequences\ndef score_seqs_motif(seqs):\n    '''\n    Calculate the scores for a list of sequences based on \n    the above score_dict\n    '''\n    data = []\n    for seq in seqs:\n        # get the average score by nucleotide\n        score = np.mean([score_dict[base] for base in seq],dtype=np.float32)\n        \n        # give a + or - bump if this k-mer has a specific motif\n        if 'TAT' in seq:\n            score += 10\n        if 'GCG' in seq:\n            score -= 10\n        data.append([seq,score])\n        \n    df = pd.DataFrame(data, columns=['seq','score'])\n    return df\n\n\nmer8 = score_seqs_motif(seqs8)\nmer8.head()\n\n\n\n\n\n\n\n\nseq\nscore\n\n\n\n\n0\nAAAAAAAA\n20.000\n\n\n1\nAAAAAAAC\n19.625\n\n\n2\nAAAAAAAG\n19.250\n\n\n3\nAAAAAAAT\n18.875\n\n\n4\nAAAAAACA\n19.625\n\n\n\n\n\n\n\nSpot check scores of a couple seqs with motifs:\n\nmer8[mer8['seq'].isin(['TGCGTTTT','CCCCCTAT'])]\n\n\n\n\n\n\n\n\nseq\nscore\n\n\n\n\n21875\nCCCCCTAT\n25.875\n\n\n59135\nTGCGTTTT\n2.500"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#plot-distribution-of-motif-scores",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#plot-distribution-of-motif-scores",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "plot distribution of motif scores",
    "text": "plot distribution of motif scores\n\nplt.hist(mer8['score'].values,bins=20)\nplt.title(\"8-mer with Motifs score distribution\")\nplt.xlabel(\"seq score\",fontsize=14)\nplt.ylabel(\"count\",fontsize=14)\nplt.show()\n\n\n\n\n\n\n\n\nAs expected, the distribution of scores across all 8-mers has 3 groups: * No motif (centered around ~15) * contains TAT (~25) * contains GCG (~5)"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-1",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-1",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "Question 1",
    "text": "Question 1\nModify the scoring function to create a more complex pattern. Instead of giving fixed bonuses for “TAT” and “GCG”, implement a position-dependent scoring where a motif gets a higher bonus if it appears at the beginning of the sequence compared to the end. How does this change the distribution of scores?"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#next-we-want-to-train-a-model-to-predict-this-score-by-from-the-dna-sequence",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#next-we-want-to-train-a-model-to-predict-this-score-by-from-the-dna-sequence",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "Next, we want to train a model to predict this score by from the DNA sequence",
    "text": "Next, we want to train a model to predict this score by from the DNA sequence\n ## 2. Prepare data for Pytorch training\nFor neural networks to make predictions, you have to give it your input as a matrix of numbers. For example, to classify images by whether or not they contain a cat, a network “sees” the image as a matrix of pixel values and learns relevant patterns in the relative arrangement of pixels (e.g. patterns that correspond to cat ears, or a nose with whiskers).\nWe similarly need to turn our DNA sequences (strings of ACGTs) into a matrix of numbers. So how do we pretend our DNA is a cat?\nOne common strategy is to one-hot encode the DNA: treat each nucleotide as a vector of length 4, where 3 positions are 0 and one position is a 1, depending on the nucleotide.\n\nturn DNA sequences into numbers with one hot encoding\n\nThis one-hot encoding has the nice property that it makes your DNA appear like how a computer sees a picture of a cat!\n\ndef one_hot_encode(seq):\n    \"\"\"\n    Given a DNA sequence, return its one-hot encoding\n    \"\"\"\n    # Make sure seq has only allowed bases\n    allowed = set(\"ACTGN\")\n    if not set(seq).issubset(allowed):\n        invalid = set(seq) - allowed\n        raise ValueError(f\"Sequence contains chars not in allowed DNA alphabet (ACGTN): {invalid}\")\n        \n    # Dictionary returning one-hot encoding for each nucleotide \n    nuc_d = {'A':[1.0,0.0,0.0,0.0],\n             'C':[0.0,1.0,0.0,0.0],\n             'G':[0.0,0.0,1.0,0.0],\n             'T':[0.0,0.0,0.0,1.0],\n             'N':[0.0,0.0,0.0,0.0]}\n    \n    # Create array from nucleotide sequence\n    vec=np.array([nuc_d[x] for x in seq], dtype=np.float32)\n        \n    return vec\n\n\n# one hot encoding of 8 As\na8 = one_hot_encode(\"AAAAAAAA\")\nprint(\"AAAAAA:\\n\",a8)\n\n\nAAAAAA:\n [[1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]\n [1. 0. 0. 0.]]\n\n\n\n# one hot encoding of another DNA\ns = one_hot_encode(\"AGGTACCT\")\nprint(\"AGGTACC:\\n\",s)\nprint(\"shape:\",s.shape)\n\nAGGTACC:\n [[1. 0. 0. 0.]\n [0. 0. 1. 0.]\n [0. 0. 1. 0.]\n [0. 0. 0. 1.]\n [1. 0. 0. 0.]\n [0. 1. 0. 0.]\n [0. 1. 0. 0.]\n [0. 0. 0. 1.]]\nshape: (8, 4)\n\n\n\n\nsplit data into train, validation, and test\nData are typically split into training, validation, and test sets. This helps avoid overfitting and achieve better generalization to new data.\n\nTraining set (e.g. 70% of the data)\n\nused to train the model\nmodel learns patterns from this data\nanalogy: studying for an exam\n\nValidation or tuning set (e.g. 15% of the data)\n\nused to tune hypterparameters\nhelps prevent overfitting\nanalogy: pratice problems while studying\n\nTest set or held out set (e.g. 15% of the data)\n\nused only to evaluate final model performance\nnever used during training or tuning\nanalogy: actual exam\n\n\nNote: in this tutorial, that uses the quick_split function defined here splits the data into 20% for the test set, 80% of the remaining 80% as the training set (i.e. 64% of the total) and 20% of the non test sets as validation (i.e. 16%).\n\ndefine quick splitting function\n\n# define function for splitting data\ndef quick_split(df, split_frac=0.8, verbose=False):\n    '''\n    Given a df of samples, randomly split indices between\n    train and test at the desired fraction\n    '''\n    cols = df.columns # original columns, use to clean up reindexed cols\n    df = df.reset_index()\n\n    # shuffle indices\n    idxs = list(range(df.shape[0]))\n    random.shuffle(idxs)\n\n    # split shuffled index list by split_frac\n    split = int(len(idxs)*split_frac)\n    train_idxs = idxs[:split]\n    test_idxs = idxs[split:]\n    \n    # split dfs and return\n    train_df = df[df.index.isin(train_idxs)]\n    test_df = df[df.index.isin(test_idxs)]\n        \n    return train_df[cols], test_df[cols]\n\n\n\nsplit data into train, validation, and test sets\n\n# split data into train, validation, and test\nfull_train_df, test_df = quick_split(mer8)\ntrain_df, val_df = quick_split(full_train_df)\n\nprint(\"Train:\", train_df.shape)\nprint(\"Val:\", val_df.shape)\nprint(\"Test:\", test_df.shape)\n\ntrain_df.head()\n\nTrain: (41942, 2)\nVal: (10486, 2)\nTest: (13108, 2)\n\n\n\n\n\n\n\n\n\nseq\nscore\n\n\n\n\n0\nAAAAAAAA\n20.000\n\n\n1\nAAAAAAAC\n19.625\n\n\n2\nAAAAAAAG\n19.250\n\n\n3\nAAAAAAAT\n18.875\n\n\n4\nAAAAAACC\n19.250\n\n\n\n\n\n\n\n\n\n\nplot distribution of train, validation, and test data and check that they are similarly distributed\n\ndef plot_train_test_hist(train_df, val_df,test_df,bins=20):\n    ''' Check distribution of train/test scores, sanity check that its not skewed'''\n    plt.hist(train_df['score'].values,bins=bins,label='train',alpha=0.5)\n    plt.hist(val_df['score'].values,bins=bins,label='val',alpha=0.75)\n    plt.hist(test_df['score'].values,bins=bins,label='test',alpha=0.4)\n    plt.legend()\n    plt.xlabel(\"seq score\",fontsize=14)\n    plt.ylabel(\"count\",fontsize=14)\n    plt.show()\n\nWith the below histogram, we can confirm that the train, test, and val sets contain example sequences from each bucket of the distribution (each set has some examples with each kind of motif)\n\nplot_train_test_hist(train_df, val_df,test_df)\n\n\n\n\n\n\n\n\n\n\ndefine dataset and dataloader classes\nDataset and DataLoader classes allow efficient data handling in deep learning.\nDataset class allows standardized way to access and preprocess data.\nDataloader handles batching, shuffling, parallel data loading to make it easier to feed the data for training.\nYou can read more about DataLoader and Dataset objects.\n\nfrom torch.utils.data import Dataset, DataLoader\n\n\ndefine one hot encoded dataset class\nThis class is essential for preparing DNA sequence data for deep learning models, converting the DNA sequences into a numerical format that neural networks can process.\n\nclass SeqDatasetOHE(Dataset):\n    '''\n    Dataset for one-hot-encoded sequences\n    '''\n    def __init__(self, df, seq_col='seq', target_col='score'):\n        # Input: DataFrame with DNA sequences and their scores\n        self.seqs = list(df[seq_col].values)  # Get DNA sequences\n        self.seq_len = len(self.seqs[0])      # Length of each sequence\n        \n        # Convert DNA sequences to one-hot encoding\n        self.ohe_seqs = torch.stack([torch.tensor(one_hot_encode(x)) for x in self.seqs])\n        \n        # Get target scores\n        self.labels = torch.tensor(list(df[target_col].values)).unsqueeze(1)\n        \n    def __len__(self): return len(self.seqs)\n    \n    def __getitem__(self,idx):\n        # Given an index, return a tuple of an X with it's associated Y\n        # This is called inside DataLoader\n        seq = self.ohe_seqs[idx]\n        label = self.labels[idx]\n        \n        return seq, label\n\n\n\nconstruct DataLoaders from Datasets.\n\ndef build_dataloaders(train_df,\n                      test_df,\n                      seq_col='seq',\n                      target_col='score',\n                      batch_size=128,\n                      shuffle=True\n                     ):\n    '''\n    Given a train and test df with some batch construction\n    details, put them into custom SeqDatasetOHE() objects. \n    Give the Datasets to the DataLoaders and return.\n    '''\n    \n    # create Datasets    \n    train_ds = SeqDatasetOHE(train_df,seq_col=seq_col,target_col=target_col)\n    test_ds = SeqDatasetOHE(test_df,seq_col=seq_col,target_col=target_col)\n\n    # Put DataSets into DataLoaders\n    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle)\n    test_dl = DataLoader(test_ds, batch_size=batch_size)\n\n    \n    return train_dl,test_dl\n\n\ntrain_dl, val_dl = build_dataloaders(train_df, val_df)\n\nThese dataloaders are now ready to be used in a training loop!\n ## 3. Define Pytorch models The primary model I was interested in trying was a Convolutional Neural Network, as these have been shown to be useful for learning motifs from genomic data. But as a point of comparison, I included a simple Linear model. Here are some model definitions:\n\n# very simple linear model\nclass DNA_Linear(nn.Module):\n    def __init__(self, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        # the 4 is for our one-hot encoded vector length 4!\n        self.lin = nn.Linear(4*seq_len, 1)\n\n    def forward(self, xb):\n        # reshape to flatten sequence dimension\n        xb = xb.view(xb.shape[0],self.seq_len*4)\n        # Linear wraps up the weights/bias dot product operations\n        out = self.lin(xb)\n        return out\n    \n## CNN model\nclass DNA_CNN(nn.Module):\n    def __init__(self, seq_len, num_filters=32, kernel_size=3):\n        super().__init__()\n        self.seq_len = seq_len\n        \n        # Define layers individually\n        self.conv = nn.Conv1d(4, num_filters, kernel_size=kernel_size)\n        self.relu = nn.ReLU(inplace=True)\n        self.linear = nn.Linear(num_filters*(seq_len-kernel_size+1), 1)\n\n    def forward(self, xb):\n        # reshape view to batch_size x 4channel x seq_len\n        xb = xb.permute(0, 2, 1)\n        \n        # Apply layers step by step\n        x = self.conv(xb)\n        x = self.relu(x)\n        x = x.flatten(1)  # flatten all dimensions except batch\n        out = self.linear(x)\n        return out \n\nThese aren’t optimized models, just something to start with (again, we’re just practicing connecting the Pytorch tubes in the context of DNA!). * The Linear model tries to predict the score by simply weighting the nucleotides that appears in each position. * The CNN model uses 32 filters of length (kernel_size) 3 to scan across the 8-mer sequences for informative 3-mer patterns.\n ## 4. Define the training loop functions\nThe model training process is structured into a series of modular functions, each responsible for a specific part of the workflow. This design improves clarity, reusability, and flexibility when working with different models, optimizers, or loss functions.\nThe overall structure is as follows:\n# Initializes optimizer and loss function if not provided, then trains the model\nrun_training()\n\n    # Iterates over multiple epochs\n    train_loop()\n\n        # Performs one complete pass over the training dataset\n        train_epoch()\n\n            # Computes loss and backprops for a single batch\n            process_batch()\n\n        # Performs one complete pass over the validation dataset\n        val_epoch()\n\n            # Computes loss for a single batch no gradient updates\n            process_batch()\n\n# +--------------------------------+\n# | Training and fitting functions |\n# +--------------------------------+\n\ndef process_batch(model, loss_func, xb, yb, opt=None,verbose=False):\n    '''\n    Apply loss function to a batch of inputs. If no optimizer\n    is provided, skip the back prop step.\n    '''\n    if verbose:\n        print('loss batch ****')\n        print(\"xb shape:\",xb.shape)\n        print(\"yb shape:\",yb.shape)\n        print(\"yb shape:\",yb.squeeze(1).shape)\n        #print(\"yb\",yb)\n\n    # get the batch output from the model given your input batch \n    # ** This is the model's prediction for the y labels! **\n    xb_out = model(xb.float())\n    \n    if verbose:\n        print(\"model out pre loss\", xb_out.shape)\n        #print('xb_out', xb_out)\n        print(\"xb_out:\",xb_out.shape)\n        print(\"yb:\",yb.shape)\n        print(\"yb.long:\",yb.long().shape)\n    \n    loss = loss_func(xb_out, yb.float()) # for MSE/regression\n    # __FOOTNOTE 2__\n    \n    if opt is not None: # if opt\n        opt.zero_grad() ## moved zero grad up to make sure it's not accumulating grads from previous batches\n        loss.backward()\n        opt.step()\n    return loss.item(), len(xb)\n\n\ndef train_epoch(model, train_dl, loss_func, device, opt):\n    '''\n    Execute 1 set of batched training within an epoch\n    '''\n    # Set model to Training mode\n    model.train()\n    tl = [] # train losses\n    ns = [] # batch sizes, n\n    \n    # loop through train DataLoader\n    for xb, yb in train_dl:\n        # put on GPU\n        xb, yb = xb.to(device),yb.to(device)\n        \n        # provide opt so backprop happens\n        t, n = process_batch(model, loss_func, xb, yb, opt=opt)\n        \n        # collect train loss and batch sizes\n        tl.append(t)\n        ns.append(n)\n    \n    # average the losses over all batches    \n    train_loss = np.sum(np.multiply(tl, ns)) / np.sum(ns)\n    \n    return train_loss\n\ndef val_epoch(model, val_dl, loss_func, device):\n    '''\n    Execute 1 set of batched validation within an epoch\n    '''\n    # Set model to Evaluation mode\n    model.eval()\n    with torch.no_grad():\n        vl = [] # val losses\n        ns = [] # batch sizes, n\n        \n        # loop through validation DataLoader\n        for xb, yb in val_dl:\n            # put on GPU\n            xb, yb = xb.to(device),yb.to(device)\n\n            # Do NOT provide opt here, so backprop does not happen\n            v, n = process_batch(model, loss_func, xb, yb)\n\n            # collect val loss and batch sizes\n            vl.append(v)\n            ns.append(n)\n\n    # average the losses over all batches\n    val_loss = np.sum(np.multiply(vl, ns)) / np.sum(ns)\n    \n    return val_loss\n\n\ndef train_loop(epochs, model, loss_func, opt, train_dl, val_dl,device,patience=1000):\n    '''\n    Fit the model params to the training data, eval on unseen data.\n    Loop for a number of epochs and keep train of train and val losses \n    along the way\n    '''\n    # keep track of losses\n    train_losses = []    \n    val_losses = []\n    \n    # loop through epochs\n    for epoch in range(epochs):\n        # take a training step\n        train_loss = train_epoch(model,train_dl,loss_func,device,opt)\n        train_losses.append(train_loss)\n\n        # take a validation step\n        val_loss = val_epoch(model,val_dl,loss_func,device)\n        val_losses.append(val_loss)\n        \n        print(f\"E{epoch} | train loss: {train_loss:.3f} | val loss: {val_loss:.3f}\")\n\n    return train_losses, val_losses\n\n\ndef run_training(train_dl,val_dl,model,device,\n              lr=0.01, epochs=50, \n              lossf=None,opt=None\n             ):\n    '''\n    Given train and val DataLoaders and a NN model, fit the model to the training\n    data. By default, use MSE loss and an SGD optimizer\n    '''\n    # define optimizer\n    if opt:\n        optimizer = opt\n    else: # if no opt provided, just use SGD\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n    \n    # define loss function\n    if lossf:\n        loss_func = lossf\n    else: # if no loss function provided, just use MSE\n        loss_func = torch.nn.MSELoss()\n    \n    # run the training loop\n    train_losses, val_losses = train_loop(\n                                epochs, \n                                model, \n                                loss_func, \n                                optimizer, \n                                train_dl, \n                                val_dl, \n                                device)\n\n    return train_losses, val_losses"
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#explanation-of-the-functions-that-make-up-the-training-process",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#explanation-of-the-functions-that-make-up-the-training-process",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "Explanation of the functions that make up the training process",
    "text": "Explanation of the functions that make up the training process\n\nrun_training(): This is the top-level function that orchestrates the entire training process. It handles the initialization of the optimizer and loss function if they are not explicitly provided. It calls the train_loop() function to begin the iterative training loop.\ntrain_loop(): This function manages the training loop across a specified number of epochs. For each epoch, it calls train_epoch() and val_epoch() to perform training and validation, respectively. It also prints the training and validation losses for each epoch.\ntrain_epoch(): This function performs one full pass over the training dataset. It iterates through the train_dl (DataLoader) to get batches of training data. For each batch, it calls process_batch() with the optimizer, enabling backpropagation. It accumulates and averages the training losses.\nval_epoch(): This function performs one full pass over the validation dataset. It iterates through the val_dl (DataLoader) to get batches of validation data. For each batch, it calls process_batch() without the optimizer, preventing gradient updates. It accumulates and averages the validation losses.\nprocess_batch(): This function processes a single batch of data. It calculates the model’s predictions and computes the loss. If an optimizer is provided, it performs backpropagation and updates the model’s weights. If no optimiser is provided, it just returns the loss and batch size. This function is used for both training and validation, with the presence of the optimiser parameter determining if back propagation occurs.\n\n ## 5. Train the models First let’s try running a Linear Model on our 8-mer sequences\n\n# get the sequence length from the first seq in the df\nseq_len = len(train_df['seq'].values[0])\n\n# create Linear model object\nmodel_lin = DNA_Linear(seq_len)\n# use float32 since mps cannot handle 64\nmodel_lin = model_lin.type(torch.float32)\nmodel_lin.to(DEVICE) # put on GPU\n\n\n# run the training pipeline with default settings!\nlin_train_losses, lin_val_losses = run_training(\n    train_dl, \n    val_dl, \n    model_lin,\n    DEVICE\n)\n\nE0 | train loss: 21.238 | val loss: 12.980\nE1 | train loss: 12.969 | val loss: 12.826\nE2 | train loss: 12.918 | val loss: 12.832\nE3 | train loss: 12.915 | val loss: 12.847\nE4 | train loss: 12.916 | val loss: 12.833\nE5 | train loss: 12.918 | val loss: 12.837\nE6 | train loss: 12.915 | val loss: 12.828\nE7 | train loss: 12.917 | val loss: 12.826\nE8 | train loss: 12.917 | val loss: 12.827\nE9 | train loss: 12.917 | val loss: 12.827\nE10 | train loss: 12.918 | val loss: 12.831\nE11 | train loss: 12.914 | val loss: 12.836\nE12 | train loss: 12.918 | val loss: 12.834\nE13 | train loss: 12.916 | val loss: 12.830\nE14 | train loss: 12.917 | val loss: 12.832\nE15 | train loss: 12.917 | val loss: 12.831\nE16 | train loss: 12.917 | val loss: 12.833\nE17 | train loss: 12.915 | val loss: 12.882\nE18 | train loss: 12.916 | val loss: 12.834\nE19 | train loss: 12.916 | val loss: 12.833\nE20 | train loss: 12.917 | val loss: 12.830\nE21 | train loss: 12.918 | val loss: 12.830\nE22 | train loss: 12.917 | val loss: 12.826\nE23 | train loss: 12.914 | val loss: 12.826\nE24 | train loss: 12.915 | val loss: 12.828\nE25 | train loss: 12.916 | val loss: 12.833\nE26 | train loss: 12.916 | val loss: 12.829\nE27 | train loss: 12.916 | val loss: 12.828\nE28 | train loss: 12.918 | val loss: 12.848\nE29 | train loss: 12.916 | val loss: 12.830\nE30 | train loss: 12.916 | val loss: 12.841\nE31 | train loss: 12.917 | val loss: 12.823\nE32 | train loss: 12.917 | val loss: 12.833\nE33 | train loss: 12.916 | val loss: 12.825\nE34 | train loss: 12.918 | val loss: 12.822\nE35 | train loss: 12.916 | val loss: 12.838\nE36 | train loss: 12.917 | val loss: 12.833\nE37 | train loss: 12.914 | val loss: 12.837\nE38 | train loss: 12.917 | val loss: 12.834\nE39 | train loss: 12.917 | val loss: 12.846\nE40 | train loss: 12.916 | val loss: 12.826\nE41 | train loss: 12.917 | val loss: 12.820\nE42 | train loss: 12.917 | val loss: 12.835\nE43 | train loss: 12.916 | val loss: 12.832\nE44 | train loss: 12.916 | val loss: 12.827\nE45 | train loss: 12.915 | val loss: 12.827\nE46 | train loss: 12.916 | val loss: 12.827\nE47 | train loss: 12.918 | val loss: 12.829\nE48 | train loss: 12.915 | val loss: 12.824\nE49 | train loss: 12.917 | val loss: 12.824\n\n\nLet’s look at the loss in quick plot:\n\ndef quick_loss_plot(data_label_list,loss_type=\"MSE Loss\",sparse_n=0):\n    '''\n    For each train/test loss trajectory, plot loss by epoch\n    '''\n    for i,(train_data,test_data,label) in enumerate(data_label_list):    \n        plt.plot(train_data,linestyle='--',color=f\"C{i}\", label=f\"{label} Train\")\n        plt.plot(test_data,color=f\"C{i}\", label=f\"{label} Val\",linewidth=3.0)\n\n    plt.legend()\n    plt.ylabel(loss_type)\n    plt.xlabel(\"Epoch\")\n    plt.legend(bbox_to_anchor=(1,1),loc='upper left')\n    plt.show()\n\n\nlin_data_label = (lin_train_losses,lin_val_losses,\"Lin\")\nquick_loss_plot([lin_data_label])\n\n\n\n\n\n\n\n\nAt first glance, not much learning appears to be happening.\nNext let’s try the CNN.\n\nseq_len = len(train_df['seq'].values[0])\n\n# create Linear model object\nmodel_cnn = DNA_CNN(seq_len)\nmodel_cnn.to(DEVICE) # put on GPU\n\n# run the model with default settings!\ncnn_train_losses, cnn_val_losses = run_training(\n    train_dl, \n    val_dl, \n    model_cnn,\n    DEVICE\n)\n\nE0 | train loss: 14.640 | val loss: 10.167\nE1 | train loss: 8.625 | val loss: 7.035\nE2 | train loss: 6.305 | val loss: 4.967\nE3 | train loss: 4.507 | val loss: 3.257\nE4 | train loss: 3.123 | val loss: 2.256\nE5 | train loss: 2.408 | val loss: 2.627\nE6 | train loss: 1.997 | val loss: 2.078\nE7 | train loss: 1.827 | val loss: 4.446\nE8 | train loss: 1.547 | val loss: 1.297\nE9 | train loss: 1.438 | val loss: 1.185\nE10 | train loss: 1.249 | val loss: 1.108\nE11 | train loss: 1.200 | val loss: 1.149\nE12 | train loss: 1.140 | val loss: 1.096\nE13 | train loss: 1.011 | val loss: 1.102\nE14 | train loss: 1.022 | val loss: 1.188\nE15 | train loss: 1.027 | val loss: 1.119\nE16 | train loss: 1.045 | val loss: 1.040\nE17 | train loss: 0.999 | val loss: 1.052\nE18 | train loss: 0.965 | val loss: 1.069\nE19 | train loss: 0.944 | val loss: 1.208\nE20 | train loss: 0.945 | val loss: 1.175\nE21 | train loss: 0.925 | val loss: 1.038\nE22 | train loss: 0.927 | val loss: 1.249\nE23 | train loss: 0.938 | val loss: 1.022\nE24 | train loss: 0.917 | val loss: 1.042\nE25 | train loss: 0.930 | val loss: 1.062\nE26 | train loss: 0.917 | val loss: 1.089\nE27 | train loss: 0.913 | val loss: 1.286\nE28 | train loss: 0.932 | val loss: 1.041\nE29 | train loss: 0.890 | val loss: 1.126\nE30 | train loss: 0.903 | val loss: 1.038\nE31 | train loss: 0.918 | val loss: 1.033\nE32 | train loss: 0.914 | val loss: 1.048\nE33 | train loss: 0.913 | val loss: 1.060\nE34 | train loss: 0.900 | val loss: 1.236\nE35 | train loss: 0.890 | val loss: 1.030\nE36 | train loss: 0.901 | val loss: 1.066\nE37 | train loss: 0.895 | val loss: 1.055\nE38 | train loss: 0.914 | val loss: 1.030\nE39 | train loss: 0.894 | val loss: 1.024\nE40 | train loss: 0.903 | val loss: 1.056\nE41 | train loss: 0.897 | val loss: 1.089\nE42 | train loss: 0.917 | val loss: 1.031\nE43 | train loss: 0.904 | val loss: 1.105\nE44 | train loss: 0.894 | val loss: 1.284\nE45 | train loss: 0.903 | val loss: 1.648\nE46 | train loss: 0.904 | val loss: 1.042\nE47 | train loss: 0.909 | val loss: 1.052\nE48 | train loss: 0.891 | val loss: 1.186\nE49 | train loss: 0.891 | val loss: 1.075\n\n\n\ncnn_data_label = (cnn_train_losses,cnn_val_losses,\"CNN\")\nquick_loss_plot([lin_data_label,cnn_data_label])\n\n\n\n\n\n\n\n\nIt seems clear from the loss curves that the CNN is able to capture a pattern in the data that the Linear model is not! Let’s spot check a few sequences to see what’s going on.\n\n# oracle dict of true score for each seq\noracle = dict(mer8[['seq','score']].values)\n\ndef quick_seq_pred(model, desc, seqs, oracle):\n    '''\n    Given a model and some sequences, get the model's predictions\n    for those sequences and compare to the oracle (true) output\n    '''\n    print(f\"__{desc}__\")\n    for dna in seqs:\n        s = torch.tensor(one_hot_encode(dna)).unsqueeze(0).to(DEVICE)\n        pred = model(s.float())\n        actual = oracle[dna]\n        diff = pred.item() - actual\n        print(f\"{dna}: pred:{pred.item():.3f} actual:{actual:.3f} ({diff:.3f})\")\n\ndef quick_8mer_pred(model, oracle):\n    seqs1 = (\"poly-X seqs\",['AAAAAAAA', 'CCCCCCCC','GGGGGGGG','TTTTTTTT'])\n    seqs2 = (\"other seqs\", ['AACCAACA','CCGGTGAG','GGGTAAGG', 'TTTCGTTT'])\n    seqsTAT = (\"with TAT motif\", ['TATAAAAA','CCTATCCC','GTATGGGG','TTTATTTT'])\n    seqsGCG = (\"with GCG motif\", ['AAGCGAAA','CGCGCCCC','GGGCGGGG','TTGCGTTT'])\n    TATGCG =  (\"both TAT and GCG\",['ATATGCGA','TGCGTATT'])\n\n    for desc,seqs in [seqs1, seqs2, seqsTAT, seqsGCG, TATGCG]:\n        quick_seq_pred(model, desc, seqs, oracle)\n        print()\n\n\n# Ask the trained Linear model to make \n# predictions for some 8-mers\nquick_8mer_pred(model_lin, oracle)\n\n__poly-X seqs__\nAAAAAAAA: pred:23.415 actual:20.000 (3.415)\nCCCCCCCC: pred:13.762 actual:17.000 (-3.238)\nGGGGGGGG: pred:7.189 actual:14.000 (-6.811)\nTTTTTTTT: pred:17.841 actual:11.000 (6.841)\n\n__other seqs__\nAACCAACA: pred:18.987 actual:18.875 (0.112)\nCCGGTGAG: pred:12.330 actual:15.125 (-2.795)\nGGGTAAGG: pred:14.006 actual:15.125 (-1.119)\nTTTCGTTT: pred:14.945 actual:12.125 (2.820)\n\n__with TAT motif__\nTATAAAAA: pred:22.317 actual:27.750 (-5.433)\nCCTATCCC: pred:17.059 actual:25.875 (-8.816)\nGTATGGGG: pred:12.329 actual:24.000 (-11.671)\nTTTATTTT: pred:18.331 actual:22.125 (-3.794)\n\n__with GCG motif__\nAAGCGAAA: pred:16.972 actual:8.125 (8.847)\nCGCGCCCC: pred:12.467 actual:6.250 (6.217)\nGGGCGGGG: pred:8.121 actual:4.375 (3.746)\nTTGCGTTT: pred:13.014 actual:2.500 (10.514)\n\n__both TAT and GCG__\nATATGCGA: pred:15.874 actual:15.875 (-0.001)\nTGCGTATT: pred:14.779 actual:13.625 (1.154)\n\n\n\nFrom the above examples, it appears that the Linear model is really underpredicting sequences with a lot of G’s and overpredicting those with many T’s. This is probably because it noticed GCG made sequences have unusually low scores and TAT made sequences have unusually high scores, however since the Linear model doesn’t have a way to take into account the different context of GCG vs GAG, it just predicts that sequences with G’s should be lower. We know from our scoring scheme that this isn’t the case: it’s not that G’s in general are detrimental, but specifically GCG is.\n\n# Ask the trained CNN model to make \n# predictions for some 8-mers\nquick_8mer_pred(model_cnn, oracle)\n\n__poly-X seqs__\nAAAAAAAA: pred:19.649 actual:20.000 (-0.351)\nCCCCCCCC: pred:16.750 actual:17.000 (-0.250)\nGGGGGGGG: pred:13.576 actual:14.000 (-0.424)\nTTTTTTTT: pred:10.895 actual:11.000 (-0.105)\n\n__other seqs__\nAACCAACA: pred:18.645 actual:18.875 (-0.230)\nCCGGTGAG: pred:14.759 actual:15.125 (-0.366)\nGGGTAAGG: pred:15.118 actual:15.125 (-0.007)\nTTTCGTTT: pred:11.789 actual:12.125 (-0.336)\n\n__with TAT motif__\nTATAAAAA: pred:26.103 actual:27.750 (-1.647)\nCCTATCCC: pred:24.256 actual:25.875 (-1.619)\nGTATGGGG: pred:22.838 actual:24.000 (-1.162)\nTTTATTTT: pred:20.555 actual:22.125 (-1.570)\n\n__with GCG motif__\nAAGCGAAA: pred:9.007 actual:8.125 (0.882)\nCGCGCCCC: pred:7.091 actual:6.250 (0.841)\nGGGCGGGG: pred:5.275 actual:4.375 (0.900)\nTTGCGTTT: pred:3.411 actual:2.500 (0.911)\n\n__both TAT and GCG__\nATATGCGA: pred:15.389 actual:15.875 (-0.486)\nTGCGTATT: pred:13.185 actual:13.625 (-0.440)\n\n\n\nThe CNN however is better able to adapt to the differences between 3-mer motifs! It predicts quite well on both the sequences with and without motifs."
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-2",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-2",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "Question 2",
    "text": "Question 2\nCompare the performance of the Linear and CNN models by using different learning rates. First run both models with higher learning rates (0.05, 0.1) and lower learning rates (0.005, 0.001), then create loss plots showing: - Linear model with these learning rates - CNN model with these learning rates\nThen analyze your results by answering: 1. How does changing the learning rate affect convergence for each model? 2. Which model is more sensitive to learning rate changes, and why? 3. Based on your analysis, what learning rate would you recommend for each model type, and why?\n ## 6. Check model predictions on the test set An important evaluation step in machine learning tasks is to check if your model can make good predictions on the test set, which it never saw during training. Here, we can use a parity plot to visualize the difference between the actual sequence scores vs the model’s predicted scores.\n\n#%pip install altair ## datapane\n\n\nimport altair as alt\nfrom sklearn.metrics import r2_score\n## import datapane as dp ## compatibility issues with pandas version\nimport os\n\n\n\ndef parity_plot(model_name,df,r2):\n    '''\n    Given a dataframe of samples with their true and predicted values,\n    make a scatterplot.\n    '''\n    plt.scatter(df['truth'].values, df['pred'].values, alpha=0.2)\n    \n    # y=x line\n    xpoints = ypoints = plt.xlim()\n    plt.plot(xpoints, ypoints, linestyle='--', color='k', lw=2, scalex=False, scaley=False)\n\n    plt.ylim(xpoints)\n    plt.ylabel(\"Predicted Score\",fontsize=14)\n    plt.xlabel(\"Actual Score\",fontsize=14)\n    plt.title(f\"{model_name} (r2:{r2:.3f})\",fontsize=20)\n    plt.show()\n\n\ndef alt_parity_plot(model, df, r2, datapane=False):\n    '''\n    Make an interactive parity plot with altair\n    '''\n    import os\n    import altair as alt\n    \n    os.makedirs('alt_out', exist_ok=True)\n    \n    # Convert model name to string to avoid any issues\n    model = str(model)\n    \n    # Create a clean version of the dataframe\n    plot_df = pd.DataFrame({\n        'truth': df['truth'].astype(float),\n        'pred': df['pred'].astype(float),\n        'seq': df['seq'].astype(str)\n    })\n    \n    # Create chart\n    chart = alt.Chart(plot_df).mark_point().encode(\n        x=alt.X('truth', type='quantitative', title='True Values'),\n        y=alt.Y('pred', type='quantitative', title='Predictions'),\n        tooltip=['seq']\n    ).properties(\n        title=str(f'{model} (r2:{r2:.3f})')\n    )\n    \n    chart.save(f'alt_out/parity_plot_{model}.html')\n    display(chart)\n\n\ndef parity_pred(models, seqs, oracle,alt=False,datapane=False):\n    '''Given some sequences, get the model's predictions '''\n    dfs = {} # key: model name, value: parity_df\n    \n    \n    for model_name,model in models:\n        print(f\"Running {model_name}\")\n        data = []\n        for dna in seqs:\n            s = torch.tensor(one_hot_encode(dna)).unsqueeze(0).to(DEVICE)\n            actual = oracle[dna]\n            pred = model(s.float())\n            data.append([dna,actual,pred.item()])\n        df = pd.DataFrame(data, columns=['seq','truth','pred'])\n        r2 = r2_score(df['truth'],df['pred'])\n        dfs[model_name] = (r2,df)\n        \n        #plot parity plot\n        if alt: # make an altair plot\n            alt_parity_plot(model_name, df, r2,datapane=datapane)\n            \n        else:\n            parity_plot(model_name, df, r2)\n\n\nseqs = test_df['seq'].values\nmodels = [\n    (\"Linear\", model_lin),\n    (\"CNN\", model_cnn)\n]\nparity_pred(models, seqs, oracle)\n\nRunning Linear\n\n\n\n\n\n\n\n\n\nRunning CNN\n\n\n\n\n\n\n\n\n\nParity plots are useful for visualizing how well your model predicts individual sequences: in a perfect model, they would all land on the y=x line, meaning that the model prediction was exactly the sequence’s actual value. But if it is off the y=x line, it means the model is over- or under-predicting.\nIn the Linear model, we can see that it can somewhat predict a trend in the Test set sequences, but really gets confused by these buckets of sequences in the high and low areas of the distribution (the ones with a motif).\nHowever for the CNN, it is much better at predicting scores close to the actual value! This is expected, given that the architecture of our CNN uses 3-mer kernels to scan along the sequence for influential motifs.\nBut the CNN isn’t perfect. We could probably train it longer or adjust the hyperparameters, but the goal here isn’t perfection - this is a very simple task relative to actual regulatory grammars. Instead, I thought it would be interesting to use the Altair visualization library to interactively inspect which sequences the models get wrong:\n\nalt.data_transformers.disable_max_rows() # disable altair warning\nparity_pred(models, seqs, oracle,alt=True)\n\nRunning Linear\n\n\n\n\n\n\n\n\nRunning CNN\n\n\n\n\n\n\n\n\nIf you’re viewing this notebook in interactive mode and run the above cell (just viewing via the github preview will omit the altair plot in the rendering), you can hover over the points and see the individual 8-mer sequences (you can also pan and zoom in this plot).\nNotice that the sequences that are off the diagonal tend to have multiple instance of the motifs! In the scoring function, we only gave the sequence a +/- bump if it had at least 1 motif, but it certainly would have been reasonable to decide to add multiple bonuses if the motif was present multiple times. In this example, I arbitrarily only added the bonus for at least 1 motif occurrence, but we could have made a different scoring function.\nIn any case, I thought it was cool that the model noticed the multiple occurrences and predicted them to be important. I suppose we did fool it a little, though an R2 of 0.95 is pretty respectable :)\n ## 7. Visualize convolutional filters When training CNN models, it can be useful to visualize the first layer convolutional filters to try to understand more about what the model is learning. With image data, the first layer convolutional filters often learn patterns such as borders or colors or textures - basic image elements that can be recombined to make more complex features.\nIn DNA, convolutional filters can be thought of like motif scanners. Similar to a position weight matrix for visualizing sequence logos, a convolutional filter is like a matrix showing a particular DNA pattern, but instead of being an exact sequence, it can hold some uncertainty about which nucleotides show up in which part of the pattern. Some positions might be very certain (i.e., there’s always an A in position 2; high information content) while other positions could hold a variety of nucleotides with about equal probability (high entropy; low information content).\nThe calculations that occur within the hidden layers of neural networks can get very complex and not every convolutional filter will be an obviously relevant pattern, but sometimes patterns in the filters do emerge and can be informative for helping to explain the model’s predictions.\nBelow are some functions to visualize the first layer convolutional filters, both as a raw heatmap and as a motif logo."
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#section",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#section",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "===",
    "text": "===\nVisualizing Convolutional Filters When training convolutional neural networks (CNNs), it’s often helpful to visualize the filters in the first convolutional layer to better understand what features the model is learning from the input data.\nIn image data:\nThe first-layer filters commonly learn simple, low-level features like:\nEdges or borders Color gradients Textures or basic shapes\nThese simple elements can be combined in deeper layers to form more complex patterns, such as object parts or entire shapes.\n🧬 In DNA sequence data:\nConvolutional filters act similarly to motif detectors. They can learn biologically meaningful patterns in the sequence, much like:\n\nPosition Weight Matrices (PWMs)\nSequence logos\n\nEach filter can be thought of as a matrix scanning for a specific DNA pattern — not necessarily a fixed sequence, but one that may allow for some variation in certain positions.\nFor example:\nOne position in the filter might strongly prefer an “A” (indicating low entropy and high information content). Another position might tolerate any nucleotide (high entropy, low information content). These learned filters can sometimes correspond to known biological motifs, or reveal novel sequence patterns that are predictive for the task at hand (e.g., enhancer activity, binding sites, expression levels)."
  },
  {
    "objectID": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-3",
    "href": "post/2025-03-25-unit00/updated-basic_DNA_tutorial.html#question-3",
    "title": "Updated - DNA score prediction with Pytorch",
    "section": "Question 3",
    "text": "Question 3\nDesign an approach to improve the model’s prediction accuracy, particularly focusing on the sequences where the current model performs poorly:\n\nAfter identifying sequences where the CNN model has high prediction errors, propose and implement a modification to either the model architecture, the loss function, the training process, or the data representation\nRetrain the model with your modifications\nCreate comparative visualizations (such as parity plots, error histograms, or other appropriate plots) to demonstrate the impact of your changes\nAnalyze your results by discussing how your modification addresses the specific weaknesses you identified. What are the trade-offs involved in your approach?\n\n\nimport logomaker\n\n\ndef get_conv_layers_from_model(model):\n    '''\n    Given a trained model, extract its convolutional layers\n    '''\n    model_children = list(model.children())\n    \n    # counter to keep count of the conv layers\n    model_weights = [] # we will save the conv layer weights in this list\n    conv_layers = [] # we will save the actual conv layers in this list\n    bias_weights = []\n    counter = 0 \n\n    # append all the conv layers and their respective weights to the list\n    for i in range(len(model_children)):\n        # get model type of Conv1d\n        if type(model_children[i]) == nn.Conv1d:\n            counter += 1\n            model_weights.append(model_children[i].weight)\n            conv_layers.append(model_children[i])\n            bias_weights.append(model_children[i].bias)\n\n        # also check sequential objects' children for conv1d\n        elif type(model_children[i]) == nn.Sequential:\n            for child in model_children[i]:\n                if type(child) == nn.Conv1d:\n                    counter += 1\n                    model_weights.append(child.weight)\n                    conv_layers.append(child)\n                    bias_weights.append(child.bias)\n\n    print(f\"Total convolutional layers: {counter}\")\n    return conv_layers, model_weights, bias_weights\n\ndef view_filters(model_weights, num_cols=8):\n    model_weights = model_weights[0]\n    num_filt = model_weights.shape[0]\n    filt_width = model_weights[0].shape[1]\n    num_rows = int(np.ceil(num_filt/num_cols))\n    \n    # visualize the first conv layer filters\n    plt.figure(figsize=(20, 17))\n\n    for i, filter in enumerate(model_weights):\n        ax = plt.subplot(num_rows, num_cols, i+1)\n        ax.imshow(filter.cpu().detach(), cmap='gray')\n        ax.set_yticks(np.arange(4))\n        ax.set_yticklabels(['A', 'C', 'G','T'])\n        ax.set_xticks(np.arange(filt_width))\n        ax.set_title(f\"Filter {i}\")\n\n    plt.tight_layout()\n    plt.show()\n\nFirst, we can take a peek at the raw filters.\n\nconv_layers, model_weights, bias_weights = get_conv_layers_from_model(model_cnn)\nview_filters(model_weights)\n\nTotal convolutional layers: 1\n\n\n\n\n\n\n\n\n\n\n\ndef get_conv_output_for_seq(seq, conv_layer):\n    '''\n    Given an input sequeunce and a convolutional layer, \n    get the output tensor containing the conv filter \n    activations along each position in the sequence\n    '''\n    # format seq for input to conv layer (OHE, reshape)\n    seq = torch.tensor(one_hot_encode(seq)).unsqueeze(0).permute(0,2,1).to(DEVICE)\n\n    # run seq through conv layer\n    with torch.no_grad(): # don't want as part of gradient graph\n        # apply learned filters to input seq\n        res = conv_layer(seq.float())\n        return res[0]\n    \n\ndef get_filter_activations(seqs, conv_layer,act_thresh=0):\n    '''\n    Given a set of input sequences and a trained convolutional layer, \n    determine the subsequences for which each filter in the conv layer \n    activate most strongly. \n    \n    1.) Run seq inputs through conv layer. \n    2.) Loop through filter activations of the resulting tensor, saving the\n            position where filter activations were &gt; act_thresh. \n    3.) Compile a count matrix for each filter by accumulating subsequences which\n            activate the filter above the threshold act_thresh\n    '''\n    # initialize dict of pwms for each filter in the conv layer\n    # pwm shape: 4 nucleotides X filter width, initialize to 0.0s\n    num_filters = conv_layer.out_channels\n    filt_width = conv_layer.kernel_size[0]\n    filter_pwms = dict((i,torch.zeros(4,filt_width)) for i in range(num_filters))\n    \n    print(\"Num filters\", num_filters)\n    print(\"filt_width\", filt_width)\n    \n    # loop through a set of sequences and collect subseqs where each filter activated\n    for seq in seqs:\n        # get a tensor of each conv filter activation along the input seq\n        res = get_conv_output_for_seq(seq, conv_layer)\n\n        # for each filter and it's activation vector\n        for filt_id,act_vec in enumerate(res):\n            # collect the indices where the activation level \n            # was above the threshold\n            act_idxs = torch.where(act_vec&gt;act_thresh)[0]\n            activated_positions = [x.item() for x in act_idxs]\n\n            # use activated indicies to extract the actual DNA\n            # subsequences that caused filter to activate\n            for pos in activated_positions:\n                subseq = seq[pos:pos+filt_width]\n                #print(\"subseq\",pos, subseq)\n                # transpose OHE to match PWM orientation\n                subseq_tensor = torch.tensor(one_hot_encode(subseq)).T\n\n                # add this subseq to the pwm count for this filter\n                filter_pwms[filt_id] += subseq_tensor            \n            \n    return filter_pwms\n\ndef view_filters_and_logos(model_weights,filter_activations, num_cols=8):\n    '''\n    Given some convolutional model weights and filter activation PWMs, \n    visualize the heatmap and motif logo pairs in a simple grid\n    '''\n    model_weights = model_weights[0].squeeze(1)\n    print(model_weights.shape)\n\n    # make sure the model weights agree with the number of filters\n    assert(model_weights.shape[0] == len(filter_activations))\n    \n    num_filts = len(filter_activations)\n    num_rows = int(np.ceil(num_filts/num_cols))*2+1 \n    # ^ not sure why +1 is needed... complained otherwise\n    \n    plt.figure(figsize=(20, 17))\n\n    j=0 # use to make sure a filter and it's logo end up vertically paired\n    for i, filter in enumerate(model_weights):\n        if (i)%num_cols == 0:\n            j += num_cols\n\n        # display raw filter\n        ax1 = plt.subplot(num_rows, num_cols, i+j+1)\n        ax1.imshow(filter.cpu().detach(), cmap='gray')\n        ax1.set_yticks(np.arange(4))\n        ax1.set_yticklabels(['A', 'C', 'G','T'])\n        ax1.set_xticks(np.arange(model_weights.shape[2]))\n        ax1.set_title(f\"Filter {i}\")\n\n        # display sequence logo\n        ax2 = plt.subplot(num_rows, num_cols, i+j+1+num_cols)\n        filt_df = pd.DataFrame(filter_activations[i].T.numpy(),columns=['A','C','G','T'])\n        filt_df_info = logomaker.transform_matrix(filt_df,from_type='counts',to_type='information')\n        logo = logomaker.Logo(filt_df_info,ax=ax2)\n        ax2.set_ylim(0,2)\n        ax2.set_title(f\"Filter {i}\")\n\n    plt.tight_layout()\n\n\n# just use some seqs from test_df to activate filters\nsome_seqs = random.choices(seqs, k=3000)\n\nfilter_activations = get_filter_activations(some_seqs, conv_layers[0])\nview_filters_and_logos(model_weights,filter_activations)\n\nNum filters 32\nfilt_width 3\ntorch.Size([32, 4, 3])\n\n\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n\n\n\n\n\n\n\n\n\n\nVisualize filters using a stronger activation threshold\nact_thresh = 1 instead of 0. (Some filters have no subsequence matches above the threshold and result in an empty motif logo)\n\nfilter_activations = get_filter_activations(some_seqs, conv_layers[0],act_thresh=1)\nview_filters_and_logos(model_weights,filter_activations)\n\nNum filters 32\nfilt_width 3\ntorch.Size([32, 4, 3])\n\n\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n/Users/haekyungim/miniconda3/envs/gene46100/lib/python3.12/site-packages/logomaker/src/Logo.py:1001: UserWarning: Attempting to set identical low and high ylims makes transformation singular; automatically expanding.\n  self.ax.set_ylim([ymin, ymax])\n\n\n\n\n\n\n\n\n\nFrom this particular CNN training, we can see a few filters have picked up on the strong TAT and GCG motifs, but other filters have focused on other patterns as well. There is some debate about how relevant convolutional filter visualizations are for model interpretability. In deep models with multiple convolutional layers, convolutional filters can be recombined in more complex ways inside the hidden layers, so the first layer filters may not be as informative on their own (Koo and Eddy, 2019). Much of the field has since moved towards attention mechanisms and other explainability methods, but should you be curious to visualize your filters as potential motifs, these functions may help get you started!\n ## 8. Conclusion This tutorial shows some basic Pytorch structure for building CNN models that work with DNA sequences. The practice task used in this demo is not reflective of real biological signals; rather, we designed the scoring method to simulate the presence of regulatory motifs in very short sequences that were easy for us humans to inspect and verify that Pytorch was behaving as expected. From this small example, we observed how a basic CNN with sliding filters was able to predict our scoring scheme better than a basic linear model that only accounted for absolute nucleotide position (without local context).\nTo read more about CNN’s applied to DNA in the wild, check out the following foundational papers: * DeepBind: Alipanahi et al 2015 * DeepSea: Zhou and Troyanskaya 2015 * Basset: Kelley et al 2016\nI hope other new-to-ML folks interested in tackling biological questions may find this helpful for getting started with using Pytorch to model DNA sequences :)"
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-prediction-starter.html",
    "href": "post/2025-03-25-unit00/tf-binding-prediction-starter.html",
    "title": "TF Binding prediction project",
    "section": "",
    "text": "TF binding prediction model\nThe goal of this project is to create a neural network model that predicts TF binding strength in a DNA sequence.\nTo do this, we have extracted 300 base pair-long DNA sequences that have a predicted binding site(s) from a TF, from a couple of chromosomes.\nThe training data is the following:\n\nThe sequences files: chr#_sequences.txt.gz store the 300 bp-long DNA sequences. A “window_name” in the format chr#_start_end has been assigned to each one.\nThe scores files: chr#_scores.txt.gz store a 300 bases long vector for each DNA sequence. Each position in these vectors correspond to a the sequence position. The values for each position represent the “binding score” that was predicted to that site by Homer, which is a widely used tool to discover motif binding sites for a given TF across the genome.\n\n\n1. Read-in the data\nThe data files for a couple of chromosomes are stored in the following link. Download them to your local folder.\nLet’s explore how the sequence and score data look like:\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport numpy as np\n\nDefine data paths, this should be changed to your personal paths:\n\nPROJECT = '/Users/sofiasalazar/Library/CloudStorage/Box-Box/imlab-data/Courses/AI-in-Genomics-2025/'\nDATA = os.path.join(PROJECT, 'data')\n\nSequence data for chromosome 22\n\nsequences = pd.read_csv(os.path.join(DATA, 'chr22_sequences.txt.gz'), sep=\"\\t\", compression='gzip')\n\n\nsequences.head()\n\n\n\n\n\n\n\n\nsequence\nwindow_name\n\n\n\n\n0\nGCAAGACTCAGTCTCAAGGAAAAAAAAAAGCTCGAAAAATGTTTGC...\nchr22_10510500_10510799\n\n\n1\nAATCAAAAAGAATATTAGAAAACAAGCTGACAAAAAAATAAAAAAA...\nchr22_10512900_10513199\n\n\n2\nAGAAAAAGATATAAAGGCATCCAAATTGGAAAGGAAGAAGTAAGTA...\nchr22_10514100_10514399\n\n\n3\nCAAATGGATTGAAGACTTAAATGTAAGAACTAAAGCTGTAAAACTA...\nchr22_10515300_10515599\n\n\n4\nAAAATAGACCTACCATATGATGCAGCAATCCCACTTGTGGGCATTT...\nchr22_10515900_10516199\n\n\n\n\n\n\n\n\nsequences.shape\n\n(23139, 2)\n\n\nTF binding scores for chromosome 22\nHere, each column has 300 values for each sequence, each value is the TF binding score for each position of the sequence. Most positions have ‘0’ as no motif is predicted to bind at those positions. One motif is a couple of bp-long and all of those bp will have the same score since they belong to the same motif.\n\nscores = pd.read_csv(os.path.join(DATA, 'chr22_scores.txt.gz'), sep=\"\\t\", compression='gzip')\n\n\nnp.array(scores.iloc[:, 0])\n\narray([0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 9.708916, 9.708916, 9.708916,\n       9.708916, 9.708916, 9.708916, 9.708916, 9.708916, 9.708916,\n       9.708916, 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       7.859208, 7.859208, 7.859208, 7.859208, 7.859208, 7.859208,\n       7.859208, 7.859208, 7.859208, 7.859208, 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       7.693852, 7.693852, 7.693852, 7.693852, 7.693852, 7.693852,\n       7.693852, 7.693852, 7.693852, 7.693852, 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ,\n       0.      , 0.      , 0.      , 0.      , 0.      , 0.      ])\n\n\nTake a look at how these score vectors look like, the blue sections represent the predicted binding sites for this TF:\n\nx = np.arange(300)\nbar_width = 0.4\nplt.figure(figsize=(12, 5))\nplt.bar(x - bar_width, scores.iloc[:, 0], width=bar_width, label=\"Predicted\", alpha=0.7, color='b')\nplt.xlabel(\"Position in sequence window\")\nplt.ylabel(\"Homer score\")\nplt.grid(axis='y', linestyle='--', alpha=0.6)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n2. Model training\nNow, the goal is to use these sequences to train a neural network model that predicts the scores vectors. Overall, the structure of the code should more or less follow these steps:\n\nOne-hot-encode the DNA sequences\nSplit sequences and their corresponding scores into training, test and validation sets\nBuild dataloaders for the training and test sets using sequences as predictor features and the scores as targets\nDefine a NN model architecture\nTrain the model\nTest the model on the test sequences\n\nThis process will be iterative as you find an optimal set of hyperparameters. Please share your best-performing model and we will test it on a set of held-out-data.\nAdditional notes\n\nNote how the scores are values greater than 1, you can try binarizing these values so they are between 0 and 1 and compare between models\nTo assess performance, you can use the following code to compute correlations between predicted scores and the ground truth\n\n\nfrom scipy.stats import pearsonr\ndef plot_comparison(pred, obs):\n  r_value = pearsonr(pred, obs)\n  x = np.arange(len(pred))\n  bar_width = 0.4\n  plt.figure(figsize=(12, 5))\n  plt.bar(x - bar_width, pred, width=bar_width, label=\"Predicted\", alpha=0.7, color='b')\n  plt.bar(x + bar_width, obs, width=bar_width, label=\"Observed\", alpha=0.7, color='r')\n  plt.xlabel(\"Position sequence window\")\n  plt.ylabel(\"Value\")\n  plt.title(\"Comparison of sequence scores\")\n  plt.legend(title=f\"Pearson R: {r_value:.2f}\")\n  plt.grid(axis='y', linestyle='--', alpha=0.6)\n  plt.show()\n\n\n\n\n\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "DRAFTS/measuring-info-in-DNA-seq.html",
    "href": "DRAFTS/measuring-info-in-DNA-seq.html",
    "title": "Measuring information in DNA sequence",
    "section": "",
    "text": "In the analysis of DNA sequences, particularly when studying motifs and patterns, two key concepts from information theory are entropy and information content. These measures help quantify the variability and conservation of nucleotides at specific positions within a set of aligned sequences.\n\nEntropy (H):\n\nEntropy measures the uncertainty or randomness at a given position. For DNA with four possible bases (A, C, G, T) and their respective probabilities at a position being p(A), p(C), p(G), and p(T) (where p(A) + p(C) + p(G) + p(T) = 1), the Shannon entropy (H) in bits is calculated using the following formula:\nH = - (p(A) * log2(p(A)) + p(C) * log2(p(C)) + p(G) * log2(p(G)) + p(T) * log2(p(T))) A higher entropy value indicates greater uncertainty or variability in the nucleotides at that position. A lower entropy value indicates less uncertainty, meaning one or a few nucleotides are more dominant. The maximum possible entropy for DNA (when all four bases are equally likely) is 2 bits. The minimum possible entropy (when only one base is present) is 0 bits. 2. Information Content (IC):\nInformation content measures the amount of information gained by knowing the nucleotide at a particular position. It reflects the conservation of that position relative to a background distribution (often assumed to be uniform). The information content (IC) in bits is calculated as:\nIC = log2(N) - H Where:\nN is the number of possible nucleotides (4 for DNA).\nlog2(N) represents the maximum possible information content (2 bits for DNA).\nH is the Shannon entropy calculated for that position.\nA higher information content value indicates greater conservation and importance of specific nucleotides at that position.\nA lower information content value indicates less conservation and more variability.\nThe maximum possible information content for DNA is 2 bits (when entropy is 0).\nThe minimum possible information content (when entropy is maximum) is 0 bits.\n\nimport math\n\ndef calculate_entropy(pA, pC, pG, pT):\n    \"\"\"\n    Calculates the Shannon entropy for DNA bases given their probabilities.\n\n    Args:\n        pA (float): Probability of Adenine (A).\n        pC (float): Probability of Cytosine (C).\n        pG (float): Probability of Guanine (G).\n        pT (float): Probability of Thymine (T).\n\n    Returns:\n        float: The Shannon entropy in bits.\n    \"\"\"\n    entropy = 0\n    if pA &gt; 0:\n        entropy -= pA * math.log2(pA)\n    if pC &gt; 0:\n        entropy -= pC * math.log2(pC)\n    if pG &gt; 0:\n        entropy -= pG * math.log2(pG)\n    if pT &gt; 0:\n        entropy -= pT * math.log2(pT)\n    return entropy\n\ndef calculate_information_content(pA, pC, pG, pT):\n    \"\"\"\n    Calculates the information content (in bits) at a given position\n    based on the nucleotide probabilities.\n\n    Args:\n        pA (float): Probability of Adenine (A).\n        pC (float): Probability of Cytosine (C).\n        pG (float): Probability of Guanine (G).\n        pT (float): Probability of Thymine (T).\n\n    Returns:\n        float: The information content in bits.\n    \"\"\"\n    num_nucleotides = 4  # A, C, G, T\n    max_entropy = math.log2(num_nucleotides)  # Maximum entropy for 4 equally likely bases\n    entropy = calculate_entropy(pA, pC, pG, pT)\n    information_content = max_entropy - entropy\n    return information_content\n\n# Example Usage:\nprobability_A = 0.3\nprobability_C = 0.2\nprobability_G = 0.25\nprobability_T = 0.25\n\nentropy_value = calculate_entropy(probability_A, probability_C, probability_G, probability_T)\ninformation_content_value = calculate_information_content(probability_A, probability_C, probability_G, probability_T)\n\nprint(f\"Probabilities: p(A)={probability_A:.2f}, p(C)={probability_C:.2f}, p(G)={probability_G:.2f}, p(T)={probability_T:.2f}\")\nprint(f\"Entropy: {entropy_value:.4f} bits\")\nprint(f\"Information Content: {information_content_value:.4f} bits\")\n\n# Example with uniform distribution:\nentropy_uniform = calculate_entropy(0.25, 0.25, 0.25, 0.25)\ninformation_content_uniform = calculate_information_content(0.25, 0.25, 0.25, 0.25)\nprint(f\"\\nProbabilities (uniform): p(A)=0.25, p(C)=0.25, p(G)=0.25, p(T)=0.25\")\nprint(f\"Entropy (uniform): {entropy_uniform:.4f} bits\")\nprint(f\"Information Content (uniform): {information_content_uniform:.4f} bits\")\n\n# Example with a fully conserved position:\nentropy_conserved = calculate_entropy(1.0, 0.0, 0.0, 0.0)\ninformation_content_conserved = calculate_information_content(1.0, 0.0, 0.0, 0.0)\nprint(f\"\\nProbabilities (fully conserved): p(A)=1.00, p(C)=0.00, p(G)=0.00, p(T)=0.00\")\nprint(f\"Entropy (fully conserved): {entropy_conserved:.4f} bits\")\nprint(f\"Information Content (fully conserved): {information_content_conserved:.4f} bits\")\n\nProbabilities: p(A)=0.30, p(C)=0.20, p(G)=0.25, p(T)=0.25\nEntropy: 1.9855 bits\nInformation Content: 0.0145 bits\n\nProbabilities (uniform): p(A)=0.25, p(C)=0.25, p(G)=0.25, p(T)=0.25\nEntropy (uniform): 2.0000 bits\nInformation Content (uniform): 0.0000 bits\n\nProbabilities (fully conserved): p(A)=1.00, p(C)=0.00, p(G)=0.00, p(T)=0.00\nEntropy (fully conserved): 0.0000 bits\nInformation Content (fully conserved): 2.0000 bits\n\n\n** text and code generated with gemini 2.0 via a series of prompting **\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "DRAFTS/measuring-info-in-DNA-seq.html#understanding-entropy-and-information-content-in-dna-sequence-analysis",
    "href": "DRAFTS/measuring-info-in-DNA-seq.html#understanding-entropy-and-information-content-in-dna-sequence-analysis",
    "title": "Measuring information in DNA sequence",
    "section": "",
    "text": "In the analysis of DNA sequences, particularly when studying motifs and patterns, two key concepts from information theory are entropy and information content. These measures help quantify the variability and conservation of nucleotides at specific positions within a set of aligned sequences.\n\nEntropy (H):\n\nEntropy measures the uncertainty or randomness at a given position. For DNA with four possible bases (A, C, G, T) and their respective probabilities at a position being p(A), p(C), p(G), and p(T) (where p(A) + p(C) + p(G) + p(T) = 1), the Shannon entropy (H) in bits is calculated using the following formula:\nH = - (p(A) * log2(p(A)) + p(C) * log2(p(C)) + p(G) * log2(p(G)) + p(T) * log2(p(T))) A higher entropy value indicates greater uncertainty or variability in the nucleotides at that position. A lower entropy value indicates less uncertainty, meaning one or a few nucleotides are more dominant. The maximum possible entropy for DNA (when all four bases are equally likely) is 2 bits. The minimum possible entropy (when only one base is present) is 0 bits. 2. Information Content (IC):\nInformation content measures the amount of information gained by knowing the nucleotide at a particular position. It reflects the conservation of that position relative to a background distribution (often assumed to be uniform). The information content (IC) in bits is calculated as:\nIC = log2(N) - H Where:\nN is the number of possible nucleotides (4 for DNA).\nlog2(N) represents the maximum possible information content (2 bits for DNA).\nH is the Shannon entropy calculated for that position.\nA higher information content value indicates greater conservation and importance of specific nucleotides at that position.\nA lower information content value indicates less conservation and more variability.\nThe maximum possible information content for DNA is 2 bits (when entropy is 0).\nThe minimum possible information content (when entropy is maximum) is 0 bits.\n\nimport math\n\ndef calculate_entropy(pA, pC, pG, pT):\n    \"\"\"\n    Calculates the Shannon entropy for DNA bases given their probabilities.\n\n    Args:\n        pA (float): Probability of Adenine (A).\n        pC (float): Probability of Cytosine (C).\n        pG (float): Probability of Guanine (G).\n        pT (float): Probability of Thymine (T).\n\n    Returns:\n        float: The Shannon entropy in bits.\n    \"\"\"\n    entropy = 0\n    if pA &gt; 0:\n        entropy -= pA * math.log2(pA)\n    if pC &gt; 0:\n        entropy -= pC * math.log2(pC)\n    if pG &gt; 0:\n        entropy -= pG * math.log2(pG)\n    if pT &gt; 0:\n        entropy -= pT * math.log2(pT)\n    return entropy\n\ndef calculate_information_content(pA, pC, pG, pT):\n    \"\"\"\n    Calculates the information content (in bits) at a given position\n    based on the nucleotide probabilities.\n\n    Args:\n        pA (float): Probability of Adenine (A).\n        pC (float): Probability of Cytosine (C).\n        pG (float): Probability of Guanine (G).\n        pT (float): Probability of Thymine (T).\n\n    Returns:\n        float: The information content in bits.\n    \"\"\"\n    num_nucleotides = 4  # A, C, G, T\n    max_entropy = math.log2(num_nucleotides)  # Maximum entropy for 4 equally likely bases\n    entropy = calculate_entropy(pA, pC, pG, pT)\n    information_content = max_entropy - entropy\n    return information_content\n\n# Example Usage:\nprobability_A = 0.3\nprobability_C = 0.2\nprobability_G = 0.25\nprobability_T = 0.25\n\nentropy_value = calculate_entropy(probability_A, probability_C, probability_G, probability_T)\ninformation_content_value = calculate_information_content(probability_A, probability_C, probability_G, probability_T)\n\nprint(f\"Probabilities: p(A)={probability_A:.2f}, p(C)={probability_C:.2f}, p(G)={probability_G:.2f}, p(T)={probability_T:.2f}\")\nprint(f\"Entropy: {entropy_value:.4f} bits\")\nprint(f\"Information Content: {information_content_value:.4f} bits\")\n\n# Example with uniform distribution:\nentropy_uniform = calculate_entropy(0.25, 0.25, 0.25, 0.25)\ninformation_content_uniform = calculate_information_content(0.25, 0.25, 0.25, 0.25)\nprint(f\"\\nProbabilities (uniform): p(A)=0.25, p(C)=0.25, p(G)=0.25, p(T)=0.25\")\nprint(f\"Entropy (uniform): {entropy_uniform:.4f} bits\")\nprint(f\"Information Content (uniform): {information_content_uniform:.4f} bits\")\n\n# Example with a fully conserved position:\nentropy_conserved = calculate_entropy(1.0, 0.0, 0.0, 0.0)\ninformation_content_conserved = calculate_information_content(1.0, 0.0, 0.0, 0.0)\nprint(f\"\\nProbabilities (fully conserved): p(A)=1.00, p(C)=0.00, p(G)=0.00, p(T)=0.00\")\nprint(f\"Entropy (fully conserved): {entropy_conserved:.4f} bits\")\nprint(f\"Information Content (fully conserved): {information_content_conserved:.4f} bits\")\n\nProbabilities: p(A)=0.30, p(C)=0.20, p(G)=0.25, p(T)=0.25\nEntropy: 1.9855 bits\nInformation Content: 0.0145 bits\n\nProbabilities (uniform): p(A)=0.25, p(C)=0.25, p(G)=0.25, p(T)=0.25\nEntropy (uniform): 2.0000 bits\nInformation Content (uniform): 0.0000 bits\n\nProbabilities (fully conserved): p(A)=1.00, p(C)=0.00, p(G)=0.00, p(T)=0.00\nEntropy (fully conserved): 0.0000 bits\nInformation Content (fully conserved): 2.0000 bits\n\n\n** text and code generated with gemini 2.0 via a series of prompting **"
  },
  {
    "objectID": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html",
    "href": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html",
    "title": "Fit linear mode to linear data",
    "section": "",
    "text": "We generate a data Y linearly dependent on features X: Y = X\\beta + \\epsilon\nWe fit a linear model\n\nwith gradient descent\nwith pytorch framework\nwith traditional linear regression estimate \\hat\\beta = (X^TX)^{-1}X^TY\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nfrom plotnine import *\n\n# Generate synthetic data\nnp.random.seed(42)\nX = np.random.randn(100, 1) * 2  # 100 samples, 1 feature\ny = 3 * X + 2 + np.random.randn(100, 1) * 0.5  # y = 3x + 2 + noise\n\n# Convert to PyTorch tensors\nX_tensor = torch.FloatTensor(X)\ny_tensor = torch.FloatTensor(y)\n\n# define a linear model\nclass LinearModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(1, 1)  # 1 input feature, 1 output\n    def forward(self, x):\n        return self.linear(x)\n\n# instantiate the model\nmodel = LinearModel()\n\n# define a loss function\nloss_fn = nn.MSELoss()\n\n# define an optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Train: iterate over the data and update the model\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Forward pass: compute model predictions and loss\n    y_pred = model(X_tensor)\n    loss = loss_fn(y_pred, y_tensor)\n    \n    # Backward pass and parameter update\n    optimizer.zero_grad()  # Clear previous gradients\n    loss.backward()        # Compute gradients via backpropagation\n    optimizer.step()       # Update model parameters\n    \n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# check performance\n\n# compute PyTorch model predictions\nmodel.eval()\nwith torch.no_grad():\n    y_pred_pytorch = model(X_tensor).numpy()\n\n# compute scikit-learn predictions\nlr = LinearRegression()\nlr.fit(X, y)\ny_pred_sklearn = lr.predict(X)\n\n# compare coefficients\nprint(\"\\nPyTorch model coefficients:\")\nprint(f\"Slope: {model.linear.weight.item():.4f}\")\nprint(f\"Intercept: {model.linear.bias.item():.4f}\")\n\nprint(\"\\nScikit-learn coefficients:\")\nprint(f\"Slope: {lr.coef_[0][0]:.4f}\")\nprint(f\"Intercept: {lr.intercept_[0]:.4f}\")\n\n# compare MSE\nmse_pytorch = np.mean((y - y_pred_pytorch) ** 2)\nmse_sklearn = np.mean((y - y_pred_sklearn) ** 2)\nprint(f\"\\nPyTorch MSE: {mse_pytorch:.4f}\")\nprint(f\"Scikit-learn MSE: {mse_sklearn:.4f}\")\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html#fit-a-linear-model-to-linear-data",
    "href": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html#fit-a-linear-model-to-linear-data",
    "title": "Fit linear mode to linear data",
    "section": "",
    "text": "We generate a data Y linearly dependent on features X: Y = X\\beta + \\epsilon\nWe fit a linear model\n\nwith gradient descent\nwith pytorch framework\nwith traditional linear regression estimate \\hat\\beta = (X^TX)^{-1}X^TY\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nfrom plotnine import *\n\n# Generate synthetic data\nnp.random.seed(42)\nX = np.random.randn(100, 1) * 2  # 100 samples, 1 feature\ny = 3 * X + 2 + np.random.randn(100, 1) * 0.5  # y = 3x + 2 + noise\n\n# Convert to PyTorch tensors\nX_tensor = torch.FloatTensor(X)\ny_tensor = torch.FloatTensor(y)\n\n# define a linear model\nclass LinearModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(1, 1)  # 1 input feature, 1 output\n    def forward(self, x):\n        return self.linear(x)\n\n# instantiate the model\nmodel = LinearModel()\n\n# define a loss function\nloss_fn = nn.MSELoss()\n\n# define an optimizer\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Train: iterate over the data and update the model\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Forward pass: compute model predictions and loss\n    y_pred = model(X_tensor)\n    loss = loss_fn(y_pred, y_tensor)\n    \n    # Backward pass and parameter update\n    optimizer.zero_grad()  # Clear previous gradients\n    loss.backward()        # Compute gradients via backpropagation\n    optimizer.step()       # Update model parameters\n    \n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# check performance\n\n# compute PyTorch model predictions\nmodel.eval()\nwith torch.no_grad():\n    y_pred_pytorch = model(X_tensor).numpy()\n\n# compute scikit-learn predictions\nlr = LinearRegression()\nlr.fit(X, y)\ny_pred_sklearn = lr.predict(X)\n\n# compare coefficients\nprint(\"\\nPyTorch model coefficients:\")\nprint(f\"Slope: {model.linear.weight.item():.4f}\")\nprint(f\"Intercept: {model.linear.bias.item():.4f}\")\n\nprint(\"\\nScikit-learn coefficients:\")\nprint(f\"Slope: {lr.coef_[0][0]:.4f}\")\nprint(f\"Intercept: {lr.intercept_[0]:.4f}\")\n\n# compare MSE\nmse_pytorch = np.mean((y - y_pred_pytorch) ** 2)\nmse_sklearn = np.mean((y - y_pred_sklearn) ** 2)\nprint(f\"\\nPyTorch MSE: {mse_pytorch:.4f}\")\nprint(f\"Scikit-learn MSE: {mse_sklearn:.4f}\")"
  },
  {
    "objectID": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html#plot-predictions-vs-true",
    "href": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html#plot-predictions-vs-true",
    "title": "Fit linear mode to linear data",
    "section": "plot predictions vs true",
    "text": "plot predictions vs true\n\n# Create a plot comparing predictions\n# Create a DataFrame for plotting\nplot_df = pd.DataFrame({\n    'y_true': y.flatten(),\n    'y_pred_pytorch': y_pred_pytorch.flatten(),\n    'y_pred_sklearn': y_pred_sklearn.flatten()\n})\n\n# Create the plot\np = (ggplot(plot_df)\n    + geom_point(aes(x='y_true', y='y_pred_pytorch', color='\"PyTorch\"'), size=1)\n    + geom_point(aes(x='y_true', y='y_pred_sklearn', color='\"Scikit-learn\"'), size=1)\n    + geom_abline(intercept=0, slope=1, color='gray', linetype='dashed', alpha=0.5)  # Identity line\n    + scale_color_manual(values=['red', 'blue'])\n    + labs(x='True Values', y='Predicted Values', \n           title='True vs Predicted Values',\n           color='Model')\n    + theme_minimal()\n)\n\np\n\n\n\n\n\n\n\n\n\n(ggplot(plot_df)\n    + geom_point(aes(x='y_pred_sklearn', y='y_pred_pytorch'), size=1)\n    + geom_abline(intercept=0, slope=1, color='gray', linetype='dashed', alpha=0.5)  # Identity line\n    + labs(x='Y sklearn', y='Y pytorch', \n           title='ptorch vs sklearn predictions',\n           color='Model')\n    + theme_minimal()\n)"
  },
  {
    "objectID": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html#fit-linear-model-using-the-gpu",
    "href": "DRAFTS/2020-01-01-TODO/fit-linear-model-to-linear.html#fit-linear-model-using-the-gpu",
    "title": "Fit linear mode to linear data",
    "section": "Fit linear model using the GPU",
    "text": "Fit linear model using the GPU\n\n# Train on GPU (checking for both MPS and CUDA)\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nfrom plotnine import *\n\n# Check for available GPU devices\nif torch.backends.mps.is_available():\n    device = torch.device('mps')  # Apple Silicon\n    print(\"Using MPS (Apple Silicon GPU)\")\nelif torch.cuda.is_available():\n    device = torch.device('cuda')  # NVIDIA GPU\n    print(\"Using CUDA (NVIDIA GPU)\")\nelse:\n    device = torch.device('cpu')  # Fallback to CPU\n    print(\"Using CPU (no GPU available)\")\n\n# Generate synthetic data (stays on CPU)\nnp.random.seed(42)\nX = np.random.randn(100, 1) * 2\ny = 3 * X + 2 + np.random.randn(100, 1) * 0.5\n\n# Convert to PyTorch tensors and move to GPU\nX_tensor = torch.FloatTensor(X).to(device)\ny_tensor = torch.FloatTensor(y).to(device)\n\n# Define and move model to GPU\nclass LinearModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(1, 1)\n\n    def forward(self, x):\n        return self.linear(x)\n\nmodel = LinearModel().to(device)\nloss_fn = nn.MSELoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# Training loop (same as before, but data is on GPU)\nnum_epochs = 100\nfor epoch in range(num_epochs):\n    # Forward pass\n    y_pred = model(X_tensor)\n    loss = loss_fn(y_pred, y_tensor)\n    \n    # Backward pass and optimize\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Get predictions back to CPU for comparison\nmodel.eval()\nwith torch.no_grad():\n    y_pred_pytorch = model(X_tensor).cpu().numpy()  # Move back to CPU before converting to numpy\n\n# Compare with scikit-learn (on CPU)\nlr = LinearRegression()\nlr.fit(X, y)\ny_pred_sklearn = lr.predict(X)\n\n# Compare coefficients\nprint(\"\\nPyTorch model coefficients:\")\nprint(f\"Slope: {model.linear.weight.item():.4f}\")\nprint(f\"Intercept: {model.linear.bias.item():.4f}\")\n\nprint(\"\\nScikit-learn coefficients:\")\nprint(f\"Slope: {lr.coef_[0][0]:.4f}\")\nprint(f\"Intercept: {lr.intercept_[0]:.4f}\")\n\n# Compare MSE\nmse_pytorch = np.mean((y - y_pred_pytorch) ** 2)\nmse_sklearn = np.mean((y - y_pred_sklearn) ** 2)\nprint(f\"\\nPyTorch MSE: {mse_pytorch:.4f}\")\nprint(f\"Scikit-learn MSE: {mse_sklearn:.4f}\")\n\n# Create a plot comparing predictions\nplot_df = pd.DataFrame({\n    'y_true': y.flatten(),\n    'y_pred_pytorch': y_pred_pytorch.flatten(),\n    'y_pred_sklearn': y_pred_sklearn.flatten()\n})\n\n# Create the plot\np = (ggplot(plot_df)\n    + geom_point(aes(x='y_true', y='y_pred_pytorch', color='\"PyTorch\"'), size=1)\n    + geom_point(aes(x='y_true', y='y_pred_sklearn', color='\"Scikit-learn\"'), size=1)\n    + geom_abline(intercept=0, slope=1, color='gray', linetype='dashed', alpha=0.5)  # Identity line\n    + scale_color_manual(values=['red', 'blue'])\n    + labs(x='True Values', y='Predicted Values', \n           title=f'True vs Predicted Values (Training on {device})',\n           color='Model')\n    + theme_minimal()\n)\n\np"
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-challenge.html",
    "href": "post/2025-03-25-unit00/tf-binding-challenge.html",
    "title": "TF binding prediction challenge",
    "section": "",
    "text": "Goal: Predict transcription factor (TF) binding scores in DNA sequences\nInput: 300bp human DNA sequences\nTarget: Binding scores for transcription factors\n\n\n\n\nThe challenge uses real genomic data to predict TF binding scores:\n\nSequence Data: chr#_sequences.txt.gz files containing 300bp DNA sequences\n\nEach sequence has a unique identifier in format chr#_start_end\n\nTarget Data: chr#_scores.txt.gz files containing binding scores\n\nEach sequence has a corresponding 300-long vector of binding scores\nScores are predicted using Homer, a widely used motif discovery tool\nEach position in the vector represents the binding score at that position in the sequence\n\n\nData prepared by Sofia Salazar.\n\n\n\n\nData download link\nStarter notebook\nExample implementation\n\n\n\n\n\nTraining Sessions:\n\nTuesday, April 8: Sofia will review implementation of using the code in the basic DNA scoring model. Students will continue working on the project. Charles, Sofia, and Ran will be available to help.\nThursday, April 10: Sofia will explain how to use weights and biases to calibrate hyperparameters of the model (learning rate, number of filters, kernel size, etc). Charles, Sofia, Ran, and Haky will be available to help.\n\nPresentation Day (Thursday, April 10): Students will present:\n\nModel architecture\nModel performance\nFilter interpretation (time permitting)\nLessons learned\n\nSubmission Deadline: April 18\n\nSubmit best model to Canvas\nTA will test on held-out data\nLeaderboard will be created\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-challenge.html#overview",
    "href": "post/2025-03-25-unit00/tf-binding-challenge.html#overview",
    "title": "TF binding prediction challenge",
    "section": "",
    "text": "Goal: Predict transcription factor (TF) binding scores in DNA sequences\nInput: 300bp human DNA sequences\nTarget: Binding scores for transcription factors"
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-challenge.html#data-description",
    "href": "post/2025-03-25-unit00/tf-binding-challenge.html#data-description",
    "title": "TF binding prediction challenge",
    "section": "",
    "text": "The challenge uses real genomic data to predict TF binding scores:\n\nSequence Data: chr#_sequences.txt.gz files containing 300bp DNA sequences\n\nEach sequence has a unique identifier in format chr#_start_end\n\nTarget Data: chr#_scores.txt.gz files containing binding scores\n\nEach sequence has a corresponding 300-long vector of binding scores\nScores are predicted using Homer, a widely used motif discovery tool\nEach position in the vector represents the binding score at that position in the sequence\n\n\nData prepared by Sofia Salazar."
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-challenge.html#getting-started",
    "href": "post/2025-03-25-unit00/tf-binding-challenge.html#getting-started",
    "title": "TF binding prediction challenge",
    "section": "",
    "text": "Data download link\nStarter notebook\nExample implementation"
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-challenge.html#timeline",
    "href": "post/2025-03-25-unit00/tf-binding-challenge.html#timeline",
    "title": "TF binding prediction challenge",
    "section": "",
    "text": "Training Sessions:\n\nTuesday, April 8: Sofia will review implementation of using the code in the basic DNA scoring model. Students will continue working on the project. Charles, Sofia, and Ran will be available to help.\nThursday, April 10: Sofia will explain how to use weights and biases to calibrate hyperparameters of the model (learning rate, number of filters, kernel size, etc). Charles, Sofia, Ran, and Haky will be available to help.\n\nPresentation Day (Thursday, April 10): Students will present:\n\nModel architecture\nModel performance\nFilter interpretation (time permitting)\nLessons learned\n\nSubmission Deadline: April 18\n\nSubmit best model to Canvas\nTA will test on held-out data\nLeaderboard will be created"
  },
  {
    "objectID": "post/2025-03-25-unit00/homework-03.html",
    "href": "post/2025-03-25-unit00/homework-03.html",
    "title": "homework 3",
    "section": "",
    "text": "Implement hyperparameter tuning using Weights & Biases (wandb)\nChoose one of the following problems to apply this to:\n\nTF binding prediction\nSimple DNA scoring\nCubic function with MLP\n\nReference example: TF Binding with wandb\nDocument the hyperparameters you tuned and their impact on model performance\n\n\n\n\n\nUpload your complete Jupyter notebook containing:\n\nModel architecture\nTraining code\nEvaluation metrics\nPrediction code\n\nInclude a separate, self-contained prediction script that:\n\nLoads all required packages\nLoads the trained model architecture and weights\nMakes predictions on the hold-out test set\nCan be run independently without executing the entire notebook\n\n\n\n\n\n\nSubmit your Jupyter notebook with all code and documentation\nInclude a separate prediction script as described above\nDocument your hyperparameter tuning process and results\nProvide clear instructions for running the prediction script\n\n\n\n\n\nProper implementation of hyperparameter tuning\nModel performance on test set\nCode organization and documentation\nReproducibility of results\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/homework-03.html#part-1-hyperparameter-calibration-with-weights-biases",
    "href": "post/2025-03-25-unit00/homework-03.html#part-1-hyperparameter-calibration-with-weights-biases",
    "title": "homework 3",
    "section": "",
    "text": "Implement hyperparameter tuning using Weights & Biases (wandb)\nChoose one of the following problems to apply this to:\n\nTF binding prediction\nSimple DNA scoring\nCubic function with MLP\n\nReference example: TF Binding with wandb\nDocument the hyperparameters you tuned and their impact on model performance"
  },
  {
    "objectID": "post/2025-03-25-unit00/homework-03.html#part-2-model-deployment-and-testing",
    "href": "post/2025-03-25-unit00/homework-03.html#part-2-model-deployment-and-testing",
    "title": "homework 3",
    "section": "",
    "text": "Upload your complete Jupyter notebook containing:\n\nModel architecture\nTraining code\nEvaluation metrics\nPrediction code\n\nInclude a separate, self-contained prediction script that:\n\nLoads all required packages\nLoads the trained model architecture and weights\nMakes predictions on the hold-out test set\nCan be run independently without executing the entire notebook"
  },
  {
    "objectID": "post/2025-03-25-unit00/homework-03.html#submission-requirements",
    "href": "post/2025-03-25-unit00/homework-03.html#submission-requirements",
    "title": "homework 3",
    "section": "",
    "text": "Submit your Jupyter notebook with all code and documentation\nInclude a separate prediction script as described above\nDocument your hyperparameter tuning process and results\nProvide clear instructions for running the prediction script"
  },
  {
    "objectID": "post/2025-03-25-unit00/homework-03.html#evaluation-criteria",
    "href": "post/2025-03-25-unit00/homework-03.html#evaluation-criteria",
    "title": "homework 3",
    "section": "",
    "text": "Proper implementation of hyperparameter tuning\nModel performance on test set\nCode organization and documentation\nReproducibility of results"
  },
  {
    "objectID": "post/2025-03-25-unit00/homework-01.html",
    "href": "post/2025-03-25-unit00/homework-01.html",
    "title": "homework 1",
    "section": "",
    "text": "Homework 1\n\nset up the environment on macos as described in the page (10 points)\n\nUpload the environment-gene46100.yml and requirements-gene46100.txt with the canvas assignment\nIf you have any issues, please let me know.\n\ngit clone the course repository to your local machine (10 points)\n\nUpload the screenshot of the terminal command you used to clone the repository with the canvas assignment\n\nrun all the code in the notebook. You should also be able to find the notebook in your cloned course repository. (20 points)\ncomplete the exercises in the notebook (30 points)\nsubmit the jupyter notebook with the results to canvas assignment\n\n\n\n\n\n\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "index-all.html",
    "href": "index-all.html",
    "title": "Deep Learning in Genomics Course Material - ALL",
    "section": "",
    "text": "This page contains ALL material for the GENE46100 Deep learning in genomics course, including test notes.\nFind the 2025 syllabus here.\nEdit github source here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunit 03\n\n\n\n\n\n\ngene46100\n\n\n\n\n\n\n\n\n\nMay 1, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nEmbeddings Analysis\n\n\n\n\n\n\n\n\n\n\n\nApr 23, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 4\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nApr 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a GPT - companion notebook annotated\n\n\n\n\n\nCompanion notebook from Karpathys video on building a minimal GPT, annotated by cursors LLM with summary from gemini.\n\n\n\n\n\nApr 15, 2025\n\n\nAndrey Karpathy\n\n\n\n\n\n\n\n\n\n\n\n\nhow to install local llm\n\n\n\n\n\n\nhow-to\n\n\nllm\n\n\n\n\n\n\n\n\n\nApr 14, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 3\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nApr 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCalibrating hyperparameters with weights and biases\n\n\n\n\n\n\ngene46100\n\n\nproject\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nApr 10, 2025\n\n\nSofia Salazar\n\n\n\n\n\n\n\n\n\n\n\n\nTF binding prediction challenge\n\n\n\n\n\n\ngene46100\n\n\nproject\n\n\n\nCompetition details for TF binding prediction challenge\n\n\n\n\n\nApr 8, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 2\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nApr 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nUpdated - DNA score prediction with Pytorch\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nApr 4, 2025\n\n\nErin Wilson\n\n\n\n\n\n\n\n\n\n\n\n\nTF Binding prediction project\n\n\n\n\n\n\ngene46100\n\n\nproject\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nApr 3, 2025\n\n\nSofia Salazar\n\n\n\n\n\n\n\n\n\n\n\n\nFit linear mode to linear data\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nMeasuring information in DNA sequence\n\n\n\n\n\n\n\n\n\n\n\nApr 3, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 1\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMar 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nQuick introduction to deep learning\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nMar 25, 2025\n\n\nBoxiang Liu, modified by Haky Im\n\n\n\n\n\n\n\n\n\n\n\n\npreparing environment for unit 00\n\n\n\n\n\n\ngene46100\n\n\nhow-to\n\n\n\nsetting up conda environement and installing packages\n\n\n\n\n\nMar 24, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a GPT From Scratch summary\n\n\n\n\n\n\ngene46100\n\n\nslides\n\n\ngpt\n\n\n\n\n\n\n\n\n\nMar 24, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nGradient descent illustration\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nConda Environments for Quarto Documents\n\n\n\n\n\n\nhow-to\n\n\n\nHow to specify a specific Conda environment when rendering quarto documents.\n\n\n\n\n\nJan 6, 2024\n\n\nRich Leyshon\n\n\n\n\n\n\n\n\n\n\n\n\nEnformer usage neanderthal\n\n\n\n\n\npredict neanderthal epigenome with enformer\n\n\n\n\n\nAug 22, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSlides unit 00\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTODO\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2020\n\n\n\n\n\n\nNo matching items\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "DRAFTS/slides-00.html",
    "href": "DRAFTS/slides-00.html",
    "title": "Slides unit 00",
    "section": "",
    "text": "An MLP is a fully connected feedforward neural network — the classic deep learning architecture.\n\n\n\nInput Layer → Hidden Layer(s) → Output Layer\n\n\n\nMLP with 2 features in the input layer, one hidden layer, one dimentional output layer\n\n\nEach layer performs:\noutput = activation(Wx + b)\nWhere:\nW = weight matrix\n\nx = input vector\n\nb = bias\n\nactivation = non-linear function like ReLU\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "DRAFTS/slides-00.html#core-idea",
    "href": "DRAFTS/slides-00.html#core-idea",
    "title": "Slides unit 00",
    "section": "",
    "text": "An MLP is a fully connected feedforward neural network — the classic deep learning architecture."
  },
  {
    "objectID": "DRAFTS/slides-00.html#architecture",
    "href": "DRAFTS/slides-00.html#architecture",
    "title": "Slides unit 00",
    "section": "",
    "text": "Input Layer → Hidden Layer(s) → Output Layer\n\n\n\nMLP with 2 features in the input layer, one hidden layer, one dimentional output layer\n\n\nEach layer performs:\noutput = activation(Wx + b)\nWhere:\nW = weight matrix\n\nx = input vector\n\nb = bias\n\nactivation = non-linear function like ReLU"
  },
  {
    "objectID": "DRAFTS/slides-00.html#data---model-input",
    "href": "DRAFTS/slides-00.html#data---model-input",
    "title": "Slides unit 00",
    "section": "1. Data -> Model Input",
    "text": "1. Data -&gt; Model Input\nDNA/RNA/protein sequences → One-hot encoded or embedded tensors\n\nWrapped in custom Dataset + DataLoader for batching"
  },
  {
    "objectID": "DRAFTS/slides-00.html#training-loop",
    "href": "DRAFTS/slides-00.html#training-loop",
    "title": "Slides unit 00",
    "section": "2. Training Loop",
    "text": "2. Training Loop\nfor xb, yb in train_loader: optimizer.zero_grad() # Step 1: Clear old gradients preds = model(xb) # Step 2: Forward pass loss = loss_fn(preds, yb) # Step 3: Compute loss loss.backward() # Step 4: Backprop: compute gradients optimizer.step() # Step 5: Update weights"
  },
  {
    "objectID": "DRAFTS/slides-00.html#model-components",
    "href": "DRAFTS/slides-00.html#model-components",
    "title": "Slides unit 00",
    "section": "3. Model Components",
    "text": "3. Model Components\nnn.Module: Defines layers and forward pass\n\nnn.Conv1d, nn.Linear, nn.ReLU, etc.\n\nOutputs: regression (e.g. expression levels) or classification (e.g. binding sites)"
  },
  {
    "objectID": "DRAFTS/slides-00.html#loss-function",
    "href": "DRAFTS/slides-00.html#loss-function",
    "title": "Slides unit 00",
    "section": "4. Loss Function",
    "text": "4. Loss Function\nnn.MSELoss() for expression prediction\n\nnn.CrossEntropyLoss() for classification\n\nnn.BCELoss() for binary classification, this is just -log likelihood for a bernoulli distribution"
  },
  {
    "objectID": "DRAFTS/slides-00.html#optimizer",
    "href": "DRAFTS/slides-00.html#optimizer",
    "title": "Slides unit 00",
    "section": "5. Optimizer",
    "text": "5. Optimizer\nCommon: torch.optim.Adam, SGD, etc.\n\nControls learning rate and parameter updates"
  },
  {
    "objectID": "DRAFTS/slides-00.html#evaluation",
    "href": "DRAFTS/slides-00.html#evaluation",
    "title": "Slides unit 00",
    "section": "6. Evaluation",
    "text": "6. Evaluation\nUse model.eval() + torch.no_grad() during validation\n\nTrack metrics like loss, R², accuracy, AUC"
  },
  {
    "objectID": "DRAFTS/2020-01-01-TODO/index.html",
    "href": "DRAFTS/2020-01-01-TODO/index.html",
    "title": "TODO",
    "section": "",
    "text": "TODO\n\n\n\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "DRAFTS/08-quarto-conda-env.html",
    "href": "DRAFTS/08-quarto-conda-env.html",
    "title": "Conda Environments for Quarto Documents",
    "section": "",
    "text": "borrowed from https://thedatasavvycorner.com/blogs/08-quarto-conda-env modified for GENE-46100\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "DRAFTS/08-quarto-conda-env.html#introduction",
    "href": "DRAFTS/08-quarto-conda-env.html#introduction",
    "title": "Conda Environments for Quarto Documents",
    "section": "Introduction",
    "text": "Introduction\nThis article sets out minimal instructions on rendering quarto documents that rely on specified conda virtual environments. This article collates information from:\n\nQuarto Documentation: Using Conda\nQuarto Documentation: Using Python\nnb_conda_kernels readme\n\nIt is presumed that you will be working within a git repository at times. If this is not the case, ignoring steps specifying git instructions should not affect your ability to successfully render the quarto documents.\n\n\n\n\n\n\nA Note on the Purpose\n\n\n\n\n\nThe purpose of this article is not to explore reasons for using conda enviroments or to compare the pros and cons of the many different options for managing python virtual environments. It aims to help the reader configure quarto documents to run with a specified conda environment while remaining minimal, opting to link to sources of further information where discussion may complement the content.\n\n\n\n\nIntended Audience\nPython practitioners familiar with conda environment management @CondaDocsEnvMgmt who are less familiar working with quarto documents @HelloQuarto.\n\n\nThe Scenario\nYou are writing a quarto document that contains python code. You would like to use conda to manage your python dependencies. You are encountering problems in having quarto select the appropriate conda environment.\n\n\n\n\n\n\nNamed Environments\n\n\n\n\n\nThis article covers having quarto execute with “prefix” conda environments. This setup may be useful specifically for a website where different site pages have different dependencies.\nHowever, many readers may wish for a simpler solution. It is possible to have quarto websites and documents execute with a named environment instead. If you have an environment created like below:\nconda create -n my-awesome-env python -y\nThen including the following statement within either the _quarto.yml or quarto document’s YAML header should be enough to guarantee that the target environment is picked up when rendering:\njupyter: my-awesome-env\nAdditionally, if you would just prefer the site or document to render with whatever version of python is available in the currently active environment, then use:\njupyter: python3\nMany thanks to Ethan for this tip.\n\n\n\n\n\nWhat You’ll Need:\n\nConda or miniconda\nQuarto\nText editor eg VS Code\nPython package manager (eg pip)\nAccess to a command line interface (CLI) such as terminal / Bash.\nrequirements.txt:\n\n\n\nrequirements.txt\n\nnbclient\nnbformat\npalmerpenguins\n\n\ngit (optional)"
  },
  {
    "objectID": "DRAFTS/08-quarto-conda-env.html#configuring-quarto-with-conda-env",
    "href": "DRAFTS/08-quarto-conda-env.html#configuring-quarto-with-conda-env",
    "title": "Conda Environments for Quarto Documents",
    "section": "Configuring Quarto with Conda Env",
    "text": "Configuring Quarto with Conda Env\n\nProject Structure\n\nCreate a new project folder. Open a terminal and change directory to the new project.\nSave the requirements to a requirements.txt file.\nCreate a new quarto document in the project root. In VS Code, use\nFile  New File...  Quarto Document.\nWrite the following content to a python code chunk in the quarto file and save as penguins.qmd\n\n\n\npenguins.qmd\n\ndf = penguins.load_penguins().dropna()\ndf.head()\n\n\n\nConfigure the Environment\n\nCreate a new conda environment with the -p flag and give it a suitably descriptive name 1. Ensure that the environment is built with python 3.11 2.\n\n\n\nCLI\n\nconda create -p ~/Github/web-GENE-46100/.venv python=3.12 -y\n\n\nActivate the environment.\n\n\n\nCLI\n\nconda activate ~/Github/web-GENE-46100/.venv\n\n\nInstall the requirements file.\n\n\n\nCLI\n\npip install -r ~/Github/web-GENE-46100/requirements-qmd46100.txt\n\n\nAdd a .gitignore file and include the name of the local environment directory created in step 4.\n\n\n\n.gitignore\n\nSOME_ENV_NAME/\n\n\n\nConfigure the Quarto Project\n\nCreate a _quarto.yml configuration file in the project root. In this file, we will specify that the quarto render command should render any qmd files and ignore any files found within your local environment. Add the following content:\n\n\n\n_quarto.yaml\n\nproject:\n  type: website\n  output-dir: docs\n  render:\n    - \"*.qmd\"\n    - \"!/./SOME_ENV_NAME/\"\n\n\nUse conda to install the nb_conda_kernels package. This is used to manage python jupyter kernels for notebooks.\n\n\n\nCLI\n\nconda install nb_conda_kernels\n\n\nCopy the path returned from the below command\n\n\n\nCLI\n\njupyter --config-dir\n\n/Users/haekyungim/.jupyter\n\nCreate a jupyter_config.json in the jupyter config directory:\n\n\n\nCLI\n\ntouch &lt;INSERT_YOUR_CONFIG_DIR&gt;/jupyter_config.json\n\n\nWrite the below content to this file and save.\n\n\n\nCLI\n\necho -e '{\\n  \"CondaKernelSpecManager\": {\\n    \"kernelspec_path\": \"--user\"\\n  }\\n}' &gt;&gt; &lt;INSERT_YOUR_CONFIG_DIR&gt;/jupyter_config.json\n\n\nRun the below command to return a list of available kernels:\n\n\n\nCLI\n\npython -m nb_conda_kernels list\n\n\nCopy the name (not the path) for the environment that you created with the format conda-env-&lt;YOUR_ENV_NAME&gt;-py.\nOpen penguins.qmd. Adjust the YAML header so that it contains the following:\n\n\n\npenguins.qmd\n\njupyter: \n  kernelspec:\n    name: \"conda-env-&lt;YOUR_ENV_NAME&gt;-py\"\n    language: \"python\"\n    display_name: \"&lt;YOUR_ENV_NAME&gt;\"\n\n\nYou should now be able to render the quarto project, confirming that the target environment was activated in the CLI output. eg:\n\n\n\nCLI\n\nquarto render\n\nStarting &lt;YOUR_ENV_NAME&gt; kernel...Done\n\n\nTips\n\nWhen encountering issues with quarto render, it can be informative to examine the output of quarto check or quarto check jupyter in the CLI.\nAs there are many steps to configuring conda, it may be a good idea to create a dedicated conda environment for all of your quarto projects. Quarto attempts to select an appropriate kernel based upon the content of the first executable python code chunk in your quarto document. Usually, this chunk would contain the import statements. However, over time this would likely result in package conflicts over time.\nThe approach set out in this how-to would be a good fit for a website built with quarto, where the configuration steps can be performed only once in a parent website environment, and then specific, minimal environments created for each article requiring a python environment.\nAlternatively, consider using venv or poetry to manage python environments @DO4DSPKGLayer for quarto projects."
  },
  {
    "objectID": "DRAFTS/08-quarto-conda-env.html#troubleshooting",
    "href": "DRAFTS/08-quarto-conda-env.html#troubleshooting",
    "title": "Conda Environments for Quarto Documents",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nYou’ve created a new environment and it is not discovered when running python -m nb_conda_kernels list:\n\nActivate your new environment\npip install ipykernel\nRun:\n\npython -m ipykernel install --user --name &lt;INSERT_ENV_NAME&gt; --display-name \"&lt;INSERT_DISPLAY_NAME&gt;\"\n\nDeactivate the new environment.\nRun python -m nb_conda_kernels list once more and the new env should appear.\nTaken from this SO thread"
  },
  {
    "objectID": "DRAFTS/08-quarto-conda-env.html#conclusion",
    "href": "DRAFTS/08-quarto-conda-env.html#conclusion",
    "title": "Conda Environments for Quarto Documents",
    "section": "Conclusion",
    "text": "Conclusion\nThis article has walked the reader through setting up a basic quarto project, creating a conda environment, and configuring a quarto document to render with a specified environment. For more help with quarto, consult the quarto-cli GitHub repository @QuartoCLI and the RStudio Community @PositCommunity (soon to rebrand to the Posit Community).\n\nfin!"
  },
  {
    "objectID": "DRAFTS/08-quarto-conda-env.html#footnotes",
    "href": "DRAFTS/08-quarto-conda-env.html#footnotes",
    "title": "Conda Environments for Quarto Documents",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhen creating conda environments, the use of generic names such as env will result in conda prepending the environment name with numbers to avoid conflicts. Use descriptive environment names in order to avoid this, eg penguins-env.↩︎\nnb_conda_kernels (a package required in a later step) does not currently work with python 3.12 or newer.↩︎"
  },
  {
    "objectID": "DRAFTS/embeding-analysis.html",
    "href": "DRAFTS/embeding-analysis.html",
    "title": "Embeddings Analysis",
    "section": "",
    "text": "** embeddings provided by Henry from his DNA GPT model**\n\nsuppressMessages(library(tidyverse))\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\nsuppressMessages(library(glue))\n\nWarning: package 'glue' was built under R version 4.3.3\n\nsuppressMessages(library(ggrepel))\n\nWarning: package 'ggrepel' was built under R version 4.3.3\n\n# suppressMessages(library(qvalue))\n# suppressMessages(library(devtools))\n# suppressMessages(source_gist(\"38431b74c6c0bf90c12f\")) ## get qqunif\n# suppressMessages(library(googlesheets4))\n\n\nWEBDATA = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data\"\n\nDATA = glue(\"{WEBDATA}/web-GENE-46100\")\n\nif(!file.exists(DATA)) system(glue::glue(\"mkdir {DATA}\"))\n##system(glue(\"open {DATA}\")) ## this will open the folder \n\n\nembeddings = read_csv(glue(\"{DATA}/henry-dna-gpt-embed_df.csv.gz\"))\n\nNew names:\nRows: 40 Columns: 385\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(1): ...1 dbl (384): 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,\n17, 18,...\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\nnames(embeddings)[1] = \"token\"\n\nunique(embeddings$token)\n\n [1] \"\\n\" \"0\"  \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"&gt;\"  \"A\"  \"B\"  \"C\" \n[16] \"G\"  \"H\"  \"I\"  \"J\"  \"K\"  \"L\"  \"M\"  \"N\"  \"T\"  \"U\"  \"X\"  \"Y\"  \"_\"  \"a\"  \"c\" \n[31] \"d\"  \"g\"  \"h\"  \"l\"  \"m\"  \"n\"  \"o\"  \"r\"  \"t\"  \"v\" \n\nsvdfit = svd(embeddings[,-1])\n\n\nACGT cluster togeter, acgt also cluster together.\n\n\nN is closer to ACGT and acgt, probably because of N appears more frequently than the remainig tokens.\n\n\npng2file = FALSE\n\n# if(png2file) png(glue(\"{DATA}/svd-plot.png\"), width=1000, height=1000)\n# plot(svdfit$u[,1], svdfit$u[,2], type=\"n\", xlab=\"\", ylab=\"\", main=\"SVD of DNA sequence embeddings\")\n# text(svdfit$u[,1], svdfit$u[,2], embeddings$token, cex=1)\n# if(png2file) dev.off()\n\n# plot(svdfit$v[,1],svdfit$v[,2],type=\"n\",xlab=\"\",ylab=\"\",main=\"SVD of DNA sequence embeddings\")\n# text(svdfit$v[,1],svdfit$v[,2],names(embeddings)[-1],cex=2)\n\n\n# Create a data frame for plotting\nplot_data &lt;- data.frame(\n  x = svdfit$u[,1],\n  y = svdfit$u[,2],\n  label = embeddings$token\n)\n\n# Create the plot\nggplot(plot_data, aes(x = x, y = y, label = label)) +\n  geom_point(alpha = 0.5) +\n  geom_label_repel(\n    size = 4,\n    max.overlaps = 20,\n    box.padding = 0.5,\n    point.padding = 0.5,\n    segment.color = 'grey50',\n    segment.alpha = 0.5,\n    fill = \"white\",\n    alpha = 0.8,\n    label.size = 0.2\n  ) +\n  theme_minimal() +\n  labs(\n    title = \"SVD of DNA Sequence Embeddings\",\n    x = \"First Principal Component\",\n    y = \"Second Principal Component\"\n  ) +\n  theme(\n    plot.title = element_text(hjust = 0.5, size = 14, face = \"bold\"),\n    axis.title = element_text(size = 12),\n    panel.grid = element_line(color = \"grey90\")\n  )\n\n\n\n\n\n\n\n# Save to file if needed\nif(png2file) {\n  ggsave(\n    glue(\"{DATA}/svd-plot-ggplot.png\"),\n    width = 10,\n    height = 10,\n    dpi = 300\n  )\n}\n\n\n\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "how-to/01-how-to-install-local-llm.html",
    "href": "how-to/01-how-to-install-local-llm.html",
    "title": "how to install local llm",
    "section": "",
    "text": "#how-to install ollama open webui local llm on a macbook adapted using conda instead of pyenv from https://medium.com/@hautel.alex2000/build-your-local-ai-from-zero-to-a-custom-chatgpt-interface-with-ollama-open-webui-6bee2c5abba3\n\ninstall homebrew if not already installed on your mac\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n\ninstall and start ollama service\n\nbrew install ollama\nbrew services start ollama\n\nrun ollama with deepseek-r1-14b or other models from https://ollama.com/library\n\nollama run deepseek-r1:14b\n\ninstall miniconda if not already installed\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh -O ~/miniconda.sh\nbash ~/miniconda.sh -b -p $HOME/miniconda\n\ncreate and activate conda environment\n\nconda create -n open-webui python=3.11\nconda activate open-webui\n\ninstall openwebui\n\npip install open-webui\n\nstart open-webui\n\nopen-webui serve\n\nopen browser and go to http://localhost:8080\n\n\n\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Deep Learning in Genomics Course Material",
    "section": "",
    "text": "This page contains material for the GENE46100 Deep learning in genomics course.\nFind the 2025 syllabus here.\nEdit github source here\n\nSee all notes here\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nunit 03\n\n\n\n\n\n\ngene46100\n\n\n\n\n\n\n\n\n\nMay 1, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 4\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nApr 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a GPT - companion notebook annotated\n\n\n\n\n\nCompanion notebook from Karpathys video on building a minimal GPT, annotated by cursors LLM with summary from gemini.\n\n\n\n\n\nApr 15, 2025\n\n\nAndrey Karpathy\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 3\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nApr 10, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nCalibrating hyperparameters with weights and biases\n\n\n\n\n\n\ngene46100\n\n\nproject\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nApr 10, 2025\n\n\nSofia Salazar\n\n\n\n\n\n\n\n\n\n\n\n\nTF binding prediction challenge\n\n\n\n\n\n\ngene46100\n\n\nproject\n\n\n\nCompetition details for TF binding prediction challenge\n\n\n\n\n\nApr 8, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 2\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nApr 4, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nUpdated - DNA score prediction with Pytorch\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nApr 4, 2025\n\n\nErin Wilson\n\n\n\n\n\n\n\n\n\n\n\n\nTF Binding prediction project\n\n\n\n\n\n\ngene46100\n\n\nproject\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nApr 3, 2025\n\n\nSofia Salazar\n\n\n\n\n\n\n\n\n\n\n\n\nhomework 1\n\n\n\n\n\n\ngene46100\n\n\nhomework\n\n\n\n\n\n\n\n\n\nMar 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nQuick introduction to deep learning\n\n\n\n\n\n\ngene46100\n\n\nnotebook\n\n\n\n\n\n\n\n\n\nMar 25, 2025\n\n\nBoxiang Liu, modified by Haky Im\n\n\n\n\n\n\n\n\n\n\n\n\npreparing environment for unit 00\n\n\n\n\n\n\ngene46100\n\n\nhow-to\n\n\n\nsetting up conda environement and installing packages\n\n\n\n\n\nMar 24, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a GPT From Scratch summary\n\n\n\n\n\n\ngene46100\n\n\nslides\n\n\ngpt\n\n\n\n\n\n\n\n\n\nMar 24, 2025\n\n\nHaky Im\n\n\n\n\n\n\n\n\n\n\n\n\nEnformer usage neanderthal\n\n\n\n\n\npredict neanderthal epigenome with enformer\n\n\n\n\n\nAug 22, 2023\n\n\n\n\n\n\nNo matching items\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/homework-02.html",
    "href": "post/2025-03-25-unit00/homework-02.html",
    "title": "homework 2",
    "section": "",
    "text": "Homework 2\n\nrun all the code in the DNA scoring tutorial notebook. You should also be able to find the notebook in your cloned course repository. (20 points)\ncomplete the exercises in the notebook (30 points)\nsubmit the jupyter notebook with the results to canvas assignment\n\n\n\n\n\n\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html",
    "href": "post/2025-03-25-unit00/index.html",
    "title": "preparing environment for unit 00",
    "section": "",
    "text": "```{bash}\n\n## download miniconda from the command line\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\n\n## or manually from https://www.anaconda.com/docs/getting-started/miniconda/main\n\n## install miniconda\nbash Miniconda3-latest-Linux-x86_64.sh\n\n```\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#install-conda",
    "href": "post/2025-03-25-unit00/index.html#install-conda",
    "title": "preparing environment for unit 00",
    "section": "",
    "text": "```{bash}\n\n## download miniconda from the command line\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\n\n## or manually from https://www.anaconda.com/docs/getting-started/miniconda/main\n\n## install miniconda\nbash Miniconda3-latest-Linux-x86_64.sh\n\n```"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#create-conda-environment",
    "href": "post/2025-03-25-unit00/index.html#create-conda-environment",
    "title": "preparing environment for unit 00",
    "section": "create conda environment",
    "text": "create conda environment\n```{bash}\nconda create -n gene46100 python=3.12\nconda activate gene46100\n\n```"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#do-not-install-packages-with-conda",
    "href": "post/2025-03-25-unit00/index.html#do-not-install-packages-with-conda",
    "title": "preparing environment for unit 00",
    "section": "do not install packages with conda",
    "text": "do not install packages with conda\n(I ran into lots of issues with torch version incompatibility with torchvision and torchmetrics)\n```{bash}\n# DONT USE CONDA TO INSTALL PYTORCH at least for now\n# conda install scikit-learn plotnine pytorch \n\n## installing torchvision and torchmetrics forced downgrading torch to 2.3.1 which was not compatible with mps\n\n```"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#install-all-packages-within-jupyter-notebook-with-pip",
    "href": "post/2025-03-25-unit00/index.html#install-all-packages-within-jupyter-notebook-with-pip",
    "title": "preparing environment for unit 00",
    "section": "install all packages within jupyter notebook with %pip",
    "text": "install all packages within jupyter notebook with %pip\nas of 2025-03-24\nusing %pip will make sure that the packages are accessible by the kernel you are using for the jupyter notebook\n```{bash}\n%pip install scikit-learn plotnine tqdm pandas \n%pip install torch\n%pip install torchvision\n%pip install torchmetrics\n```"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#install-cursor-or-vscode-to-run-the-jupyter-notebook",
    "href": "post/2025-03-25-unit00/index.html#install-cursor-or-vscode-to-run-the-jupyter-notebook",
    "title": "preparing environment for unit 00",
    "section": "install cursor or vscode to run the jupyter notebook",
    "text": "install cursor or vscode to run the jupyter notebook\n\ninstall the python extension for cursor or vscode.\nselect the python interpreter to be the one in the conda environment you created (gene46100)"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#save-the-environment-for-reproducibility",
    "href": "post/2025-03-25-unit00/index.html#save-the-environment-for-reproducibility",
    "title": "preparing environment for unit 00",
    "section": "Save the environment for reproducibility",
    "text": "Save the environment for reproducibility\n\nto reproduce the environment exactly, save the environment\n```{bash}\n# Save conda packages with their sources\nconda env export --from-history &gt; environment-gene46100.yml\n\n# Save pip-installed packages separately\npip list --format=freeze &gt; requirements-gene46100.txt\n\n# Save full environment state (for reference)\nconda env export &gt; environment_full-gene46100.yml\n```"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#do-not-run-the-following-unless-need-to-reinstall-the-environment-for-some-reason",
    "href": "post/2025-03-25-unit00/index.html#do-not-run-the-following-unless-need-to-reinstall-the-environment-for-some-reason",
    "title": "preparing environment for unit 00",
    "section": "DO NOT RUN THE FOLLOWING UNLESS NEED TO REINSTALL THE ENVIRONMENT FOR SOME REASON",
    "text": "DO NOT RUN THE FOLLOWING UNLESS NEED TO REINSTALL THE ENVIRONMENT FOR SOME REASON\n\nreinstall the environment\n```{bash}\n# Create environment from conda packages\nconda env create -f environment-gene46100.yml\n\n# Activate the environment\nconda activate gene46100\n\n# Install pip packages\npip install -r requirements-gene46100.txt\n\n```"
  },
  {
    "objectID": "post/2025-03-25-unit00/index.html#configuration-to-run-python-in-qmd-files",
    "href": "post/2025-03-25-unit00/index.html#configuration-to-run-python-in-qmd-files",
    "title": "preparing environment for unit 00",
    "section": "configuration to run python in qmd files",
    "text": "configuration to run python in qmd files\nFollow instructions in https://thedatasavvycorner.com/blogs/08-quarto-conda-en copied to /DRAFTS/2025-02-20-testin/08-quarto-conda-env.qmd\nBelow is a shortened version of the instructions when you already have a conda environment.\n```{bash}\nconda activate gene46100\npip install nbclient nbformat\n\n# Use conda to install the nb_conda_kernels package. This is used to manage python jupyter kernels for notebooks.\n\nconda install nb_conda_kernels\n\n# Copy the path returned from the below command\njupyter --config-dir\n\n# in my case it was ~/.jupyter\n\n# Create a jupyter_config.json in the jupyter config directory:\ntouch ~/.jupyter/jupyter_config.json\necho '{\n  \"CondaKernelSpecManager\": {\n    \"kernelspec_path\": \"--user\"\n  }\n}' &gt;&gt; ~/.jupyter/jupyter_config.json\n\n# Run the below command to return a list of available kernels:\npython -m nb_conda_kernels list\n\n# Copy the name (not the path) for the environment that you created with the format `conda-env-&lt;YOUR_ENV_NAME&gt;-py`.\n```\nadd the following to the yaml header in the qmd file with python blocks\n```{yaml}\njupyter: \n  kernelspec:\n    name: \"conda-env-&lt;YOUR_ENV_NAME&gt;-py\"\n    language: \"python\"\n    display_name: \"&lt;YOUR_ENV_NAME&gt;\"\n```\nmy environment was conda-env-gene46100-py you can run quarto render and it will be rendered with the conda environment you specified when running interactively, you can choose the kernel, similarly to a jupyter notebook\n```{yaml}\njupyter: \n  kernelspec:\n    name: \"conda-env-gene46100-py\"\n    language: \"python\"\n    display_name: \"gene46100\"\nfreeze: true\n## you may want to choose freeze true if you don't want to run the code every time you render the qmd file\n\n```"
  },
  {
    "objectID": "post/2025-04-15-unit01/homework-04.html",
    "href": "post/2025-04-15-unit01/homework-04.html",
    "title": "homework 4",
    "section": "",
    "text": "Homework 4\nWatch Karpathy’s nanoGPT tutorial on youtube link and answer the questions in the form\nlist of questions in text format is here\n\n\n\n\n\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-04-15-unit01/nano-gpt/annotated-gpt_dev.html",
    "href": "post/2025-04-15-unit01/nano-gpt/annotated-gpt_dev.html",
    "title": "Building a GPT - companion notebook annotated",
    "section": "",
    "text": "Companion notebook to the Zero To Hero video on GPT. Downloaded from here\n(https://github.com/karpathy/nanoGPT)\n\n\n\n\nShow the code\n# Download the tiny shakespeare dataset\n#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n\n\n\nShow the code\n# read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n\n\n\nShow the code\n# print the length of the dataset\nprint(\"length of dataset in characters: \", len(text))\n\n\nlength of dataset in characters:  1115394\n\n\n\n\nShow the code\n# let's look at the first 1000 characters\nprint(text[:1000])\n\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n\n\n\n\nShow the code\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n\n\n\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n\n\n\n\n\n\n\nShow the code\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n\n\n[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n\n\n\n\n\n\n\nShow the code\n# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch # we use PyTorch: [https://pytorch.org](https://pytorch.org)\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this\n\n\ntorch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n\n\n\n\n\n\n\nShow the code\n# split up the data into train and validation sets\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n\n\n\n\n\n\nShow the code\nblock_size = 8\ntrain_data[:block_size+1]\n\n\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n\n\n\n\n\n\n\nShow the code\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n\n\nwhen input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n\n\n\n\n\n\n\nShow the code\ntorch.manual_seed(1337)\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n\n\ninputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\nwhen input is [24] the target: 43\nwhen input is [24, 43] the target: 58\nwhen input is [24, 43, 58] the target: 5\nwhen input is [24, 43, 58, 5] the target: 57\nwhen input is [24, 43, 58, 5, 57] the target: 1\nwhen input is [24, 43, 58, 5, 57, 1] the target: 46\nwhen input is [24, 43, 58, 5, 57, 1, 46] the target: 43\nwhen input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\nwhen input is [44] the target: 53\nwhen input is [44, 53] the target: 56\nwhen input is [44, 53, 56] the target: 1\nwhen input is [44, 53, 56, 1] the target: 58\nwhen input is [44, 53, 56, 1, 58] the target: 46\nwhen input is [44, 53, 56, 1, 58, 46] the target: 39\nwhen input is [44, 53, 56, 1, 58, 46, 39] the target: 58\nwhen input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52] the target: 58\nwhen input is [52, 58] the target: 1\nwhen input is [52, 58, 1] the target: 58\nwhen input is [52, 58, 1, 58] the target: 46\nwhen input is [52, 58, 1, 58, 46] the target: 39\nwhen input is [52, 58, 1, 58, 46, 39] the target: 58\nwhen input is [52, 58, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\nwhen input is [25] the target: 17\nwhen input is [25, 17] the target: 27\nwhen input is [25, 17, 27] the target: 10\nwhen input is [25, 17, 27, 10] the target: 0\nwhen input is [25, 17, 27, 10, 0] the target: 21\nwhen input is [25, 17, 27, 10, 0, 21] the target: 1\nwhen input is [25, 17, 27, 10, 0, 21, 1] the target: 54\nwhen input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n\n\n\n\n\n\n\nShow the code\n# define the bigram language model\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\n\n\n\n\nLoss = -\\sum_{i}(y_i * \\log(p_i))x\nwhere:\ny_i = actual probability (0 or 1 for the i-th class) p_i = predicted probability for the i-th class \\sum = sum over all classes (characters)\nThis is the loss for a single token prediction. The total loss reported by F.cross_entropy is the average loss across all B*T tokens in the batch, where:\nB = batch_size T = block_size (sequence length)\nBefore training, we would expect the model to predict the next character from a uniform distribution (random guessing). The probability for the correct character would be 1 / \\text{vocab_size}.\nExpected initial loss \\approx - \\log(1 / \\text{vocab_size}) = \\log(\\text{vocab_size}) \\log(65) \\approx 4.1744\n\n\n\n\n\nShow the code\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb) # xb/yb are from the previous cell (B=4, T=8)\nprint(logits.shape) # Expected: (B, T, C) = (4, 8, 65)\nprint(loss) # Expected: Around 4.17\n\n\ntorch.Size([32, 65])\ntensor(4.8786, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\n\n\n\nShow the code\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\n\n\nSKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\nwnYWmnxKWWev-tDqXErVKLgJ\n\n\n\n\n\n\n\nShow the code\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\n\n\n\n\n\n\nShow the code\nbatch_size = 32 # Redefine batch size for training\nfor steps in range(100): # # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n\n\n4.65630578994751\n\n\n\n\n\n\n\nShow the code\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n\n\n\noTo.JUZ!!zqe!\nxBP qbs$Gy'AcOmrLwwt\np$x;Seh-onQbfM?OjKbn'NwUAW -Np3fkz$FVwAUEa-wzWC -wQo-R!v -Mj?,SPiTyZ;o-opr$mOiPJEYD-CfigkzD3p3?zvS;ADz;.y?o,ivCuC'zqHxcVT cHA\nrT'Fd,SBMZyOslg!NXeF$sBe,juUzLq?w-wzP-h\nERjjxlgJzPbHxf$ q,q,KCDCU fqBOQT\nSV&CW:xSVwZv'DG'NSPypDhKStKzC -$hslxIVzoivnp ,ethA:NCCGoi\ntN!ljjP3fwJMwNelgUzzPGJlgihJ!d?q.d\npSPYgCuCJrIFtb\njQXg\npA.P LP,SPJi\nDBcuBM:CixjJ$Jzkq,OLf3KLQLMGph$O 3DfiPHnXKuHMlyjxEiyZib3FaHV-oJa!zoc'XSP :CKGUhd?lgCOF$;;DTHZMlvvcmZAm;:iv'MMgO&Ywbc;BLCUd&vZINLIzkuTGZa\nD.?\n\n\n\n\n\ntoy example illustrating how matrix multiplication can be used for a “weighted aggregation”\n\n\nShow the code\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3)) # Lower triangular matrix of 1s\na = a / torch.sum(a, 1, keepdim=True) # Normalize rows to sum to 1\nb = torch.randint(0,10,(3,2)).float() # Some data\nc = a @ b # Matrix multiply\nprint('a=')\nprint(a)\nprint('--')\nprint('b=')\nprint(b)\nprint('--')\nprint('c=')\nprint(c)\n\n\na=\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n--\nb=\ntensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n--\nc=\ntensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n\n\n\n\nShow the code\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2 # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\n\ntorch.Size([4, 8, 2])\n\n\n\n\n\n\n\nShow the code\n# We want x[b,t] = mean_{i&lt;=t} x[b,i]\nxbow = torch.zeros((B,T,C)) # x bag-of-words (running average)\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # Select vectors from start up to time t: shape (t+1, C)\n        xbow[b,t] = torch.mean(xprev, 0) # Compute mean along the time dimension (dim 0)\n\n\n\n\n\n\n\nShow the code\n# Create the averaging weight matrix\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True) # Normalize rows to sum to 1\n# Perform batched matrix multiplication\nxbow2 = wei @ x # (T, T) @ (B, T, C) -&gt; (B, T, C) via broadcasting\ntorch.allclose(xbow, xbow2) # Check if results are identical\n\n\nTrue\n\n\n\n\n\n\n\nShow the code\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\n# Mask out future positions by setting them to -infinity before softmax\nwei = wei.masked_fill(tril == 0, float('-inf'))\n# Apply softmax to get row-wise probability distributions (weights)\nwei = F.softmax(wei, dim=-1)\n# Perform weighted aggregation\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3) # Check if results are identical\n\n\nTrue\n\n\n\n\n\nsoftmax(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n\n\n\n\n\nShow the code\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels (embedding dimension)\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)   # (B, T, head_size)\nq = query(x) # (B, T, head_size)\n# Compute attention scores (\"affinities\")\nwei = q @ k.transpose(-2, -1) # (B, T, hs) @ (B, hs, T) ---&gt; (B, T, T)\n\n# Scale the scores\n# Note: Karpathy uses C**-0.5 here (sqrt(embedding_dim)). Standard Transformer uses sqrt(head_size).\nwei = wei * (C**-0.5)\n\n# Apply causal mask\ntril = torch.tril(torch.ones(T, T))\n#wei = torch.zeros((T,T)) # This line is commented out in original, was from softmax demo\nwei = wei.masked_fill(tril == 0, float('-inf')) # Mask future tokens\n\n# Apply softmax to get attention weights\nwei = F.softmax(wei, dim=-1) # (B, T, T)\n\n# Perform weighted aggregation of Values\nv = value(x) # (B, T, head_size)\nout = wei @ v # (B, T, T) @ (B, T, hs) ---&gt; (B, T, hs)\n#out = wei @ x # This would aggregate original x, not the projected values 'v'\n\nout.shape # Expected: (B, T, head_size) = (4, 8, 16)\n\n\ntorch.Size([4, 8, 16])\n\n\n\n\nShow the code\nwei[0] # Show attention weights for the first sequence in the batch\n\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.4264, 0.5736, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3151, 0.3022, 0.3827, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3007, 0.2272, 0.2467, 0.2253, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1635, 0.2048, 0.1776, 0.1616, 0.2926, 0.0000, 0.0000, 0.0000],\n        [0.1403, 0.2272, 0.1454, 0.1244, 0.2678, 0.0949, 0.0000, 0.0000],\n        [0.1554, 0.1815, 0.1224, 0.1213, 0.1428, 0.1603, 0.1164, 0.0000],\n        [0.0952, 0.1217, 0.1130, 0.1453, 0.1137, 0.1180, 0.1467, 0.1464]],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\n\n\n\n\nnC = 64\nX = matrix(rnorm(4*64), nrow=4, ncol=nC)\n## make it so that the third token is similar to the last one\nX[2,] = X[4,]*0.5 + X[2,]*0.5\n## normalize X\nX = t(scale(t(X)))\n\nq = X\nk = X\nv = X\n\nqkt = q %*% t(k)/(nC-1)\nxcor = cor(t(q),t(k))\ndim(xcor)\ndim(qkt)\ncat(\"xcor\\n\")\nxcor\ncat(\"---\\n qkt\\n\")\nqkt\n\ncat(\"are xcor and qkt equal?\")\nall.equal(xcor, qkt)\n\npar(mar=c(5, 6, 4, 2) + 0.1)  # increase left margin to avoid cutting of the y label\npar(pty=\"s\")  # Set plot type to \"square\"\nplot(c(xcor), c(qkt),cex=3,cex.lab=3,cex.axis=2,cex.main=2,cex.sub=2); abline(0,1)\npar(pty=\"m\")  # Reset to default plot type\npar(mar=c(5, 4, 4, 2) + 0.1)  # Reset to default margins\n\n\n\n\nAttention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n\nThere is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens. example: “the cat sat on the mat” should be different from “the mat sat on the cat”\nEach example across batch dimension is of course processed completely independently and never “talk” to each other.\nIn an “encoder” attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a “decoder” attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n“self-attention” just means that the keys and values are produced from the same source as queries (all come from x). In “cross-attention”, the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n\n\n\n\n“Scaled” attention additionaly divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n\n\nShow the code\n# Demonstrate variance without scaling\nk_unscaled = torch.randn(B,T,head_size)\nq_unscaled = torch.randn(B,T,head_size)\nwei_unscaled = q_unscaled @ k_unscaled.transpose(-2, -1)\nprint(f\"k var: {k_unscaled.var():.4f}, q var: {q_unscaled.var():.4f}, wei (unscaled) var: {wei_unscaled.var():.4f}\")\n\n# Demonstrate variance *with* scaling (using head_size for illustration)\nk = torch.randn(B,T,head_size)\nq = torch.randn(B,T,head_size)\nwei = q @ k.transpose(-2, -1) * head_size**-0.5 # Scale by sqrt(head_size)\nprint(f\"k var: {k.var():.4f}, q var: {q.var():.4f}, wei (scaled) var: {wei.var():.4f}\") # Variance should be closer to 1\n\n\nk var: 1.0449, q var: 1.0700, wei (unscaled) var: 17.4690\nk var: 0.9006, q var: 1.0037, wei (scaled) var: 0.9957\n\n\n\n\nShow the code\nk.var() # Should be close to 1\n\n\ntensor(0.9006)\n\n\n\n\nShow the code\nq.var() # Should be close to 1\n\n\ntensor(1.0037)\n\n\n\n\nShow the code\nwei.var() # With scaling, should be closer to 1 than head_size (16)\n\n\ntensor(0.9957)\n\n\n\n\nShow the code\n# Softmax with small inputs (diffuse distribution)\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n\n\ntensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n\n\n\n\nShow the code\n# Softmax with large inputs (simulating unscaled attention scores) -&gt; peaks\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot\n\n\ntensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n\n\n\n\n\n\n\nShow the code\nclass LayerNorm1d: # (used to be BatchNorm1d)\n    def __init__(self, dim, eps=1e-5, momentum=0.1): # Momentum is not used in typical LayerNorm\n        self.eps = eps\n        # Learnable scale and shift parameters, initialized to 1 and 0\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        # Calculate mean over the *last* dimension (features/embedding)\n        xmean = x.mean(1, keepdim=True) # batch mean (shape B, 1, C if input B, T, C) --&gt; Needs adjustment for (B,C) input shape here. Assumes input is (B, dim)\n        # Correction: x is (32, 100). dim=1 is correct for features. Shape (32, 1)\n        xvar = x.var(1, keepdim=True) # batch variance (shape 32, 1)\n        # Normalize each feature vector independently\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n        # Apply scale and shift\n        self.out = self.gamma * xhat + self.beta\n        return self.out\n\n    def parameters(self):\n        # Expose gamma and beta as learnable parameters\n        return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = LayerNorm1d(100) # Create LayerNorm for 100 features\nx = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\nx = module(x)\nx.shape # Should be (32, 100)\n\n\ntorch.Size([32, 100])\n\n\nExplanation of layernorm\nInput shape: (B, T, C) where: B = batch size T = sequence length (number of tokens) C = embedding dimension (features of each token) For each token in the sequence (each position T), LayerNorm: Takes its embedding vector of size C Calculates the mean and standard deviation of just that vector Normalizes that vector by subtracting its mean and dividing by its standard deviation Applies the learnable scale (gamma) and shift (beta) parameters So if you have a sequence like “The cat sat”, and each word is represented by a 64-dimensional embedding vector, LayerNorm would: Take “The”’s 64-dimensional vector and normalize it Take “cat”’s 64-dimensional vector and normalize it Take “sat”’s 64-dimensional vector and normalize it Each token’s vector is normalized independently of the others. This is different from BatchNorm, which would normalize across the batch dimension (i.e., looking at the same position across different examples in the batch). This per-token normalization helps maintain stable gradients during training and is particularly important in Transformers where the attention mechanism needs to work with normalized vectors to compute meaningful attention scores.\n\n\nShow the code\n# Mean and std of the first feature *across the batch*. Not expected to be 0 and 1.\nx[:,0].mean(), x[:,0].std()\n\n\n(tensor(0.1469), tensor(0.8803))\n\n\n\n\nShow the code\n# Mean and std *across features* for the first item in the batch. Expected to be ~0 and ~1.\nx[0,:].mean(), x[0,:].std()\n\n\n(tensor(2.3842e-09), tensor(1.0000))\n\n\n\n\n\n\n\nShow the code\n# &lt;--------- ENCODE ------------------&gt;&lt;--------------- DECODE -----------------&gt;\n# les réseaux de neurones sont géniaux! &lt;START&gt; neural networks are awesome!&lt;END&gt;\n\n\n\n\n\n\n\nShow the code\n# Import necessary PyTorch modules\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# ===== HYPERPARAMETERS =====\nbatch_size = 16       # Number of sequences per batch (Smaller than Bigram training)\nblock_size = 32       # Context length (Larger than Bigram demo)\nmax_iters = 5000      # Total training iterations (More substantial training) TODO change to 5000 later\neval_interval = 100   # How often to check validation loss\nlearning_rate = 1e-3  # Optimizer learning rate\neval_iters = 200      # Number of batches to average for validation loss estimate\nn_embd = 64           # Embedding dimension (Size of token vectors)\nn_head = 4            # Number of attention heads\nn_layer = 4           # Number of Transformer blocks (layers)\ndropout = 0.0         # Dropout probability (0.0 means no dropout here)\n# ==========================\n\n# Device selection: MPS (Apple Silicon) &gt; CUDA &gt; CPU\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")   # Apple Silicon GPU\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")  # NVIDIA GPU\nelse:\n    device = torch.device(\"cpu\")   # CPU fallback\nprint(f\"Using device: {device}\")\n\n# Set random seed for reproducibility\ntorch.manual_seed(1337)\nif device.type == 'cuda':\n    torch.cuda.manual_seed(1337)\nelif device.type == 'mps':\n    torch.mps.manual_seed(1337)\n\n# Load and read the training text (assuming input.txt is available)\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# ===== DATA PREPROCESSING =====\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nstoi = { ch:i for i,ch in enumerate(chars) }   # string to index\nitos = { i:ch for i,ch in enumerate(chars) }   # index to string\nencode = lambda s: [stoi[c] for c in s]   # convert string to list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])   # convert list of integers to string\n\n# Split data into training and validation sets\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data))   # first 90% for training\ntrain_data = data[:n]\nval_data = data[n:]\n# =============================\n\n# ===== DATA LOADING FUNCTION =====\ndef get_batch(split):\n    \"\"\"Generate a batch of data for training or validation.\"\"\"\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device) # Move data to the target device\n    return x, y\n# ================================\n\n# ===== LOSS ESTIMATION FUNCTION =====\n@torch.no_grad()   # Disable gradient calculation for efficiency\ndef estimate_loss():\n    \"\"\"Estimate the loss on training and validation sets.\"\"\"\n    out = {}\n    model.eval()   # Set model to evaluation mode\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()  # Set model back to training mode\n    return out\n# ===================================\n\n# ===== ATTENTION HEAD IMPLEMENTATION =====\nclass Head(nn.Module):\n    \"\"\"Single head of self-attention.\"\"\"\n    \n    def __init__(self, head_size):\n        super().__init__()\n        # Linear projections for Key, Query, Value\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        # Causal mask (tril). 'register_buffer' makes it part of the model state but not a parameter to be trained.\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        # Dropout layer (applied after softmax)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape # C here is n_embd\n        # Project input to K, Q, V\n        k = self.key(x)   # (B,T,head_size)\n        q = self.query(x) # (B,T,head_size)\n        # Compute attention scores, scale, mask, softmax\n        # Note the scaling by C**-0.5 (sqrt(n_embd)) as discussed before\n        wei = q @ k.transpose(-2,-1) * C**-0.5   # (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))   # Use dynamic slicing [:T, :T] for flexibility if T &lt; block_size\n        wei = F.softmax(wei, dim=-1)   # (B, T, T)\n        wei = self.dropout(wei) # Apply dropout to attention weights\n        # Weighted aggregation of values\n        v = self.value(x) # (B,T,head_size)\n        out = wei @ v # (B, T, T) @ (B, T, head_size) -&gt; (B, T, head_size)\n        return out\n# ========================================\n\n# ===== MULTI-HEAD ATTENTION =====\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        # Linear layer after concatenating heads\n        self.proj = nn.Linear(n_embd, n_embd) # Projects back to n_embd dimension\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # Compute attention for each head and concatenate results\n        out = torch.cat([h(x) for h in self.heads], dim=-1) # Shape (B, T, num_heads * head_size) = (B, T, n_embd)\n        # Apply final projection and dropout\n        out = self.dropout(self.proj(out))\n        return out\n# ===============================\n\n# ===== FEED-FORWARD NETWORK =====\nclass FeedFoward(nn.Module):\n    \"\"\"Simple position-wise feed-forward network with one hidden layer.\"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),   # Expand dimension (common practice)\n            nn.ReLU(),                      # Non-linearity\n            nn.Linear(4 * n_embd, n_embd),   # Project back to original dimension\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n# ==============================\n\n# ===== TRANSFORMER BLOCK =====\nclass Block(nn.Module):\n    \"\"\"Transformer block: communication (attention) followed by computation (FFN).\"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head   # Calculate size for each head\n        self.sa = MultiHeadAttention(n_head, head_size) # Self-Attention layer\n        self.ffwd = FeedFoward(n_embd) # Feed-Forward layer\n        self.ln1 = nn.LayerNorm(n_embd) # LayerNorm for Attention input\n        self.ln2 = nn.LayerNorm(n_embd) # LayerNorm for FFN input\n\n    def forward(self, x):\n        # Pre-Normalization variant: Norm -&gt; Sublayer -&gt; Residual\n        x = x + self.sa(self.ln1(x))  # Attention block\n        x = x + self.ffwd(self.ln2(x)) # Feed-forward block\n        return x\n# ============================\n\n# ===== LANGUAGE MODEL =====\nclass BigramLanguageModel(nn.Module):\n    \"\"\"GPT-like language model using Transformer blocks.\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        # Token Embedding Table: Maps character index to embedding vector. (vocab_size, n_embd)\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        # Position Embedding Table: Maps position index (0 to block_size-1) to embedding vector. (block_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        # Sequence of Transformer Blocks\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        # Final Layer Normalization (applied after blocks)\n        self.ln_f = nn.LayerNorm(n_embd)   # Final layer norm\n        # Linear Head: Maps final embedding back to vocabulary size to get logits. (n_embd, vocab_size)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        \n        # Get token embeddings from indices: (B, T) -&gt; (B, T, n_embd)\n        tok_emb = self.token_embedding_table(idx)\n        # Get position embeddings: Create indices 0..T-1, look up embeddings -&gt; (T, n_embd)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n        # Combine token and position embeddings by addition: (B, T, n_embd). Broadcasting handles the addition.\n        x = tok_emb + pos_emb   # (B,T,C)\n        # Pass through Transformer blocks: (B, T, n_embd) -&gt; (B, T, n_embd)\n        x = self.blocks(x)\n        # Apply final LayerNorm\n        x = self.ln_f(x)\n        # Map to vocabulary logits: (B, T, n_embd) -&gt; (B, T, vocab_size)\n        logits = self.lm_head(x)\n\n        # Calculate loss if targets are provided (same as before)\n        if targets is None:\n            loss = None\n        else:\n            # Reshape for cross_entropy: (B*T, vocab_size) and (B*T)\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        \"\"\"Generate new text given a starting sequence.\"\"\"\n        for _ in range(max_new_tokens):\n            # Crop context `idx` to the last `block_size` tokens. Important as position embeddings only go up to block_size.\n            idx_cond = idx[:, -block_size:]\n            # Get predictions (logits) from the model\n            logits, loss = self(idx_cond)\n            # Focus on the logits for the *last* time step: (B, C)\n            logits = logits[:, -1, :]\n            # Convert logits to probabilities via softmax\n            probs = F.softmax(logits, dim=-1)   # (B, C)\n            # Sample next token index from the probability distribution\n            idx_next = torch.multinomial(probs, num_samples=1)   # (B, 1)\n            # Append the sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1)   # (B, T+1)\n        return idx\n# =========================\n\n# ===== MODEL INITIALIZATION AND TRAINING =====\n# Create model instance and move it to the selected device\nmodel = BigramLanguageModel()\nm = model.to(device)\n# Print number of parameters (useful for understanding model size)\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters') # Calculate and print M parameters\n\n# Create optimizer (AdamW again)\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor iter in range(max_iters):\n    # Evaluate loss periodically\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss() # Get train/val loss using the helper function\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\") # Print losses\n\n    # Sample a batch of data\n    xb, yb = get_batch('train')\n\n    # Forward pass: Evaluate loss\n    logits, loss = model(xb, yb)\n    # Backward pass: Calculate gradients\n    optimizer.zero_grad(set_to_none=True) # Zero gradients\n    loss.backward() # Backpropagation\n    # Update parameters\n    optimizer.step() # Optimizer step\n\n# Generate text from the trained model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device) # Starting context: [[0]]\nprint(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n# ============================================\n\n\nUsing device: mps\n0.209729 M parameters\nstep 0: train loss 4.4116, val loss 4.4022\nstep 100: train loss 2.6568, val loss 2.6670\nstep 200: train loss 2.5091, val loss 2.5059\nstep 300: train loss 2.4194, val loss 2.4336\nstep 400: train loss 2.3499, val loss 2.3563\nstep 500: train loss 2.2963, val loss 2.3126\nstep 600: train loss 2.2411, val loss 2.2501\nstep 700: train loss 2.2053, val loss 2.2188\nstep 800: train loss 2.1645, val loss 2.1882\nstep 900: train loss 2.1238, val loss 2.1498\nstep 1000: train loss 2.1027, val loss 2.1297\nstep 1100: train loss 2.0699, val loss 2.1186\nstep 1200: train loss 2.0394, val loss 2.0806\nstep 1300: train loss 2.0255, val loss 2.0644\nstep 1400: train loss 1.9924, val loss 2.0376\nstep 1500: train loss 1.9697, val loss 2.0303\nstep 1600: train loss 1.9644, val loss 2.0482\nstep 1700: train loss 1.9413, val loss 2.0122\nstep 1800: train loss 1.9087, val loss 1.9949\nstep 1900: train loss 1.9106, val loss 1.9898\nstep 2000: train loss 1.8858, val loss 1.9993\nstep 2100: train loss 1.8722, val loss 1.9762\nstep 2200: train loss 1.8602, val loss 1.9636\nstep 2300: train loss 1.8577, val loss 1.9551\nstep 2400: train loss 1.8442, val loss 1.9467\nstep 2500: train loss 1.8153, val loss 1.9439\nstep 2600: train loss 1.8224, val loss 1.9363\nstep 2700: train loss 1.8125, val loss 1.9370\nstep 2800: train loss 1.8054, val loss 1.9250\nstep 2900: train loss 1.8045, val loss 1.9336\nstep 3000: train loss 1.7950, val loss 1.9202\nstep 3100: train loss 1.7707, val loss 1.9197\nstep 3200: train loss 1.7545, val loss 1.9107\nstep 3300: train loss 1.7569, val loss 1.9075\nstep 3400: train loss 1.7533, val loss 1.8942\nstep 3500: train loss 1.7374, val loss 1.8960\nstep 3600: train loss 1.7268, val loss 1.8909\nstep 3700: train loss 1.7277, val loss 1.8814\nstep 3800: train loss 1.7188, val loss 1.8889\nstep 3900: train loss 1.7194, val loss 1.8714\nstep 4000: train loss 1.7127, val loss 1.8636\nstep 4100: train loss 1.7073, val loss 1.8710\nstep 4200: train loss 1.7022, val loss 1.8597\nstep 4300: train loss 1.6994, val loss 1.8488\nstep 4400: train loss 1.7048, val loss 1.8664\nstep 4500: train loss 1.6860, val loss 1.8461\nstep 4600: train loss 1.6854, val loss 1.8304\nstep 4700: train loss 1.6841, val loss 1.8469\nstep 4800: train loss 1.6655, val loss 1.8454\nstep 4900: train loss 1.6713, val loss 1.8387\nstep 4999: train loss 1.6656, val loss 1.8277\n\nFoast.\n\nMENENIUS:\nPraviely your niews? I cank, CORiced aggele;\nOr heave worth sunt bone Ammiod, Lord,\nWho is make thy batted oub! servilings\nToke as lihtch you basw to see swife,\nIs letsts lown'd us; to lace and though mistrair took\nAnd the proply enstriaghte for a shien.\nWhy, they foul tlead,\nup is later and\nbehoy cried men as thou beatt his you.\n\nHERRY VI:\nThere, you weaks mirre and all was imper, Then death, doth those I will read;\nWeas sul't is King me, I what lady so not this dire.\n\nROMEO:\nO, upon to death! him not this bornorow-prove.\n\nMUCIOND:\nWhy leave ye no you?\n\nDUCUCHESTEH:\nBut one thyies, if will the save your blages wore I mong father you hast;\nAlaitle not arm thither crown tow doth.\n\nFROM WTARDit't me reven.\n\nWARWICK:\nOr, as extress womb voishmas!\nGood me you; and incaes up done! make,\nOr I serigh to emmequerel, to speak, herse to supomet?\n\nLUCIO:\nThe like, But twast on was theirs\npoor of thou do\nAs hath lay but so bredaint, forweet of For which his lictless me,\nThat while fumseriands thy unclity,\nWheree I wam my broth? am the too to virsant, whould enterfuly,\nAll there, ontreman one his him;\nWhen whom to Luvinge one the rews,\nWarwixt kill himfined me the bights the with and\nThost will in him,\nMor Sonme man, make to men, Must took.\n\nServer:\nIs aid the underer you: if\nThe I holseld at most lost! Comioli his but a bedrip thy lord,\nAnd then you pringent, and what you kingle is a gestreface is ears.\nBut take me. Tis basdeh,--\nCendom to nie,\nYou lordone turn to mine hath dels in woo forth.\nPoy devisecity, Ineed and encont\nOnking, pleasiness, here's me?\nWhat the have of the doet.\n\nClaytAM:\nNow tweett, cour is plose,\nOstate, and you raint this made untu\nWith ould to Warwith that me bone;\nWill him drown the have wesest: doth,\nAre goody gent yours the pot opings, time same, that BI thirself have gative. I' cown love this mind,\nNether if thou her fortune they have fight my ftlair aggainst for him burry.\n\nBRUTUS:\nWhoth lost for for leth\nAnd, being eyes\nAnd if for\n\n\n\n\n\n\n\n\nNote\n\n\n\nWith 5000 iterations, the model is able to generate text that is similar to the training text.\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-04-15-unit01/nano-gpt/annotated-gpt_dev.html#building-a-gpt",
    "href": "post/2025-04-15-unit01/nano-gpt/annotated-gpt_dev.html#building-a-gpt",
    "title": "Building a GPT - companion notebook annotated",
    "section": "",
    "text": "Companion notebook to the Zero To Hero video on GPT. Downloaded from here\n(https://github.com/karpathy/nanoGPT)\n\n\n\n\nShow the code\n# Download the tiny shakespeare dataset\n#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n\n\n\nShow the code\n# read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n\n\n\nShow the code\n# print the length of the dataset\nprint(\"length of dataset in characters: \", len(text))\n\n\nlength of dataset in characters:  1115394\n\n\n\n\nShow the code\n# let's look at the first 1000 characters\nprint(text[:1000])\n\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n\n\n\n\nShow the code\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n\n\n\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n\n\n\n\n\n\n\nShow the code\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n\n\n[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n\n\n\n\n\n\n\nShow the code\n# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch # we use PyTorch: [https://pytorch.org](https://pytorch.org)\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this\n\n\ntorch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n\n\n\n\n\n\n\nShow the code\n# split up the data into train and validation sets\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n\n\n\n\n\n\nShow the code\nblock_size = 8\ntrain_data[:block_size+1]\n\n\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n\n\n\n\n\n\n\nShow the code\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n\n\nwhen input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n\n\n\n\n\n\n\nShow the code\ntorch.manual_seed(1337)\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n\n\ninputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\nwhen input is [24] the target: 43\nwhen input is [24, 43] the target: 58\nwhen input is [24, 43, 58] the target: 5\nwhen input is [24, 43, 58, 5] the target: 57\nwhen input is [24, 43, 58, 5, 57] the target: 1\nwhen input is [24, 43, 58, 5, 57, 1] the target: 46\nwhen input is [24, 43, 58, 5, 57, 1, 46] the target: 43\nwhen input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\nwhen input is [44] the target: 53\nwhen input is [44, 53] the target: 56\nwhen input is [44, 53, 56] the target: 1\nwhen input is [44, 53, 56, 1] the target: 58\nwhen input is [44, 53, 56, 1, 58] the target: 46\nwhen input is [44, 53, 56, 1, 58, 46] the target: 39\nwhen input is [44, 53, 56, 1, 58, 46, 39] the target: 58\nwhen input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52] the target: 58\nwhen input is [52, 58] the target: 1\nwhen input is [52, 58, 1] the target: 58\nwhen input is [52, 58, 1, 58] the target: 46\nwhen input is [52, 58, 1, 58, 46] the target: 39\nwhen input is [52, 58, 1, 58, 46, 39] the target: 58\nwhen input is [52, 58, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\nwhen input is [25] the target: 17\nwhen input is [25, 17] the target: 27\nwhen input is [25, 17, 27] the target: 10\nwhen input is [25, 17, 27, 10] the target: 0\nwhen input is [25, 17, 27, 10, 0] the target: 21\nwhen input is [25, 17, 27, 10, 0, 21] the target: 1\nwhen input is [25, 17, 27, 10, 0, 21, 1] the target: 54\nwhen input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n\n\n\n\n\n\n\nShow the code\n# define the bigram language model\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\n\n\n\n\nLoss = -\\sum_{i}(y_i * \\log(p_i))x\nwhere:\ny_i = actual probability (0 or 1 for the i-th class) p_i = predicted probability for the i-th class \\sum = sum over all classes (characters)\nThis is the loss for a single token prediction. The total loss reported by F.cross_entropy is the average loss across all B*T tokens in the batch, where:\nB = batch_size T = block_size (sequence length)\nBefore training, we would expect the model to predict the next character from a uniform distribution (random guessing). The probability for the correct character would be 1 / \\text{vocab_size}.\nExpected initial loss \\approx - \\log(1 / \\text{vocab_size}) = \\log(\\text{vocab_size}) \\log(65) \\approx 4.1744\n\n\n\n\n\nShow the code\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb) # xb/yb are from the previous cell (B=4, T=8)\nprint(logits.shape) # Expected: (B, T, C) = (4, 8, 65)\nprint(loss) # Expected: Around 4.17\n\n\ntorch.Size([32, 65])\ntensor(4.8786, grad_fn=&lt;NllLossBackward0&gt;)\n\n\n\n\n\n\n\nShow the code\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\n\n\nSKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\nwnYWmnxKWWev-tDqXErVKLgJ\n\n\n\n\n\n\n\nShow the code\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\n\n\n\n\n\n\nShow the code\nbatch_size = 32 # Redefine batch size for training\nfor steps in range(100): # # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n\n\n4.65630578994751\n\n\n\n\n\n\n\nShow the code\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n\n\n\noTo.JUZ!!zqe!\nxBP qbs$Gy'AcOmrLwwt\np$x;Seh-onQbfM?OjKbn'NwUAW -Np3fkz$FVwAUEa-wzWC -wQo-R!v -Mj?,SPiTyZ;o-opr$mOiPJEYD-CfigkzD3p3?zvS;ADz;.y?o,ivCuC'zqHxcVT cHA\nrT'Fd,SBMZyOslg!NXeF$sBe,juUzLq?w-wzP-h\nERjjxlgJzPbHxf$ q,q,KCDCU fqBOQT\nSV&CW:xSVwZv'DG'NSPypDhKStKzC -$hslxIVzoivnp ,ethA:NCCGoi\ntN!ljjP3fwJMwNelgUzzPGJlgihJ!d?q.d\npSPYgCuCJrIFtb\njQXg\npA.P LP,SPJi\nDBcuBM:CixjJ$Jzkq,OLf3KLQLMGph$O 3DfiPHnXKuHMlyjxEiyZib3FaHV-oJa!zoc'XSP :CKGUhd?lgCOF$;;DTHZMlvvcmZAm;:iv'MMgO&Ywbc;BLCUd&vZINLIzkuTGZa\nD.?\n\n\n\n\n\ntoy example illustrating how matrix multiplication can be used for a “weighted aggregation”\n\n\nShow the code\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3)) # Lower triangular matrix of 1s\na = a / torch.sum(a, 1, keepdim=True) # Normalize rows to sum to 1\nb = torch.randint(0,10,(3,2)).float() # Some data\nc = a @ b # Matrix multiply\nprint('a=')\nprint(a)\nprint('--')\nprint('b=')\nprint(b)\nprint('--')\nprint('c=')\nprint(c)\n\n\na=\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n--\nb=\ntensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n--\nc=\ntensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n\n\n\n\nShow the code\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2 # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\n\ntorch.Size([4, 8, 2])\n\n\n\n\n\n\n\nShow the code\n# We want x[b,t] = mean_{i&lt;=t} x[b,i]\nxbow = torch.zeros((B,T,C)) # x bag-of-words (running average)\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # Select vectors from start up to time t: shape (t+1, C)\n        xbow[b,t] = torch.mean(xprev, 0) # Compute mean along the time dimension (dim 0)\n\n\n\n\n\n\n\nShow the code\n# Create the averaging weight matrix\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True) # Normalize rows to sum to 1\n# Perform batched matrix multiplication\nxbow2 = wei @ x # (T, T) @ (B, T, C) -&gt; (B, T, C) via broadcasting\ntorch.allclose(xbow, xbow2) # Check if results are identical\n\n\nTrue\n\n\n\n\n\n\n\nShow the code\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\n# Mask out future positions by setting them to -infinity before softmax\nwei = wei.masked_fill(tril == 0, float('-inf'))\n# Apply softmax to get row-wise probability distributions (weights)\nwei = F.softmax(wei, dim=-1)\n# Perform weighted aggregation\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3) # Check if results are identical\n\n\nTrue\n\n\n\n\n\nsoftmax(z_i) = \\frac{e^{z_i}}{\\sum_j e^{z_j}}\n\n\n\n\n\nShow the code\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels (embedding dimension)\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)   # (B, T, head_size)\nq = query(x) # (B, T, head_size)\n# Compute attention scores (\"affinities\")\nwei = q @ k.transpose(-2, -1) # (B, T, hs) @ (B, hs, T) ---&gt; (B, T, T)\n\n# Scale the scores\n# Note: Karpathy uses C**-0.5 here (sqrt(embedding_dim)). Standard Transformer uses sqrt(head_size).\nwei = wei * (C**-0.5)\n\n# Apply causal mask\ntril = torch.tril(torch.ones(T, T))\n#wei = torch.zeros((T,T)) # This line is commented out in original, was from softmax demo\nwei = wei.masked_fill(tril == 0, float('-inf')) # Mask future tokens\n\n# Apply softmax to get attention weights\nwei = F.softmax(wei, dim=-1) # (B, T, T)\n\n# Perform weighted aggregation of Values\nv = value(x) # (B, T, head_size)\nout = wei @ v # (B, T, T) @ (B, T, hs) ---&gt; (B, T, hs)\n#out = wei @ x # This would aggregate original x, not the projected values 'v'\n\nout.shape # Expected: (B, T, head_size) = (4, 8, 16)\n\n\ntorch.Size([4, 8, 16])\n\n\n\n\nShow the code\nwei[0] # Show attention weights for the first sequence in the batch\n\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.4264, 0.5736, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3151, 0.3022, 0.3827, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3007, 0.2272, 0.2467, 0.2253, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1635, 0.2048, 0.1776, 0.1616, 0.2926, 0.0000, 0.0000, 0.0000],\n        [0.1403, 0.2272, 0.1454, 0.1244, 0.2678, 0.0949, 0.0000, 0.0000],\n        [0.1554, 0.1815, 0.1224, 0.1213, 0.1428, 0.1603, 0.1164, 0.0000],\n        [0.0952, 0.1217, 0.1130, 0.1453, 0.1137, 0.1180, 0.1467, 0.1464]],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\n\n\n\n\nnC = 64\nX = matrix(rnorm(4*64), nrow=4, ncol=nC)\n## make it so that the third token is similar to the last one\nX[2,] = X[4,]*0.5 + X[2,]*0.5\n## normalize X\nX = t(scale(t(X)))\n\nq = X\nk = X\nv = X\n\nqkt = q %*% t(k)/(nC-1)\nxcor = cor(t(q),t(k))\ndim(xcor)\ndim(qkt)\ncat(\"xcor\\n\")\nxcor\ncat(\"---\\n qkt\\n\")\nqkt\n\ncat(\"are xcor and qkt equal?\")\nall.equal(xcor, qkt)\n\npar(mar=c(5, 6, 4, 2) + 0.1)  # increase left margin to avoid cutting of the y label\npar(pty=\"s\")  # Set plot type to \"square\"\nplot(c(xcor), c(qkt),cex=3,cex.lab=3,cex.axis=2,cex.main=2,cex.sub=2); abline(0,1)\npar(pty=\"m\")  # Reset to default plot type\npar(mar=c(5, 4, 4, 2) + 0.1)  # Reset to default margins\n\n\n\n\nAttention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n\nThere is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens. example: “the cat sat on the mat” should be different from “the mat sat on the cat”\nEach example across batch dimension is of course processed completely independently and never “talk” to each other.\nIn an “encoder” attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a “decoder” attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n“self-attention” just means that the keys and values are produced from the same source as queries (all come from x). In “cross-attention”, the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n\n\n\n\n“Scaled” attention additionaly divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n\n\nShow the code\n# Demonstrate variance without scaling\nk_unscaled = torch.randn(B,T,head_size)\nq_unscaled = torch.randn(B,T,head_size)\nwei_unscaled = q_unscaled @ k_unscaled.transpose(-2, -1)\nprint(f\"k var: {k_unscaled.var():.4f}, q var: {q_unscaled.var():.4f}, wei (unscaled) var: {wei_unscaled.var():.4f}\")\n\n# Demonstrate variance *with* scaling (using head_size for illustration)\nk = torch.randn(B,T,head_size)\nq = torch.randn(B,T,head_size)\nwei = q @ k.transpose(-2, -1) * head_size**-0.5 # Scale by sqrt(head_size)\nprint(f\"k var: {k.var():.4f}, q var: {q.var():.4f}, wei (scaled) var: {wei.var():.4f}\") # Variance should be closer to 1\n\n\nk var: 1.0449, q var: 1.0700, wei (unscaled) var: 17.4690\nk var: 0.9006, q var: 1.0037, wei (scaled) var: 0.9957\n\n\n\n\nShow the code\nk.var() # Should be close to 1\n\n\ntensor(0.9006)\n\n\n\n\nShow the code\nq.var() # Should be close to 1\n\n\ntensor(1.0037)\n\n\n\n\nShow the code\nwei.var() # With scaling, should be closer to 1 than head_size (16)\n\n\ntensor(0.9957)\n\n\n\n\nShow the code\n# Softmax with small inputs (diffuse distribution)\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n\n\ntensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n\n\n\n\nShow the code\n# Softmax with large inputs (simulating unscaled attention scores) -&gt; peaks\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot\n\n\ntensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n\n\n\n\n\n\n\nShow the code\nclass LayerNorm1d: # (used to be BatchNorm1d)\n    def __init__(self, dim, eps=1e-5, momentum=0.1): # Momentum is not used in typical LayerNorm\n        self.eps = eps\n        # Learnable scale and shift parameters, initialized to 1 and 0\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        # Calculate mean over the *last* dimension (features/embedding)\n        xmean = x.mean(1, keepdim=True) # batch mean (shape B, 1, C if input B, T, C) --&gt; Needs adjustment for (B,C) input shape here. Assumes input is (B, dim)\n        # Correction: x is (32, 100). dim=1 is correct for features. Shape (32, 1)\n        xvar = x.var(1, keepdim=True) # batch variance (shape 32, 1)\n        # Normalize each feature vector independently\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n        # Apply scale and shift\n        self.out = self.gamma * xhat + self.beta\n        return self.out\n\n    def parameters(self):\n        # Expose gamma and beta as learnable parameters\n        return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = LayerNorm1d(100) # Create LayerNorm for 100 features\nx = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\nx = module(x)\nx.shape # Should be (32, 100)\n\n\ntorch.Size([32, 100])\n\n\nExplanation of layernorm\nInput shape: (B, T, C) where: B = batch size T = sequence length (number of tokens) C = embedding dimension (features of each token) For each token in the sequence (each position T), LayerNorm: Takes its embedding vector of size C Calculates the mean and standard deviation of just that vector Normalizes that vector by subtracting its mean and dividing by its standard deviation Applies the learnable scale (gamma) and shift (beta) parameters So if you have a sequence like “The cat sat”, and each word is represented by a 64-dimensional embedding vector, LayerNorm would: Take “The”’s 64-dimensional vector and normalize it Take “cat”’s 64-dimensional vector and normalize it Take “sat”’s 64-dimensional vector and normalize it Each token’s vector is normalized independently of the others. This is different from BatchNorm, which would normalize across the batch dimension (i.e., looking at the same position across different examples in the batch). This per-token normalization helps maintain stable gradients during training and is particularly important in Transformers where the attention mechanism needs to work with normalized vectors to compute meaningful attention scores.\n\n\nShow the code\n# Mean and std of the first feature *across the batch*. Not expected to be 0 and 1.\nx[:,0].mean(), x[:,0].std()\n\n\n(tensor(0.1469), tensor(0.8803))\n\n\n\n\nShow the code\n# Mean and std *across features* for the first item in the batch. Expected to be ~0 and ~1.\nx[0,:].mean(), x[0,:].std()\n\n\n(tensor(2.3842e-09), tensor(1.0000))\n\n\n\n\n\n\n\nShow the code\n# &lt;--------- ENCODE ------------------&gt;&lt;--------------- DECODE -----------------&gt;\n# les réseaux de neurones sont géniaux! &lt;START&gt; neural networks are awesome!&lt;END&gt;\n\n\n\n\n\n\n\nShow the code\n# Import necessary PyTorch modules\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# ===== HYPERPARAMETERS =====\nbatch_size = 16       # Number of sequences per batch (Smaller than Bigram training)\nblock_size = 32       # Context length (Larger than Bigram demo)\nmax_iters = 5000      # Total training iterations (More substantial training) TODO change to 5000 later\neval_interval = 100   # How often to check validation loss\nlearning_rate = 1e-3  # Optimizer learning rate\neval_iters = 200      # Number of batches to average for validation loss estimate\nn_embd = 64           # Embedding dimension (Size of token vectors)\nn_head = 4            # Number of attention heads\nn_layer = 4           # Number of Transformer blocks (layers)\ndropout = 0.0         # Dropout probability (0.0 means no dropout here)\n# ==========================\n\n# Device selection: MPS (Apple Silicon) &gt; CUDA &gt; CPU\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")   # Apple Silicon GPU\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")  # NVIDIA GPU\nelse:\n    device = torch.device(\"cpu\")   # CPU fallback\nprint(f\"Using device: {device}\")\n\n# Set random seed for reproducibility\ntorch.manual_seed(1337)\nif device.type == 'cuda':\n    torch.cuda.manual_seed(1337)\nelif device.type == 'mps':\n    torch.mps.manual_seed(1337)\n\n# Load and read the training text (assuming input.txt is available)\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# ===== DATA PREPROCESSING =====\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nstoi = { ch:i for i,ch in enumerate(chars) }   # string to index\nitos = { i:ch for i,ch in enumerate(chars) }   # index to string\nencode = lambda s: [stoi[c] for c in s]   # convert string to list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])   # convert list of integers to string\n\n# Split data into training and validation sets\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data))   # first 90% for training\ntrain_data = data[:n]\nval_data = data[n:]\n# =============================\n\n# ===== DATA LOADING FUNCTION =====\ndef get_batch(split):\n    \"\"\"Generate a batch of data for training or validation.\"\"\"\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device) # Move data to the target device\n    return x, y\n# ================================\n\n# ===== LOSS ESTIMATION FUNCTION =====\n@torch.no_grad()   # Disable gradient calculation for efficiency\ndef estimate_loss():\n    \"\"\"Estimate the loss on training and validation sets.\"\"\"\n    out = {}\n    model.eval()   # Set model to evaluation mode\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()  # Set model back to training mode\n    return out\n# ===================================\n\n# ===== ATTENTION HEAD IMPLEMENTATION =====\nclass Head(nn.Module):\n    \"\"\"Single head of self-attention.\"\"\"\n    \n    def __init__(self, head_size):\n        super().__init__()\n        # Linear projections for Key, Query, Value\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        # Causal mask (tril). 'register_buffer' makes it part of the model state but not a parameter to be trained.\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        # Dropout layer (applied after softmax)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape # C here is n_embd\n        # Project input to K, Q, V\n        k = self.key(x)   # (B,T,head_size)\n        q = self.query(x) # (B,T,head_size)\n        # Compute attention scores, scale, mask, softmax\n        # Note the scaling by C**-0.5 (sqrt(n_embd)) as discussed before\n        wei = q @ k.transpose(-2,-1) * C**-0.5   # (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))   # Use dynamic slicing [:T, :T] for flexibility if T &lt; block_size\n        wei = F.softmax(wei, dim=-1)   # (B, T, T)\n        wei = self.dropout(wei) # Apply dropout to attention weights\n        # Weighted aggregation of values\n        v = self.value(x) # (B,T,head_size)\n        out = wei @ v # (B, T, T) @ (B, T, head_size) -&gt; (B, T, head_size)\n        return out\n# ========================================\n\n# ===== MULTI-HEAD ATTENTION =====\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        # Linear layer after concatenating heads\n        self.proj = nn.Linear(n_embd, n_embd) # Projects back to n_embd dimension\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # Compute attention for each head and concatenate results\n        out = torch.cat([h(x) for h in self.heads], dim=-1) # Shape (B, T, num_heads * head_size) = (B, T, n_embd)\n        # Apply final projection and dropout\n        out = self.dropout(self.proj(out))\n        return out\n# ===============================\n\n# ===== FEED-FORWARD NETWORK =====\nclass FeedFoward(nn.Module):\n    \"\"\"Simple position-wise feed-forward network with one hidden layer.\"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),   # Expand dimension (common practice)\n            nn.ReLU(),                      # Non-linearity\n            nn.Linear(4 * n_embd, n_embd),   # Project back to original dimension\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n# ==============================\n\n# ===== TRANSFORMER BLOCK =====\nclass Block(nn.Module):\n    \"\"\"Transformer block: communication (attention) followed by computation (FFN).\"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head   # Calculate size for each head\n        self.sa = MultiHeadAttention(n_head, head_size) # Self-Attention layer\n        self.ffwd = FeedFoward(n_embd) # Feed-Forward layer\n        self.ln1 = nn.LayerNorm(n_embd) # LayerNorm for Attention input\n        self.ln2 = nn.LayerNorm(n_embd) # LayerNorm for FFN input\n\n    def forward(self, x):\n        # Pre-Normalization variant: Norm -&gt; Sublayer -&gt; Residual\n        x = x + self.sa(self.ln1(x))  # Attention block\n        x = x + self.ffwd(self.ln2(x)) # Feed-forward block\n        return x\n# ============================\n\n# ===== LANGUAGE MODEL =====\nclass BigramLanguageModel(nn.Module):\n    \"\"\"GPT-like language model using Transformer blocks.\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        # Token Embedding Table: Maps character index to embedding vector. (vocab_size, n_embd)\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        # Position Embedding Table: Maps position index (0 to block_size-1) to embedding vector. (block_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        # Sequence of Transformer Blocks\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        # Final Layer Normalization (applied after blocks)\n        self.ln_f = nn.LayerNorm(n_embd)   # Final layer norm\n        # Linear Head: Maps final embedding back to vocabulary size to get logits. (n_embd, vocab_size)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        \n        # Get token embeddings from indices: (B, T) -&gt; (B, T, n_embd)\n        tok_emb = self.token_embedding_table(idx)\n        # Get position embeddings: Create indices 0..T-1, look up embeddings -&gt; (T, n_embd)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n        # Combine token and position embeddings by addition: (B, T, n_embd). Broadcasting handles the addition.\n        x = tok_emb + pos_emb   # (B,T,C)\n        # Pass through Transformer blocks: (B, T, n_embd) -&gt; (B, T, n_embd)\n        x = self.blocks(x)\n        # Apply final LayerNorm\n        x = self.ln_f(x)\n        # Map to vocabulary logits: (B, T, n_embd) -&gt; (B, T, vocab_size)\n        logits = self.lm_head(x)\n\n        # Calculate loss if targets are provided (same as before)\n        if targets is None:\n            loss = None\n        else:\n            # Reshape for cross_entropy: (B*T, vocab_size) and (B*T)\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        \"\"\"Generate new text given a starting sequence.\"\"\"\n        for _ in range(max_new_tokens):\n            # Crop context `idx` to the last `block_size` tokens. Important as position embeddings only go up to block_size.\n            idx_cond = idx[:, -block_size:]\n            # Get predictions (logits) from the model\n            logits, loss = self(idx_cond)\n            # Focus on the logits for the *last* time step: (B, C)\n            logits = logits[:, -1, :]\n            # Convert logits to probabilities via softmax\n            probs = F.softmax(logits, dim=-1)   # (B, C)\n            # Sample next token index from the probability distribution\n            idx_next = torch.multinomial(probs, num_samples=1)   # (B, 1)\n            # Append the sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1)   # (B, T+1)\n        return idx\n# =========================\n\n# ===== MODEL INITIALIZATION AND TRAINING =====\n# Create model instance and move it to the selected device\nmodel = BigramLanguageModel()\nm = model.to(device)\n# Print number of parameters (useful for understanding model size)\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters') # Calculate and print M parameters\n\n# Create optimizer (AdamW again)\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor iter in range(max_iters):\n    # Evaluate loss periodically\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss() # Get train/val loss using the helper function\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\") # Print losses\n\n    # Sample a batch of data\n    xb, yb = get_batch('train')\n\n    # Forward pass: Evaluate loss\n    logits, loss = model(xb, yb)\n    # Backward pass: Calculate gradients\n    optimizer.zero_grad(set_to_none=True) # Zero gradients\n    loss.backward() # Backpropagation\n    # Update parameters\n    optimizer.step() # Optimizer step\n\n# Generate text from the trained model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device) # Starting context: [[0]]\nprint(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n# ============================================\n\n\nUsing device: mps\n0.209729 M parameters\nstep 0: train loss 4.4116, val loss 4.4022\nstep 100: train loss 2.6568, val loss 2.6670\nstep 200: train loss 2.5091, val loss 2.5059\nstep 300: train loss 2.4194, val loss 2.4336\nstep 400: train loss 2.3499, val loss 2.3563\nstep 500: train loss 2.2963, val loss 2.3126\nstep 600: train loss 2.2411, val loss 2.2501\nstep 700: train loss 2.2053, val loss 2.2188\nstep 800: train loss 2.1645, val loss 2.1882\nstep 900: train loss 2.1238, val loss 2.1498\nstep 1000: train loss 2.1027, val loss 2.1297\nstep 1100: train loss 2.0699, val loss 2.1186\nstep 1200: train loss 2.0394, val loss 2.0806\nstep 1300: train loss 2.0255, val loss 2.0644\nstep 1400: train loss 1.9924, val loss 2.0376\nstep 1500: train loss 1.9697, val loss 2.0303\nstep 1600: train loss 1.9644, val loss 2.0482\nstep 1700: train loss 1.9413, val loss 2.0122\nstep 1800: train loss 1.9087, val loss 1.9949\nstep 1900: train loss 1.9106, val loss 1.9898\nstep 2000: train loss 1.8858, val loss 1.9993\nstep 2100: train loss 1.8722, val loss 1.9762\nstep 2200: train loss 1.8602, val loss 1.9636\nstep 2300: train loss 1.8577, val loss 1.9551\nstep 2400: train loss 1.8442, val loss 1.9467\nstep 2500: train loss 1.8153, val loss 1.9439\nstep 2600: train loss 1.8224, val loss 1.9363\nstep 2700: train loss 1.8125, val loss 1.9370\nstep 2800: train loss 1.8054, val loss 1.9250\nstep 2900: train loss 1.8045, val loss 1.9336\nstep 3000: train loss 1.7950, val loss 1.9202\nstep 3100: train loss 1.7707, val loss 1.9197\nstep 3200: train loss 1.7545, val loss 1.9107\nstep 3300: train loss 1.7569, val loss 1.9075\nstep 3400: train loss 1.7533, val loss 1.8942\nstep 3500: train loss 1.7374, val loss 1.8960\nstep 3600: train loss 1.7268, val loss 1.8909\nstep 3700: train loss 1.7277, val loss 1.8814\nstep 3800: train loss 1.7188, val loss 1.8889\nstep 3900: train loss 1.7194, val loss 1.8714\nstep 4000: train loss 1.7127, val loss 1.8636\nstep 4100: train loss 1.7073, val loss 1.8710\nstep 4200: train loss 1.7022, val loss 1.8597\nstep 4300: train loss 1.6994, val loss 1.8488\nstep 4400: train loss 1.7048, val loss 1.8664\nstep 4500: train loss 1.6860, val loss 1.8461\nstep 4600: train loss 1.6854, val loss 1.8304\nstep 4700: train loss 1.6841, val loss 1.8469\nstep 4800: train loss 1.6655, val loss 1.8454\nstep 4900: train loss 1.6713, val loss 1.8387\nstep 4999: train loss 1.6656, val loss 1.8277\n\nFoast.\n\nMENENIUS:\nPraviely your niews? I cank, CORiced aggele;\nOr heave worth sunt bone Ammiod, Lord,\nWho is make thy batted oub! servilings\nToke as lihtch you basw to see swife,\nIs letsts lown'd us; to lace and though mistrair took\nAnd the proply enstriaghte for a shien.\nWhy, they foul tlead,\nup is later and\nbehoy cried men as thou beatt his you.\n\nHERRY VI:\nThere, you weaks mirre and all was imper, Then death, doth those I will read;\nWeas sul't is King me, I what lady so not this dire.\n\nROMEO:\nO, upon to death! him not this bornorow-prove.\n\nMUCIOND:\nWhy leave ye no you?\n\nDUCUCHESTEH:\nBut one thyies, if will the save your blages wore I mong father you hast;\nAlaitle not arm thither crown tow doth.\n\nFROM WTARDit't me reven.\n\nWARWICK:\nOr, as extress womb voishmas!\nGood me you; and incaes up done! make,\nOr I serigh to emmequerel, to speak, herse to supomet?\n\nLUCIO:\nThe like, But twast on was theirs\npoor of thou do\nAs hath lay but so bredaint, forweet of For which his lictless me,\nThat while fumseriands thy unclity,\nWheree I wam my broth? am the too to virsant, whould enterfuly,\nAll there, ontreman one his him;\nWhen whom to Luvinge one the rews,\nWarwixt kill himfined me the bights the with and\nThost will in him,\nMor Sonme man, make to men, Must took.\n\nServer:\nIs aid the underer you: if\nThe I holseld at most lost! Comioli his but a bedrip thy lord,\nAnd then you pringent, and what you kingle is a gestreface is ears.\nBut take me. Tis basdeh,--\nCendom to nie,\nYou lordone turn to mine hath dels in woo forth.\nPoy devisecity, Ineed and encont\nOnking, pleasiness, here's me?\nWhat the have of the doet.\n\nClaytAM:\nNow tweett, cour is plose,\nOstate, and you raint this made untu\nWith ould to Warwith that me bone;\nWill him drown the have wesest: doth,\nAre goody gent yours the pot opings, time same, that BI thirself have gative. I' cown love this mind,\nNether if thou her fortune they have fight my ftlair aggainst for him burry.\n\nBRUTUS:\nWhoth lost for for leth\nAnd, being eyes\nAnd if for\n\n\n\n\n\n\n\n\nNote\n\n\n\nWith 5000 iterations, the model is able to generate text that is similar to the training text."
  },
  {
    "objectID": "post/2025-04-15-unit01/nano-gpt/slides-gpt.html",
    "href": "post/2025-04-15-unit01/nano-gpt/slides-gpt.html",
    "title": "Building a GPT From Scratch summary",
    "section": "",
    "text": "Karpathy’s gpt_dev.ipynb summarized by gemini, reviewed by me.\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-04-15-unit01/nano-gpt/slides-gpt.html#building-a-gpt-from-scratch-a-character-level-approach",
    "href": "post/2025-04-15-unit01/nano-gpt/slides-gpt.html#building-a-gpt-from-scratch-a-character-level-approach",
    "title": "Building a GPT From Scratch summary",
    "section": "Building a GPT From Scratch: A Character-Level Approach",
    "text": "Building a GPT From Scratch: A Character-Level Approach\n\nUnderstanding the Core Concepts (Based on A. Karpathy’s NanoGPT)"
  },
  {
    "objectID": "DRAFTS/2025-02-20-testing/gradient-descent-illustration.html",
    "href": "DRAFTS/2025-02-20-testing/gradient-descent-illustration.html",
    "title": "Gradient descent illustration",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nfrom plotnine import *\n\n\n# Loss function and its gradient\ndef loss(beta):\n    return beta ** 2\n\ndef grad(beta):\n    return 2 * beta\n\n# Simulate gradient descent\nbeta = 3.5  # random initial value\nlearning_rate = 0.2\nsteps = []\n\nfor i in range(10):\n    current_loss = loss(beta)\n    steps.append({'step': i, 'beta': beta, 'loss': current_loss})\n    beta -= learning_rate * grad(beta)\n\ndf = pd.DataFrame(steps)\n\n# Create full curve for loss function\nbeta_vals = np.linspace(-4, 4, 200)\nloss_vals = loss(beta_vals)\ndf_curve = pd.DataFrame({'beta': beta_vals, 'loss': loss_vals})\n\n# Create arrows for learning steps\ndf_arrows = df[:-1].copy()\ndf_arrows['beta_end'] = df['beta'][1:].values\ndf_arrows['loss_end'] = df['loss'][1:].values\n\n# Plot\np = (\n    ggplot(df_curve, aes('beta', 'loss')) +\n    geom_line(size=1.5, color=\"gray\") +\n    geom_point(df, aes('beta', 'loss'), color='purple', size=3) +\n    geom_point(df.tail(1), aes('beta', 'loss'), color='yellow', size=4) +\n    geom_segment(df_arrows,\n                 aes(x='beta', y='loss', xend='beta_end', yend='loss_end'),\n                 arrow=arrow(length=0.15, type='closed'),\n                 color='gray') +\n    annotate('text', x=3.5, y=12, label='Random\\ninitial value', ha='left') +\n#    annotate('text', x=1.5, y=5, label='Learning step', ha='left') +\n#    annotate('text', x=0, y=0.5, label='Minimum', ha='left') +\n    geom_vline(xintercept=0, linetype='dashed', color='gray') +\n    labs(x=r'$\\beta$', y='Loss') +\n    theme_minimal() +\n    theme(\n        figure_size=(7, 4),\n        axis_title=element_text(size=12),\n        axis_text=element_text(size=10)\n    )\n)\n\np\n\n\n\n\n\n\n\n\n\n\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "notebooks/05-train-gpt.html",
    "href": "notebooks/05-train-gpt.html",
    "title": "Building a GPT - from colab",
    "section": "",
    "text": "Companion notebook to the Zero To Hero video on GPT. Downloaded from https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing\n\n# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n--2023-01-17 01:39:27--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‘input.txt’\n\ninput.txt           100%[===================&gt;]   1.06M  --.-KB/s    in 0.04s   \n\n2023-01-17 01:39:28 (29.0 MB/s) - ‘input.txt’ saved [1115394/1115394]\n\n\n\n\n# read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n\nprint(\"length of dataset in characters: \", len(text))\n\nlength of dataset in characters:  1115394\n\n\n\n# let's look at the first 1000 characters\nprint(text[:1000])\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n\n\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n\n\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n\n\n\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n\n[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n\n\n\n# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch # we use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this\n\ntorch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n\n\n\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n\nblock_size = 8\ntrain_data[:block_size+1]\n\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n\n\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n\nwhen input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n\n\n\ntorch.manual_seed(1337)\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n\ninputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\nwhen input is [24] the target: 43\nwhen input is [24, 43] the target: 58\nwhen input is [24, 43, 58] the target: 5\nwhen input is [24, 43, 58, 5] the target: 57\nwhen input is [24, 43, 58, 5, 57] the target: 1\nwhen input is [24, 43, 58, 5, 57, 1] the target: 46\nwhen input is [24, 43, 58, 5, 57, 1, 46] the target: 43\nwhen input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\nwhen input is [44] the target: 53\nwhen input is [44, 53] the target: 56\nwhen input is [44, 53, 56] the target: 1\nwhen input is [44, 53, 56, 1] the target: 58\nwhen input is [44, 53, 56, 1, 58] the target: 46\nwhen input is [44, 53, 56, 1, 58, 46] the target: 39\nwhen input is [44, 53, 56, 1, 58, 46, 39] the target: 58\nwhen input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52] the target: 58\nwhen input is [52, 58] the target: 1\nwhen input is [52, 58, 1] the target: 58\nwhen input is [52, 58, 1, 58] the target: 46\nwhen input is [52, 58, 1, 58, 46] the target: 39\nwhen input is [52, 58, 1, 58, 46, 39] the target: 58\nwhen input is [52, 58, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\nwhen input is [25] the target: 17\nwhen input is [25, 17] the target: 27\nwhen input is [25, 17, 27] the target: 10\nwhen input is [25, 17, 27, 10] the target: 0\nwhen input is [25, 17, 27, 10, 0] the target: 21\nwhen input is [25, 17, 27, 10, 0, 21] the target: 1\nwhen input is [25, 17, 27, 10, 0, 21, 1] the target: 54\nwhen input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n\n\n\nprint(xb) # our input to the transformer\n\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\ntorch.Size([32, 65])\ntensor(4.8786, grad_fn=&lt;NllLossBackward0&gt;)\n\nSKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\nwnYWmnxKWWev-tDqXErVKLgJ\n\n\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\n\nbatch_size = 32\nfor steps in range(100): # increase number of steps for good results...\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n\n4.65630578994751\n\n\n\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n\n\noTo.JUZ!!zqe!\nxBP qbs$Gy'AcOmrLwwt\np$x;Seh-onQbfM?OjKbn'NwUAW -Np3fkz$FVwAUEa-wzWC -wQo-R!v -Mj?,SPiTyZ;o-opr$mOiPJEYD-CfigkzD3p3?zvS;ADz;.y?o,ivCuC'zqHxcVT cHA\nrT'Fd,SBMZyOslg!NXeF$sBe,juUzLq?w-wzP-h\nERjjxlgJzPbHxf$ q,q,KCDCU fqBOQT\nSV&CW:xSVwZv'DG'NSPypDhKStKzC -$hslxIVzoivnp ,ethA:NCCGoi\ntN!ljjP3fwJMwNelgUzzPGJlgihJ!d?q.d\npSPYgCuCJrIFtb\njQXg\npA.P LP,SPJi\nDBcuBM:CixjJ$Jzkq,OLf3KLQLMGph$O 3DfiPHnXKuHMlyjxEiyZib3FaHV-oJa!zoc'XSP :CKGUhd?lgCOF$;;DTHZMlvvcmZAm;:iv'MMgO&Ywbc;BLCUd&vZINLIzkuTGZa\nD.?\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "notebooks/05-train-gpt.html#building-a-gpt",
    "href": "notebooks/05-train-gpt.html#building-a-gpt",
    "title": "Building a GPT - from colab",
    "section": "",
    "text": "Companion notebook to the Zero To Hero video on GPT. Downloaded from https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing\n\n# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n--2023-01-17 01:39:27--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 1115394 (1.1M) [text/plain]\nSaving to: ‘input.txt’\n\ninput.txt           100%[===================&gt;]   1.06M  --.-KB/s    in 0.04s   \n\n2023-01-17 01:39:28 (29.0 MB/s) - ‘input.txt’ saved [1115394/1115394]\n\n\n\n\n# read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n\nprint(\"length of dataset in characters: \", len(text))\n\nlength of dataset in characters:  1115394\n\n\n\n# let's look at the first 1000 characters\nprint(text[:1000])\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n\n\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n\n\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n\n\n\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n\n[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n\n\n\n# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch # we use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this\n\ntorch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n\n\n\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n\nblock_size = 8\ntrain_data[:block_size+1]\n\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n\n\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n\nwhen input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n\n\n\ntorch.manual_seed(1337)\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n\ninputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\nwhen input is [24] the target: 43\nwhen input is [24, 43] the target: 58\nwhen input is [24, 43, 58] the target: 5\nwhen input is [24, 43, 58, 5] the target: 57\nwhen input is [24, 43, 58, 5, 57] the target: 1\nwhen input is [24, 43, 58, 5, 57, 1] the target: 46\nwhen input is [24, 43, 58, 5, 57, 1, 46] the target: 43\nwhen input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\nwhen input is [44] the target: 53\nwhen input is [44, 53] the target: 56\nwhen input is [44, 53, 56] the target: 1\nwhen input is [44, 53, 56, 1] the target: 58\nwhen input is [44, 53, 56, 1, 58] the target: 46\nwhen input is [44, 53, 56, 1, 58, 46] the target: 39\nwhen input is [44, 53, 56, 1, 58, 46, 39] the target: 58\nwhen input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52] the target: 58\nwhen input is [52, 58] the target: 1\nwhen input is [52, 58, 1] the target: 58\nwhen input is [52, 58, 1, 58] the target: 46\nwhen input is [52, 58, 1, 58, 46] the target: 39\nwhen input is [52, 58, 1, 58, 46, 39] the target: 58\nwhen input is [52, 58, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\nwhen input is [25] the target: 17\nwhen input is [25, 17] the target: 27\nwhen input is [25, 17, 27] the target: 10\nwhen input is [25, 17, 27, 10] the target: 0\nwhen input is [25, 17, 27, 10, 0] the target: 21\nwhen input is [25, 17, 27, 10, 0, 21] the target: 1\nwhen input is [25, 17, 27, 10, 0, 21, 1] the target: 54\nwhen input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n\n\n\nprint(xb) # our input to the transformer\n\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\n\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n\ntorch.Size([32, 65])\ntensor(4.8786, grad_fn=&lt;NllLossBackward0&gt;)\n\nSKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\nwnYWmnxKWWev-tDqXErVKLgJ\n\n\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\n\nbatch_size = 32\nfor steps in range(100): # increase number of steps for good results...\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n\n4.65630578994751\n\n\n\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n\n\noTo.JUZ!!zqe!\nxBP qbs$Gy'AcOmrLwwt\np$x;Seh-onQbfM?OjKbn'NwUAW -Np3fkz$FVwAUEa-wzWC -wQo-R!v -Mj?,SPiTyZ;o-opr$mOiPJEYD-CfigkzD3p3?zvS;ADz;.y?o,ivCuC'zqHxcVT cHA\nrT'Fd,SBMZyOslg!NXeF$sBe,juUzLq?w-wzP-h\nERjjxlgJzPbHxf$ q,q,KCDCU fqBOQT\nSV&CW:xSVwZv'DG'NSPypDhKStKzC -$hslxIVzoivnp ,ethA:NCCGoi\ntN!ljjP3fwJMwNelgUzzPGJlgihJ!d?q.d\npSPYgCuCJrIFtb\njQXg\npA.P LP,SPJi\nDBcuBM:CixjJ$Jzkq,OLf3KLQLMGph$O 3DfiPHnXKuHMlyjxEiyZib3FaHV-oJa!zoc'XSP :CKGUhd?lgCOF$;;DTHZMlvvcmZAm;:iv'MMgO&Ywbc;BLCUd&vZINLIzkuTGZa\nD.?"
  },
  {
    "objectID": "notebooks/05-train-gpt.html#the-mathematical-trick-in-self-attention",
    "href": "notebooks/05-train-gpt.html#the-mathematical-trick-in-self-attention",
    "title": "Building a GPT - from colab",
    "section": "The mathematical trick in self-attention",
    "text": "The mathematical trick in self-attention\n\n# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0,10,(3,2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('--')\nprint('b=')\nprint(b)\nprint('--')\nprint('c=')\nprint(c)\n\na=\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n--\nb=\ntensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n--\nc=\ntensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n\n\n\n# consider the following toy example:\n\ntorch.manual_seed(1337)\nB,T,C = 4,8,2 # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n\n\n# We want x[b,t] = mean_{i&lt;=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n\n\n# version 2: using matrix multiply for a weighted aggregation\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x # (B, T, T) @ (B, T, C) ----&gt; (B, T, C)\ntorch.allclose(xbow, xbow2)\n\nTrue\n\n\n\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n\nTrue\n\n\n\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)   # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\n#wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n#out = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 16])\n\n\n\nwei[0]\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n       grad_fn=&lt;SelectBackward0&gt;)\n\n\nNotes: - Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights. - There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens. - Each example across batch dimension is of course processed completely independently and never “talk” to each other - In an “encoder” attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a “decoder” attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling. - “self-attention” just means that the keys and values are produced from the same source as queries. In “cross-attention”, the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module) - “Scaled” attention additional divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n\nk = torch.randn(B,T,head_size)\nq = torch.randn(B,T,head_size)\nwei = q @ k.transpose(-2, -1) * head_size**-0.5\n\n\nk.var()\n\ntensor(1.0449)\n\n\n\nq.var()\n\ntensor(1.0700)\n\n\n\nwei.var()\n\ntensor(1.0918)\n\n\n\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n\ntensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n\n\n\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot\n\ntensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n\n\n\nclass LayerNorm1d: # (used to be BatchNorm1d)\n\n  def __init__(self, dim, eps=1e-5, momentum=0.1):\n    self.eps = eps\n    self.gamma = torch.ones(dim)\n    self.beta = torch.zeros(dim)\n\n  def __call__(self, x):\n    # calculate the forward pass\n    xmean = x.mean(1, keepdim=True) # batch mean\n    xvar = x.var(1, keepdim=True) # batch variance\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n    return self.out\n\n  def parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = LayerNorm1d(100)\nx = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\nx = module(x)\nx.shape\n\ntorch.Size([32, 100])\n\n\n\nx[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs\n\n(tensor(0.1469), tensor(0.8803))\n\n\n\nx[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features\n\n(tensor(-9.5367e-09), tensor(1.0000))\n\n\n\n# French to English translation example:\n\n# &lt;--------- ENCODE ------------------&gt;&lt;--------------- DECODE -----------------&gt;\n# les réseaux de neurones sont géniaux! &lt;START&gt; neural networks are awesome!&lt;END&gt;\n\n\n\nFull finished code, for reference\nYou may want to refer directly to the git repo instead though.\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 16 # how many independent sequences will we process in parallel?\nblock_size = 32 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 64\nn_head = 4\nn_layer = 4\ndropout = 0.0\n# ------------\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -&gt; (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,C)\n        out = wei @ v # (B, T, T) @ (B, T, C) -&gt; (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = BigramLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n\n0.209729 M parameters\nstep 0: train loss 4.4116, val loss 4.4022\nstep 100: train loss 2.6568, val loss 2.6670\nstep 200: train loss 2.5090, val loss 2.5058\nstep 300: train loss 2.4198, val loss 2.4340\nstep 400: train loss 2.3503, val loss 2.3567\nstep 500: train loss 2.2970, val loss 2.3136\nstep 600: train loss 2.2410, val loss 2.2506\nstep 700: train loss 2.2062, val loss 2.2198\nstep 800: train loss 2.1638, val loss 2.1871\nstep 900: train loss 2.1232, val loss 2.1494\nstep 1000: train loss 2.1020, val loss 2.1293\nstep 1100: train loss 2.0704, val loss 2.1196\nstep 1200: train loss 2.0382, val loss 2.0798\nstep 1300: train loss 2.0249, val loss 2.0640\nstep 1400: train loss 1.9922, val loss 2.0354\nstep 1500: train loss 1.9707, val loss 2.0308\nstep 1600: train loss 1.9614, val loss 2.0474\nstep 1700: train loss 1.9393, val loss 2.0130\nstep 1800: train loss 1.9070, val loss 1.9943\nstep 1900: train loss 1.9057, val loss 1.9871\nstep 2000: train loss 1.8834, val loss 1.9954\nstep 2100: train loss 1.8719, val loss 1.9758\nstep 2200: train loss 1.8582, val loss 1.9623\nstep 2300: train loss 1.8546, val loss 1.9517\nstep 2400: train loss 1.8410, val loss 1.9476\nstep 2500: train loss 1.8167, val loss 1.9455\nstep 2600: train loss 1.8263, val loss 1.9401\nstep 2700: train loss 1.8108, val loss 1.9340\nstep 2800: train loss 1.8040, val loss 1.9247\nstep 2900: train loss 1.8044, val loss 1.9304\nstep 3000: train loss 1.7963, val loss 1.9242\nstep 3100: train loss 1.7687, val loss 1.9147\nstep 3200: train loss 1.7547, val loss 1.9102\nstep 3300: train loss 1.7557, val loss 1.9037\nstep 3400: train loss 1.7547, val loss 1.8946\nstep 3500: train loss 1.7385, val loss 1.8968\nstep 3600: train loss 1.7260, val loss 1.8914\nstep 3700: train loss 1.7257, val loss 1.8808\nstep 3800: train loss 1.7204, val loss 1.8919\nstep 3900: train loss 1.7215, val loss 1.8788\nstep 4000: train loss 1.7146, val loss 1.8639\nstep 4100: train loss 1.7095, val loss 1.8724\nstep 4200: train loss 1.7079, val loss 1.8707\nstep 4300: train loss 1.7035, val loss 1.8502\nstep 4400: train loss 1.7043, val loss 1.8693\nstep 4500: train loss 1.6914, val loss 1.8522\nstep 4600: train loss 1.6853, val loss 1.8357\nstep 4700: train loss 1.6862, val loss 1.8483\nstep 4800: train loss 1.6671, val loss 1.8434\nstep 4900: train loss 1.6736, val loss 1.8415\nstep 4999: train loss 1.6635, val loss 1.8226\n\nFlY BOLINGLO:\nThem thrumply towiter arts the\nmuscue rike begatt the sea it\nWhat satell in rowers that some than othis Marrity.\n\nLUCENTVO:\nBut userman these that, where can is not diesty rege;\nWhat and see to not. But's eyes. What?\n\nJOHN MARGARET:\nThan up I wark, what out, I ever of and love,\none these do sponce, vois I me;\nBut my pray sape to ries all to the not erralied in may.\n\nBENVOLIO:\nTo spits as stold's bewear I would and say mesby all\non sworn make he anough\nAs cousins the solle, whose be my conforeful may lie them yet\nnobe allimely untraled to be thre I say be,\nNotham a brotes theme an make come,\nAnd that his reach to the duke ento\nthe grmeants bell! and now there king-liff-or grief?\n\nGLOUCESTER:\nAll the bettle dreene, for To his like thou thron!\n\nMENENIUS:\nThen, if I knom her all.\nMy lord, but terruly friend\nRish of the ploceiness and wilt tends sure?\nIs you knows a fasir wead\nThat with him my spaut,\nI shall not tas where's not, becomity; my coulds sting,\nthen the wit be dong to tyget our hereefore,\nWho strop me, mend here, if agains, bitten, thy lack.\nThe but these it were is tus. For the her skeep the fasting. joy tweet Bumner:-\nHow the enclady: It you and how,\nI am in him, And ladderle:\nTheir hand whose wife, it my hithre,\nRoman and where sposs gives'd you.\n\nTROMIOLANUS:\nBut livants you great, I shom mistrot come, for to she to lot\nfor smy to men ventry mehus. Gazise;\nFull't were some the cause, and stouch set,\nOr promises, which a kingsasted to your gove them; and sterrer,\nAnd that wae love him.\n\nBRUTUS:\nYou shape with these sweet.\n\nCORTENGONO:\nLo, where 'twon elmes, 'morth young agres;\nSir, azavoust to striel accurded we missery sets crave.\n\nANGOLUM:\nFor is Henry to have gleise the dreason\nThat I ant shorfold wefth their servy in enscy.\n\nISABELLA:\nO, I better you eyse such formfetrews.\n\nBUCKINGHARENT:\nQead my lightle this righanneds flase them\nWam which an take was our some pleasurs,\nLovisoname to me, then fult me?--have it?\n\nHENRY BOLINGBROY:\nThat wha"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html",
    "title": "Quick introduction to deep learning",
    "section": "",
    "text": "Understand and implement gradient descent to fit a linear model\nBuild and train a multi-layer perceptron (MLP) using PyTorch\nLearn the basics of PyTorch, including model definition, forward pass, and optimization\n\n\n\nWe’ll start by making sure we have all the required Python packages installed and ready to go. PyTorch is the main library we’ll use for deep learning.\n\n## install packages if needed\nif False:\n    %pip install scikit-learn plotnine tqdm pandas\n\n\nfrom sklearn.datasets import make_regression\nimport numpy as np\nfrom numpy.linalg import inv\nfrom plotnine import qplot, ggplot, geom_point, geom_line, aes, geom_abline\nfrom plotnine.themes import theme_bw\nfrom plotnine.geoms import annotate\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#simulate-linear-data",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#simulate-linear-data",
    "title": "Quick introduction to deep learning",
    "section": "Simulate linear data",
    "text": "Simulate linear data\nWe generate 1000 samples, each with 2 features. We’ll define the “true” coefficients and add a bit of noise to make it realistic.\n\nx: predictors/features (shape: 1000×2)\ny: response variable (shape: 1000×1)\ncoef = [\\beta_1 \\beta_2]: the true coefficients used to generate y\n\n\nnp.random.seed(42)\nbias = 0\nnoise = 10\nx, y, coef = make_regression(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_targets=1,\n    bias=bias,\n    noise=noise,\n    coef=True\n    )\n\n\nshow simulated x, y, and coef= [\\beta_1 \\beta_2]\n\nprint('x.shape:', x.shape)\nprint('y.shape:', y.shape)\nprint('coef.shape:', coef.shape)\n\nx.shape: (1000, 2)\ny.shape: (1000,)\ncoef.shape: (2,)\n\n\n\nprint('x:\\n',x,'\\n')\nprint('y[0:10]:\\n',y[0:10],'\\n')\nprint('coef:\\n',coef,'\\n')\n\nx:\n [[-0.16711808  0.14671369]\n [-0.02090159  0.11732738]\n [ 0.15041891  0.364961  ]\n ...\n [ 0.30263547 -0.75427585]\n [ 0.38193545  0.43004165]\n [ 0.07736831 -0.8612842 ]] \n\ny[0:10]:\n [-14.99694989 -12.67808888  17.77545452   6.66146467 -14.19552996\n -25.24484815 -39.23162627 -52.01803821   5.76368853 -50.11860295] \n\ncoef:\n [40.71064891  6.60098441]"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#predict-y-with-ground-truth-parameters",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#predict-y-with-ground-truth-parameters",
    "title": "Quick introduction to deep learning",
    "section": "Predict y with ground truth parameters",
    "text": "Predict y with ground truth parameters\nSince we know the true coefficients, we can directly compute the predicted y and visualize how well they fit.\n\ny_hat = x.dot(coef) + bias\n\n\ncompare \\hat{y} and y\n\n(qplot(x=y, y=y_hat, geom=\"point\", xlab=\"y\", ylab=\"y_hat\") + geom_abline(intercept=0, slope=1, color='gray', linetype='dashed'))"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#compute-analytical-solution",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#compute-analytical-solution",
    "title": "Quick introduction to deep learning",
    "section": "Compute analytical solution",
    "text": "Compute analytical solution\nFor linear regression, there’s a closed-form solution known as the normal equation:\n\\hat{\\beta} = (X^T X)^{-1} X^T y\nThis gives us the optimal coefficients that minimize mean square errors.\nLet’s get \\hat{\\beta} and compare it with the ground truth.\n\nExercise\nDerive the normal equation from the model, using matrix algebra\n\nvar = x.transpose().dot(x)\ncov = x.transpose().dot(y)\nb_hat = inv(var).dot(cov)\n\n\nprint(\"Estimated\")\nprint(b_hat)\n\nprint(\"Ground truth\")\nprint(coef)\n\nEstimated\n[41.06972678  6.79965716]\nGround truth\n[40.71064891  6.60098441]\n\n\n\n\nResult Comparison\nEstimated coefficients using the normal equation are close to the true ones, despite noise. That’s a good sanity check.\n\n\nExercise\n\nPlot the prediction with the analystical estimates vs the target y"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#define-stochastic-gradient-descent-sgd",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#define-stochastic-gradient-descent-sgd",
    "title": "Quick introduction to deep learning",
    "section": "Define Stochastic Gradient Descent (SGD)",
    "text": "Define Stochastic Gradient Descent (SGD)\nWhen analytical solutions aren’t available (which is often), we rely on numerical optimization like gradient descent.\nAlthough linear regression has analytical solutions, this is unfortunately not the case for many other models. We may need to resort to numerical approximations to find the optimal parameters. One of the most popular numerical optimizers is called stochastic gradient descent."
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-a-gradient",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-a-gradient",
    "title": "Quick introduction to deep learning",
    "section": "What is a Gradient?",
    "text": "What is a Gradient?\nA gradient is the derivative of a function, and it tells us how to adjust parameters to reduce the error.\n$f’() = _{→ 0} = $"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-gradient-descent",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-gradient-descent",
    "title": "Quick introduction to deep learning",
    "section": "What is Gradient Descent?",
    "text": "What is Gradient Descent?\nAn optimization algorithm that iteratively updates parameters in the direction of steepest descent (negative gradient) to minimize a loss function.\n\nLearning rate (\\alpha): step size. It is a hyperparameters that determines how fast we move in the direction of the gradient.\nLoss surface: the landscape we are optimizing over\n\nAt a particular β_i, we find the gradient f'(\\beta), and take a step along the direction of the gradient to find the next point \\beta_{i+1}.\n\\beta_{i+1} = \\beta_i - \\alpha f'(\\beta_i)\nThe \\alpha is called the learning rate.\n\n\n\ngradient descent"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-stochastic-gradient-descent",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#what-is-stochastic-gradient-descent",
    "title": "Quick introduction to deep learning",
    "section": "What is stochastic gradient descent?",
    "text": "What is stochastic gradient descent?\nIn real-world applications, the size of our dataset is so large that it is impossible to calculate the gradient using all data points. Therefore, we take a small chunk (called a “batch”) of the dataset to calcalate the gradient. This approximates the full-data gradients, thus the word stochastic."
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#three-components-of-machine-learning",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#three-components-of-machine-learning",
    "title": "Quick introduction to deep learning",
    "section": "Three Components of Machine Learning",
    "text": "Three Components of Machine Learning\nTo build a machine learning model, we need:\n\nModel: Defines the hypothesis space (e.g., linear model)\nLoss Function: Measures how well the model fits\nOptimizer: Updates parameters to reduce the loss (e.g., SGD)"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#define-the-loss-function",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#define-the-loss-function",
    "title": "Quick introduction to deep learning",
    "section": "Define the Loss Function",
    "text": "Define the Loss Function\nCommon choice for regression: Mean Squared Error (MSE)\n\\ell(\\beta) = \\frac{1}{2}\\sum\\limits_{i=1}^m (f(\\beta)^{i} - y^{i})^2 / m"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#compute-the-gradient",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#compute-the-gradient",
    "title": "Quick introduction to deep learning",
    "section": "Compute the gradient",
    "text": "Compute the gradient\n\\frac{\\partial}{\\partial \\beta_j}\\ell(\\beta) = \\frac{\\partial}{\\partial \\beta_j} \\frac{1}{2} \\sum_i (f(\\beta)^i - y^i)^2 =  \\sum_i (f(\\beta)^i - y^i) x_j\nRecall that in our example $ f() = = _1 x_1 + _2 x_2$. Here \\beta_j is either \\beta_1 or \\beta_2.\n\nExercise\nShow that the derivative of the loss function is as stated above."
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#optimize-estimate-parameters-with-gradient-descent",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#optimize-estimate-parameters-with-gradient-descent",
    "title": "Quick introduction to deep learning",
    "section": "Optimize: estimate parameters with gradient descent",
    "text": "Optimize: estimate parameters with gradient descent\n\nlr = 0.1 # learning rate\nb = [0.0, 0.0] # initialize all betas to 0\nn_examples = len(y)\ntrajectory = [b]\n\nfor _ in range(50): # 50 steps\n  diff = x.dot(b) - y\n  grad = diff.dot(x) / n_examples\n  b -= lr*grad\n  trajectory.append(b.copy())\n\ntrajectory = np.stack(trajectory, axis=1)\n\n\nqplot(x=trajectory[0], y=trajectory[1], xlab=\"beta_1\", ylab=\"beta_2\", geom=[\"point\", \"line\"]) + theme_bw() + annotate(geom=\"point\", x=coef[0], y=coef[1], size=8, color=\"blue\",shape='+',stroke=1)\n\n\n\n\n\n\n\n\n\nExercise\n\nAdd to the plot the normal equation estimates (traditional linear regression) of the coefficients in a different green\n\n\n\nExercise\n\nSimulate y with larger noise, estimate the regression coefficients using the normal equation and the gradient descent method, and plot the trajectory, the ground truth, and the two estimates for comparison."
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#simulate-non-linear-data",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#simulate-non-linear-data",
    "title": "Quick introduction to deep learning",
    "section": "Simulate non linear data",
    "text": "Simulate non linear data\nLet’s start by simulating data according to the generative model:\n$ y = x_1^3$\n\nx = np.random.normal(size=1000)\ny = x ** 3"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#predict-with-simple-linear-model",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#predict-with-simple-linear-model",
    "title": "Quick introduction to deep learning",
    "section": "Predict with simple linear model",
    "text": "Predict with simple linear model\nLet’s try to predict with a simple linear regression model\n$ y = X $\n\nQuestion\nhow many parameters do we need to estimate for this simple linear model?\n\nlr = 0.1 # learning rate\nb = 0.0 # initialize all betas to 0\nn_examples = len(y)\n\nfor _ in range(50): # 50 steps\n  diff = x.dot(b) - y\n  grad = diff.dot(x) / n_examples\n  b -= lr*grad\n\n\ny_hat = x.dot(b)\n( qplot(x = y, y=y_hat, geom=\"point\", xlab=\"y\", ylab=\"y_hat\") +\ngeom_abline(intercept=0, slope=1, color='gray', linetype='dashed') )\n\n\n\n\n\n\n\n\n\n\nWe would like to be closer to the dashed gray, identity line! Let’s try MLP."
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#install-packages-if-needed",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#install-packages-if-needed",
    "title": "Quick introduction to deep learning",
    "section": "Install packages if needed",
    "text": "Install packages if needed\n\n## install packages if needed\nif False:  # Change to True to run the commands\n    %pip install tqdm\n    %pip install torch\n    %pip install torchvision torchmetrics\n\n\nimport torch\nprint(torch.__version__)\n## if on a mac check if mps is available so you use gpu\n## print(torch.backends.mps.is_available())\n\n2.6.0\n\n\n\nDefine a non linear MLP\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n#device = torch.device(\"cuda\")\ndevice = torch.device(\"mps\") ## use this line instead of the one above withb cuda\n\n\nclass MLP(nn.Module):\n  def __init__(self, input_dim, hid_dim, output_dim):\n    super().__init__()\n    self.fc1 = nn.Linear(input_dim, hid_dim)\n    self.fc2 = nn.Linear(hid_dim, output_dim)\n\n  def forward(self, x):\n    x = F.relu(self.fc1(x))\n    y = self.fc2(x)\n\n    return y.squeeze(1)\n\nmlp = MLP(input_dim=1, hid_dim=1024, output_dim=1).to(device)"
  },
  {
    "objectID": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#train-mlp-model-using-pytorch",
    "href": "post/2025-03-25-unit00/hands-on-introduction_to_deep_learning.html#train-mlp-model-using-pytorch",
    "title": "Quick introduction to deep learning",
    "section": "train MLP model using pytorch",
    "text": "train MLP model using pytorch\nWe are now ready to train our model. To understand how our model is doing, we record the loss vs step, which is called the learning curve in ML literature.\n\nx_tensor = torch.Tensor(x).unsqueeze(1).to(device)\ny_tensor = torch.Tensor(y).to(device)\nlearning_curve = []\n\noptimizer = torch.optim.SGD(mlp.parameters(), lr=0.001)\nloss_fn = nn.MSELoss()\n\nfor epoch in range(10000):\n  optimizer.zero_grad()\n  y_hat = mlp(x_tensor)\n  loss = loss_fn(y_hat, y_tensor)\n  learning_curve.append(loss.item())\n  loss.backward()\n  optimizer.step()\n\n\nqplot(x=range(10000), y=learning_curve, xlab=\"epoch\", ylab=\"loss\")\n\n\n\n\n\n\n\n\n\ny_hat = mlp(x_tensor)\ny_hat = y_hat.detach().cpu().numpy()\n#qplot(x=y, y=y_hat, geom=\"point\", xlab=\"y\", ylab=\"y_hat\")\nqplot(x=y, y=y_hat, geom=[\"point\", \"abline\"],\n      xlab=\"y\", ylab=\"y_hat\",\n      abline=dict(slope=1, intercept=0, color='red', linetype='dashed'))\n\n\n\n\n\n\n\n\nNow this looks much better!"
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-wandb.html",
    "href": "post/2025-03-25-unit00/tf-binding-wandb.html",
    "title": "Calibrating hyperparameters with weights and biases",
    "section": "",
    "text": "Goal: Learn how to use weights and biases to calibrate the hyperparameters of a DL model.\n\nWeights and biases is a platform used for AI developers to track, visualize and manage their ML models and experiments. The coolest part is that W&B allows you to log various performance metrics during training, like training and validation loss, test set correlations, etc. Additionally, it allows you to compare between different experiments or versions of your models. Making it easier to identify the best performing models and see which hyperparameter configuration is the optimal.\nIn this notebook, we will focus on using W&B as a tool to help callibrate the hyperparameters of the TF binding prediction model to find an optimal solution. However, we encourage you to explore other applications that W&B offers.\n\n\n\nFirst install w&b in your environment with the following command, it should take only a couple of seconds\n\n%pip install wandb onnx -Uq\n%pip install nbformat\n\nLoad all libraries\n\nimport torch\nfrom torch import nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport os\nimport pandas as pd\nimport wandb\nfrom scipy.stats import pearsonr\n\n\n\n\nIf you don’t already have a w&b account, sign up here. Then, run the following command which will prompt you to insert your API key\n\nwandb.login()\n\nwandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\nwandb: Currently logged in as: ssalazar_02 to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n\n\nTrue\n\n\n\n\n\nThen, define your functions, this will remain unchanged\n\ndef get_device():\n  \"\"\"\n  Determines the device to use for PyTorch computations.\n\n  Prioritizes Metal Performance Shaders (MPS), then CUDA, then CPU.\n\n  Returns:\n    torch.device: The selected device.\n  \"\"\"\n  if torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    print(\"Using MPS device.\")\n  elif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"Using CUDA device.\")\n  else:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU device.\")\n  return device\n\n# Example usage:\ndevice = get_device()\n\nUsing MPS device.\n\n\n\ndef one_hot_encode(seq):\n    \"\"\"\n    Given a DNA sequence, return its one-hot encoding\n    \"\"\"\n    # Make sure seq has only allowed bases\n    allowed = set(\"ACTGN\")\n    if not set(seq).issubset(allowed):\n        invalid = set(seq) - allowed\n        print(seq)\n        raise ValueError(f\"Sequence contains chars not in allowed DNA alphabet (ACGTN): {invalid}\")\n\n    # Dictionary returning one-hot encoding for each nucleotide\n    nuc_d = {'A':[1.0,0.0,0.0,0.0],\n             'C':[0.0,1.0,0.0,0.0],\n             'G':[0.0,0.0,1.0,0.0],\n             'T':[0.0,0.0,0.0,1.0],\n             'N':[0.0,0.0,0.0,0.0]}\n\n    # Create array from nucleotide sequence\n    vec=np.array([nuc_d[x] for x in seq],dtype='float32')\n\n    return vec\n\n\ndef quick_split(df, split_frac=0.8, verbose=False):\n    '''\n    Given a df of samples, randomly split indices between\n    train and test at the desired fraction\n    '''\n    cols = df.columns # original columns, use to clean up reindexed cols\n    df = df.reset_index()\n\n    # shuffle indices\n    idxs = list(range(df.shape[0]))\n    random.shuffle(idxs)\n\n    # split shuffled index list by split_frac\n    split = int(len(idxs)*split_frac)\n    train_idxs = idxs[:split]\n    test_idxs = idxs[split:]\n\n    # split dfs and return\n    train_df = df[df.index.isin(train_idxs)]\n    test_df = df[df.index.isin(test_idxs)]\n\n    return train_df[cols], test_df[cols]\n\n\ndef split_sequences(sequences_df):\n    full_train_sequences, test_sequences = quick_split(sequences_df)\n    train_sequences, val_sequences = quick_split(full_train_sequences)\n    print(\"Train:\", train_sequences.shape)\n    print(\"Val:\", val_sequences.shape)\n    print(\"Test:\", test_sequences.shape)\n    return train_sequences, val_sequences, test_sequences\n\n\ndef get_data_tensors(scores_df, sequences_df):\n    # split sequences in train, validation and test sets\n    train_sequences, val_sequences, test_sequences = split_sequences(sequences_df)\n    # get scores for each set of sequences\n    train_scores = scores_df[train_sequences['window_name'].to_list()].transpose().values.astype('float32') # shape is (num_sequences, 300)\n    val_scores = scores_df[val_sequences['window_name'].to_list()].transpose().values.astype('float32')\n    test_scores = scores_df[test_sequences['window_name'].to_list()].transpose().values.astype('float32')\n\n    train_scores = torch.tensor(train_scores, dtype=torch.float32).to(device)\n    val_scores = torch.tensor(val_scores, dtype=torch.float32).to(device)\n    test_scores = torch.tensor(test_scores, dtype=torch.float32).to(device)\n\n    # get one hot encoded sequences for each set\n    train_one_hot = [one_hot_encode(seq) for seq in train_sequences['sequence'].to_list()]\n    train_sequences_tensor = torch.tensor(np.stack(train_one_hot))\n\n    val_one_hot = [one_hot_encode(seq) for seq in val_sequences['sequence'].to_list()]\n    val_sequences_tensor = torch.tensor(np.stack(val_one_hot))\n\n    test_one_hot = [one_hot_encode(seq) for seq in test_sequences['sequence'].to_list()]\n    test_sequences_tensor = torch.tensor(np.stack(test_one_hot))\n\n    return train_scores, train_sequences_tensor, val_scores, val_sequences_tensor, test_scores, test_sequences_tensor\n\n\n\ndef create_dataloader(predictors, targets, batch_size, is_train = True):\n    '''\n    features: one hot encoded sequences\n    targets: sequence scores\n    batch_size\n    is_train: if True, data is reshuffled at every epoch\n    '''\n\n    dataset = torch.utils.data.TensorDataset(predictors, targets)\n    return torch.utils.data.DataLoader(dataset, batch_size, shuffle = is_train)\n\n\nclass DNA_CNN(nn.Module):\n    def __init__(self,\n                 seq_len,\n                 num_filters=16,\n                 kernel_size=10,\n                 add_sigmoid=False):\n        super().__init__()\n        self.seq_len = seq_len\n        self.add_sigmoid = add_sigmoid\n        # Define layers individually\n        self.conv = nn.Conv1d(in_channels = 4, out_channels = num_filters, kernel_size=kernel_size)\n        self.relu = nn.ReLU(inplace=True)\n        self.linear = nn.Linear(num_filters*(seq_len-kernel_size+1), 300)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, xb):\n        # reshape view to batch_size x 4channel x seq_len\n        # permute to put channel in correct order\n        xb = xb.permute(0,2,1) # (batch_size, 300, 4) to (batch_size, 4, 300)\n\n        # Apply layers step by step\n        x = self.conv(xb)\n        x = self.relu(x)\n        x = x.flatten(1)  # flatten all dimensions except batch\n        out = self.linear(x)\n        \n        if self.add_sigmoid:\n            out = self.sigmoid(out)\n        return out\n\n\ndef process_batch(model, loss_func, x_batch, y_batch, opt=None):\n    xb_out = model(x_batch.to(torch.float32))\n\n    loss = loss_func(xb_out, y_batch)\n\n    if opt is not None: # backpropagate if train step (optimizer given)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    return loss.item(), len(x_batch)\n\n\ndef train_epoch(model, train_dl, loss_func, device, opt):\n\n    model.train()\n    tl = [] # train losses\n    ns = [] # batch sizes, n\n\n    # loop through batches\n    for x_batch, y_batch in train_dl:\n\n        x_batch, y_batch = x_batch.to(device),y_batch.to(device)\n\n        t, n = process_batch(model, loss_func, x_batch, y_batch, opt=opt)\n\n        # collect train loss and batch sizes\n        tl.append(t)\n        ns.append(n)\n\n    # average the losses over all batches\n    train_loss = np.sum(np.multiply(tl, ns)) / np.sum(ns)\n\n    return train_loss\n\n\ndef val_epoch(model, val_dl, loss_func, device):\n\n    # Set model to Evaluation mode\n    model.eval()\n    with torch.no_grad():\n        vl = [] # val losses\n        ns = [] # batch sizes, n\n\n        # loop through validation DataLoader\n        for x_batch, y_batch in val_dl:\n\n            x_batch, y_batch = x_batch.to(device),y_batch.to(device)\n\n            v, n = process_batch(model, loss_func, x_batch, y_batch)\n\n            # collect val loss and batch sizes\n            vl.append(v)\n            ns.append(n)\n\n    # average the losses over all batches\n    val_loss = np.sum(np.multiply(vl, ns)) / np.sum(ns)\n\n    return val_loss\n\n\n\n\nThe only function that we need to change is the train_loop function, because here is where we are recovering the parameters that we want to track with w&b. We will use wandb.log and create a dictionary with the parmeters that we want to track.\n\ndef train_loop(epochs, model, loss_func, opt, train_dl, val_dl, device):\n\n    # keep track of losses\n    train_losses = []\n    val_losses = []\n\n    # loop through epochs\n    for epoch in range(epochs):\n        # take a training step\n        train_loss = train_epoch(model,train_dl,loss_func,device,opt)\n        train_losses.append(train_loss)\n\n        # take a validation step\n        val_loss = val_epoch(model,val_dl,loss_func,device)\n        val_losses.append(val_loss)\n\n        print(f\"Epoch {epoch + 1} | train loss: {train_loss:.3f} | val loss: {val_loss:.3f}\")\n        wandb.log({\"epoch\": epoch + 1,\n                   \"train_loss\": train_loss,\n                   \"val_loss\": val_loss})\n\n    return train_losses, val_losses\n\nDefining the train_model function. We omit the plot_curves function since all performance metrics will be tracked on w&b.\n\ndef train_model(train_dl,val_dl,model,device, lr=0.01, epochs=50, lossf=None,opt=None):\n\n    # define optimizer\n    if opt:\n        optimizer = opt(model.parameters(), lr=lr)\n    else: # if no opt provided, just use SGD\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n\n    # define loss function\n    if lossf:\n        loss_func = lossf\n    else: # if no loss function provided, just use MSE\n        loss_func = torch.nn.MSELoss()\n\n    # run the training loop\n    train_losses, val_losses = train_loop(\n                                epochs,\n                                model,\n                                loss_func,\n                                optimizer,\n                                train_dl,\n                                val_dl,\n                                device)\n\n\n\n\n\nAs a way to evaluate each model, let’s modify the test_model() function so that wand also keeps track of the performance metrics. In this case the metrics are pearson_per_sample, test_pearson_r and best_test.\n\ndef test_model(model, test_features, test_targets):\n  model.eval()\n  predictions = model(test_features.to(torch.float32).to(device)).detach().cpu().numpy()\n  observations = test_targets.cpu().numpy()\n  pearson_per_sample = np.array([pearsonr(predictions[i], observations[i])[0] for i in range(300)])\n  test_pearsonr = pearson_per_sample.mean()\n  best_test = pearson_per_sample.max()\n  wandb.log({'test_avg_pearsonr': test_pearsonr,\n             'beast_pearsonr': best_test})\n\n\n\n\nA sweep is the training and testing of a single model with a given configuration of hyperparameters. with wandb.sweep we define the set of hyperparameters to test, which will be then combined in different configurations each sweep.\n\nsweep_config = {\n    'method': 'random',\n    'metric': {'name': 'test_avg_pearsonr', 'goal': 'maximize'},\n    'parameters': {\n        'num_filters': {'values': [4, 16]},\n        'kernel_size': {'values': [5, 10]},\n        'add_sigmoid': {'values': [True, False],},\n        'learning_rate':{'values':[0.1, 0.05]},\n        'batch_size': {'values':[16, 32, 64]},\n        'optimizer': {'values': ['SGD','Adam']}\n    }\n}\n\nCreate a project ID for your model, all your tests will be saved in this project\n\nsweep_id = wandb.sweep(sweep_config, project=\"DNA_model\")\n\nCreate sweep with ID: yfwcc62j\nSweep URL: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n\n\n\n\nDIR = '/Users/sofiasalazar/Library/CloudStorage/Box-Box/imlab-data/Courses/AI-in-Genomics-2025/data/'\nsequences = pd.read_csv(os.path.join(DIR, 'chr22_sequences.txt.gz'), sep=\"\\t\", compression='gzip')\nscores = pd.read_csv(os.path.join(DIR, 'chr22_scores.txt.gz'), sep=\"\\t\", compression='gzip',dtype='float32')\n\n\ntrain_scores, train_sequences_tensor, val_scores, val_sequences_tensor, test_scores, test_sequences_tensor = get_data_tensors(scores, sequences)\n\nTrain: (14808, 2)\nVal: (3703, 2)\nTest: (4628, 2)\n\n\n\n\n\nWith wandb.init we initialize one sweep and what we want to do in each. This consists on\n\nLoading a configuration of hyperparameters with wandb.config\nLoading the model\nTelling wandb to track the training with wandb.watch\nCreate the dataloaders\nTrain and test the model\n\n\ndef train_sweep():\n    with wandb.init(project = \"DNA_model\"):\n        config = wandb.config\n        model = DNA_CNN(seq_len=300, num_filters=config.num_filters, kernel_size=config.kernel_size, add_sigmoid=config.add_sigmoid).to(device)\n        wandb.watch(model, log=\"all\", log_freq=10) # log all: logs all gradients and parameters, every log_freq number of training steps (batches) \n        train_loader = create_dataloader(train_sequences_tensor, train_scores, batch_size=config.batch_size)\n        val_loader = create_dataloader(val_sequences_tensor, val_scores, batch_size=config.batch_size, is_train=False)\n        if config.optimizer == 'SGD':\n            opt = torch.optim.SGD\n        else: opt = torch.optim.Adam \n\n        train_model(train_loader, val_loader, model, device, epochs=30, lr = config.learning_rate, opt=opt)\n        test_model(model, test_sequences_tensor, test_scores)\n\nFinally, we train with wandb.agent, the argument count is the number of combinations of hyperparameters I want to try. The maximum in my case is 240 combinations\n\n# wandb.agent(sweep_id, train_sweep, count=240)\nwandb.agent(sweep_id, train_sweep, count=6)\n\nwandb: Agent Starting Run: 0un6by39 with config:\nwandb:  add_sigmoid: True\nwandb:  batch_size: 16\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.1\nwandb:  num_filters: 4\nwandb:  optimizer: SGD\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151021-0un6by39\n\n\nSyncing run stellar-sweep-1 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/0un6by39\n\n\nEpoch 1 | train loss: 2.423 | val loss: 2.452\nEpoch 2 | train loss: 2.417 | val loss: 2.447\nEpoch 3 | train loss: 2.402 | val loss: 2.417\nEpoch 4 | train loss: 2.352 | val loss: 2.363\nEpoch 5 | train loss: 2.314 | val loss: 2.344\nEpoch 6 | train loss: 2.297 | val loss: 2.334\nEpoch 7 | train loss: 2.287 | val loss: 2.328\nEpoch 8 | train loss: 2.279 | val loss: 2.324\nEpoch 9 | train loss: 2.273 | val loss: 2.321\nEpoch 10 | train loss: 2.267 | val loss: 2.318\nEpoch 11 | train loss: 2.262 | val loss: 2.317\nEpoch 12 | train loss: 2.257 | val loss: 2.313\nEpoch 13 | train loss: 2.252 | val loss: 2.311\nEpoch 14 | train loss: 2.246 | val loss: 2.311\nEpoch 15 | train loss: 2.240 | val loss: 2.308\nEpoch 16 | train loss: 2.235 | val loss: 2.305\nEpoch 17 | train loss: 2.229 | val loss: 2.303\nEpoch 18 | train loss: 2.223 | val loss: 2.301\nEpoch 19 | train loss: 2.217 | val loss: 2.301\nEpoch 20 | train loss: 2.211 | val loss: 2.299\nEpoch 21 | train loss: 2.205 | val loss: 2.298\nEpoch 22 | train loss: 2.199 | val loss: 2.296\nEpoch 23 | train loss: 2.193 | val loss: 2.295\nEpoch 24 | train loss: 2.187 | val loss: 2.295\nEpoch 25 | train loss: 2.181 | val loss: 2.294\nEpoch 26 | train loss: 2.176 | val loss: 2.294\nEpoch 27 | train loss: 2.170 | val loss: 2.293\nEpoch 28 | train loss: 2.165 | val loss: 2.293\nEpoch 29 | train loss: 2.160 | val loss: 2.292\nEpoch 30 | train loss: 2.155 | val loss: 2.292\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n▁\n\n\nepoch\n▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n\n\ntest_avg_pearsonr\n▁\n\n\ntrain_loss\n██▇▆▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁\n\n\nval_loss\n██▆▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\n\n\n\n\n\nbeast_pearsonr\n0.68177\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n0.23191\n\n\ntrain_loss\n2.15465\n\n\nval_loss\n2.29175\n\n\n\n\n\n\n View run stellar-sweep-1 at: https://wandb.ai/ssalazar_02/DNA_model/runs/0un6by39 View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_151021-0un6by39/logs\n\n\nwandb: Agent Starting Run: bt4sqsaj with config:\nwandb:  add_sigmoid: False\nwandb:  batch_size: 16\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.1\nwandb:  num_filters: 4\nwandb:  optimizer: SGD\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151218-bt4sqsaj\n\n\nSyncing run vivid-sweep-2 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/bt4sqsaj\n\n\nEpoch 1 | train loss: 2.387 | val loss: 2.354\nEpoch 2 | train loss: 2.291 | val loss: 2.311\nEpoch 3 | train loss: 2.259 | val loss: 2.314\nEpoch 4 | train loss: 2.244 | val loss: 2.289\nEpoch 5 | train loss: 2.229 | val loss: 2.259\nEpoch 6 | train loss: 2.128 | val loss: 2.137\nEpoch 7 | train loss: 2.045 | val loss: 2.091\nEpoch 8 | train loss: 1.995 | val loss: 2.044\nEpoch 9 | train loss: 1.934 | val loss: 1.987\nEpoch 10 | train loss: 1.873 | val loss: 1.942\nEpoch 11 | train loss: 1.815 | val loss: 1.886\nEpoch 12 | train loss: 1.757 | val loss: 1.846\nEpoch 13 | train loss: 1.708 | val loss: 1.807\nEpoch 14 | train loss: 1.666 | val loss: 1.774\nEpoch 15 | train loss: 1.632 | val loss: 1.746\nEpoch 16 | train loss: 1.602 | val loss: 1.724\nEpoch 17 | train loss: 1.576 | val loss: 1.702\nEpoch 18 | train loss: 1.552 | val loss: 1.684\nEpoch 19 | train loss: 1.530 | val loss: 1.667\nEpoch 20 | train loss: 1.509 | val loss: 1.650\nEpoch 21 | train loss: 1.489 | val loss: 1.637\nEpoch 22 | train loss: 1.473 | val loss: 1.635\nEpoch 23 | train loss: 1.458 | val loss: 1.622\nEpoch 24 | train loss: 1.446 | val loss: 1.608\nEpoch 25 | train loss: 1.435 | val loss: 1.603\nEpoch 26 | train loss: 1.425 | val loss: 1.599\nEpoch 27 | train loss: 1.416 | val loss: 1.593\nEpoch 28 | train loss: 1.407 | val loss: 1.586\nEpoch 29 | train loss: 1.397 | val loss: 1.584\nEpoch 30 | train loss: 1.388 | val loss: 1.580\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n▁\n\n\nepoch\n▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n\n\ntest_avg_pearsonr\n▁\n\n\ntrain_loss\n█▇▇▇▇▆▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n\n\nval_loss\n███▇▇▆▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\n\n\n\n\n\n\n\nbeast_pearsonr\n0.92833\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n0.57389\n\n\ntrain_loss\n1.38804\n\n\nval_loss\n1.57982\n\n\n\n\n\n\n View run vivid-sweep-2 at: https://wandb.ai/ssalazar_02/DNA_model/runs/bt4sqsaj View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_151218-bt4sqsaj/logs\n\n\nwandb: Agent Starting Run: 7ca57ikz with config:\nwandb:  add_sigmoid: True\nwandb:  batch_size: 16\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.05\nwandb:  num_filters: 32\nwandb:  optimizer: SGD\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151414-7ca57ikz\n\n\nSyncing run unique-sweep-3 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/7ca57ikz\n\n\nEpoch 1 | train loss: 2.423 | val loss: 2.451\nEpoch 2 | train loss: 2.415 | val loss: 2.447\nEpoch 3 | train loss: 2.408 | val loss: 2.436\nEpoch 4 | train loss: 2.385 | val loss: 2.404\nEpoch 5 | train loss: 2.345 | val loss: 2.370\nEpoch 6 | train loss: 2.316 | val loss: 2.350\nEpoch 7 | train loss: 2.300 | val loss: 2.340\nEpoch 8 | train loss: 2.289 | val loss: 2.336\nEpoch 9 | train loss: 2.281 | val loss: 2.328\nEpoch 10 | train loss: 2.274 | val loss: 2.325\nEpoch 11 | train loss: 2.268 | val loss: 2.323\nEpoch 12 | train loss: 2.262 | val loss: 2.321\nEpoch 13 | train loss: 2.257 | val loss: 2.318\nEpoch 14 | train loss: 2.251 | val loss: 2.318\nEpoch 15 | train loss: 2.247 | val loss: 2.316\nEpoch 16 | train loss: 2.242 | val loss: 2.316\nEpoch 17 | train loss: 2.237 | val loss: 2.315\nEpoch 18 | train loss: 2.232 | val loss: 2.315\nEpoch 19 | train loss: 2.227 | val loss: 2.313\nEpoch 20 | train loss: 2.222 | val loss: 2.314\nEpoch 21 | train loss: 2.217 | val loss: 2.314\nEpoch 22 | train loss: 2.212 | val loss: 2.314\nEpoch 23 | train loss: 2.206 | val loss: 2.313\nEpoch 24 | train loss: 2.201 | val loss: 2.314\nEpoch 25 | train loss: 2.196 | val loss: 2.315\nEpoch 26 | train loss: 2.191 | val loss: 2.315\nEpoch 27 | train loss: 2.186 | val loss: 2.315\nEpoch 28 | train loss: 2.181 | val loss: 2.315\nEpoch 29 | train loss: 2.176 | val loss: 2.315\nEpoch 30 | train loss: 2.171 | val loss: 2.316\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n▁\n\n\nepoch\n▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n\n\ntest_avg_pearsonr\n▁\n\n\ntrain_loss\n███▇▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁\n\n\nval_loss\n██▇▆▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\n\n\n\n\n\nbeast_pearsonr\n0.6957\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n0.21384\n\n\ntrain_loss\n2.17106\n\n\nval_loss\n2.31633\n\n\n\n\n\n\n View run unique-sweep-3 at: https://wandb.ai/ssalazar_02/DNA_model/runs/7ca57ikz View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_151414-7ca57ikz/logs\n\n\nwandb: Agent Starting Run: 2pk9jgsh with config:\nwandb:  add_sigmoid: False\nwandb:  batch_size: 32\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.1\nwandb:  num_filters: 32\nwandb:  optimizer: Adam\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151710-2pk9jgsh\n\n\nSyncing run breezy-sweep-4 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/2pk9jgsh\n\n\nEpoch 1 | train loss: 8.430 | val loss: 2.453\nEpoch 2 | train loss: 2.422 | val loss: 2.457\nEpoch 3 | train loss: 2.423 | val loss: 2.455\nEpoch 4 | train loss: 2.424 | val loss: 2.457\nEpoch 5 | train loss: 2.425 | val loss: 2.457\nEpoch 6 | train loss: 2.425 | val loss: 2.459\nEpoch 7 | train loss: 2.427 | val loss: 2.464\nEpoch 8 | train loss: 2.428 | val loss: 2.463\nEpoch 9 | train loss: 2.429 | val loss: 2.461\nEpoch 10 | train loss: 2.430 | val loss: 2.463\nEpoch 11 | train loss: 2.429 | val loss: 2.459\nEpoch 12 | train loss: 2.430 | val loss: 2.465\nEpoch 13 | train loss: 2.430 | val loss: 2.461\nEpoch 14 | train loss: 2.431 | val loss: 2.458\nEpoch 15 | train loss: 2.430 | val loss: 2.462\nEpoch 16 | train loss: 2.431 | val loss: 2.464\nEpoch 17 | train loss: 2.431 | val loss: 2.463\nEpoch 18 | train loss: 2.430 | val loss: 2.461\nEpoch 19 | train loss: 2.431 | val loss: 2.458\nEpoch 20 | train loss: 2.430 | val loss: 2.466\nEpoch 21 | train loss: 2.431 | val loss: 2.466\nEpoch 22 | train loss: 2.431 | val loss: 2.463\nEpoch 23 | train loss: 2.431 | val loss: 2.463\nEpoch 24 | train loss: 2.430 | val loss: 2.470\nEpoch 25 | train loss: 2.431 | val loss: 2.460\nEpoch 26 | train loss: 2.431 | val loss: 2.459\nEpoch 27 | train loss: 2.431 | val loss: 2.463\nEpoch 28 | train loss: 2.431 | val loss: 2.462\nEpoch 29 | train loss: 2.431 | val loss: 2.462\nEpoch 30 | train loss: 2.431 | val loss: 2.461\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n▁\n\n\nepoch\n▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n\n\ntest_avg_pearsonr\n▁\n\n\ntrain_loss\n█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nval_loss\n▁▃▂▂▃▄▆▅▄▅▄▆▄▃▅▅▅▄▃▆▆▅▅█▄▃▅▅▅▄\n\n\n\n\n\n\n\nbeast_pearsonr\n0.35183\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n-0.00231\n\n\ntrain_loss\n2.43134\n\n\nval_loss\n2.46144\n\n\n\n\n\n\n View run breezy-sweep-4 at: https://wandb.ai/ssalazar_02/DNA_model/runs/2pk9jgsh View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_151710-2pk9jgsh/logs\n\n\nwandb: Agent Starting Run: ohia2jz9 with config:\nwandb:  add_sigmoid: True\nwandb:  batch_size: 32\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.05\nwandb:  num_filters: 32\nwandb:  optimizer: Adam\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151916-ohia2jz9\n\n\nSyncing run vibrant-sweep-5 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/ohia2jz9\n\n\nEpoch 1 | train loss: 2.520 | val loss: 2.554\nEpoch 2 | train loss: 2.520 | val loss: 2.556\nEpoch 3 | train loss: 2.520 | val loss: 2.556\nEpoch 4 | train loss: 2.520 | val loss: 2.556\nEpoch 5 | train loss: 2.520 | val loss: 2.556\nEpoch 6 | train loss: 2.520 | val loss: 2.556\nEpoch 7 | train loss: 2.520 | val loss: 2.556\nEpoch 8 | train loss: 2.520 | val loss: 2.556\nEpoch 9 | train loss: 2.520 | val loss: 2.556\nEpoch 10 | train loss: 2.520 | val loss: 2.556\nEpoch 11 | train loss: 2.520 | val loss: 2.556\nEpoch 12 | train loss: 2.520 | val loss: 2.556\nEpoch 13 | train loss: 2.520 | val loss: 2.556\nEpoch 14 | train loss: 2.520 | val loss: 2.556\nEpoch 15 | train loss: 2.520 | val loss: 2.556\nEpoch 16 | train loss: 2.520 | val loss: 2.556\nEpoch 17 | train loss: 2.520 | val loss: 2.556\nEpoch 18 | train loss: 2.520 | val loss: 2.556\nEpoch 19 | train loss: 2.520 | val loss: 2.556\nEpoch 20 | train loss: 2.520 | val loss: 2.556\nEpoch 21 | train loss: 2.520 | val loss: 2.556\nEpoch 22 | train loss: 2.520 | val loss: 2.556\nEpoch 23 | train loss: 2.520 | val loss: 2.556\nEpoch 24 | train loss: 2.520 | val loss: 2.556\nEpoch 25 | train loss: 2.520 | val loss: 2.556\nEpoch 26 | train loss: 2.520 | val loss: 2.556\nEpoch 27 | train loss: 2.520 | val loss: 2.556\nEpoch 28 | train loss: 2.520 | val loss: 2.556\nEpoch 29 | train loss: 2.520 | val loss: 2.556\nEpoch 30 | train loss: 2.520 | val loss: 2.556\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n▁\n\n\nepoch\n▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n\n\ntest_avg_pearsonr\n▁\n\n\ntrain_loss\n▁█████████████████████████████\n\n\nval_loss\n▁█████████████████████████████\n\n\n\n\n\n\n\nbeast_pearsonr\n0.20144\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n-0.00629\n\n\ntrain_loss\n2.52015\n\n\nval_loss\n2.5559\n\n\n\n\n\n\n View run vibrant-sweep-5 at: https://wandb.ai/ssalazar_02/DNA_model/runs/ohia2jz9 View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_151916-ohia2jz9/logs\n\n\nwandb: Agent Starting Run: gmg0gdbw with config:\nwandb:  add_sigmoid: False\nwandb:  batch_size: 64\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.1\nwandb:  num_filters: 16\nwandb:  optimizer: SGD\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_152128-gmg0gdbw\n\n\nSyncing run upbeat-sweep-6 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/gmg0gdbw\n\n\nEpoch 1 | train loss: 2.424 | val loss: 2.440\nEpoch 2 | train loss: 2.370 | val loss: 2.371\nEpoch 3 | train loss: 2.307 | val loss: 2.336\nEpoch 4 | train loss: 2.278 | val loss: 2.320\nEpoch 5 | train loss: 2.260 | val loss: 2.312\nEpoch 6 | train loss: 2.245 | val loss: 2.304\nEpoch 7 | train loss: 2.231 | val loss: 2.297\nEpoch 8 | train loss: 2.216 | val loss: 2.294\nEpoch 9 | train loss: 2.202 | val loss: 2.290\nEpoch 10 | train loss: 2.185 | val loss: 2.285\nEpoch 11 | train loss: 2.168 | val loss: 2.281\nEpoch 12 | train loss: 2.148 | val loss: 2.271\nEpoch 13 | train loss: 2.119 | val loss: 2.255\nEpoch 14 | train loss: 2.080 | val loss: 2.228\nEpoch 15 | train loss: 2.039 | val loss: 2.208\nEpoch 16 | train loss: 2.001 | val loss: 2.185\nEpoch 17 | train loss: 1.966 | val loss: 2.169\nEpoch 18 | train loss: 1.933 | val loss: 2.157\nEpoch 19 | train loss: 1.903 | val loss: 2.140\nEpoch 20 | train loss: 1.875 | val loss: 2.126\nEpoch 21 | train loss: 1.848 | val loss: 2.113\nEpoch 22 | train loss: 1.822 | val loss: 2.107\nEpoch 23 | train loss: 1.799 | val loss: 2.098\nEpoch 24 | train loss: 1.776 | val loss: 2.092\nEpoch 25 | train loss: 1.755 | val loss: 2.086\nEpoch 26 | train loss: 1.736 | val loss: 2.081\nEpoch 27 | train loss: 1.717 | val loss: 2.075\nEpoch 28 | train loss: 1.699 | val loss: 2.075\nEpoch 29 | train loss: 1.682 | val loss: 2.069\nEpoch 30 | train loss: 1.666 | val loss: 2.068\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n▁\n\n\nepoch\n▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n\n\ntest_avg_pearsonr\n▁\n\n\ntrain_loss\n██▇▇▆▆▆▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁\n\n\nval_loss\n█▇▆▆▆▅▅▅▅▅▅▅▅▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\n\n\n\n\n\n\n\nbeast_pearsonr\n0.80236\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n0.38552\n\n\ntrain_loss\n1.66555\n\n\nval_loss\n2.06831\n\n\n\n\n\n\n View run upbeat-sweep-6 at: https://wandb.ai/ssalazar_02/DNA_model/runs/gmg0gdbw View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_152128-gmg0gdbw/logs\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-03-25-unit00/tf-binding-wandb.html#calibrating-hyperparameters-with-weights-and-biases",
    "href": "post/2025-03-25-unit00/tf-binding-wandb.html#calibrating-hyperparameters-with-weights-and-biases",
    "title": "Calibrating hyperparameters with weights and biases",
    "section": "",
    "text": "Goal: Learn how to use weights and biases to calibrate the hyperparameters of a DL model.\n\nWeights and biases is a platform used for AI developers to track, visualize and manage their ML models and experiments. The coolest part is that W&B allows you to log various performance metrics during training, like training and validation loss, test set correlations, etc. Additionally, it allows you to compare between different experiments or versions of your models. Making it easier to identify the best performing models and see which hyperparameter configuration is the optimal.\nIn this notebook, we will focus on using W&B as a tool to help callibrate the hyperparameters of the TF binding prediction model to find an optimal solution. However, we encourage you to explore other applications that W&B offers.\n\n\n\nFirst install w&b in your environment with the following command, it should take only a couple of seconds\n\n%pip install wandb onnx -Uq\n%pip install nbformat\n\nLoad all libraries\n\nimport torch\nfrom torch import nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nimport os\nimport pandas as pd\nimport wandb\nfrom scipy.stats import pearsonr\n\n\n\n\nIf you don’t already have a w&b account, sign up here. Then, run the following command which will prompt you to insert your API key\n\nwandb.login()\n\nwandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\nwandb: Currently logged in as: ssalazar_02 to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n\n\nTrue\n\n\n\n\n\nThen, define your functions, this will remain unchanged\n\ndef get_device():\n  \"\"\"\n  Determines the device to use for PyTorch computations.\n\n  Prioritizes Metal Performance Shaders (MPS), then CUDA, then CPU.\n\n  Returns:\n    torch.device: The selected device.\n  \"\"\"\n  if torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    print(\"Using MPS device.\")\n  elif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"Using CUDA device.\")\n  else:\n    device = torch.device(\"cpu\")\n    print(\"Using CPU device.\")\n  return device\n\n# Example usage:\ndevice = get_device()\n\nUsing MPS device.\n\n\n\ndef one_hot_encode(seq):\n    \"\"\"\n    Given a DNA sequence, return its one-hot encoding\n    \"\"\"\n    # Make sure seq has only allowed bases\n    allowed = set(\"ACTGN\")\n    if not set(seq).issubset(allowed):\n        invalid = set(seq) - allowed\n        print(seq)\n        raise ValueError(f\"Sequence contains chars not in allowed DNA alphabet (ACGTN): {invalid}\")\n\n    # Dictionary returning one-hot encoding for each nucleotide\n    nuc_d = {'A':[1.0,0.0,0.0,0.0],\n             'C':[0.0,1.0,0.0,0.0],\n             'G':[0.0,0.0,1.0,0.0],\n             'T':[0.0,0.0,0.0,1.0],\n             'N':[0.0,0.0,0.0,0.0]}\n\n    # Create array from nucleotide sequence\n    vec=np.array([nuc_d[x] for x in seq],dtype='float32')\n\n    return vec\n\n\ndef quick_split(df, split_frac=0.8, verbose=False):\n    '''\n    Given a df of samples, randomly split indices between\n    train and test at the desired fraction\n    '''\n    cols = df.columns # original columns, use to clean up reindexed cols\n    df = df.reset_index()\n\n    # shuffle indices\n    idxs = list(range(df.shape[0]))\n    random.shuffle(idxs)\n\n    # split shuffled index list by split_frac\n    split = int(len(idxs)*split_frac)\n    train_idxs = idxs[:split]\n    test_idxs = idxs[split:]\n\n    # split dfs and return\n    train_df = df[df.index.isin(train_idxs)]\n    test_df = df[df.index.isin(test_idxs)]\n\n    return train_df[cols], test_df[cols]\n\n\ndef split_sequences(sequences_df):\n    full_train_sequences, test_sequences = quick_split(sequences_df)\n    train_sequences, val_sequences = quick_split(full_train_sequences)\n    print(\"Train:\", train_sequences.shape)\n    print(\"Val:\", val_sequences.shape)\n    print(\"Test:\", test_sequences.shape)\n    return train_sequences, val_sequences, test_sequences\n\n\ndef get_data_tensors(scores_df, sequences_df):\n    # split sequences in train, validation and test sets\n    train_sequences, val_sequences, test_sequences = split_sequences(sequences_df)\n    # get scores for each set of sequences\n    train_scores = scores_df[train_sequences['window_name'].to_list()].transpose().values.astype('float32') # shape is (num_sequences, 300)\n    val_scores = scores_df[val_sequences['window_name'].to_list()].transpose().values.astype('float32')\n    test_scores = scores_df[test_sequences['window_name'].to_list()].transpose().values.astype('float32')\n\n    train_scores = torch.tensor(train_scores, dtype=torch.float32).to(device)\n    val_scores = torch.tensor(val_scores, dtype=torch.float32).to(device)\n    test_scores = torch.tensor(test_scores, dtype=torch.float32).to(device)\n\n    # get one hot encoded sequences for each set\n    train_one_hot = [one_hot_encode(seq) for seq in train_sequences['sequence'].to_list()]\n    train_sequences_tensor = torch.tensor(np.stack(train_one_hot))\n\n    val_one_hot = [one_hot_encode(seq) for seq in val_sequences['sequence'].to_list()]\n    val_sequences_tensor = torch.tensor(np.stack(val_one_hot))\n\n    test_one_hot = [one_hot_encode(seq) for seq in test_sequences['sequence'].to_list()]\n    test_sequences_tensor = torch.tensor(np.stack(test_one_hot))\n\n    return train_scores, train_sequences_tensor, val_scores, val_sequences_tensor, test_scores, test_sequences_tensor\n\n\n\ndef create_dataloader(predictors, targets, batch_size, is_train = True):\n    '''\n    features: one hot encoded sequences\n    targets: sequence scores\n    batch_size\n    is_train: if True, data is reshuffled at every epoch\n    '''\n\n    dataset = torch.utils.data.TensorDataset(predictors, targets)\n    return torch.utils.data.DataLoader(dataset, batch_size, shuffle = is_train)\n\n\nclass DNA_CNN(nn.Module):\n    def __init__(self,\n                 seq_len,\n                 num_filters=16,\n                 kernel_size=10,\n                 add_sigmoid=False):\n        super().__init__()\n        self.seq_len = seq_len\n        self.add_sigmoid = add_sigmoid\n        # Define layers individually\n        self.conv = nn.Conv1d(in_channels = 4, out_channels = num_filters, kernel_size=kernel_size)\n        self.relu = nn.ReLU(inplace=True)\n        self.linear = nn.Linear(num_filters*(seq_len-kernel_size+1), 300)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, xb):\n        # reshape view to batch_size x 4channel x seq_len\n        # permute to put channel in correct order\n        xb = xb.permute(0,2,1) # (batch_size, 300, 4) to (batch_size, 4, 300)\n\n        # Apply layers step by step\n        x = self.conv(xb)\n        x = self.relu(x)\n        x = x.flatten(1)  # flatten all dimensions except batch\n        out = self.linear(x)\n        \n        if self.add_sigmoid:\n            out = self.sigmoid(out)\n        return out\n\n\ndef process_batch(model, loss_func, x_batch, y_batch, opt=None):\n    xb_out = model(x_batch.to(torch.float32))\n\n    loss = loss_func(xb_out, y_batch)\n\n    if opt is not None: # backpropagate if train step (optimizer given)\n        loss.backward()\n        opt.step()\n        opt.zero_grad()\n\n    return loss.item(), len(x_batch)\n\n\ndef train_epoch(model, train_dl, loss_func, device, opt):\n\n    model.train()\n    tl = [] # train losses\n    ns = [] # batch sizes, n\n\n    # loop through batches\n    for x_batch, y_batch in train_dl:\n\n        x_batch, y_batch = x_batch.to(device),y_batch.to(device)\n\n        t, n = process_batch(model, loss_func, x_batch, y_batch, opt=opt)\n\n        # collect train loss and batch sizes\n        tl.append(t)\n        ns.append(n)\n\n    # average the losses over all batches\n    train_loss = np.sum(np.multiply(tl, ns)) / np.sum(ns)\n\n    return train_loss\n\n\ndef val_epoch(model, val_dl, loss_func, device):\n\n    # Set model to Evaluation mode\n    model.eval()\n    with torch.no_grad():\n        vl = [] # val losses\n        ns = [] # batch sizes, n\n\n        # loop through validation DataLoader\n        for x_batch, y_batch in val_dl:\n\n            x_batch, y_batch = x_batch.to(device),y_batch.to(device)\n\n            v, n = process_batch(model, loss_func, x_batch, y_batch)\n\n            # collect val loss and batch sizes\n            vl.append(v)\n            ns.append(n)\n\n    # average the losses over all batches\n    val_loss = np.sum(np.multiply(vl, ns)) / np.sum(ns)\n\n    return val_loss\n\n\n\n\nThe only function that we need to change is the train_loop function, because here is where we are recovering the parameters that we want to track with w&b. We will use wandb.log and create a dictionary with the parmeters that we want to track.\n\ndef train_loop(epochs, model, loss_func, opt, train_dl, val_dl, device):\n\n    # keep track of losses\n    train_losses = []\n    val_losses = []\n\n    # loop through epochs\n    for epoch in range(epochs):\n        # take a training step\n        train_loss = train_epoch(model,train_dl,loss_func,device,opt)\n        train_losses.append(train_loss)\n\n        # take a validation step\n        val_loss = val_epoch(model,val_dl,loss_func,device)\n        val_losses.append(val_loss)\n\n        print(f\"Epoch {epoch + 1} | train loss: {train_loss:.3f} | val loss: {val_loss:.3f}\")\n        wandb.log({\"epoch\": epoch + 1,\n                   \"train_loss\": train_loss,\n                   \"val_loss\": val_loss})\n\n    return train_losses, val_losses\n\nDefining the train_model function. We omit the plot_curves function since all performance metrics will be tracked on w&b.\n\ndef train_model(train_dl,val_dl,model,device, lr=0.01, epochs=50, lossf=None,opt=None):\n\n    # define optimizer\n    if opt:\n        optimizer = opt(model.parameters(), lr=lr)\n    else: # if no opt provided, just use SGD\n        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n\n    # define loss function\n    if lossf:\n        loss_func = lossf\n    else: # if no loss function provided, just use MSE\n        loss_func = torch.nn.MSELoss()\n\n    # run the training loop\n    train_losses, val_losses = train_loop(\n                                epochs,\n                                model,\n                                loss_func,\n                                optimizer,\n                                train_dl,\n                                val_dl,\n                                device)\n\n\n\n\n\nAs a way to evaluate each model, let’s modify the test_model() function so that wand also keeps track of the performance metrics. In this case the metrics are pearson_per_sample, test_pearson_r and best_test.\n\ndef test_model(model, test_features, test_targets):\n  model.eval()\n  predictions = model(test_features.to(torch.float32).to(device)).detach().cpu().numpy()\n  observations = test_targets.cpu().numpy()\n  pearson_per_sample = np.array([pearsonr(predictions[i], observations[i])[0] for i in range(300)])\n  test_pearsonr = pearson_per_sample.mean()\n  best_test = pearson_per_sample.max()\n  wandb.log({'test_avg_pearsonr': test_pearsonr,\n             'beast_pearsonr': best_test})\n\n\n\n\nA sweep is the training and testing of a single model with a given configuration of hyperparameters. with wandb.sweep we define the set of hyperparameters to test, which will be then combined in different configurations each sweep.\n\nsweep_config = {\n    'method': 'random',\n    'metric': {'name': 'test_avg_pearsonr', 'goal': 'maximize'},\n    'parameters': {\n        'num_filters': {'values': [4, 16]},\n        'kernel_size': {'values': [5, 10]},\n        'add_sigmoid': {'values': [True, False],},\n        'learning_rate':{'values':[0.1, 0.05]},\n        'batch_size': {'values':[16, 32, 64]},\n        'optimizer': {'values': ['SGD','Adam']}\n    }\n}\n\nCreate a project ID for your model, all your tests will be saved in this project\n\nsweep_id = wandb.sweep(sweep_config, project=\"DNA_model\")\n\nCreate sweep with ID: yfwcc62j\nSweep URL: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n\n\n\n\nDIR = '/Users/sofiasalazar/Library/CloudStorage/Box-Box/imlab-data/Courses/AI-in-Genomics-2025/data/'\nsequences = pd.read_csv(os.path.join(DIR, 'chr22_sequences.txt.gz'), sep=\"\\t\", compression='gzip')\nscores = pd.read_csv(os.path.join(DIR, 'chr22_scores.txt.gz'), sep=\"\\t\", compression='gzip',dtype='float32')\n\n\ntrain_scores, train_sequences_tensor, val_scores, val_sequences_tensor, test_scores, test_sequences_tensor = get_data_tensors(scores, sequences)\n\nTrain: (14808, 2)\nVal: (3703, 2)\nTest: (4628, 2)\n\n\n\n\n\nWith wandb.init we initialize one sweep and what we want to do in each. This consists on\n\nLoading a configuration of hyperparameters with wandb.config\nLoading the model\nTelling wandb to track the training with wandb.watch\nCreate the dataloaders\nTrain and test the model\n\n\ndef train_sweep():\n    with wandb.init(project = \"DNA_model\"):\n        config = wandb.config\n        model = DNA_CNN(seq_len=300, num_filters=config.num_filters, kernel_size=config.kernel_size, add_sigmoid=config.add_sigmoid).to(device)\n        wandb.watch(model, log=\"all\", log_freq=10) # log all: logs all gradients and parameters, every log_freq number of training steps (batches) \n        train_loader = create_dataloader(train_sequences_tensor, train_scores, batch_size=config.batch_size)\n        val_loader = create_dataloader(val_sequences_tensor, val_scores, batch_size=config.batch_size, is_train=False)\n        if config.optimizer == 'SGD':\n            opt = torch.optim.SGD\n        else: opt = torch.optim.Adam \n\n        train_model(train_loader, val_loader, model, device, epochs=30, lr = config.learning_rate, opt=opt)\n        test_model(model, test_sequences_tensor, test_scores)\n\nFinally, we train with wandb.agent, the argument count is the number of combinations of hyperparameters I want to try. The maximum in my case is 240 combinations\n\n# wandb.agent(sweep_id, train_sweep, count=240)\nwandb.agent(sweep_id, train_sweep, count=6)\n\nwandb: Agent Starting Run: 0un6by39 with config:\nwandb:  add_sigmoid: True\nwandb:  batch_size: 16\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.1\nwandb:  num_filters: 4\nwandb:  optimizer: SGD\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151021-0un6by39\n\n\nSyncing run stellar-sweep-1 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/0un6by39\n\n\nEpoch 1 | train loss: 2.423 | val loss: 2.452\nEpoch 2 | train loss: 2.417 | val loss: 2.447\nEpoch 3 | train loss: 2.402 | val loss: 2.417\nEpoch 4 | train loss: 2.352 | val loss: 2.363\nEpoch 5 | train loss: 2.314 | val loss: 2.344\nEpoch 6 | train loss: 2.297 | val loss: 2.334\nEpoch 7 | train loss: 2.287 | val loss: 2.328\nEpoch 8 | train loss: 2.279 | val loss: 2.324\nEpoch 9 | train loss: 2.273 | val loss: 2.321\nEpoch 10 | train loss: 2.267 | val loss: 2.318\nEpoch 11 | train loss: 2.262 | val loss: 2.317\nEpoch 12 | train loss: 2.257 | val loss: 2.313\nEpoch 13 | train loss: 2.252 | val loss: 2.311\nEpoch 14 | train loss: 2.246 | val loss: 2.311\nEpoch 15 | train loss: 2.240 | val loss: 2.308\nEpoch 16 | train loss: 2.235 | val loss: 2.305\nEpoch 17 | train loss: 2.229 | val loss: 2.303\nEpoch 18 | train loss: 2.223 | val loss: 2.301\nEpoch 19 | train loss: 2.217 | val loss: 2.301\nEpoch 20 | train loss: 2.211 | val loss: 2.299\nEpoch 21 | train loss: 2.205 | val loss: 2.298\nEpoch 22 | train loss: 2.199 | val loss: 2.296\nEpoch 23 | train loss: 2.193 | val loss: 2.295\nEpoch 24 | train loss: 2.187 | val loss: 2.295\nEpoch 25 | train loss: 2.181 | val loss: 2.294\nEpoch 26 | train loss: 2.176 | val loss: 2.294\nEpoch 27 | train loss: 2.170 | val loss: 2.293\nEpoch 28 | train loss: 2.165 | val loss: 2.293\nEpoch 29 | train loss: 2.160 | val loss: 2.292\nEpoch 30 | train loss: 2.155 | val loss: 2.292\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n▁\n\n\nepoch\n▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n\n\ntest_avg_pearsonr\n▁\n\n\ntrain_loss\n██▇▆▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁\n\n\nval_loss\n██▆▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\n\n\n\n\n\nbeast_pearsonr\n0.68177\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n0.23191\n\n\ntrain_loss\n2.15465\n\n\nval_loss\n2.29175\n\n\n\n\n\n\n View run stellar-sweep-1 at: https://wandb.ai/ssalazar_02/DNA_model/runs/0un6by39 View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_151021-0un6by39/logs\n\n\nwandb: Agent Starting Run: bt4sqsaj with config:\nwandb:  add_sigmoid: False\nwandb:  batch_size: 16\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.1\nwandb:  num_filters: 4\nwandb:  optimizer: SGD\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151218-bt4sqsaj\n\n\nSyncing run vivid-sweep-2 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/bt4sqsaj\n\n\nEpoch 1 | train loss: 2.387 | val loss: 2.354\nEpoch 2 | train loss: 2.291 | val loss: 2.311\nEpoch 3 | train loss: 2.259 | val loss: 2.314\nEpoch 4 | train loss: 2.244 | val loss: 2.289\nEpoch 5 | train loss: 2.229 | val loss: 2.259\nEpoch 6 | train loss: 2.128 | val loss: 2.137\nEpoch 7 | train loss: 2.045 | val loss: 2.091\nEpoch 8 | train loss: 1.995 | val loss: 2.044\nEpoch 9 | train loss: 1.934 | val loss: 1.987\nEpoch 10 | train loss: 1.873 | val loss: 1.942\nEpoch 11 | train loss: 1.815 | val loss: 1.886\nEpoch 12 | train loss: 1.757 | val loss: 1.846\nEpoch 13 | train loss: 1.708 | val loss: 1.807\nEpoch 14 | train loss: 1.666 | val loss: 1.774\nEpoch 15 | train loss: 1.632 | val loss: 1.746\nEpoch 16 | train loss: 1.602 | val loss: 1.724\nEpoch 17 | train loss: 1.576 | val loss: 1.702\nEpoch 18 | train loss: 1.552 | val loss: 1.684\nEpoch 19 | train loss: 1.530 | val loss: 1.667\nEpoch 20 | train loss: 1.509 | val loss: 1.650\nEpoch 21 | train loss: 1.489 | val loss: 1.637\nEpoch 22 | train loss: 1.473 | val loss: 1.635\nEpoch 23 | train loss: 1.458 | val loss: 1.622\nEpoch 24 | train loss: 1.446 | val loss: 1.608\nEpoch 25 | train loss: 1.435 | val loss: 1.603\nEpoch 26 | train loss: 1.425 | val loss: 1.599\nEpoch 27 | train loss: 1.416 | val loss: 1.593\nEpoch 28 | train loss: 1.407 | val loss: 1.586\nEpoch 29 | train loss: 1.397 | val loss: 1.584\nEpoch 30 | train loss: 1.388 | val loss: 1.580\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n▁\n\n\nepoch\n▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n\n\ntest_avg_pearsonr\n▁\n\n\ntrain_loss\n█▇▇▇▇▆▆▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁\n\n\nval_loss\n███▇▇▆▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁\n\n\n\n\n\n\n\nbeast_pearsonr\n0.92833\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n0.57389\n\n\ntrain_loss\n1.38804\n\n\nval_loss\n1.57982\n\n\n\n\n\n\n View run vivid-sweep-2 at: https://wandb.ai/ssalazar_02/DNA_model/runs/bt4sqsaj View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_151218-bt4sqsaj/logs\n\n\nwandb: Agent Starting Run: 7ca57ikz with config:\nwandb:  add_sigmoid: True\nwandb:  batch_size: 16\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.05\nwandb:  num_filters: 32\nwandb:  optimizer: SGD\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151414-7ca57ikz\n\n\nSyncing run unique-sweep-3 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/7ca57ikz\n\n\nEpoch 1 | train loss: 2.423 | val loss: 2.451\nEpoch 2 | train loss: 2.415 | val loss: 2.447\nEpoch 3 | train loss: 2.408 | val loss: 2.436\nEpoch 4 | train loss: 2.385 | val loss: 2.404\nEpoch 5 | train loss: 2.345 | val loss: 2.370\nEpoch 6 | train loss: 2.316 | val loss: 2.350\nEpoch 7 | train loss: 2.300 | val loss: 2.340\nEpoch 8 | train loss: 2.289 | val loss: 2.336\nEpoch 9 | train loss: 2.281 | val loss: 2.328\nEpoch 10 | train loss: 2.274 | val loss: 2.325\nEpoch 11 | train loss: 2.268 | val loss: 2.323\nEpoch 12 | train loss: 2.262 | val loss: 2.321\nEpoch 13 | train loss: 2.257 | val loss: 2.318\nEpoch 14 | train loss: 2.251 | val loss: 2.318\nEpoch 15 | train loss: 2.247 | val loss: 2.316\nEpoch 16 | train loss: 2.242 | val loss: 2.316\nEpoch 17 | train loss: 2.237 | val loss: 2.315\nEpoch 18 | train loss: 2.232 | val loss: 2.315\nEpoch 19 | train loss: 2.227 | val loss: 2.313\nEpoch 20 | train loss: 2.222 | val loss: 2.314\nEpoch 21 | train loss: 2.217 | val loss: 2.314\nEpoch 22 | train loss: 2.212 | val loss: 2.314\nEpoch 23 | train loss: 2.206 | val loss: 2.313\nEpoch 24 | train loss: 2.201 | val loss: 2.314\nEpoch 25 | train loss: 2.196 | val loss: 2.315\nEpoch 26 | train loss: 2.191 | val loss: 2.315\nEpoch 27 | train loss: 2.186 | val loss: 2.315\nEpoch 28 | train loss: 2.181 | val loss: 2.315\nEpoch 29 | train loss: 2.176 | val loss: 2.315\nEpoch 30 | train loss: 2.171 | val loss: 2.316\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n▁\n\n\nepoch\n▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n\n\ntest_avg_pearsonr\n▁\n\n\ntrain_loss\n███▇▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁\n\n\nval_loss\n██▇▆▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\n\n\n\n\n\nbeast_pearsonr\n0.6957\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n0.21384\n\n\ntrain_loss\n2.17106\n\n\nval_loss\n2.31633\n\n\n\n\n\n\n View run unique-sweep-3 at: https://wandb.ai/ssalazar_02/DNA_model/runs/7ca57ikz View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_151414-7ca57ikz/logs\n\n\nwandb: Agent Starting Run: 2pk9jgsh with config:\nwandb:  add_sigmoid: False\nwandb:  batch_size: 32\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.1\nwandb:  num_filters: 32\nwandb:  optimizer: Adam\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151710-2pk9jgsh\n\n\nSyncing run breezy-sweep-4 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/2pk9jgsh\n\n\nEpoch 1 | train loss: 8.430 | val loss: 2.453\nEpoch 2 | train loss: 2.422 | val loss: 2.457\nEpoch 3 | train loss: 2.423 | val loss: 2.455\nEpoch 4 | train loss: 2.424 | val loss: 2.457\nEpoch 5 | train loss: 2.425 | val loss: 2.457\nEpoch 6 | train loss: 2.425 | val loss: 2.459\nEpoch 7 | train loss: 2.427 | val loss: 2.464\nEpoch 8 | train loss: 2.428 | val loss: 2.463\nEpoch 9 | train loss: 2.429 | val loss: 2.461\nEpoch 10 | train loss: 2.430 | val loss: 2.463\nEpoch 11 | train loss: 2.429 | val loss: 2.459\nEpoch 12 | train loss: 2.430 | val loss: 2.465\nEpoch 13 | train loss: 2.430 | val loss: 2.461\nEpoch 14 | train loss: 2.431 | val loss: 2.458\nEpoch 15 | train loss: 2.430 | val loss: 2.462\nEpoch 16 | train loss: 2.431 | val loss: 2.464\nEpoch 17 | train loss: 2.431 | val loss: 2.463\nEpoch 18 | train loss: 2.430 | val loss: 2.461\nEpoch 19 | train loss: 2.431 | val loss: 2.458\nEpoch 20 | train loss: 2.430 | val loss: 2.466\nEpoch 21 | train loss: 2.431 | val loss: 2.466\nEpoch 22 | train loss: 2.431 | val loss: 2.463\nEpoch 23 | train loss: 2.431 | val loss: 2.463\nEpoch 24 | train loss: 2.430 | val loss: 2.470\nEpoch 25 | train loss: 2.431 | val loss: 2.460\nEpoch 26 | train loss: 2.431 | val loss: 2.459\nEpoch 27 | train loss: 2.431 | val loss: 2.463\nEpoch 28 | train loss: 2.431 | val loss: 2.462\nEpoch 29 | train loss: 2.431 | val loss: 2.462\nEpoch 30 | train loss: 2.431 | val loss: 2.461\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n▁\n\n\nepoch\n▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n\n\ntest_avg_pearsonr\n▁\n\n\ntrain_loss\n█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n\n\nval_loss\n▁▃▂▂▃▄▆▅▄▅▄▆▄▃▅▅▅▄▃▆▆▅▅█▄▃▅▅▅▄\n\n\n\n\n\n\n\nbeast_pearsonr\n0.35183\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n-0.00231\n\n\ntrain_loss\n2.43134\n\n\nval_loss\n2.46144\n\n\n\n\n\n\n View run breezy-sweep-4 at: https://wandb.ai/ssalazar_02/DNA_model/runs/2pk9jgsh View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_151710-2pk9jgsh/logs\n\n\nwandb: Agent Starting Run: ohia2jz9 with config:\nwandb:  add_sigmoid: True\nwandb:  batch_size: 32\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.05\nwandb:  num_filters: 32\nwandb:  optimizer: Adam\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_151916-ohia2jz9\n\n\nSyncing run vibrant-sweep-5 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/ohia2jz9\n\n\nEpoch 1 | train loss: 2.520 | val loss: 2.554\nEpoch 2 | train loss: 2.520 | val loss: 2.556\nEpoch 3 | train loss: 2.520 | val loss: 2.556\nEpoch 4 | train loss: 2.520 | val loss: 2.556\nEpoch 5 | train loss: 2.520 | val loss: 2.556\nEpoch 6 | train loss: 2.520 | val loss: 2.556\nEpoch 7 | train loss: 2.520 | val loss: 2.556\nEpoch 8 | train loss: 2.520 | val loss: 2.556\nEpoch 9 | train loss: 2.520 | val loss: 2.556\nEpoch 10 | train loss: 2.520 | val loss: 2.556\nEpoch 11 | train loss: 2.520 | val loss: 2.556\nEpoch 12 | train loss: 2.520 | val loss: 2.556\nEpoch 13 | train loss: 2.520 | val loss: 2.556\nEpoch 14 | train loss: 2.520 | val loss: 2.556\nEpoch 15 | train loss: 2.520 | val loss: 2.556\nEpoch 16 | train loss: 2.520 | val loss: 2.556\nEpoch 17 | train loss: 2.520 | val loss: 2.556\nEpoch 18 | train loss: 2.520 | val loss: 2.556\nEpoch 19 | train loss: 2.520 | val loss: 2.556\nEpoch 20 | train loss: 2.520 | val loss: 2.556\nEpoch 21 | train loss: 2.520 | val loss: 2.556\nEpoch 22 | train loss: 2.520 | val loss: 2.556\nEpoch 23 | train loss: 2.520 | val loss: 2.556\nEpoch 24 | train loss: 2.520 | val loss: 2.556\nEpoch 25 | train loss: 2.520 | val loss: 2.556\nEpoch 26 | train loss: 2.520 | val loss: 2.556\nEpoch 27 | train loss: 2.520 | val loss: 2.556\nEpoch 28 | train loss: 2.520 | val loss: 2.556\nEpoch 29 | train loss: 2.520 | val loss: 2.556\nEpoch 30 | train loss: 2.520 | val loss: 2.556\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n▁\n\n\nepoch\n▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n\n\ntest_avg_pearsonr\n▁\n\n\ntrain_loss\n▁█████████████████████████████\n\n\nval_loss\n▁█████████████████████████████\n\n\n\n\n\n\n\nbeast_pearsonr\n0.20144\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n-0.00629\n\n\ntrain_loss\n2.52015\n\n\nval_loss\n2.5559\n\n\n\n\n\n\n View run vibrant-sweep-5 at: https://wandb.ai/ssalazar_02/DNA_model/runs/ohia2jz9 View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_151916-ohia2jz9/logs\n\n\nwandb: Agent Starting Run: gmg0gdbw with config:\nwandb:  add_sigmoid: False\nwandb:  batch_size: 64\nwandb:  kernel_size: 10\nwandb:  learning_rate: 0.1\nwandb:  num_filters: 16\nwandb:  optimizer: SGD\n\n\nIgnoring project 'DNA_model' when running a sweep.\n\n\nTracking run with wandb version 0.19.9\n\n\nRun data is saved locally in /Users/sofiasalazar/Desktop/IM_lab/DNA_model/wandb/run-20250407_152128-gmg0gdbw\n\n\nSyncing run upbeat-sweep-6 to Weights & Biases (docs)Sweep page: https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View project at https://wandb.ai/ssalazar_02/DNA_model\n\n\n View sweep at https://wandb.ai/ssalazar_02/DNA_model/sweeps/yfwcc62j\n\n\n View run at https://wandb.ai/ssalazar_02/DNA_model/runs/gmg0gdbw\n\n\nEpoch 1 | train loss: 2.424 | val loss: 2.440\nEpoch 2 | train loss: 2.370 | val loss: 2.371\nEpoch 3 | train loss: 2.307 | val loss: 2.336\nEpoch 4 | train loss: 2.278 | val loss: 2.320\nEpoch 5 | train loss: 2.260 | val loss: 2.312\nEpoch 6 | train loss: 2.245 | val loss: 2.304\nEpoch 7 | train loss: 2.231 | val loss: 2.297\nEpoch 8 | train loss: 2.216 | val loss: 2.294\nEpoch 9 | train loss: 2.202 | val loss: 2.290\nEpoch 10 | train loss: 2.185 | val loss: 2.285\nEpoch 11 | train loss: 2.168 | val loss: 2.281\nEpoch 12 | train loss: 2.148 | val loss: 2.271\nEpoch 13 | train loss: 2.119 | val loss: 2.255\nEpoch 14 | train loss: 2.080 | val loss: 2.228\nEpoch 15 | train loss: 2.039 | val loss: 2.208\nEpoch 16 | train loss: 2.001 | val loss: 2.185\nEpoch 17 | train loss: 1.966 | val loss: 2.169\nEpoch 18 | train loss: 1.933 | val loss: 2.157\nEpoch 19 | train loss: 1.903 | val loss: 2.140\nEpoch 20 | train loss: 1.875 | val loss: 2.126\nEpoch 21 | train loss: 1.848 | val loss: 2.113\nEpoch 22 | train loss: 1.822 | val loss: 2.107\nEpoch 23 | train loss: 1.799 | val loss: 2.098\nEpoch 24 | train loss: 1.776 | val loss: 2.092\nEpoch 25 | train loss: 1.755 | val loss: 2.086\nEpoch 26 | train loss: 1.736 | val loss: 2.081\nEpoch 27 | train loss: 1.717 | val loss: 2.075\nEpoch 28 | train loss: 1.699 | val loss: 2.075\nEpoch 29 | train loss: 1.682 | val loss: 2.069\nEpoch 30 | train loss: 1.666 | val loss: 2.068\n\n\n\n\n\n    \n\n\n\nbeast_pearsonr\n▁\n\n\nepoch\n▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███\n\n\ntest_avg_pearsonr\n▁\n\n\ntrain_loss\n██▇▇▆▆▆▆▆▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁\n\n\nval_loss\n█▇▆▆▆▅▅▅▅▅▅▅▅▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁\n\n\n\n\n\n\n\nbeast_pearsonr\n0.80236\n\n\nepoch\n30\n\n\ntest_avg_pearsonr\n0.38552\n\n\ntrain_loss\n1.66555\n\n\nval_loss\n2.06831\n\n\n\n\n\n\n View run upbeat-sweep-6 at: https://wandb.ai/ssalazar_02/DNA_model/runs/gmg0gdbw View project at: https://wandb.ai/ssalazar_02/DNA_modelSynced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n\n\nFind logs at: ./wandb/run-20250407_152128-gmg0gdbw/logs"
  },
  {
    "objectID": "post/2025-05-01-unit03/index.html",
    "href": "post/2025-05-01-unit03/index.html",
    "title": "unit 03",
    "section": "",
    "text": "Slides for Unit 3 - Enformer and PrediXcan link\n\n\n\n\n\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer-minimal-neanderthal.html",
    "href": "post/2025-05-01-unit03/enformer-minimal-neanderthal.html",
    "title": "Enformer usage neanderthal",
    "section": "",
    "text": "This notebook demonstrates how to - Make predictions with Enformer using human reference - Make predictions using Neanderthal vcf\n© HakyImLab and Listed Authors - CC BY 4.0 License"
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer-minimal-neanderthal.html#set-up-and-function-definitions",
    "href": "post/2025-05-01-unit03/enformer-minimal-neanderthal.html#set-up-and-function-definitions",
    "title": "Enformer usage neanderthal",
    "section": "1. set up and function definitions",
    "text": "1. set up and function definitions\nThis is Sabrina’s EnformerVCF.py file with functions necessary to run vcf modified enformer, based on functions from Temi and Sai, in turn based on Avsec et al’s code\n\nimport packages\n\nimport tensorflow as tf\n# Make sure the GPU is enabled \nassert tf.config.list_physical_devices('GPU'), 'Start the colab kernel with GPU: Runtime -&gt; Change runtime type -&gt; GPU'\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\nimport tensorflow_hub as hub # for interacting with saved models and tensorflow hub\nimport joblib\nimport gzip # for manipulating compressed files\nimport kipoiseq # for manipulating fasta files\nfrom kipoiseq import Interval # same as above, really\nimport pyfaidx # to index our reference genome file\nimport pandas as pd # for manipulating dataframes\nimport numpy as np # for numerical computations\nimport matplotlib.pyplot as plt # for plotting\nimport matplotlib as mpl # for plotting\nimport seaborn as sns # for plotting\nimport pickle # for saving large objects\nimport os, sys # functions for interacting with the operating system\nimport cyvcf2\n\nimport Bio\nfrom Bio.Seq import Seq\ndef create_rev_complement(dna_string):\n    return(str(Seq(dna_string).reverse_complement()))\n\nimport io\nimport os\nimport gzip\n\n\n\ndefine classes and functions\n\n# @title `Enformer`, `EnformerScoreVariantsNormalized`, `EnformerScoreVariantsPCANormalized`,\nSEQUENCE_LENGTH = 393216\n\n\nclass Enformer:\n\n  def __init__(self, tfhub_url):\n    self._model = hub.load(tfhub_url).model\n\n  def predict_on_batch(self, inputs):\n    predictions = self._model.predict_on_batch(inputs)\n    return {k: v.numpy() for k, v in predictions.items()}\n\n  @tf.function\n  def contribution_input_grad(self, input_sequence,\n                              target_mask, output_head='human'):\n    input_sequence = input_sequence[tf.newaxis]\n\n    target_mask_mass = tf.reduce_sum(target_mask)\n    with tf.GradientTape() as tape:\n      tape.watch(input_sequence)\n      prediction = tf.reduce_sum(\n          target_mask[tf.newaxis] *\n          self._model.predict_on_batch(input_sequence)[output_head]) / target_mask_mass\n\n    input_grad = tape.gradient(prediction, input_sequence) * input_sequence\n    input_grad = tf.squeeze(input_grad, axis=0)\n    return tf.reduce_sum(input_grad, axis=-1)\n\nclass EnformerScoreVariantsRaw:\n\n  def __init__(self, tfhub_url, organism='human'):\n    self._model = Enformer(tfhub_url)\n    self._organism = organism\n\n  def predict_on_batch(self, inputs):\n    ref_prediction = self._model.predict_on_batch(inputs['ref'])[self._organism]\n    alt_prediction = self._model.predict_on_batch(inputs['alt'])[self._organism]\n\n    return alt_prediction.mean(axis=1) - ref_prediction.mean(axis=1)\n\nclass EnformerScoreVariantsNormalized:\n\n  def __init__(self, tfhub_url, transform_pkl_path,\n               organism='human'):\n    assert organism == 'human', 'Transforms only compatible with organism=human'\n    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n      transform_pipeline = joblib.load(f)\n    self._transform = transform_pipeline.steps[0][1]  # StandardScaler.\n\n  def predict_on_batch(self, inputs):\n    scores = self._model.predict_on_batch(inputs)\n    return self._transform.transform(scores)\n\nclass EnformerScoreVariantsPCANormalized:\n\n  def __init__(self, tfhub_url, transform_pkl_path,\n               organism='human', num_top_features=500):\n    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n      self._transform = joblib.load(f)\n    self._num_top_features = num_top_features\n\n  def predict_on_batch(self, inputs):\n    scores = self._model.predict_on_batch(inputs)\n    return self._transform.transform(scores)[:, :self._num_top_features]\n\n# @title `variant_centered_sequences`\n\nclass FastaStringExtractor:\n\n    def __init__(self, fasta_file):\n        self.fasta = pyfaidx.Fasta(fasta_file)\n        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n    #import pd.Interval as Interval\n    def extract(self, interval: Interval, **kwargs) -&gt; str:\n        # Truncate interval if it extends beyond the chromosome lengths.\n        chromosome_length = self._chromosome_sizes[interval.chrom]\n        trimmed_interval = Interval(interval.chrom,\n                                    max(interval.start, 0),\n                                    min(interval.end, chromosome_length),\n                                    )\n        # pyfaidx wants a 1-based interval\n        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n                                          trimmed_interval.start + 1,\n                                          trimmed_interval.stop).seq).upper()\n        # Fill truncated values with N's.\n        pad_upstream = 'N' * max(-interval.start, 0)\n        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n        return pad_upstream + sequence + pad_downstream\n\n    def close(self):\n        return self.fasta.close()\n\n\ndef one_hot_encode(sequence):\n  return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)\n\n# @title `plot_tracks`\n\ndef plot_tracks(tracks, interval, height=1.5):\n  fig, axes = plt.subplots(len(tracks), 1, figsize=(20, height * len(tracks)), sharex=True)\n  for ax, (title, y) in zip(axes, tracks.items()):\n    ax.fill_between(np.linspace(interval.start, interval.end, num=len(y)), y)\n    ax.set_title(title)\n    sns.despine(top=True, right=True, bottom=True)\n  ax.set_xlabel(str(interval))\n  plt.tight_layout()\n\ndef read_vcf(path):\n    with gzip.open(path, 'rt') as f:\n        lines = [l for l in f if not l.startswith('##')]\n    return pd.read_csv(\n        io.StringIO(''.join(lines)),\n        dtype={'#CHROM': str, 'POS': int, 'ID': str, 'REF': str, 'ALT': str,\n               'QUAL': str, 'FILTER': str, 'INFO': str},\n        sep='\\t'\n    ).rename(columns={'#CHROM': 'CHROM'})\n\ndef vcf_to_seq(target_interval, individual, vcf, fasta_extractor):\n  target_fa = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))\n  window_coords = target_interval.resize(SEQUENCE_LENGTH)\n      # two haplotypes per individual\n  haplo_1 = list(target_fa[:])\n  haplo_2 = list(target_fa[:])\n\n  ref_mismatch_count = 0\n  for i,row in vcf.iterrows():\n    geno = row[individual].split(\"|\")\n    if (row[\"POS\"]-window_coords.start-1) &gt;= len(haplo_2):\n      continue\n    if (row[\"POS\"]-window_coords.start-1) &lt; 0:\n      continue\n    if geno[0] == \"1\":\n      haplo_1[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n    if geno[1] == \"1\":\n      haplo_2[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n  return haplo_1, haplo_2\n\n\ndef vcf_to_seq_faster(target_interval, individual, vcf_file, fasta_extractor):\n  # target_inverval is a kipoiseq interval, e.g. kipoiseq.Interval(\"chr22\", 18118779, 18145669)\n  # individual is the id of the individual in the vcf file\n  \n  target_fa = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))\n  window_coords = target_interval.resize(SEQUENCE_LENGTH)\n      # two haplotypes per individual\n  haplo_1 = list(target_fa[:])\n  haplo_2 = list(target_fa[:])\n\n  # Open the VCF file\n  vcf_reader = cyvcf2.VCF(vcf_file)\n\n  # Specific genomic region\n  CHROM = window_coords.chrom\n  START = max(1,window_coords.start)\n  END = min(window_coords.end, fasta_extractor._chromosome_sizes[CHROM]-1)\n\n  count1 = 0\n  count2 = 0\n\n  # Iterate through variants in the specified region\n  for variant in vcf_reader(CHROM + ':' + str(START) + '-' + str(END)):\n      # Access the individual's genotype using index (0-based) of the sample\n      individual_index = vcf_reader.samples.index(individual)\n      genotype = variant.genotypes[individual_index]\n      ALT=variant.ALT[0]\n      REF=variant.REF\n      POS=variant.POS\n      if REF == target_fa[POS - window_coords.start - 1]:\n        if genotype[0] == 1:\n          haplo_1[POS - window_coords.start - 1] = ALT\n          count1 = count1 + 1\n        if genotype[1] == 1:\n          haplo_2[POS - window_coords.start - 1] = ALT  \n          count2 = count2 + 1\n      else:\n        print(\"ERROR: REF in vcf \"+ REF + \"!= REF from the build\" + target_fa[POS - window_coords.start - 1])\n  print(\"number of changes haplo1:\")\n  print(count1)\n  print(\"number of changes haplo2:\")\n  print(count2)\n  return haplo_1, haplo_2\n\n\n\ndefine comparison functions\n\n# Comparison functions \n\ndef get_diffmat(mat1, mat2):\n    \n    diffmat = mat1 - mat2\n    abs_diffmat = np.abs(diffmat)\n\n    colwise_maxes1 = np.max(mat1, axis=0)\n    colwise_maxes2 = np.max(mat2, axis=0)\n\n    colwise_maxes_maxes = np.maximum(colwise_maxes1, colwise_maxes2)\n\n    relmax3_diffmat = diffmat / colwise_maxes_maxes\n    relmax3_diffmat = np.abs(relmax3_diffmat)\n\n    return relmax3_diffmat\n\n\ndef get_summary(arr):\n    summary = {\n        \"mean\": np.mean(arr),\n        \"median\": np.median(arr),\n        \"minimum\": np.min(arr),\n        \"maximum\": np.max(arr),\n        \"q1\": np.percentile(arr, 25),\n        \"q3\": np.percentile(arr, 75),\n    }\n    return summary\n\n\ndef plot_hist(arr, bin_num, xlab='Value', ylab='Frequency', title='Histogram'):\n    plt.hist(arr, bins=bin_num)\n    plt.title(title)\n    plt.xlabel(xlab)\n    plt.ylabel(ylab)\n    plt.show()\n\ndef column_correlations(mat1, mat2):\n    if mat1.shape != mat2.shape:\n        raise ValueError(\"Input matrices must have the same shape\")\n\n    num_columns = mat1.shape[1]\n    correlations = np.empty(num_columns)\n\n    for col in range(num_columns):\n        correlation = np.corrcoef(mat1[:, col], mat2[:, col])[0, 1]\n        correlations[col] = correlation\n\n    return correlations"
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer-minimal-neanderthal.html#define-locations-load-model-target-annotations",
    "href": "post/2025-05-01-unit03/enformer-minimal-neanderthal.html#define-locations-load-model-target-annotations",
    "title": "Enformer usage neanderthal",
    "section": "2. define locations, load model, target annotations",
    "text": "2. define locations, load model, target annotations\n\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/Reference-Data/\"\n\nmodel_path = PRE + \"models/enformer/raw\"\nfasta_file = PRE + \"ref_sequences/hg19/raw/genome.fa\"\n## check whether specific reference fasta used for the calling of the neanderthal vcf should be used\n#fasta_file = PRE + \"ref_sequences/hg38/Homo_sapiens_assembly38.fasta\"\n\nmodel = Enformer(model_path) # here we load the model architecture.\nfasta_extractor = FastaStringExtractor(fasta_file)\n\n\n## load target data\ntargets_txt = 'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_human.txt'\n# df_targets = pd.read_csv(targets_txt, sep='\\t')\ntargets_slim_file = PRE + \"models/enformer/targets_slims.csv\"\ntargets_slim_df = pd.read_csv(targets_slim_file)"
  },
  {
    "objectID": "post/2025-05-01-unit03/enformer-minimal-neanderthal.html#load-neanderthal-variants-and-run-enformer",
    "href": "post/2025-05-01-unit03/enformer-minimal-neanderthal.html#load-neanderthal-variants-and-run-enformer",
    "title": "Enformer usage neanderthal",
    "section": "3. load neanderthal variants and run enformer",
    "text": "3. load neanderthal variants and run enformer\nDownload Altai ch5 filtered vcf brew install htslib bgzip AltaiNea.hg19_1000g.5.vcf tabix -p vcf AltaiNea.hg19_1000g.5.vcf.gz\ncreate file filter-add-chr.sh with the following content chmod u+x filter-add-chr.sh to make it executable\n\nfor NUM in {1..22}; do\n    # Filter missing genotypes and non-variant sites\n    bcftools view -e '(GT=\"./.\") || (GT=\"0/0\") || (ALT=\".\")' AltaiNea.hg19_1000g.${NUM}.vcf.gz &gt; AltaiNea.hg19_1000g.${NUM}.nomiss.vcf\n\n    # Compress the resulting VCF\n    bgzip AltaiNea.hg19_1000g.${NUM}.nomiss.vcf\n    \n    # Add \"chr\" prefix to all non-header lines and compress\n    # zcat &lt; ... is used on a mac terminal; in linux, it should be without &lt;,i.e., zcat AltaiNea...\n    zcat &lt; AltaiNea.hg19_1000g.${NUM}.nomiss.vcf.gz | awk 'BEGIN{OFS=FS=\"\\t\"} /^#/ {print; next} {print \"chr\"$0}' | bgzip &gt; AltaiNea.hg19_1000g.chr${NUM}.nomiss.vcf.gz\n    \n    # Filter to retain only SNPs\n    bcftools view -i 'strlen(REF) = 1 && strlen(ALT) = 1' AltaiNea.hg19_1000g.chr${NUM}.nomiss.vcf.gz &gt; AltaiNea.hg19_1000g.chr${NUM}.nomiss.snps_only.vcf\ndone\n\n# read VCFs and encode haplotypes\nCHROM='chr5'\nvcf_file = PRE + \"neanderthal/AltaiNea.hg19_1000g.\" + CHROM + \".nomiss.snps_only.vcf.gz\"\n\ntarget_interval = kipoiseq.Interval(CHROM,96875939 , 96919716)\nhaplo1, haplo2 = vcf_to_seq_faster(target_interval, 'AltaiNea', vcf_file, fasta_extractor)\nhaplo0 = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))\n\nhaplo1_enc = one_hot_encode(\"\".join(haplo1))[np.newaxis]\nhaplo2_enc = one_hot_encode(\"\".join(haplo2))[np.newaxis]\nhaplo0_enc = one_hot_encode(\"\".join(haplo0))[np.newaxis]\n\nprint(\"number of changes\");print(np.sum(haplo2_enc != haplo0_enc))\n\npred_human = model.predict_on_batch(haplo0_enc)['human'][0]\npred_altai = model.predict_on_batch(haplo1_enc + haplo2_enc)['human'][0]\n\n\npredictions = pred_human\ntracks = {'DNASE:CD14-positive monocyte female': predictions[:, 41],\n          'DNASE:keratinocyte female': predictions[:, 42],\n          'CHIP:H3K27ac:keratinocyte female': predictions[:, 706],\n          'CAGE:LCL': np.log10(1 + predictions[:, 5110])}\nplot_tracks(tracks, target_interval)\n\n\npredictions = pred_altai\ntracks = {'DNASE:CD14-positive monocyte female': predictions[:, 41],\n          'DNASE:keratinocyte female': predictions[:, 42],\n          'CHIP:H3K27ac:keratinocyte female': predictions[:, 706],\n          'CAGE:LCL': np.log10(1 + predictions[:, 5110])}\nplot_tracks(tracks, target_interval)\n\n\nget_summary(get_diffmat(pred_human,pred_altai))\n\n\nget_summary(column_correlations(pred_human,pred_altai))"
  }
]