<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Henry Raeder">
<meta name="dcterms.date" content="2025-04-22">
<meta name="description" content="Henry’s genomic language model trained on human reference genome with nanogpt">

<title>Genomic language model training with nanoGPT - qmd version – GENE 46100</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-dcc7d9e706e4adfbab8e3d6c7c60bab2.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">GENE 46100</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#steps" id="toc-steps" class="nav-link" data-scroll-target="#steps">Steps</a></li>
  <li><a href="#setup" id="toc-setup" class="nav-link" data-scroll-target="#setup">Setup</a></li>
  <li><a href="#download-and-read-the-reference-genome" id="toc-download-and-read-the-reference-genome" class="nav-link" data-scroll-target="#download-and-read-the-reference-genome">Download and Read the Reference Genome</a></li>
  <li><a href="#mapping-characters-to-integers-i.e.-tokenization" id="toc-mapping-characters-to-integers-i.e.-tokenization" class="nav-link" data-scroll-target="#mapping-characters-to-integers-i.e.-tokenization">Mapping Characters to Integers (i.e.&nbsp;Tokenization)</a></li>
  <li><a href="#creating-the-model" id="toc-creating-the-model" class="nav-link" data-scroll-target="#creating-the-model">Creating the Model</a></li>
  <li><a href="#training" id="toc-training" class="nav-link" data-scroll-target="#training">Training</a></li>
  <li><a href="#sampling-from-the-model" id="toc-sampling-from-the-model" class="nav-link" data-scroll-target="#sampling-from-the-model">Sampling from the model</a></li>
  <li><a href="#fine-tuning-promoter-classification" id="toc-fine-tuning-promoter-classification" class="nav-link" data-scroll-target="#fine-tuning-promoter-classification">Fine-Tuning (Promoter Classification)</a></li>
  <li><a href="#fine-tuning-enhancer-classification" id="toc-fine-tuning-enhancer-classification" class="nav-link" data-scroll-target="#fine-tuning-enhancer-classification">Fine-Tuning (Enhancer Classification)</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Genomic language model training with nanoGPT - qmd version</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">notebook</div>
    <div class="quarto-category">gene46100</div>
  </div>
  </div>

<div>
  <div class="description">
    Henry’s genomic language model trained on human reference genome with nanogpt
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Henry Raeder </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 22, 2025</p>
    </div>
  </div>
  
    <div>
    <div class="quarto-title-meta-heading">Modified</div>
    <div class="quarto-title-meta-contents">
      <p class="date-modified">June 9, 2025</p>
    </div>
  </div>
    
  </div>
  


</header>


<p><strong>Acknowledgement</strong>:</p>
<ul>
<li>Andrej Karpathy (nanoGPT model framework)</li>
<li>Dalla-torre et. al.&nbsp;(Nucleotide Transformer workbook)</li>
</ul>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In this notebook, we explore how one can pretrain and fine-tune a basic genomic language model (gLM). As the basis of our model, we use Andrej Karpathy’s publicly-available <a href="https://github.com/karpathy/nanoGPT">nanoGPT framework</a> (with some slight edits). This is a flexible implementation of a basic GPT model (based on GPT-2) that can take generic text as input. Here, we showcase how relatively sophisticated models can be trained efficiently, and how these can be fine-tuned to specific prediction tasks using datasets pulled from the <a href="https://github.com/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling.ipynb">Nucleotide Transformer training notebook</a> (direct sources will be cited when datasets are introduced). Generally, the goal of this notebook is to provide hands-on experience in working with language models, and give insight into the fundamentals of attention-based deep learning.</p>
</section>
<section id="steps" class="level2">
<h2 class="anchored" data-anchor-id="steps">Steps</h2>
<p>This notebook demonstrates how to:</p>
<ul>
<li>Tokenize genomic data to be read by a language model</li>
<li>Train and sample from a generative language model</li>
<li>Fine-tune a model with additional training to perform specific tasks</li>
</ul>
</section>
<section id="setup" class="level2">
<h2 class="anchored" data-anchor-id="setup">Setup</h2>
<p>First, it is necessary to connect to the GPU. If you are working in Colab, this can be done in the upper right corner by selecting “T4 GPU” as your runtime option.</p>
<p>Next, we need to install a few things that help us build and run the model efficiently.</p>
<div id="66cb4954" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">#%pip install torch numpy transformers datasets tiktoken wandb tqdm biopython huggingface_hub accelerate</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> inspect</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gc</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> contextlib <span class="im">import</span> nullcontext</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.parallel <span class="im">import</span> DistributedDataParallel <span class="im">as</span> DDP</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributed <span class="im">import</span> init_process_group, destroy_process_group</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset, Dataset</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, TensorDataset</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co">#from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> matthews_corrcoef, f1_score</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="download-and-read-the-reference-genome" class="level2">
<h2 class="anchored" data-anchor-id="download-and-read-the-reference-genome">Download and Read the Reference Genome</h2>
<p>For our training, we will be using the GRCh38 human reference genome (credit to the Genome Reference Consortium: https://www.ncbi.nlm.nih.gov/grc). In the next block, we simply create a place to store our genome in the Colab filesystem and download the genome file remotely.</p>
<div id="49746c35" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>running_in_colab <span class="op">=</span> <span class="va">False</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> running_in_colab:</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  WORKDIR <span class="op">=</span> <span class="st">'/root/data/'</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>  <span class="op">!</span>mkdir <span class="op">/</span>root<span class="op">/</span>data</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>  fasta_file <span class="op">=</span> WORKDIR <span class="op">+</span> <span class="st">'genome.fa'</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>  <span class="op">!</span>wget <span class="op">-</span>O <span class="op">-</span> https:<span class="op">//</span>hgdownload.soe.ucsc.edu<span class="op">/</span>goldenPath<span class="op">/</span>hg38<span class="op">/</span>bigZips<span class="op">/</span>hg38.fa.gz <span class="op">|</span> gunzip <span class="op">-</span>c <span class="op">&gt;</span> {fasta_file}</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>  input_file_path <span class="op">=</span> os.path.join(WORKDIR, <span class="st">'genome.fa'</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>  WORKDIR <span class="op">=</span> <span class="st">'~/Downloads/'</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>  fasta_path <span class="op">=</span> <span class="st">'/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/Reference-Data/ref_sequences/hg38/'</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>  fasta_file <span class="op">=</span> fasta_path <span class="op">+</span> <span class="st">'Homo_sapiens_assembly38.fasta'</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>  input_file_path <span class="op">=</span> fasta_file</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Properly expand the home directory if using ~</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>  WORKDIR <span class="op">=</span> os.path.expanduser(<span class="st">'~/Downloads'</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Create directory if it doesn't exist</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>  os.makedirs(WORKDIR, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a><span class="co">Prepare the hg38 genomic dataset for character-level language modeling.</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co">So instead of encoding with BPE tokens, we just map characters to ints.</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co">Will save train.bin, val.bin containing the ids, and meta.pkl containing the</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co">encoder and decoder and some other related info.</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that we have the genome file saved locally, we have to read it into our environment in order to make it usable. To do this, we just read in the entire file as one long string, which we can then subset and investigate as we wish. For example, we may want to know the total length of our dataset, and all of the unique characters we have:</p>
<div id="82824195" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(input_file_path, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> f.read()</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"length of dataset in characters: </span><span class="sc">{</span><span class="bu">len</span>(data)<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># get all the unique characters that occur in this text</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(data)))</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(chars)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"all the unique characters:"</span>, <span class="st">''</span>.join(chars))</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"vocab size: </span><span class="sc">{</span>vocab_size<span class="sc">:,}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Great! The number of characters we have is approximately the same as the number of nucleotides in the human genome (about 3.2 billion), so it looks like our data imported correctly. However, you may notice that we have more characters than just “A”, “T”, “C”, “G”, and “N” as you may expect from genomic data. This is because our text file includes headers for each part of the genome assembly (i.e.&nbsp;chromosomes and fragments).</p>
<p>UPDATE: Daniel tested that performance of downstream task is better when removing non DNA characters. <del>One nice thing about working with a language model is that we don’t need to worry about this. Even if the input data is a bit messy, the model is essentially designed to predict the most likely characters based on a general input. Since the headers make up a tiny fraction of the total number of characters, our model will assign a very very low probability to each of the non-ATCGN characters, and they will not influence our outputs.</del> To keep the example simple, we will keep using the full genome file with non-DNA characters.</p>
</section>
<section id="mapping-characters-to-integers-i.e.-tokenization" class="level2">
<h2 class="anchored" data-anchor-id="mapping-characters-to-integers-i.e.-tokenization">Mapping Characters to Integers (i.e.&nbsp;Tokenization)</h2>
<p>Now that we have our text, we need to put it into a format that is more easily readable for our model. Generally, this process is called Tokenization. There are many different methods for this, and each will impact the performance of the model differently (for example byte pair encoding or non-overlapping kmer tokenization used by other gLMs), but for the purpose of this notebook we will use a simple character-level tokenization. This means that we will simply assign a different integer to each unique character, and then transform our input into a long list of integers to be read by our model:</p>
<div id="b4ab15b0" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a mapping from characters to integers</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>stoi <span class="op">=</span> { ch:i <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>itos <span class="op">=</span> { i:ch <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="d7423e28" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> encode(s):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [stoi[c] <span class="cf">for</span> c <span class="kw">in</span> s] <span class="co"># encoder: take a string, output a list of integers</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> decode(l):</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">''</span>.join([itos[i] <span class="cf">for</span> i <span class="kw">in</span> l]) <span class="co"># decoder: take a list of integers, output a string</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that we have our functions defined, it is time to actually do the tokenization. This will take a few minutes, so go ahead and run the block, and see the description of what it is doing below:</p>
<div id="57b8ebb8" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create the train and test splits</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(data)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> data[:<span class="bu">int</span>(n<span class="op">*</span><span class="fl">0.9</span>)]</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>val_data <span class="op">=</span> data[<span class="bu">int</span>(n<span class="op">*</span><span class="fl">0.9</span>):]</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co">#save memory</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> data</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co">#helper function to store/append data to binary files and save RAM</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> append_to_binary_file(filename, obj):</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(filename, <span class="st">'ab'</span>) <span class="im">as</span> <span class="bu">file</span>:  <span class="co"># Open in append-binary mode</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>        obj.tofile(<span class="bu">file</span>)</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="co"># encode both to integers</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>ntrain<span class="op">=</span><span class="bu">len</span>(train_data)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>nval <span class="op">=</span> <span class="bu">len</span>(val_data)</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> np.arange(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.05</span>):</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"Tokenizing: </span><span class="sc">{</span><span class="bu">int</span>(i<span class="op">*</span><span class="dv">100</span>)<span class="sc">:,}</span><span class="ss">% complete"</span>)</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>  train_ids <span class="op">=</span> encode(train_data[<span class="bu">int</span>(ntrain<span class="op">*</span>i):<span class="bu">int</span>(ntrain<span class="op">*</span>(i<span class="op">+</span><span class="fl">0.05</span>))])</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>  val_ids <span class="op">=</span> encode(val_data[<span class="bu">int</span>(nval<span class="op">*</span>i):<span class="bu">int</span>(nval<span class="op">*</span>(i<span class="op">+</span><span class="fl">0.05</span>))])</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>  train_ids <span class="op">=</span> np.array(train_ids, dtype<span class="op">=</span>np.uint16)</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>  val_ids <span class="op">=</span> np.array(val_ids, dtype<span class="op">=</span>np.uint16)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>  append_to_binary_file(os.path.join(WORKDIR, <span class="st">'train.bin'</span>), train_ids)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>  append_to_binary_file(os.path.join(WORKDIR, <span class="st">'val.bin'</span>), val_ids)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"train has </span><span class="sc">{</span>ntrain<span class="sc">:,}</span><span class="ss"> tokens"</span>)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"val has </span><span class="sc">{</span>nval<span class="sc">:,}</span><span class="ss"> tokens"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ol type="1">
<li>First, we are dividing the genome into a training and testing dataset using a simple 90/10 split.</li>
<li>Next, we create a helper function that will help us save the integer lists locally. These end up taking up a lot of space in our Python environment, so it is better to save them to the disk and only access them when needed rather than having them sit and take up RAM.</li>
<li>Finally, we actually go through and encode/tokenize our data. Since the dataset is so large, we don’t have enough memory to do this all at once in Colab, so we encode our data in batches and then push each new batch to our locally-saved files.</li>
</ol>
<p>We also want to save some of our metadata (i.e.&nbsp;vocabulary size, and our mappings from characters to integers and back) to pull back in later.</p>
<div id="a8ea1ebb" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co"># save the meta information as well, to help us encode/decode later</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>meta <span class="op">=</span> {</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'vocab_size'</span>: vocab_size,</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'itos'</span>: itos,</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'stoi'</span>: stoi,</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(os.path.join(WORKDIR, <span class="st">'meta.pkl'</span>), <span class="st">'wb'</span>) <span class="im">as</span> f:</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    pickle.dump(meta, f)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="50ab9022" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> running_in_colab:</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>  os.kill(os.getpid(), <span class="dv">9</span>) <span class="co">#If running in a low-memory environment, this will clear our RAM. After this, go run the very first block again!</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<section id="questions" class="level4">
<h4 class="anchored" data-anchor-id="questions">Questions:</h4>
<ol type="1">
<li>What does each “token” in our model represent? I.e. a single base, a stretch of a few bases, some functional region, etc.?</li>
<li>What is the purpose of tokenization? Why is it necessary to train our model?</li>
</ol>
</section>
</section>
<section id="creating-the-model" class="level2">
<h2 class="anchored" data-anchor-id="creating-the-model">Creating the Model</h2>
<p>Now that we have our training data ready to go, it’s time to actually create the gLM. First, we define the function we are going to use to do the heavy lifting in the attention layers. We will go into more detail on what we mean by attention during training, but if you are interested in what the implementation looks like, see the block below (and be sure to run it either way!)</p>
<div id="9f08fe47" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co">#Defining attention function (copied from Pytorch documentation, just need to save weights correctly)</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> scaled_dot_product_attention(query, key, value, attn_mask<span class="op">=</span><span class="va">None</span>, dropout_p<span class="op">=</span><span class="fl">0.0</span>, is_causal<span class="op">=</span><span class="va">False</span>, scale<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    L, S <span class="op">=</span> query.size(<span class="op">-</span><span class="dv">2</span>), key.size(<span class="op">-</span><span class="dv">2</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    scale_factor <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> math.sqrt(query.size(<span class="op">-</span><span class="dv">1</span>)) <span class="cf">if</span> scale <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> scale</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    attn_bias <span class="op">=</span> torch.zeros(L, S, dtype<span class="op">=</span>query.dtype, device<span class="op">=</span>query.device)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> is_causal:</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> attn_mask <span class="kw">is</span> <span class="va">None</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>        temp_mask <span class="op">=</span> torch.ones(L, S, dtype<span class="op">=</span>torch.<span class="bu">bool</span>, device<span class="op">=</span>query.device).tril(diagonal<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>        attn_bias.masked_fill_(temp_mask.logical_not(), <span class="bu">float</span>(<span class="st">"-inf"</span>))</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>        attn_bias.to(query.dtype)</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> attn_mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> attn_mask.dtype <span class="op">==</span> torch.<span class="bu">bool</span>:</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>            attn_bias.masked_fill_(attn_mask.logical_not(), <span class="bu">float</span>(<span class="st">"-inf"</span>))</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>            attn_bias <span class="op">+=</span> attn_mask</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>    attn_weight <span class="op">=</span> query <span class="op">@</span> key.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> scale_factor</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>    attn_weight <span class="op">+=</span> attn_bias</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    attn_weight <span class="op">=</span> torch.softmax(attn_weight, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>    attn_weight <span class="op">=</span> torch.dropout(attn_weight, dropout_p, train<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> attn_weight, attn_weight <span class="op">@</span> value</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that we have our attention function, it’s time to define what our model will look like. This is a lot of code, so we will provide a short summary of what each layer is doing and the order in which they occur. If you are interested in the specifics, feel free to read through the code in detail.</p>
<p>The main model is created as a class simply called <strong>GPT</strong>. Within this model, we have a few different types of layers/elements:</p>
<ul>
<li><strong>Embedding</strong>: In our model, we project our relatively simple vocabulary (40 characters) into a higher-dimensional space in order to increase the model’s capacity to understand more complex relationships between tokens. This is a bit difficult to visualize, but in essence Embedding layers just perform this projection and return the resulting dictionary to use later in our model. For language models, we usually embed both the tokens and the positions of the tokens, for reasons we will discuss later.</li>
<li><strong>LayerNorm</strong>: As the name suggests, this layer simply applies normalization across the input features. This stabilizes the training, as it ensures each feature has a similar scale.</li>
<li><strong>Dropout</strong>: Dropout layers randomly zero-out some of the elements in the input tensor. This serves as a form of regularization, and helps avoid overfitting our training data.</li>
<li><strong>Causal Self-Attention</strong>: The mechanism of this layer will be described in further detail below, but this is the layer where our model learns the relationship between each of the tokens in the input, which is what gives it predictive power to generate what token will come next.</li>
<li><strong>MLP</strong>: One can think of the MLP class as a mini neural network within our model. It is made up of a linear layer, a GELU activation layer, another linear layer, and finally a dropout layer. In short, the MLP element introduces non-linearity into our model, and helps us transform the output of our attention layer back into something we can pass through attention again, and decode into DNA sequence.</li>
<li><strong>Block</strong>: This is not really a layer, it is more a sequence of layers. Specifically, we define a block as LayerNorm → Causal Self-Attention → LayerNorm → MLP</li>
</ul>
<p>Now that we have these modules defined, the general outline of our GPT model is as below (assume the outputs of each step are passed to the next):</p>
<ol type="1">
<li>Token and Position Embedding Layers</li>
<li>Dropout layer</li>
<li>Blocks (repeated <span class="math inline">n</span> times, i.e.&nbsp;if we want 6 attention layers we will have 6 Blocks)</li>
<li>LayerNorm</li>
<li>Linear layer to transform embedded outputs back into our vocab size</li>
<li>Loss calculation (if training)</li>
</ol>
<div id="e2fa2583" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co">Full definition of a GPT Language Model, all of it in this single file.</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">References:</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">1) the official GPT-2 TensorFlow implementation released by OpenAI:</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">https://github.com/openai/gpt-2/blob/master/src/model.py</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">2) huggingface/transformers PyTorch implementation:</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co">https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>gc.collect()</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LayerNorm(nn.Module):</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False """</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ndim, bias):</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weight <span class="op">=</span> nn.Parameter(torch.ones(ndim))</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.zeros(ndim)) <span class="cf">if</span> bias <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> F.layer_norm(<span class="bu">input</span>, <span class="va">self</span>.weight.shape, <span class="va">self</span>.weight, <span class="va">self</span>.bias, <span class="fl">1e-5</span>)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CausalSelfAttention(nn.Module):</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> config.n_embd <span class="op">%</span> config.n_head <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># key, query, value projections for all heads, but in a batch</span></span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c_attn <span class="op">=</span> nn.Linear(config.n_embd, <span class="dv">3</span> <span class="op">*</span> config.n_embd, bias<span class="op">=</span>config.bias)</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># output projection</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c_proj <span class="op">=</span> nn.Linear(config.n_embd, config.n_embd, bias<span class="op">=</span>config.bias)</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>        <span class="co"># regularization</span></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn_dropout <span class="op">=</span> nn.Dropout(config.dropout)</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.resid_dropout <span class="op">=</span> nn.Dropout(config.dropout)</span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_head <span class="op">=</span> config.n_head</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_embd <span class="op">=</span> config.n_embd</span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> config.dropout</span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>        <span class="co"># flash attention make GPU go brrrrr but support is only in PyTorch &gt;= 2.0</span></span>
<span id="cb10-38"><a href="#cb10-38" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flash <span class="op">=</span> <span class="bu">hasattr</span>(torch.nn.functional, <span class="st">'scaled_dot_product_attention'</span>)</span>
<span id="cb10-39"><a href="#cb10-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-40"><a href="#cb10-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb10-41"><a href="#cb10-41" aria-hidden="true" tabindex="-1"></a>        B, T, C <span class="op">=</span> x.size() <span class="co"># batch size, sequence length, embedding dimensionality (n_embd)</span></span>
<span id="cb10-42"><a href="#cb10-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-43"><a href="#cb10-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calculate query, key, values for all heads in batch and move head forward to be the batch dim</span></span>
<span id="cb10-44"><a href="#cb10-44" aria-hidden="true" tabindex="-1"></a>        q, k, v  <span class="op">=</span> <span class="va">self</span>.c_attn(x).split(<span class="va">self</span>.n_embd, dim<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb10-45"><a href="#cb10-45" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> k.view(B, T, <span class="va">self</span>.n_head, C <span class="op">//</span> <span class="va">self</span>.n_head).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, nh, T, hs)</span></span>
<span id="cb10-46"><a href="#cb10-46" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> q.view(B, T, <span class="va">self</span>.n_head, C <span class="op">//</span> <span class="va">self</span>.n_head).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, nh, T, hs)</span></span>
<span id="cb10-47"><a href="#cb10-47" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> v.view(B, T, <span class="va">self</span>.n_head, C <span class="op">//</span> <span class="va">self</span>.n_head).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, nh, T, hs)</span></span>
<span id="cb10-48"><a href="#cb10-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-49"><a href="#cb10-49" aria-hidden="true" tabindex="-1"></a>        <span class="co"># causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)</span></span>
<span id="cb10-50"><a href="#cb10-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.flash:</span>
<span id="cb10-51"><a href="#cb10-51" aria-hidden="true" tabindex="-1"></a>            <span class="co"># efficient attention using Flash Attention CUDA kernels</span></span>
<span id="cb10-52"><a href="#cb10-52" aria-hidden="true" tabindex="-1"></a>            attn_weight, y <span class="op">=</span> scaled_dot_product_attention(q, k, v, attn_mask<span class="op">=</span><span class="va">None</span>, dropout_p<span class="op">=</span><span class="va">self</span>.dropout <span class="cf">if</span> <span class="va">self</span>.training <span class="cf">else</span> <span class="dv">0</span>, is_causal<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-53"><a href="#cb10-53" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(B, T, C) <span class="co"># re-assemble all head outputs side by side</span></span>
<span id="cb10-54"><a href="#cb10-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-55"><a href="#cb10-55" aria-hidden="true" tabindex="-1"></a>        <span class="co"># output projection</span></span>
<span id="cb10-56"><a href="#cb10-56" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> <span class="va">self</span>.resid_dropout(<span class="va">self</span>.c_proj(y))</span>
<span id="cb10-57"><a href="#cb10-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y, torch.mean(attn_weight, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb10-58"><a href="#cb10-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-59"><a href="#cb10-59" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb10-60"><a href="#cb10-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-61"><a href="#cb10-61" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb10-62"><a href="#cb10-62" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-63"><a href="#cb10-63" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c_fc    <span class="op">=</span> nn.Linear(config.n_embd, <span class="dv">4</span> <span class="op">*</span> config.n_embd, bias<span class="op">=</span>config.bias)</span>
<span id="cb10-64"><a href="#cb10-64" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gelu    <span class="op">=</span> nn.GELU()</span>
<span id="cb10-65"><a href="#cb10-65" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c_proj  <span class="op">=</span> nn.Linear(<span class="dv">4</span> <span class="op">*</span> config.n_embd, config.n_embd, bias<span class="op">=</span>config.bias)</span>
<span id="cb10-66"><a href="#cb10-66" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.dropout)</span>
<span id="cb10-67"><a href="#cb10-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-68"><a href="#cb10-68" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb10-69"><a href="#cb10-69" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.c_fc(x)</span>
<span id="cb10-70"><a href="#cb10-70" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.gelu(x)</span>
<span id="cb10-71"><a href="#cb10-71" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.c_proj(x)</span>
<span id="cb10-72"><a href="#cb10-72" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb10-73"><a href="#cb10-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb10-74"><a href="#cb10-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-75"><a href="#cb10-75" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb10-76"><a href="#cb10-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-77"><a href="#cb10-77" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb10-78"><a href="#cb10-78" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-79"><a href="#cb10-79" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln_1 <span class="op">=</span> LayerNorm(config.n_embd, bias<span class="op">=</span>config.bias)</span>
<span id="cb10-80"><a href="#cb10-80" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> CausalSelfAttention(config)</span>
<span id="cb10-81"><a href="#cb10-81" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln_2 <span class="op">=</span> LayerNorm(config.n_embd, bias<span class="op">=</span>config.bias)</span>
<span id="cb10-82"><a href="#cb10-82" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp <span class="op">=</span> MLP(config)</span>
<span id="cb10-83"><a href="#cb10-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-84"><a href="#cb10-84" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb10-85"><a href="#cb10-85" aria-hidden="true" tabindex="-1"></a>        val, attn_weight <span class="op">=</span> <span class="va">self</span>.attn(<span class="va">self</span>.ln_1(x))</span>
<span id="cb10-86"><a href="#cb10-86" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> val</span>
<span id="cb10-87"><a href="#cb10-87" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mlp(<span class="va">self</span>.ln_2(x))</span>
<span id="cb10-88"><a href="#cb10-88" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x, attn_weight</span>
<span id="cb10-89"><a href="#cb10-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-90"><a href="#cb10-90" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb10-91"><a href="#cb10-91" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GPTConfig: <span class="co">#Default values for configuration, originally designed to match GPT-2, will override for training</span></span>
<span id="cb10-92"><a href="#cb10-92" aria-hidden="true" tabindex="-1"></a>    block_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1024</span></span>
<span id="cb10-93"><a href="#cb10-93" aria-hidden="true" tabindex="-1"></a>    vocab_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">50304</span> <span class="co"># GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency</span></span>
<span id="cb10-94"><a href="#cb10-94" aria-hidden="true" tabindex="-1"></a>    n_layer: <span class="bu">int</span> <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb10-95"><a href="#cb10-95" aria-hidden="true" tabindex="-1"></a>    n_head: <span class="bu">int</span> <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb10-96"><a href="#cb10-96" aria-hidden="true" tabindex="-1"></a>    n_embd: <span class="bu">int</span> <span class="op">=</span> <span class="dv">768</span></span>
<span id="cb10-97"><a href="#cb10-97" aria-hidden="true" tabindex="-1"></a>    dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb10-98"><a href="#cb10-98" aria-hidden="true" tabindex="-1"></a>    bias: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span> <span class="co"># True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster</span></span>
<span id="cb10-99"><a href="#cb10-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-100"><a href="#cb10-100" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GPT(nn.Module):</span>
<span id="cb10-101"><a href="#cb10-101" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-102"><a href="#cb10-102" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb10-103"><a href="#cb10-103" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb10-104"><a href="#cb10-104" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> config.vocab_size <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span>
<span id="cb10-105"><a href="#cb10-105" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> config.block_size <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span>
<span id="cb10-106"><a href="#cb10-106" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.config <span class="op">=</span> config</span>
<span id="cb10-107"><a href="#cb10-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-108"><a href="#cb10-108" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transformer <span class="op">=</span> nn.ModuleDict(<span class="bu">dict</span>(</span>
<span id="cb10-109"><a href="#cb10-109" aria-hidden="true" tabindex="-1"></a>            wte <span class="op">=</span> nn.Embedding(config.vocab_size, config.n_embd),</span>
<span id="cb10-110"><a href="#cb10-110" aria-hidden="true" tabindex="-1"></a>            wpe <span class="op">=</span> nn.Embedding(config.block_size, config.n_embd),</span>
<span id="cb10-111"><a href="#cb10-111" aria-hidden="true" tabindex="-1"></a>            drop <span class="op">=</span> nn.Dropout(config.dropout),</span>
<span id="cb10-112"><a href="#cb10-112" aria-hidden="true" tabindex="-1"></a>            h <span class="op">=</span> nn.ModuleList([Block(config) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.n_layer)]),</span>
<span id="cb10-113"><a href="#cb10-113" aria-hidden="true" tabindex="-1"></a>            ln_f <span class="op">=</span> LayerNorm(config.n_embd, bias<span class="op">=</span>config.bias),</span>
<span id="cb10-114"><a href="#cb10-114" aria-hidden="true" tabindex="-1"></a>        ))</span>
<span id="cb10-115"><a href="#cb10-115" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lm_head <span class="op">=</span> nn.Linear(config.n_embd, config.vocab_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb10-116"><a href="#cb10-116" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transformer.wte.weight <span class="op">=</span> <span class="va">self</span>.lm_head.weight <span class="co"># https://paperswithcode.com/method/weight-tying</span></span>
<span id="cb10-117"><a href="#cb10-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-118"><a href="#cb10-118" aria-hidden="true" tabindex="-1"></a>        <span class="co"># init all weights</span></span>
<span id="cb10-119"><a href="#cb10-119" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">apply</span>(<span class="va">self</span>._init_weights)</span>
<span id="cb10-120"><a href="#cb10-120" aria-hidden="true" tabindex="-1"></a>        <span class="co"># apply special scaled init to the residual projections, per GPT-2 paper</span></span>
<span id="cb10-121"><a href="#cb10-121" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> pn, p <span class="kw">in</span> <span class="va">self</span>.named_parameters():</span>
<span id="cb10-122"><a href="#cb10-122" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> pn.endswith(<span class="st">'c_proj.weight'</span>):</span>
<span id="cb10-123"><a href="#cb10-123" aria-hidden="true" tabindex="-1"></a>                torch.nn.init.normal_(p, mean<span class="op">=</span><span class="fl">0.0</span>, std<span class="op">=</span><span class="fl">0.02</span><span class="op">/</span>math.sqrt(<span class="dv">2</span> <span class="op">*</span> config.n_layer))</span>
<span id="cb10-124"><a href="#cb10-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-125"><a href="#cb10-125" aria-hidden="true" tabindex="-1"></a>        <span class="co"># report number of parameters</span></span>
<span id="cb10-126"><a href="#cb10-126" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"number of parameters: </span><span class="sc">%.2f</span><span class="st">M"</span> <span class="op">%</span> (<span class="va">self</span>.get_num_params()<span class="op">/</span><span class="fl">1e6</span>,))</span>
<span id="cb10-127"><a href="#cb10-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-128"><a href="#cb10-128" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_num_params(<span class="va">self</span>, non_embedding<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb10-129"><a href="#cb10-129" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb10-130"><a href="#cb10-130" aria-hidden="true" tabindex="-1"></a><span class="co">        Return the number of parameters in the model.</span></span>
<span id="cb10-131"><a href="#cb10-131" aria-hidden="true" tabindex="-1"></a><span class="co">        For non-embedding count (default), the position embeddings get subtracted.</span></span>
<span id="cb10-132"><a href="#cb10-132" aria-hidden="true" tabindex="-1"></a><span class="co">        The token embeddings would too, except due to the parameter sharing these</span></span>
<span id="cb10-133"><a href="#cb10-133" aria-hidden="true" tabindex="-1"></a><span class="co">        params are actually used as weights in the final layer, so we include them.</span></span>
<span id="cb10-134"><a href="#cb10-134" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb10-135"><a href="#cb10-135" aria-hidden="true" tabindex="-1"></a>        n_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.parameters())</span>
<span id="cb10-136"><a href="#cb10-136" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> non_embedding:</span>
<span id="cb10-137"><a href="#cb10-137" aria-hidden="true" tabindex="-1"></a>            n_params <span class="op">-=</span> <span class="va">self</span>.transformer.wpe.weight.numel()</span>
<span id="cb10-138"><a href="#cb10-138" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> n_params</span>
<span id="cb10-139"><a href="#cb10-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-140"><a href="#cb10-140" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _init_weights(<span class="va">self</span>, module):</span>
<span id="cb10-141"><a href="#cb10-141" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(module, nn.Linear):</span>
<span id="cb10-142"><a href="#cb10-142" aria-hidden="true" tabindex="-1"></a>            torch.nn.init.normal_(module.weight, mean<span class="op">=</span><span class="fl">0.0</span>, std<span class="op">=</span><span class="fl">0.02</span>)</span>
<span id="cb10-143"><a href="#cb10-143" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> module.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb10-144"><a href="#cb10-144" aria-hidden="true" tabindex="-1"></a>                torch.nn.init.zeros_(module.bias)</span>
<span id="cb10-145"><a href="#cb10-145" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="bu">isinstance</span>(module, nn.Embedding):</span>
<span id="cb10-146"><a href="#cb10-146" aria-hidden="true" tabindex="-1"></a>            torch.nn.init.normal_(module.weight, mean<span class="op">=</span><span class="fl">0.0</span>, std<span class="op">=</span><span class="fl">0.02</span>)</span>
<span id="cb10-147"><a href="#cb10-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-148"><a href="#cb10-148" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb10-149"><a href="#cb10-149" aria-hidden="true" tabindex="-1"></a>        <span class="co">#device = idx.device</span></span>
<span id="cb10-150"><a href="#cb10-150" aria-hidden="true" tabindex="-1"></a>        b, t <span class="op">=</span> idx.size()</span>
<span id="cb10-151"><a href="#cb10-151" aria-hidden="true" tabindex="-1"></a>        weight_lst <span class="op">=</span> []</span>
<span id="cb10-152"><a href="#cb10-152" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> t <span class="op">&lt;=</span> <span class="va">self</span>.config.block_size, <span class="ss">f"Cannot forward sequence of length </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">, block size is only </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>config<span class="sc">.</span>block_size<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb10-153"><a href="#cb10-153" aria-hidden="true" tabindex="-1"></a>        pos <span class="op">=</span> torch.arange(<span class="dv">0</span>, t, dtype<span class="op">=</span>torch.<span class="bu">long</span>, device<span class="op">=</span>device) <span class="co"># shape (t)</span></span>
<span id="cb10-154"><a href="#cb10-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-155"><a href="#cb10-155" aria-hidden="true" tabindex="-1"></a>        <span class="co"># forward the GPT model itself</span></span>
<span id="cb10-156"><a href="#cb10-156" aria-hidden="true" tabindex="-1"></a>        tok_emb <span class="op">=</span> <span class="va">self</span>.transformer.wte(idx) <span class="co"># token embeddings of shape (b, t, n_embd)</span></span>
<span id="cb10-157"><a href="#cb10-157" aria-hidden="true" tabindex="-1"></a>        pos_emb <span class="op">=</span> <span class="va">self</span>.transformer.wpe(pos) <span class="co"># position embeddings of shape (t, n_embd)</span></span>
<span id="cb10-158"><a href="#cb10-158" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.transformer.drop(tok_emb <span class="op">+</span> pos_emb)</span>
<span id="cb10-159"><a href="#cb10-159" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.transformer.h:</span>
<span id="cb10-160"><a href="#cb10-160" aria-hidden="true" tabindex="-1"></a>            x, attn_weight <span class="op">=</span> block(x)</span>
<span id="cb10-161"><a href="#cb10-161" aria-hidden="true" tabindex="-1"></a>            weight_lst.append(attn_weight)</span>
<span id="cb10-162"><a href="#cb10-162" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.transformer.ln_f(x)</span>
<span id="cb10-163"><a href="#cb10-163" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-164"><a href="#cb10-164" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb10-165"><a href="#cb10-165" aria-hidden="true" tabindex="-1"></a>            <span class="co"># if we are given some desired targets also calculate the loss</span></span>
<span id="cb10-166"><a href="#cb10-166" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> <span class="va">self</span>.lm_head(x)</span>
<span id="cb10-167"><a href="#cb10-167" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> F.cross_entropy(logits.view(<span class="op">-</span><span class="dv">1</span>, logits.size(<span class="op">-</span><span class="dv">1</span>)), targets.view(<span class="op">-</span><span class="dv">1</span>), ignore_index<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb10-168"><a href="#cb10-168" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb10-169"><a href="#cb10-169" aria-hidden="true" tabindex="-1"></a>            <span class="co"># inference-time mini-optimization: only forward the lm_head on the very last position</span></span>
<span id="cb10-170"><a href="#cb10-170" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> <span class="va">self</span>.lm_head(x[:, [<span class="op">-</span><span class="dv">1</span>], :]) <span class="co"># note: using list [-1] to preserve the time dim</span></span>
<span id="cb10-171"><a href="#cb10-171" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb10-172"><a href="#cb10-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-173"><a href="#cb10-173" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits, loss, weight_lst</span>
<span id="cb10-174"><a href="#cb10-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-175"><a href="#cb10-175" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> crop_block_size(<span class="va">self</span>, block_size):</span>
<span id="cb10-176"><a href="#cb10-176" aria-hidden="true" tabindex="-1"></a>        <span class="co"># model surgery to decrease the block size if necessary</span></span>
<span id="cb10-177"><a href="#cb10-177" aria-hidden="true" tabindex="-1"></a>        <span class="co"># e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)</span></span>
<span id="cb10-178"><a href="#cb10-178" aria-hidden="true" tabindex="-1"></a>        <span class="co"># but want to use a smaller block size for some smaller, simpler model</span></span>
<span id="cb10-179"><a href="#cb10-179" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> block_size <span class="op">&lt;=</span> <span class="va">self</span>.config.block_size</span>
<span id="cb10-180"><a href="#cb10-180" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.config.block_size <span class="op">=</span> block_size</span>
<span id="cb10-181"><a href="#cb10-181" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transformer.wpe.weight <span class="op">=</span> nn.Parameter(<span class="va">self</span>.transformer.wpe.weight[:block_size])</span>
<span id="cb10-182"><a href="#cb10-182" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.transformer.h:</span>
<span id="cb10-183"><a href="#cb10-183" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">hasattr</span>(block.attn, <span class="st">'bias'</span>):</span>
<span id="cb10-184"><a href="#cb10-184" aria-hidden="true" tabindex="-1"></a>                block.attn.bias <span class="op">=</span> block.attn.bias[:,:,:block_size,:block_size]</span>
<span id="cb10-185"><a href="#cb10-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-186"><a href="#cb10-186" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> configure_optimizers(<span class="va">self</span>, weight_decay, learning_rate, betas, device_type):</span>
<span id="cb10-187"><a href="#cb10-187" aria-hidden="true" tabindex="-1"></a>        <span class="co"># start with all of the candidate parameters</span></span>
<span id="cb10-188"><a href="#cb10-188" aria-hidden="true" tabindex="-1"></a>        param_dict <span class="op">=</span> {pn: p <span class="cf">for</span> pn, p <span class="kw">in</span> <span class="va">self</span>.named_parameters()}</span>
<span id="cb10-189"><a href="#cb10-189" aria-hidden="true" tabindex="-1"></a>        <span class="co"># filter out those that do not require grad</span></span>
<span id="cb10-190"><a href="#cb10-190" aria-hidden="true" tabindex="-1"></a>        param_dict <span class="op">=</span> {pn: p <span class="cf">for</span> pn, p <span class="kw">in</span> param_dict.items() <span class="cf">if</span> p.requires_grad}</span>
<span id="cb10-191"><a href="#cb10-191" aria-hidden="true" tabindex="-1"></a>        <span class="co"># create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.</span></span>
<span id="cb10-192"><a href="#cb10-192" aria-hidden="true" tabindex="-1"></a>        <span class="co"># i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.</span></span>
<span id="cb10-193"><a href="#cb10-193" aria-hidden="true" tabindex="-1"></a>        decay_params <span class="op">=</span> [p <span class="cf">for</span> n, p <span class="kw">in</span> param_dict.items() <span class="cf">if</span> p.dim() <span class="op">&gt;=</span> <span class="dv">2</span>]</span>
<span id="cb10-194"><a href="#cb10-194" aria-hidden="true" tabindex="-1"></a>        nodecay_params <span class="op">=</span> [p <span class="cf">for</span> n, p <span class="kw">in</span> param_dict.items() <span class="cf">if</span> p.dim() <span class="op">&lt;</span> <span class="dv">2</span>]</span>
<span id="cb10-195"><a href="#cb10-195" aria-hidden="true" tabindex="-1"></a>        optim_groups <span class="op">=</span> [</span>
<span id="cb10-196"><a href="#cb10-196" aria-hidden="true" tabindex="-1"></a>            {<span class="st">'params'</span>: decay_params, <span class="st">'weight_decay'</span>: weight_decay},</span>
<span id="cb10-197"><a href="#cb10-197" aria-hidden="true" tabindex="-1"></a>            {<span class="st">'params'</span>: nodecay_params, <span class="st">'weight_decay'</span>: <span class="fl">0.0</span>}</span>
<span id="cb10-198"><a href="#cb10-198" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb10-199"><a href="#cb10-199" aria-hidden="true" tabindex="-1"></a>        num_decay_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> decay_params)</span>
<span id="cb10-200"><a href="#cb10-200" aria-hidden="true" tabindex="-1"></a>        num_nodecay_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> nodecay_params)</span>
<span id="cb10-201"><a href="#cb10-201" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"num decayed parameter tensors: </span><span class="sc">{</span><span class="bu">len</span>(decay_params)<span class="sc">}</span><span class="ss">, with </span><span class="sc">{</span>num_decay_params<span class="sc">:,}</span><span class="ss"> parameters"</span>)</span>
<span id="cb10-202"><a href="#cb10-202" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"num non-decayed parameter tensors: </span><span class="sc">{</span><span class="bu">len</span>(nodecay_params)<span class="sc">}</span><span class="ss">, with </span><span class="sc">{</span>num_nodecay_params<span class="sc">:,}</span><span class="ss"> parameters"</span>)</span>
<span id="cb10-203"><a href="#cb10-203" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create AdamW optimizer and use the fused version if it is available</span></span>
<span id="cb10-204"><a href="#cb10-204" aria-hidden="true" tabindex="-1"></a>        fused_available <span class="op">=</span> <span class="st">'fused'</span> <span class="kw">in</span> inspect.signature(torch.optim.AdamW).parameters</span>
<span id="cb10-205"><a href="#cb10-205" aria-hidden="true" tabindex="-1"></a>        use_fused <span class="op">=</span> fused_available <span class="kw">and</span> device_type <span class="op">==</span> <span class="st">'cuda'</span></span>
<span id="cb10-206"><a href="#cb10-206" aria-hidden="true" tabindex="-1"></a>        extra_args <span class="op">=</span> <span class="bu">dict</span>(fused<span class="op">=</span><span class="va">True</span>) <span class="cf">if</span> use_fused <span class="cf">else</span> <span class="bu">dict</span>()</span>
<span id="cb10-207"><a href="#cb10-207" aria-hidden="true" tabindex="-1"></a>        optimizer <span class="op">=</span> torch.optim.AdamW(optim_groups, lr<span class="op">=</span>learning_rate, betas<span class="op">=</span>betas, <span class="op">**</span>extra_args)</span>
<span id="cb10-208"><a href="#cb10-208" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"using fused AdamW: </span><span class="sc">{</span>use_fused<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb10-209"><a href="#cb10-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-210"><a href="#cb10-210" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> optimizer</span>
<span id="cb10-211"><a href="#cb10-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-212"><a href="#cb10-212" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> estimate_mfu(<span class="va">self</span>, fwdbwd_per_iter, dt):</span>
<span id="cb10-213"><a href="#cb10-213" aria-hidden="true" tabindex="-1"></a>        <span class="co">""" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS """</span></span>
<span id="cb10-214"><a href="#cb10-214" aria-hidden="true" tabindex="-1"></a>        <span class="co"># first estimate the number of flops we do per iteration.</span></span>
<span id="cb10-215"><a href="#cb10-215" aria-hidden="true" tabindex="-1"></a>        <span class="co"># see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311</span></span>
<span id="cb10-216"><a href="#cb10-216" aria-hidden="true" tabindex="-1"></a>        N <span class="op">=</span> <span class="va">self</span>.get_num_params()</span>
<span id="cb10-217"><a href="#cb10-217" aria-hidden="true" tabindex="-1"></a>        cfg <span class="op">=</span> <span class="va">self</span>.config</span>
<span id="cb10-218"><a href="#cb10-218" aria-hidden="true" tabindex="-1"></a>        L, H, Q, T <span class="op">=</span> cfg.n_layer, cfg.n_head, cfg.n_embd<span class="op">//</span>cfg.n_head, cfg.block_size</span>
<span id="cb10-219"><a href="#cb10-219" aria-hidden="true" tabindex="-1"></a>        flops_per_token <span class="op">=</span> <span class="dv">6</span><span class="op">*</span>N <span class="op">+</span> <span class="dv">12</span><span class="op">*</span>L<span class="op">*</span>H<span class="op">*</span>Q<span class="op">*</span>T</span>
<span id="cb10-220"><a href="#cb10-220" aria-hidden="true" tabindex="-1"></a>        flops_per_fwdbwd <span class="op">=</span> flops_per_token <span class="op">*</span> T</span>
<span id="cb10-221"><a href="#cb10-221" aria-hidden="true" tabindex="-1"></a>        flops_per_iter <span class="op">=</span> flops_per_fwdbwd <span class="op">*</span> fwdbwd_per_iter</span>
<span id="cb10-222"><a href="#cb10-222" aria-hidden="true" tabindex="-1"></a>        <span class="co"># express our flops throughput as ratio of A100 bfloat16 peak flops</span></span>
<span id="cb10-223"><a href="#cb10-223" aria-hidden="true" tabindex="-1"></a>        flops_achieved <span class="op">=</span> flops_per_iter <span class="op">*</span> (<span class="fl">1.0</span><span class="op">/</span>dt) <span class="co"># per second</span></span>
<span id="cb10-224"><a href="#cb10-224" aria-hidden="true" tabindex="-1"></a>        flops_promised <span class="op">=</span> <span class="fl">312e12</span> <span class="co"># A100 GPU bfloat16 peak flops is 312 TFLOPS</span></span>
<span id="cb10-225"><a href="#cb10-225" aria-hidden="true" tabindex="-1"></a>        mfu <span class="op">=</span> flops_achieved <span class="op">/</span> flops_promised</span>
<span id="cb10-226"><a href="#cb10-226" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> mfu</span>
<span id="cb10-227"><a href="#cb10-227" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-228"><a href="#cb10-228" aria-hidden="true" tabindex="-1"></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb10-229"><a href="#cb10-229" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens, temperature<span class="op">=</span><span class="fl">1.0</span>, top_k<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb10-230"><a href="#cb10-230" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb10-231"><a href="#cb10-231" aria-hidden="true" tabindex="-1"></a><span class="co">        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete</span></span>
<span id="cb10-232"><a href="#cb10-232" aria-hidden="true" tabindex="-1"></a><span class="co">        the sequence max_new_tokens times, feeding the predictions back into the model each time.</span></span>
<span id="cb10-233"><a href="#cb10-233" aria-hidden="true" tabindex="-1"></a><span class="co">        Most likely you'll want to make sure to be in model.eval() mode of operation for this.</span></span>
<span id="cb10-234"><a href="#cb10-234" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb10-235"><a href="#cb10-235" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb10-236"><a href="#cb10-236" aria-hidden="true" tabindex="-1"></a>            <span class="co"># if the sequence context is growing too long we must crop it at block_size</span></span>
<span id="cb10-237"><a href="#cb10-237" aria-hidden="true" tabindex="-1"></a>            idx_cond <span class="op">=</span> idx <span class="cf">if</span> idx.size(<span class="dv">1</span>) <span class="op">&lt;=</span> <span class="va">self</span>.config.block_size <span class="cf">else</span> idx[:, <span class="op">-</span><span class="va">self</span>.config.block_size:]</span>
<span id="cb10-238"><a href="#cb10-238" aria-hidden="true" tabindex="-1"></a>            <span class="co"># forward the model to get the logits for the index in the sequence</span></span>
<span id="cb10-239"><a href="#cb10-239" aria-hidden="true" tabindex="-1"></a>            logits, _ <span class="op">=</span> <span class="va">self</span>(idx_cond)[:<span class="dv">2</span>]</span>
<span id="cb10-240"><a href="#cb10-240" aria-hidden="true" tabindex="-1"></a>            <span class="co"># pluck the logits at the final step and scale by desired temperature</span></span>
<span id="cb10-241"><a href="#cb10-241" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="op">/</span> temperature</span>
<span id="cb10-242"><a href="#cb10-242" aria-hidden="true" tabindex="-1"></a>            <span class="co"># optionally crop the logits to only the top k options</span></span>
<span id="cb10-243"><a href="#cb10-243" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> top_k <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb10-244"><a href="#cb10-244" aria-hidden="true" tabindex="-1"></a>                v, _ <span class="op">=</span> torch.topk(logits, <span class="bu">min</span>(top_k, logits.size(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb10-245"><a href="#cb10-245" aria-hidden="true" tabindex="-1"></a>                logits[logits <span class="op">&lt;</span> v[:, [<span class="op">-</span><span class="dv">1</span>]]] <span class="op">=</span> <span class="op">-</span><span class="bu">float</span>(<span class="st">'Inf'</span>)</span>
<span id="cb10-246"><a href="#cb10-246" aria-hidden="true" tabindex="-1"></a>            <span class="co"># apply softmax to convert logits to (normalized) probabilities</span></span>
<span id="cb10-247"><a href="#cb10-247" aria-hidden="true" tabindex="-1"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb10-248"><a href="#cb10-248" aria-hidden="true" tabindex="-1"></a>            <span class="co"># sample from the distribution</span></span>
<span id="cb10-249"><a href="#cb10-249" aria-hidden="true" tabindex="-1"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-250"><a href="#cb10-250" aria-hidden="true" tabindex="-1"></a>            <span class="co"># append sampled index to the running sequence and continue</span></span>
<span id="cb10-251"><a href="#cb10-251" aria-hidden="true" tabindex="-1"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-252"><a href="#cb10-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-253"><a href="#cb10-253" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> idx</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="training" class="level2">
<h2 class="anchored" data-anchor-id="training">Training</h2>
<p>Now that our model constructors are ready, it’s time to start training! First, see below for the configuration settings we are using for the model. Most of these are not important for the purposes of this notebook, but just note that we are creating a model with 4 attention layers with 4 heads each (more on that below), our embedding size is 384 (so our vocabulary gets projected into 384-dimensional space), and our block size/context size is 300, so our model will look at 300 bp of sequence as each input. Training will take a while (about 5-10 minutes), so run the code block below and go to the end for a description of the Attention mechanism and a rundown of what our training loop does.</p>
<div id="fc4c0ca0" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co">#@title Training Time!</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>gc.collect()</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------------------------</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># default config values designed to train a gpt on hg38 reference genome</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># I/O</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>out_dir <span class="op">=</span> WORKDIR <span class="op">+</span> <span class="st">'out'</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>eval_interval <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>log_interval <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>eval_iters <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>eval_only <span class="op">=</span> <span class="va">False</span> <span class="co"># if True, script exits right after the first eval</span></span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>always_save_checkpoint <span class="op">=</span> <span class="va">False</span> <span class="co"># if True, always save a checkpoint after each eval</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>init_from <span class="op">=</span> <span class="st">'scratch'</span> <span class="co"># 'scratch' or 'resume' or 'gpt2*'</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a><span class="co"># wandb logging</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>wandb_log <span class="op">=</span> <span class="va">False</span> <span class="co"># disabled by default</span></span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>wandb_project <span class="op">=</span> <span class="st">'genome_char'</span></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>wandb_run_name <span class="op">=</span> <span class="st">'mini-gpt'</span> <span class="co"># 'run' + str(time.time())</span></span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a><span class="co"># data</span></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> <span class="st">'genome_char'</span></span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a>gradient_accumulation_steps <span class="op">=</span> <span class="dv">1</span> <span class="co"># used to simulate larger batch sizes</span></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span> <span class="co"># if gradient_accumulation_steps &gt; 1, this is the micro-batch size</span></span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a>block_size <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a><span class="co"># model</span></span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a>n_layer <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a>n_head <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a>n_embd <span class="op">=</span> <span class="dv">384</span></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>dropout <span class="op">=</span> <span class="fl">0.2</span> <span class="co"># for pretraining 0 is good, for finetuning try 0.1+</span></span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> <span class="va">False</span> <span class="co"># do we use bias inside LayerNorm and Linear layers?</span></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a><span class="co"># adamw optimizer</span></span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span> <span class="co"># max learning rate</span></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>max_iters <span class="op">=</span> <span class="dv">4000</span> <span class="co"># total number of training iterations</span></span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a>weight_decay <span class="op">=</span> <span class="fl">1e-1</span></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a>beta1 <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a>beta2 <span class="op">=</span> <span class="fl">0.99</span></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a>grad_clip <span class="op">=</span> <span class="fl">1.0</span> <span class="co"># clip gradients at this value, or disable if == 0.0</span></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a><span class="co"># learning rate decay settings</span></span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a>decay_lr <span class="op">=</span> <span class="va">True</span> <span class="co"># whether to decay the learning rate</span></span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>warmup_iters <span class="op">=</span> <span class="dv">100</span> <span class="co"># how many steps to warm up for</span></span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a>lr_decay_iters <span class="op">=</span> <span class="dv">4000</span> <span class="co"># should be ~= max_iters per Chinchilla</span></span>
<span id="cb11-40"><a href="#cb11-40" aria-hidden="true" tabindex="-1"></a>min_lr <span class="op">=</span> <span class="fl">1e-4</span> <span class="co"># minimum learning rate, should be ~= learning_rate/10 per Chinchilla</span></span>
<span id="cb11-41"><a href="#cb11-41" aria-hidden="true" tabindex="-1"></a><span class="co"># DDP settings</span></span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a><span class="co">#backend = 'nccl' # 'nccl', 'gloo', etc.</span></span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a><span class="co"># system</span></span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="co"># examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks</span></span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a>dtype <span class="op">=</span> <span class="st">'float16'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="kw">and</span> torch.cuda.is_bf16_supported() <span class="cf">else</span> <span class="st">'float16'</span> <span class="co"># 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler</span></span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available() <span class="kw">and</span> device <span class="op">==</span> <span class="st">'cuda'</span>:</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'cuda'</span>)</span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'CPU'</span>)</span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a><span class="bu">compile</span> <span class="op">=</span> <span class="va">True</span> <span class="co"># use PyTorch 2.0 to compile the model to be faster</span></span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------------------------</span></span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>config_keys <span class="op">=</span> [k <span class="cf">for</span> k,v <span class="kw">in</span> <span class="bu">globals</span>().items() <span class="cf">if</span> <span class="kw">not</span> k.startswith(<span class="st">'_'</span>) <span class="kw">and</span> <span class="bu">isinstance</span>(v, (<span class="bu">int</span>, <span class="bu">float</span>, <span class="bu">bool</span>, <span class="bu">str</span>))]</span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a><span class="co">#exec(open('configurator.py').read()) # overrides from command line or config file</span></span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> {k: <span class="bu">globals</span>()[k] <span class="cf">for</span> k <span class="kw">in</span> config_keys} <span class="co"># will be useful for logging</span></span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------------------------</span></span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a><span class="co"># various inits, derived attributes, I/O setup</span></span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a>master_process <span class="op">=</span> <span class="va">True</span></span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a>seed_offset <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a>ddp_world_size <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a>tokens_per_iter <span class="op">=</span> gradient_accumulation_steps <span class="op">*</span> ddp_world_size <span class="op">*</span> batch_size <span class="op">*</span> block_size</span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"tokens per iteration will be: </span><span class="sc">{</span>tokens_per_iter<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> master_process:</span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a>    os.makedirs(out_dir, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1337</span> <span class="op">+</span> seed_offset)</span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a>torch.backends.cuda.matmul.allow_tf32 <span class="op">=</span> <span class="va">True</span> <span class="co"># allow tf32 on matmul</span></span>
<span id="cb11-68"><a href="#cb11-68" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.allow_tf32 <span class="op">=</span> <span class="va">True</span> <span class="co"># allow tf32 on cudnn</span></span>
<span id="cb11-69"><a href="#cb11-69" aria-hidden="true" tabindex="-1"></a>device_type <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> <span class="st">'cuda'</span> <span class="kw">in</span> device <span class="cf">else</span> <span class="st">'cpu'</span> <span class="co"># for later use in torch.autocast</span></span>
<span id="cb11-70"><a href="#cb11-70" aria-hidden="true" tabindex="-1"></a><span class="co"># note: float16 data type will automatically use a GradScaler</span></span>
<span id="cb11-71"><a href="#cb11-71" aria-hidden="true" tabindex="-1"></a>ptdtype <span class="op">=</span> {<span class="st">'float32'</span>: torch.float32, <span class="st">'bfloat16'</span>: torch.bfloat16, <span class="st">'float16'</span>: torch.float16}[dtype]</span>
<span id="cb11-72"><a href="#cb11-72" aria-hidden="true" tabindex="-1"></a>ctx <span class="op">=</span> nullcontext() <span class="cf">if</span> device_type <span class="op">==</span> <span class="st">'cpu'</span> <span class="cf">else</span> torch.amp.autocast(device_type<span class="op">=</span>device_type, dtype<span class="op">=</span>ptdtype)</span>
<span id="cb11-73"><a href="#cb11-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-74"><a href="#cb11-74" aria-hidden="true" tabindex="-1"></a><span class="co"># poor man's data loader</span></span>
<span id="cb11-75"><a href="#cb11-75" aria-hidden="true" tabindex="-1"></a>data_dir <span class="op">=</span> <span class="st">'/root/data'</span></span>
<span id="cb11-76"><a href="#cb11-76" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_batch(split):</span>
<span id="cb11-77"><a href="#cb11-77" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We recreate np.memmap every batch to avoid a memory leak, as per</span></span>
<span id="cb11-78"><a href="#cb11-78" aria-hidden="true" tabindex="-1"></a>    <span class="co"># https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122</span></span>
<span id="cb11-79"><a href="#cb11-79" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> split <span class="op">==</span> <span class="st">'train'</span>:</span>
<span id="cb11-80"><a href="#cb11-80" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> np.memmap(os.path.join(data_dir, <span class="st">'train.bin'</span>), dtype<span class="op">=</span>np.uint16, mode<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb11-81"><a href="#cb11-81" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb11-82"><a href="#cb11-82" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> np.memmap(os.path.join(data_dir, <span class="st">'val.bin'</span>), dtype<span class="op">=</span>np.uint16, mode<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb11-83"><a href="#cb11-83" aria-hidden="true" tabindex="-1"></a>    ix <span class="op">=</span> torch.randint(<span class="bu">len</span>(data) <span class="op">-</span> block_size, (batch_size,))</span>
<span id="cb11-84"><a href="#cb11-84" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.stack([torch.from_numpy((data[i:i<span class="op">+</span>block_size]).astype(np.int64)) <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb11-85"><a href="#cb11-85" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> torch.stack([torch.from_numpy((data[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span><span class="dv">1</span><span class="op">+</span>block_size]).astype(np.int64)) <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb11-86"><a href="#cb11-86" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> device_type <span class="op">==</span> <span class="st">'cuda'</span>:</span>
<span id="cb11-87"><a href="#cb11-87" aria-hidden="true" tabindex="-1"></a>        <span class="co"># pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)</span></span>
<span id="cb11-88"><a href="#cb11-88" aria-hidden="true" tabindex="-1"></a>        x, y <span class="op">=</span> x.pin_memory().to(device, non_blocking<span class="op">=</span><span class="va">True</span>), y.pin_memory().to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-89"><a href="#cb11-89" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb11-90"><a href="#cb11-90" aria-hidden="true" tabindex="-1"></a>        x, y <span class="op">=</span> x.to(device), y.to(device)</span>
<span id="cb11-91"><a href="#cb11-91" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x, y</span>
<span id="cb11-92"><a href="#cb11-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-93"><a href="#cb11-93" aria-hidden="true" tabindex="-1"></a><span class="co"># init these up here, can override if init_from='resume' (i.e. from a checkpoint)</span></span>
<span id="cb11-94"><a href="#cb11-94" aria-hidden="true" tabindex="-1"></a>iter_num <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb11-95"><a href="#cb11-95" aria-hidden="true" tabindex="-1"></a>best_val_loss <span class="op">=</span> <span class="fl">1e9</span></span>
<span id="cb11-96"><a href="#cb11-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-97"><a href="#cb11-97" aria-hidden="true" tabindex="-1"></a><span class="co"># attempt to derive vocab_size from the dataset</span></span>
<span id="cb11-98"><a href="#cb11-98" aria-hidden="true" tabindex="-1"></a>meta_path <span class="op">=</span> os.path.join(data_dir, <span class="st">'meta.pkl'</span>)</span>
<span id="cb11-99"><a href="#cb11-99" aria-hidden="true" tabindex="-1"></a>meta_vocab_size <span class="op">=</span> <span class="va">None</span></span>
<span id="cb11-100"><a href="#cb11-100" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> os.path.exists(meta_path):</span>
<span id="cb11-101"><a href="#cb11-101" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(meta_path, <span class="st">'rb'</span>) <span class="im">as</span> f:</span>
<span id="cb11-102"><a href="#cb11-102" aria-hidden="true" tabindex="-1"></a>        meta <span class="op">=</span> pickle.load(f)</span>
<span id="cb11-103"><a href="#cb11-103" aria-hidden="true" tabindex="-1"></a>    meta_vocab_size <span class="op">=</span> meta[<span class="st">'vocab_size'</span>]</span>
<span id="cb11-104"><a href="#cb11-104" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"found vocab_size = </span><span class="sc">{</span>meta_vocab_size<span class="sc">}</span><span class="ss"> (inside </span><span class="sc">{</span>meta_path<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb11-105"><a href="#cb11-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-106"><a href="#cb11-106" aria-hidden="true" tabindex="-1"></a><span class="co"># model init</span></span>
<span id="cb11-107"><a href="#cb11-107" aria-hidden="true" tabindex="-1"></a>model_args <span class="op">=</span> <span class="bu">dict</span>(n_layer<span class="op">=</span>n_layer, n_head<span class="op">=</span>n_head, n_embd<span class="op">=</span>n_embd, block_size<span class="op">=</span>block_size,</span>
<span id="cb11-108"><a href="#cb11-108" aria-hidden="true" tabindex="-1"></a>                  bias<span class="op">=</span>bias, vocab_size<span class="op">=</span><span class="va">None</span>, dropout<span class="op">=</span>dropout) <span class="co"># start with model_args from command line</span></span>
<span id="cb11-109"><a href="#cb11-109" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> init_from <span class="op">==</span> <span class="st">'scratch'</span>:</span>
<span id="cb11-110"><a href="#cb11-110" aria-hidden="true" tabindex="-1"></a>    <span class="co"># init a new model from scratch</span></span>
<span id="cb11-111"><a href="#cb11-111" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Initializing a new model from scratch"</span>)</span>
<span id="cb11-112"><a href="#cb11-112" aria-hidden="true" tabindex="-1"></a>    <span class="co"># determine the vocab size we'll use for from-scratch training</span></span>
<span id="cb11-113"><a href="#cb11-113" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> meta_vocab_size <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb11-114"><a href="#cb11-114" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)"</span>)</span>
<span id="cb11-115"><a href="#cb11-115" aria-hidden="true" tabindex="-1"></a>    model_args[<span class="st">'vocab_size'</span>] <span class="op">=</span> meta_vocab_size <span class="cf">if</span> meta_vocab_size <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="dv">50304</span></span>
<span id="cb11-116"><a href="#cb11-116" aria-hidden="true" tabindex="-1"></a>    gptconf <span class="op">=</span> GPTConfig(<span class="op">**</span>model_args)</span>
<span id="cb11-117"><a href="#cb11-117" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> GPT(gptconf)</span>
<span id="cb11-118"><a href="#cb11-118" aria-hidden="true" tabindex="-1"></a><span class="co"># crop down the model block size if desired, using model surgery</span></span>
<span id="cb11-119"><a href="#cb11-119" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> block_size <span class="op">&lt;</span> model.config.block_size:</span>
<span id="cb11-120"><a href="#cb11-120" aria-hidden="true" tabindex="-1"></a>    model.crop_block_size(block_size)</span>
<span id="cb11-121"><a href="#cb11-121" aria-hidden="true" tabindex="-1"></a>    model_args[<span class="st">'block_size'</span>] <span class="op">=</span> block_size <span class="co"># so that the checkpoint will have the right value</span></span>
<span id="cb11-122"><a href="#cb11-122" aria-hidden="true" tabindex="-1"></a>model.to(device)</span>
<span id="cb11-123"><a href="#cb11-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-124"><a href="#cb11-124" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize a GradScaler. If enabled=False scaler is a no-op</span></span>
<span id="cb11-125"><a href="#cb11-125" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> torch.cuda.amp.GradScaler(enabled<span class="op">=</span>(dtype <span class="op">==</span> <span class="st">'float16'</span>))</span>
<span id="cb11-126"><a href="#cb11-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-127"><a href="#cb11-127" aria-hidden="true" tabindex="-1"></a><span class="co"># optimizer</span></span>
<span id="cb11-128"><a href="#cb11-128" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)</span>
<span id="cb11-129"><a href="#cb11-129" aria-hidden="true" tabindex="-1"></a><span class="co">#if init_from == 'resume':</span></span>
<span id="cb11-130"><a href="#cb11-130" aria-hidden="true" tabindex="-1"></a><span class="co">#    optimizer.load_state_dict(checkpoint['optimizer'])</span></span>
<span id="cb11-131"><a href="#cb11-131" aria-hidden="true" tabindex="-1"></a>checkpoint <span class="op">=</span> <span class="va">None</span> <span class="co"># free up memory</span></span>
<span id="cb11-132"><a href="#cb11-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-133"><a href="#cb11-133" aria-hidden="true" tabindex="-1"></a><span class="co"># compile the model</span></span>
<span id="cb11-134"><a href="#cb11-134" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">compile</span>:</span>
<span id="cb11-135"><a href="#cb11-135" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"compiling the model... (takes a ~minute)"</span>)</span>
<span id="cb11-136"><a href="#cb11-136" aria-hidden="true" tabindex="-1"></a>    unoptimized_model <span class="op">=</span> model</span>
<span id="cb11-137"><a href="#cb11-137" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> torch.<span class="bu">compile</span>(model) <span class="co"># requires PyTorch 2.0</span></span>
<span id="cb11-138"><a href="#cb11-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-139"><a href="#cb11-139" aria-hidden="true" tabindex="-1"></a><span class="co"># helps estimate an arbitrarily accurate loss over either split using many batches</span></span>
<span id="cb11-140"><a href="#cb11-140" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb11-141"><a href="#cb11-141" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> estimate_loss():</span>
<span id="cb11-142"><a href="#cb11-142" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> {}</span>
<span id="cb11-143"><a href="#cb11-143" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb11-144"><a href="#cb11-144" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> split <span class="kw">in</span> [<span class="st">'train'</span>, <span class="st">'val'</span>]:</span>
<span id="cb11-145"><a href="#cb11-145" aria-hidden="true" tabindex="-1"></a>        losses <span class="op">=</span> torch.zeros(eval_iters)</span>
<span id="cb11-146"><a href="#cb11-146" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(eval_iters):</span>
<span id="cb11-147"><a href="#cb11-147" aria-hidden="true" tabindex="-1"></a>            X, Y <span class="op">=</span> get_batch(split)</span>
<span id="cb11-148"><a href="#cb11-148" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> ctx:</span>
<span id="cb11-149"><a href="#cb11-149" aria-hidden="true" tabindex="-1"></a>                logits, loss <span class="op">=</span> model(X, Y)[:<span class="dv">2</span>]</span>
<span id="cb11-150"><a href="#cb11-150" aria-hidden="true" tabindex="-1"></a>            losses[k] <span class="op">=</span> loss.item()</span>
<span id="cb11-151"><a href="#cb11-151" aria-hidden="true" tabindex="-1"></a>        out[split] <span class="op">=</span> losses.mean()</span>
<span id="cb11-152"><a href="#cb11-152" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb11-153"><a href="#cb11-153" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out</span>
<span id="cb11-154"><a href="#cb11-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-155"><a href="#cb11-155" aria-hidden="true" tabindex="-1"></a><span class="co"># learning rate decay scheduler (cosine with warmup)</span></span>
<span id="cb11-156"><a href="#cb11-156" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_lr(it):</span>
<span id="cb11-157"><a href="#cb11-157" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1) linear warmup for warmup_iters steps</span></span>
<span id="cb11-158"><a href="#cb11-158" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> it <span class="op">&lt;</span> warmup_iters:</span>
<span id="cb11-159"><a href="#cb11-159" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> learning_rate <span class="op">*</span> it <span class="op">/</span> warmup_iters</span>
<span id="cb11-160"><a href="#cb11-160" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2) if it &gt; lr_decay_iters, return min learning rate</span></span>
<span id="cb11-161"><a href="#cb11-161" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> it <span class="op">&gt;</span> lr_decay_iters:</span>
<span id="cb11-162"><a href="#cb11-162" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> min_lr</span>
<span id="cb11-163"><a href="#cb11-163" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3) in between, use cosine decay down to min learning rate</span></span>
<span id="cb11-164"><a href="#cb11-164" aria-hidden="true" tabindex="-1"></a>    decay_ratio <span class="op">=</span> (it <span class="op">-</span> warmup_iters) <span class="op">/</span> (lr_decay_iters <span class="op">-</span> warmup_iters)</span>
<span id="cb11-165"><a href="#cb11-165" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="dv">0</span> <span class="op">&lt;=</span> decay_ratio <span class="op">&lt;=</span> <span class="dv">1</span></span>
<span id="cb11-166"><a href="#cb11-166" aria-hidden="true" tabindex="-1"></a>    coeff <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (<span class="fl">1.0</span> <span class="op">+</span> math.cos(math.pi <span class="op">*</span> decay_ratio)) <span class="co"># coeff ranges 0..1</span></span>
<span id="cb11-167"><a href="#cb11-167" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> min_lr <span class="op">+</span> coeff <span class="op">*</span> (learning_rate <span class="op">-</span> min_lr)</span>
<span id="cb11-168"><a href="#cb11-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-169"><a href="#cb11-169" aria-hidden="true" tabindex="-1"></a><span class="co"># logging</span></span>
<span id="cb11-170"><a href="#cb11-170" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> wandb_log <span class="kw">and</span> master_process:</span>
<span id="cb11-171"><a href="#cb11-171" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> wandb</span>
<span id="cb11-172"><a href="#cb11-172" aria-hidden="true" tabindex="-1"></a>    wandb.init(project<span class="op">=</span>wandb_project, name<span class="op">=</span>wandb_run_name, config<span class="op">=</span>config)</span>
<span id="cb11-173"><a href="#cb11-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-174"><a href="#cb11-174" aria-hidden="true" tabindex="-1"></a><span class="co"># training loop</span></span>
<span id="cb11-175"><a href="#cb11-175" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> get_batch(<span class="st">'train'</span>) <span class="co"># fetch the very first batch</span></span>
<span id="cb11-176"><a href="#cb11-176" aria-hidden="true" tabindex="-1"></a>t0 <span class="op">=</span> time.time()</span>
<span id="cb11-177"><a href="#cb11-177" aria-hidden="true" tabindex="-1"></a>local_iter_num <span class="op">=</span> <span class="dv">0</span> <span class="co"># number of iterations in the lifetime of this process</span></span>
<span id="cb11-178"><a href="#cb11-178" aria-hidden="true" tabindex="-1"></a>raw_model <span class="op">=</span> model</span>
<span id="cb11-179"><a href="#cb11-179" aria-hidden="true" tabindex="-1"></a>running_mfu <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span></span>
<span id="cb11-180"><a href="#cb11-180" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb11-181"><a href="#cb11-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-182"><a href="#cb11-182" aria-hidden="true" tabindex="-1"></a>    <span class="co"># determine and set the learning rate for this iteration</span></span>
<span id="cb11-183"><a href="#cb11-183" aria-hidden="true" tabindex="-1"></a>    lr <span class="op">=</span> get_lr(iter_num) <span class="cf">if</span> decay_lr <span class="cf">else</span> learning_rate</span>
<span id="cb11-184"><a href="#cb11-184" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param_group <span class="kw">in</span> optimizer.param_groups:</span>
<span id="cb11-185"><a href="#cb11-185" aria-hidden="true" tabindex="-1"></a>        param_group[<span class="st">'lr'</span>] <span class="op">=</span> lr</span>
<span id="cb11-186"><a href="#cb11-186" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-187"><a href="#cb11-187" aria-hidden="true" tabindex="-1"></a>    <span class="co"># evaluate the loss on train/val sets and write checkpoints</span></span>
<span id="cb11-188"><a href="#cb11-188" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> iter_num <span class="op">%</span> eval_interval <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> master_process:</span>
<span id="cb11-189"><a href="#cb11-189" aria-hidden="true" tabindex="-1"></a>        losses <span class="op">=</span> estimate_loss()</span>
<span id="cb11-190"><a href="#cb11-190" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"step </span><span class="sc">{</span>iter_num<span class="sc">}</span><span class="ss">: train loss </span><span class="sc">{</span>losses[<span class="st">'train'</span>]<span class="sc">:.4f}</span><span class="ss">, val loss </span><span class="sc">{</span>losses[<span class="st">'val'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb11-191"><a href="#cb11-191" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> wandb_log:</span>
<span id="cb11-192"><a href="#cb11-192" aria-hidden="true" tabindex="-1"></a>            wandb.log({</span>
<span id="cb11-193"><a href="#cb11-193" aria-hidden="true" tabindex="-1"></a>                <span class="st">"iter"</span>: iter_num,</span>
<span id="cb11-194"><a href="#cb11-194" aria-hidden="true" tabindex="-1"></a>                <span class="st">"train/loss"</span>: losses[<span class="st">'train'</span>],</span>
<span id="cb11-195"><a href="#cb11-195" aria-hidden="true" tabindex="-1"></a>                <span class="st">"val/loss"</span>: losses[<span class="st">'val'</span>],</span>
<span id="cb11-196"><a href="#cb11-196" aria-hidden="true" tabindex="-1"></a>                <span class="st">"lr"</span>: lr,</span>
<span id="cb11-197"><a href="#cb11-197" aria-hidden="true" tabindex="-1"></a>                <span class="st">"mfu"</span>: running_mfu<span class="op">*</span><span class="dv">100</span>, <span class="co"># convert to percentage</span></span>
<span id="cb11-198"><a href="#cb11-198" aria-hidden="true" tabindex="-1"></a>            })</span>
<span id="cb11-199"><a href="#cb11-199" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> losses[<span class="st">'val'</span>] <span class="op">&lt;</span> best_val_loss <span class="kw">or</span> always_save_checkpoint:</span>
<span id="cb11-200"><a href="#cb11-200" aria-hidden="true" tabindex="-1"></a>            best_val_loss <span class="op">=</span> losses[<span class="st">'val'</span>]</span>
<span id="cb11-201"><a href="#cb11-201" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> iter_num <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb11-202"><a href="#cb11-202" aria-hidden="true" tabindex="-1"></a>                checkpoint <span class="op">=</span> {</span>
<span id="cb11-203"><a href="#cb11-203" aria-hidden="true" tabindex="-1"></a>                    <span class="st">'model'</span>: raw_model.state_dict(),</span>
<span id="cb11-204"><a href="#cb11-204" aria-hidden="true" tabindex="-1"></a>                    <span class="st">'optimizer'</span>: optimizer.state_dict(),</span>
<span id="cb11-205"><a href="#cb11-205" aria-hidden="true" tabindex="-1"></a>                    <span class="st">'model_args'</span>: model_args,</span>
<span id="cb11-206"><a href="#cb11-206" aria-hidden="true" tabindex="-1"></a>                    <span class="st">'iter_num'</span>: iter_num,</span>
<span id="cb11-207"><a href="#cb11-207" aria-hidden="true" tabindex="-1"></a>                    <span class="st">'best_val_loss'</span>: best_val_loss,</span>
<span id="cb11-208"><a href="#cb11-208" aria-hidden="true" tabindex="-1"></a>                    <span class="st">'config'</span>: config,</span>
<span id="cb11-209"><a href="#cb11-209" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb11-210"><a href="#cb11-210" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"saving checkpoint to </span><span class="sc">{</span>out_dir<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-211"><a href="#cb11-211" aria-hidden="true" tabindex="-1"></a>                torch.save(checkpoint, os.path.join(out_dir, <span class="st">'ckpt.pt'</span>))</span>
<span id="cb11-212"><a href="#cb11-212" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> iter_num <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> eval_only:</span>
<span id="cb11-213"><a href="#cb11-213" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb11-214"><a href="#cb11-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-215"><a href="#cb11-215" aria-hidden="true" tabindex="-1"></a>    <span class="co"># forward backward update, with optional gradient accumulation to simulate larger batch size</span></span>
<span id="cb11-216"><a href="#cb11-216" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and using the GradScaler if data type is float16</span></span>
<span id="cb11-217"><a href="#cb11-217" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> micro_step <span class="kw">in</span> <span class="bu">range</span>(gradient_accumulation_steps):</span>
<span id="cb11-218"><a href="#cb11-218" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> ctx:</span>
<span id="cb11-219"><a href="#cb11-219" aria-hidden="true" tabindex="-1"></a>            logits, loss <span class="op">=</span> model(X, Y)[:<span class="dv">2</span>]</span>
<span id="cb11-220"><a href="#cb11-220" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss <span class="op">/</span> gradient_accumulation_steps <span class="co"># scale the loss to account for gradient accumulation</span></span>
<span id="cb11-221"><a href="#cb11-221" aria-hidden="true" tabindex="-1"></a>        <span class="co"># immediately async prefetch next batch while model is doing the forward pass on the GPU</span></span>
<span id="cb11-222"><a href="#cb11-222" aria-hidden="true" tabindex="-1"></a>        X, Y <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb11-223"><a href="#cb11-223" aria-hidden="true" tabindex="-1"></a>        <span class="co"># backward pass, with gradient scaling if training in fp16</span></span>
<span id="cb11-224"><a href="#cb11-224" aria-hidden="true" tabindex="-1"></a>        scaler.scale(loss).backward()</span>
<span id="cb11-225"><a href="#cb11-225" aria-hidden="true" tabindex="-1"></a>    <span class="co"># clip the gradient</span></span>
<span id="cb11-226"><a href="#cb11-226" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> grad_clip <span class="op">!=</span> <span class="fl">0.0</span>:</span>
<span id="cb11-227"><a href="#cb11-227" aria-hidden="true" tabindex="-1"></a>        scaler.unscale_(optimizer)</span>
<span id="cb11-228"><a href="#cb11-228" aria-hidden="true" tabindex="-1"></a>        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)</span>
<span id="cb11-229"><a href="#cb11-229" aria-hidden="true" tabindex="-1"></a>    <span class="co"># step the optimizer and scaler if training in fp16</span></span>
<span id="cb11-230"><a href="#cb11-230" aria-hidden="true" tabindex="-1"></a>    scaler.step(optimizer)</span>
<span id="cb11-231"><a href="#cb11-231" aria-hidden="true" tabindex="-1"></a>    scaler.update()</span>
<span id="cb11-232"><a href="#cb11-232" aria-hidden="true" tabindex="-1"></a>    <span class="co"># flush the gradients as soon as we can, no need for this memory anymore</span></span>
<span id="cb11-233"><a href="#cb11-233" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb11-234"><a href="#cb11-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-235"><a href="#cb11-235" aria-hidden="true" tabindex="-1"></a>    <span class="co"># timing and logging</span></span>
<span id="cb11-236"><a href="#cb11-236" aria-hidden="true" tabindex="-1"></a>    t1 <span class="op">=</span> time.time()</span>
<span id="cb11-237"><a href="#cb11-237" aria-hidden="true" tabindex="-1"></a>    dt <span class="op">=</span> t1 <span class="op">-</span> t0</span>
<span id="cb11-238"><a href="#cb11-238" aria-hidden="true" tabindex="-1"></a>    t0 <span class="op">=</span> t1</span>
<span id="cb11-239"><a href="#cb11-239" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> iter_num <span class="op">%</span> log_interval <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> master_process:</span>
<span id="cb11-240"><a href="#cb11-240" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get loss as float. note: this is a CPU-GPU sync point</span></span>
<span id="cb11-241"><a href="#cb11-241" aria-hidden="true" tabindex="-1"></a>        <span class="co"># scale up to undo the division above, approximating the true total loss (exact would have been a sum)</span></span>
<span id="cb11-242"><a href="#cb11-242" aria-hidden="true" tabindex="-1"></a>        lossf <span class="op">=</span> loss.item() <span class="op">*</span> gradient_accumulation_steps</span>
<span id="cb11-243"><a href="#cb11-243" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> local_iter_num <span class="op">&gt;=</span> <span class="dv">5</span>: <span class="co"># let the training loop settle a bit</span></span>
<span id="cb11-244"><a href="#cb11-244" aria-hidden="true" tabindex="-1"></a>            mfu <span class="op">=</span> raw_model.estimate_mfu(batch_size <span class="op">*</span> gradient_accumulation_steps, dt)</span>
<span id="cb11-245"><a href="#cb11-245" aria-hidden="true" tabindex="-1"></a>            running_mfu <span class="op">=</span> mfu <span class="cf">if</span> running_mfu <span class="op">==</span> <span class="op">-</span><span class="fl">1.0</span> <span class="cf">else</span> <span class="fl">0.9</span><span class="op">*</span>running_mfu <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>mfu</span>
<span id="cb11-246"><a href="#cb11-246" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"iter </span><span class="sc">{</span>iter_num<span class="sc">}</span><span class="ss">: loss </span><span class="sc">{</span>lossf<span class="sc">:.4f}</span><span class="ss">, time </span><span class="sc">{</span>dt<span class="op">*</span><span class="dv">1000</span><span class="sc">:.2f}</span><span class="ss">ms, mfu </span><span class="sc">{</span>running_mfu<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb11-247"><a href="#cb11-247" aria-hidden="true" tabindex="-1"></a>    iter_num <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb11-248"><a href="#cb11-248" aria-hidden="true" tabindex="-1"></a>    local_iter_num <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb11-249"><a href="#cb11-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-250"><a href="#cb11-250" aria-hidden="true" tabindex="-1"></a>    <span class="co"># termination conditions</span></span>
<span id="cb11-251"><a href="#cb11-251" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> iter_num <span class="op">&gt;</span> max_iters:</span>
<span id="cb11-252"><a href="#cb11-252" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that we’ve started training, let’s talk about Attention. The first paper which outlines the Transformer neural network architecture (which relies heavily on the Attention mechanism and is the basis for most large language models today) is <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention is All you Need</a>. This paper describes the application of Attention in a machine translation model, but the underlying ideas can be applied across any model that processes sequences of text.</p>
<p>At its core, the Attention mechanism allows us to analyze the pairwise relationship between every token in a given sequence. So, in a very simple example, if you’re looking at the sentence “The cow jumped over the moon”, and you are tokenizing by word, you may expect there to be a strong relationship between “jumped” and “over” (as one tends to jump over things), but not between the two iterations of “the”. Below, I describe the actual mechanism behind this in plain language. It will be an oversimplification, but hopefully it can provide some context into what our model is doing.</p>
<p>The way we model this using Attention is via Query (Q), Key (K), and Value (V) matrices. In short, the input into the Attention layer goes through three separate matrix multiplication processes with different weights to create Q, K, and V. The idea behind these matrices is that Q represents each position/token in the sequence, and what information each of those positions is “looking for”. K represents the information that each position contains, and therefore the actual Attention Score is obtained by doing matrix multiplication on Q and K. If two positions are a good match, they will have a higher attention score. The authors of the <a href="https://www.nature.com/articles/s41592-021-01252-x">Enformer</a> paper provide a good, simplified visualization of this, where darker colors represent a stronger relationship in the Key vector to the given Q position:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../post/images/key-query.png" class="img-fluid figure-img"></p>
<figcaption>query-key-value</figcaption>
</figure>
</div>
<p>Once we have our Attention score matrix, we then multiply this by the V matrix, which represents the actual information of each token. This puts the Attention scores back into the context of the original token embeddings, which can then be passed to future layers.</p>
<p>One thing to note is that this calculation is often done in parallel, across multiple “heads” within a single layer. The idea behind this is that each head can learn different weights, and therefore multiple types of relationships between tokens can be represented within the same model. The <a href="https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention is All you Need</a> paper provides a good diagram of the Attention process, and what multi-head attention looks like (note that Scaled Dot-Produce Attention is essentially the process we just described):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="../../../post/images/scale-dot-attention-multihead.png" class="img-fluid figure-img"></p>
<figcaption>Attention_Diagram</figcaption>
</figure>
</div>
<p>Now that we’ve defined the Attention mechanism, our training loop is actually pretty straightforward. See below for the steps:</p>
<ol type="1">
<li>Generate a batch from our training data: This consists of sampling a 300 bp sequence out of the genome as our “data”, then the same 300 bp but shifted over by 1 as our “labels”. Our batch size is 64, so a single batch will be 64 of these pairs of sequences</li>
<li>Plug the batch into our model to calculate loss. Our model is “causal”, so it tries to predict the next token based on all previous tokens (i.e.&nbsp;data[0] is used to predict label[0], data[0:1] is used to predict label[1], data[0:2] is used to predict label[2], etc.)</li>
<li>Once we have calculated the loss (using Cross Entropy Loss), we begin the backward pass and take a step based on the gradients of all of our parameters.</li>
<li>Grab another batch and repeat</li>
<li>Once we hit the evaluation interval (1000 batches), we pause our model training, and evaluate the average loss over 100 additional batches of the training and validation sets, to get a checkpoint of our model performance</li>
<li>If the performance on the validation data is the best we’ve seen yet, we save this version of the model as a checkpoint to use later.</li>
</ol>
<p>Obviously there is quite a bit more code than just these steps above, but this summary covers all of the functionally important parts. Much of the extra code above is related to making the model run efficiently, keeping track of how long it has been running, ensuring we are allocating GPU/CPU memory correctly, etc.</p>
</section>
<section id="sampling-from-the-model" class="level2">
<h2 class="anchored" data-anchor-id="sampling-from-the-model">Sampling from the model</h2>
<p>Great! Now that we have a trained model, let’s see what it can generate it. The code block below simply pulls out the highest-performing version of the model we just trained, and feeds it a newline character to see what it will generate:</p>
<p>Sampling from the model</p>
<div id="9d78c838" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="co">Sample from a trained model</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------------------------</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>init_from <span class="op">=</span> <span class="st">'resume'</span> <span class="co"># either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>out_dir <span class="op">=</span> <span class="st">'/root/data/out'</span> <span class="co"># ignored if init_from is not 'resume'</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="co"># or "&lt;|endoftext|&gt;" or etc. Can also specify a file, use as: "FILE:prompt.txt"</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">5</span> <span class="co"># number of samples to draw</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>max_new_tokens <span class="op">=</span> <span class="dv">250</span> <span class="co"># number of tokens generated in each sample</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>temperature <span class="op">=</span> <span class="fl">0.8</span> <span class="co"># 1.0 = no change, &lt; 1.0 = less random, &gt; 1.0 = more random, in predictions</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>top_k <span class="op">=</span> <span class="dv">200</span> <span class="co"># retain only the top_k most likely tokens, clamp others to have 0 probability</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">1337</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="co"># examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.</span></span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>dtype <span class="op">=</span> <span class="st">'bfloat16'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="kw">and</span> torch.cuda.is_bf16_supported() <span class="cf">else</span> <span class="st">'float16'</span> <span class="co"># 'float32' or 'bfloat16' or 'float16'</span></span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a><span class="bu">compile</span> <span class="op">=</span> <span class="va">True</span> <span class="co"># use PyTorch 2.0 to compile the model to be faster</span></span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------------------------</span></span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>torch.cuda.init()</span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(seed)</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>torch.cuda.manual_seed(seed)</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>torch.backends.cuda.matmul.allow_tf32 <span class="op">=</span> <span class="va">True</span> <span class="co"># allow tf32 on matmul</span></span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.allow_tf32 <span class="op">=</span> <span class="va">True</span> <span class="co"># allow tf32 on cudnn</span></span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>device_type <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> <span class="st">'cuda'</span> <span class="kw">in</span> device <span class="cf">else</span> <span class="st">'cpu'</span> <span class="co"># for later use in torch.autocast</span></span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>ptdtype <span class="op">=</span> {<span class="st">'float32'</span>: torch.float32, <span class="st">'bfloat16'</span>: torch.bfloat16, <span class="st">'float16'</span>: torch.float16}[<span class="st">'float16'</span>]</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>ctx <span class="op">=</span> nullcontext() <span class="cf">if</span> device_type <span class="op">==</span> <span class="st">'cpu'</span> <span class="cf">else</span> torch.amp.autocast(device_type<span class="op">=</span>device_type, dtype<span class="op">=</span>ptdtype)</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a><span class="co"># model</span></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> init_from <span class="op">==</span> <span class="st">'resume'</span>:</span>
<span id="cb12-30"><a href="#cb12-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># init from a model saved in a specific directory</span></span>
<span id="cb12-31"><a href="#cb12-31" aria-hidden="true" tabindex="-1"></a>    ckpt_path <span class="op">=</span> os.path.join(out_dir, <span class="st">'ckpt.pt'</span>)</span>
<span id="cb12-32"><a href="#cb12-32" aria-hidden="true" tabindex="-1"></a>    checkpoint <span class="op">=</span> torch.load(ckpt_path, map_location<span class="op">=</span>device)</span>
<span id="cb12-33"><a href="#cb12-33" aria-hidden="true" tabindex="-1"></a>    gptconf <span class="op">=</span> GPTConfig(<span class="op">**</span>checkpoint[<span class="st">'model_args'</span>])</span>
<span id="cb12-34"><a href="#cb12-34" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> GPT(gptconf)</span>
<span id="cb12-35"><a href="#cb12-35" aria-hidden="true" tabindex="-1"></a>    state_dict <span class="op">=</span> checkpoint[<span class="st">'model'</span>]</span>
<span id="cb12-36"><a href="#cb12-36" aria-hidden="true" tabindex="-1"></a>    unwanted_prefix <span class="op">=</span> <span class="st">'_orig_mod.'</span></span>
<span id="cb12-37"><a href="#cb12-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k,v <span class="kw">in</span> <span class="bu">list</span>(state_dict.items()):</span>
<span id="cb12-38"><a href="#cb12-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> k.startswith(unwanted_prefix):</span>
<span id="cb12-39"><a href="#cb12-39" aria-hidden="true" tabindex="-1"></a>            state_dict[k[<span class="bu">len</span>(unwanted_prefix):]] <span class="op">=</span> state_dict.pop(k)</span>
<span id="cb12-40"><a href="#cb12-40" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(state_dict)</span>
<span id="cb12-41"><a href="#cb12-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-42"><a href="#cb12-42" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb12-43"><a href="#cb12-43" aria-hidden="true" tabindex="-1"></a>model.to(device)</span>
<span id="cb12-44"><a href="#cb12-44" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">compile</span>:</span>
<span id="cb12-45"><a href="#cb12-45" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> torch.<span class="bu">compile</span>(model) <span class="co"># requires PyTorch 2.0 (optional)</span></span>
<span id="cb12-46"><a href="#cb12-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-47"><a href="#cb12-47" aria-hidden="true" tabindex="-1"></a><span class="co"># look for the meta pickle in case it is available in the dataset folder</span></span>
<span id="cb12-48"><a href="#cb12-48" aria-hidden="true" tabindex="-1"></a>load_meta <span class="op">=</span> <span class="va">False</span></span>
<span id="cb12-49"><a href="#cb12-49" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> init_from <span class="op">==</span> <span class="st">'resume'</span> <span class="kw">and</span> <span class="st">'config'</span> <span class="kw">in</span> checkpoint <span class="kw">and</span> <span class="st">'dataset'</span> <span class="kw">in</span> checkpoint[<span class="st">'config'</span>]: <span class="co"># older checkpoints might not have these...</span></span>
<span id="cb12-50"><a href="#cb12-50" aria-hidden="true" tabindex="-1"></a>    meta_path <span class="op">=</span> os.path.join(<span class="st">'/root/data'</span>, <span class="st">'meta.pkl'</span>)</span>
<span id="cb12-51"><a href="#cb12-51" aria-hidden="true" tabindex="-1"></a>    load_meta <span class="op">=</span> os.path.exists(meta_path)</span>
<span id="cb12-52"><a href="#cb12-52" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> load_meta:</span>
<span id="cb12-53"><a href="#cb12-53" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Loading meta from </span><span class="sc">{</span>meta_path<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb12-54"><a href="#cb12-54" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(meta_path, <span class="st">'rb'</span>) <span class="im">as</span> f:</span>
<span id="cb12-55"><a href="#cb12-55" aria-hidden="true" tabindex="-1"></a>        meta <span class="op">=</span> pickle.load(f)</span>
<span id="cb12-56"><a href="#cb12-56" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co"> want to make this more general to arbitrary encoder/decoder schemes</span></span>
<span id="cb12-57"><a href="#cb12-57" aria-hidden="true" tabindex="-1"></a>    stoi, itos <span class="op">=</span> meta[<span class="st">'stoi'</span>], meta[<span class="st">'itos'</span>]</span>
<span id="cb12-58"><a href="#cb12-58" aria-hidden="true" tabindex="-1"></a>    encode <span class="op">=</span> <span class="kw">lambda</span> s: [stoi[c] <span class="cf">for</span> c <span class="kw">in</span> s]</span>
<span id="cb12-59"><a href="#cb12-59" aria-hidden="true" tabindex="-1"></a>    decode <span class="op">=</span> <span class="kw">lambda</span> l: <span class="st">''</span>.join([itos[i] <span class="cf">for</span> i <span class="kw">in</span> l])</span>
<span id="cb12-60"><a href="#cb12-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-61"><a href="#cb12-61" aria-hidden="true" tabindex="-1"></a><span class="co"># encode the beginning of the prompt</span></span>
<span id="cb12-62"><a href="#cb12-62" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> start.startswith(<span class="st">'FILE:'</span>):</span>
<span id="cb12-63"><a href="#cb12-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(start[<span class="dv">5</span>:], <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> f:</span>
<span id="cb12-64"><a href="#cb12-64" aria-hidden="true" tabindex="-1"></a>        start <span class="op">=</span> f.read()</span>
<span id="cb12-65"><a href="#cb12-65" aria-hidden="true" tabindex="-1"></a>start_ids <span class="op">=</span> encode(start)</span>
<span id="cb12-66"><a href="#cb12-66" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> (torch.tensor(start_ids, dtype<span class="op">=</span>torch.<span class="bu">long</span>, device<span class="op">=</span>device)[<span class="va">None</span>, ...])</span>
<span id="cb12-67"><a href="#cb12-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-68"><a href="#cb12-68" aria-hidden="true" tabindex="-1"></a><span class="co"># run generation</span></span>
<span id="cb12-69"><a href="#cb12-69" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb12-70"><a href="#cb12-70" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> ctx:</span>
<span id="cb12-71"><a href="#cb12-71" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(num_samples):</span>
<span id="cb12-72"><a href="#cb12-72" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> model.generate(x, max_new_tokens, temperature<span class="op">=</span>temperature, top_k<span class="op">=</span>top_k)</span>
<span id="cb12-73"><a href="#cb12-73" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(decode(y[<span class="dv">0</span>].tolist()))</span>
<span id="cb12-74"><a href="#cb12-74" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">'---------------'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And voila, it looks like our model is generating DNA sequence! But, that begs the question, how do we evaluate whether our model is actually performing well? With natural language, it’s pretty easy to tell whether an output makes sense or not, but it is not so simple with DNA. Therefore, let’s talk about the process of fine-tuning our model for more specific tasks.</p>
<p><strong>Note for those interested</strong>: You may also notice some uppercase and lowercase sequences. This is expected, as in our original file any soft-masked bases (i.e.&nbsp;lower-confidence segments) are lowercase rather than uppercase. The fact that our model is not mixing lower and uppercase bases is a good sign, as these should not be mixed based on sequencing assembly techniques.</p>
<section id="questions-1" class="level4">
<h4 class="anchored" data-anchor-id="questions-1">Questions</h4>
<ol type="1">
<li>As mentioned above, our model is using “causal” self-attention, meaning that it is only paying attention to the ~300 tokens/base pairs that come before the current one. Does this align with how DNA interacts in a biological sense? Why or why not?</li>
<li>What effect do you think adding more attention layers/heads to our model would have? What about training for more epochs? Are there any downsides to expanding our model in this way?</li>
<li>What is the difference between the Query, Key, and Value matrices?</li>
</ol>
</section>
</section>
<section id="fine-tuning-promoter-classification" class="level2">
<h2 class="anchored" data-anchor-id="fine-tuning-promoter-classification">Fine-Tuning (Promoter Classification)</h2>
<p>Now that we have our gLM up and running, let’s try applying it to two different problems. First, let’s look at a promoter classification problem pulled from the <a href="https://github.com/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling.ipynb">Nucleotide Transformer training notebook</a>. In short, this dataset is made up of 300 bp sequences, each labelled as 1 for promoter or 0 for non-promoter. Let’s try to train our language model to separate these classes!</p>
<p>Note: This data was pulled from the <a href="https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2019.00286/full">DeePromoter</a> paper, as cited below:</p>
<p>Oubounyt, M., Louadi, Z., Tayara, H., &amp; Chong, K. T. (2019). DeePromoter: Robust Promoter Predictor Using Deep Learning. Frontiers in Genetics, 10. https://doi.org/10.3389/fgene.2019.00286</p>
<p>First, let’s actually load in the data:</p>
<div id="179a29db" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the promoter dataset from the InstaDeep Hugging Face resources</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>dataset_name <span class="op">=</span> <span class="st">"promoter_all"</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>train_dataset_promoter <span class="op">=</span> load_dataset(</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">"InstaDeepAI/nucleotide_transformer_downstream_tasks"</span>,</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>        dataset_name,</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        split<span class="op">=</span><span class="st">"train"</span>,</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        streaming<span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        trust_remote_code<span class="op">=</span><span class="va">True</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>test_dataset_promoter <span class="op">=</span> load_dataset(</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">"InstaDeepAI/nucleotide_transformer_downstream_tasks"</span>,</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>        dataset_name,</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        split<span class="op">=</span><span class="st">"test"</span>,</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        streaming<span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>        trust_remote_code<span class="op">=</span><span class="va">True</span></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>num_labels_promoter <span class="op">=</span> <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, let’s separate out the labels from the actual DNA sequences, and separate 5% of our training data to use as a validation set (the test set is already separated).</p>
<div id="6d931140" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get training data</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>train_sequences_promoter <span class="op">=</span> train_dataset_promoter[<span class="st">'sequence'</span>]</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>train_labels_promoter <span class="op">=</span> train_dataset_promoter[<span class="st">'label'</span>]</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into a training and a validation dataset</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>train_sequences_promoter, validation_sequences_promoter, train_labels_promoter, validation_labels_promoter <span class="op">=</span> train_test_split(train_sequences_promoter,</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>                                                                              train_labels_promoter, test_size<span class="op">=</span><span class="fl">0.05</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Get test data</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>test_sequences_promoter <span class="op">=</span> test_dataset_promoter[<span class="st">'sequence'</span>]</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>test_labels_promoter <span class="op">=</span> test_dataset_promoter[<span class="st">'label'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, let’s take a quick look at what one of these examples looks like:</p>
<div id="36fb447f" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>idx_sequence <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>sequence, label <span class="op">=</span> train_sequences_promoter[idx_sequence], train_labels_promoter[idx_sequence]</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The DNA sequence is </span><span class="sc">{</span>sequence<span class="sc">}</span><span class="ss">."</span>)</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Its associated label is label </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">."</span>)</span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>idx_TATA <span class="op">=</span> sequence.find(<span class="st">"TATA"</span>)</span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"This promoter is a TATA promoter, as the TATA motif is present at the </span><span class="sc">{</span>idx_TATA<span class="sc">}</span><span class="ss">st nucleotide."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that we have all of our datasets, it is time to tokenize them using the same mapping that we created for our language model. first, we load them into a Dataset object so we can work with them more easily:</p>
<div id="0b550077" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Promoter dataset</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>ds_train_promoter <span class="op">=</span> Dataset.from_dict({<span class="st">"data"</span>: train_sequences_promoter,<span class="st">'labels'</span>:train_labels_promoter})</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>ds_validation_promoter <span class="op">=</span> Dataset.from_dict({<span class="st">"data"</span>: validation_sequences_promoter,<span class="st">'labels'</span>:validation_labels_promoter})</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>ds_test_promoter <span class="op">=</span> Dataset.from_dict({<span class="st">"data"</span>: test_sequences_promoter,<span class="st">'labels'</span>:test_labels_promoter})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we do the actual tokenization. However, remember that our previous encoding function could only handle a single example at a time (since our initial data file was one long string). We define a new tokenizing function below to use the same process, but apply it to multiple separate sequences:</p>
<div id="5c0e5e5a" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_function(examples):</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>  outputs <span class="op">=</span> np.empty((<span class="dv">0</span>, <span class="dv">300</span>), dtype<span class="op">=</span><span class="st">'int16'</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> example <span class="kw">in</span> examples[<span class="st">"data"</span>]:</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> np.vstack([outputs, encode(example)])</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> {</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>      <span class="st">'input_ids'</span>: outputs,</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>      <span class="st">'attention_mask'</span>: [<span class="dv">1</span>] <span class="op">*</span> <span class="bu">len</span>(outputs),</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a>tokenized_datasets_train_promoter <span class="op">=</span> ds_train_promoter.<span class="bu">map</span>(</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a>    tokenize_function,</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a>    batched<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a>    remove_columns<span class="op">=</span>[<span class="st">'data'</span>],</span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a>tokenized_datasets_validation_promoter <span class="op">=</span> ds_validation_promoter.<span class="bu">map</span>(</span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a>    tokenize_function,</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a>    batched<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    remove_columns<span class="op">=</span>[<span class="st">'data'</span>],</span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>tokenized_datasets_test_promoter <span class="op">=</span> ds_test_promoter.<span class="bu">map</span>(</span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    tokenize_function,</span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>    batched<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>    remove_columns<span class="op">=</span>[<span class="st">'data'</span>],</span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that we’ve tokenized the data, let’s load it into Tensor format (which Pytorch uses in its models). Then, we can create a DataLoader, which will automatically create batches of our data and put them into a format which our model can read easily:</p>
<div id="5b6a0d6e" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> TensorDataset(torch.tensor(tokenized_datasets_train_promoter[<span class="st">'input_ids'</span>], device<span class="op">=</span>device),</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>                              torch.tensor(tokenized_datasets_train_promoter[<span class="st">'labels'</span>], device<span class="op">=</span>device))</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(train_dataset, shuffle<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=</span><span class="dv">64</span>)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>validation_dataset <span class="op">=</span> TensorDataset(torch.tensor(tokenized_datasets_validation_promoter[<span class="st">'input_ids'</span>], device<span class="op">=</span>device),</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>                              torch.tensor(tokenized_datasets_validation_promoter[<span class="st">'labels'</span>], device<span class="op">=</span>device))</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>validation_dataloader <span class="op">=</span> DataLoader(validation_dataset, shuffle<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=</span><span class="dv">64</span>)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> TensorDataset(torch.tensor(tokenized_datasets_test_promoter[<span class="st">'input_ids'</span>], device<span class="op">=</span>device),</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>                              torch.tensor(tokenized_datasets_test_promoter[<span class="st">'labels'</span>], device<span class="op">=</span>device))</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>test_dataloader <span class="op">=</span> DataLoader(test_dataset, shuffle<span class="op">=</span><span class="va">False</span>, batch_size<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In the last step before training, we have to actually define a slightly new type of model. Essentially all we need to do here is add one more linear layer to our old model, which takes the original output and projects it into a two-dimensional output. This means that the “new” model will output the probability of the example belonging to each class. To do this, we just create a new model which pulls in our language model and adds the new layer on top:</p>
<div id="51cb570b" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ClassificationModel(nn.Module):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, base_model, num_labels):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">super</span>(ClassificationModel, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.orig_model <span class="op">=</span> base_model</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.classifier <span class="op">=</span> nn.Linear(base_model.config.vocab_size, num_labels)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.num_labels <span class="op">=</span> num_labels</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> forward(<span class="va">self</span>, input_ids, labels<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> <span class="va">self</span>.orig_model(input_ids)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> <span class="va">self</span>.classifier(outputs[<span class="dv">0</span>][:, <span class="dv">0</span>, :])</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>    attn_weights <span class="op">=</span> outputs[<span class="dv">2</span>]</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> labels <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>      loss_fct <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>      loss <span class="op">=</span> loss_fct(logits.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.num_labels), labels.view(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (logits, loss, attn_weights) <span class="cf">if</span> loss <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> (logits, attn_weights)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that we have our new model defined, let’s pull in the model we trained previously, and use it to create this new 2-label classification model:</p>
<div id="ba1d6970" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------------------------</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>init_from <span class="op">=</span> <span class="st">'resume'</span> <span class="co"># either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>out_dir <span class="op">=</span> <span class="st">'/root/data/out'</span> <span class="co"># ignored if init_from is not 'resume'</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">1337</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="co"># examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>dtype <span class="op">=</span> <span class="st">'bfloat16'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="kw">and</span> torch.cuda.is_bf16_supported() <span class="cf">else</span> <span class="st">'float16'</span> <span class="co"># 'float32' or 'bfloat16' or 'float16'</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="bu">compile</span> <span class="op">=</span> <span class="va">True</span> <span class="co"># use PyTorch 2.0 to compile the model to be faster</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------------------------</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>torch.cuda.init()</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(seed)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>torch.cuda.manual_seed(seed)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>torch.backends.cuda.matmul.allow_tf32 <span class="op">=</span> <span class="va">True</span> <span class="co"># allow tf32 on matmul</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.allow_tf32 <span class="op">=</span> <span class="va">True</span> <span class="co"># allow tf32 on cudnn</span></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>device_type <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> <span class="st">'cuda'</span> <span class="kw">in</span> device <span class="cf">else</span> <span class="st">'cpu'</span> <span class="co"># for later use in torch.autocast</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>ptdtype <span class="op">=</span> {<span class="st">'float32'</span>: torch.float32, <span class="st">'bfloat16'</span>: torch.bfloat16, <span class="st">'float16'</span>: torch.float16}[<span class="st">'float16'</span>]</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a>ctx <span class="op">=</span> nullcontext() <span class="cf">if</span> device_type <span class="op">==</span> <span class="st">'cpu'</span> <span class="cf">else</span> torch.amp.autocast(device_type<span class="op">=</span>device_type, dtype<span class="op">=</span>ptdtype)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="co"># model</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> init_from <span class="op">==</span> <span class="st">'resume'</span>:</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># init from a model saved in a specific directory</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>    ckpt_path <span class="op">=</span> os.path.join(out_dir, <span class="st">'ckpt.pt'</span>)</span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>    checkpoint <span class="op">=</span> torch.load(ckpt_path, map_location<span class="op">=</span>device)</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>    gptconf <span class="op">=</span> GPTConfig(<span class="op">**</span>checkpoint[<span class="st">'model_args'</span>])</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> GPT(gptconf)</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a>    state_dict <span class="op">=</span> checkpoint[<span class="st">'model'</span>]</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>    unwanted_prefix <span class="op">=</span> <span class="st">'_orig_mod.'</span></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k,v <span class="kw">in</span> <span class="bu">list</span>(state_dict.items()):</span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> k.startswith(unwanted_prefix):</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>            state_dict[k[<span class="bu">len</span>(unwanted_prefix):]] <span class="op">=</span> state_dict.pop(k)</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(state_dict)</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a>new_model <span class="op">=</span> ClassificationModel(base_model<span class="op">=</span>model, num_labels<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a>new_model.<span class="bu">eval</span>()</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>new_model.to(device)</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">compile</span>:</span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a>    new_model <span class="op">=</span> torch.<span class="bu">compile</span>(new_model) <span class="co"># requires PyTorch 2.0 (optional)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, go ahead and run the next block of code to train our new model. Again, this will take about 10 minutes, so go to the next paragraph for a description of what our training is actually doing.</p>
<div id="7ed7e66c" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(new_model.parameters(), lr<span class="op">=</span><span class="fl">2e-5</span>)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>num_epochs<span class="op">=</span><span class="dv">2</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>total_steps <span class="op">=</span> <span class="bu">len</span>(train_dataloader) <span class="op">*</span> num_epochs</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_f1(predictions, true_labels):</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>    true_pos <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>    false_pos <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    false_neg <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    true_neg <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> np.arange(<span class="bu">len</span>(predictions)) :</span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> predictions[i] <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> true_labels[i] <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>            true_neg <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> predictions[i] <span class="op">==</span> <span class="dv">1</span> <span class="kw">and</span> true_labels[i] <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>            false_pos <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> predictions[i] <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> true_labels[i] <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>            false_neg <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> predictions[i] <span class="op">==</span> <span class="dv">1</span> <span class="kw">and</span> true_labels[i] <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>            true_pos <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>    precision <span class="op">=</span> true_pos <span class="op">/</span> (true_pos <span class="op">+</span> false_pos) <span class="cf">if</span> true_pos <span class="op">+</span> false_pos <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>    recall <span class="op">=</span> true_pos <span class="op">/</span> (true_pos <span class="op">+</span> false_neg)</span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>    F1 <span class="op">=</span> (<span class="dv">2</span><span class="op">*</span>precision<span class="op">*</span>recall <span class="op">/</span> (precision <span class="op">+</span> recall))</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> F1</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>val_predictions <span class="op">=</span> []</span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>val_true_labels <span class="op">=</span> []</span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a><span class="co">#Training Loop</span></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>loss_lst <span class="op">=</span> []</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>val_loss_lst <span class="op">=</span> []</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>f1_scores <span class="op">=</span> []</span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>new_model.train()</span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>    i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> train_dataloader:</span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>        input_ids, labels <span class="op">=</span> batch</span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass</span></span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> new_model(input_ids, labels<span class="op">=</span>labels)</span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> outputs[<span class="dv">1</span>]</span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a>        loss_lst.append(loss.item())</span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward pass and optimization</span></span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a>        i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">99</span>:</span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a>            new_model.<span class="bu">eval</span>()</span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> val_batch <span class="kw">in</span> validation_dataloader:</span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a>                    val_input_ids, val_labels <span class="op">=</span> val_batch</span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a>                    optimizer.zero_grad()</span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a>                    val_outputs <span class="op">=</span> new_model(val_input_ids, labels<span class="op">=</span>val_labels)</span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a>                    val_loss <span class="op">=</span> val_outputs[<span class="dv">1</span>]</span>
<span id="cb21-55"><a href="#cb21-55" aria-hidden="true" tabindex="-1"></a>                    val_loss_lst.append(val_loss.item())</span>
<span id="cb21-56"><a href="#cb21-56" aria-hidden="true" tabindex="-1"></a>                    val_logits <span class="op">=</span> val_outputs[<span class="dv">0</span>]  <span class="co"># Get the logits</span></span>
<span id="cb21-57"><a href="#cb21-57" aria-hidden="true" tabindex="-1"></a>                    val_predictions.append(val_logits)</span>
<span id="cb21-58"><a href="#cb21-58" aria-hidden="true" tabindex="-1"></a>                    val_true_labels.append(val_labels)</span>
<span id="cb21-59"><a href="#cb21-59" aria-hidden="true" tabindex="-1"></a>                val_predictions <span class="op">=</span> torch.cat(val_predictions, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb21-60"><a href="#cb21-60" aria-hidden="true" tabindex="-1"></a>                val_true_labels <span class="op">=</span> torch.cat(val_true_labels, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb21-61"><a href="#cb21-61" aria-hidden="true" tabindex="-1"></a>                val_predictions <span class="op">=</span> torch.argmax(val_predictions, dim<span class="op">=</span><span class="dv">1</span>).tolist()</span>
<span id="cb21-62"><a href="#cb21-62" aria-hidden="true" tabindex="-1"></a>                val_true_labels <span class="op">=</span> val_true_labels.tolist()</span>
<span id="cb21-63"><a href="#cb21-63" aria-hidden="true" tabindex="-1"></a>                f1_scores.append(calc_f1(val_predictions, val_true_labels))</span>
<span id="cb21-64"><a href="#cb21-64" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, Batch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>np<span class="sc">.</span>mean(loss_lst)<span class="sc">}</span><span class="ss">, Val Loss: </span><span class="sc">{</span>np<span class="sc">.</span>mean(val_loss_lst)<span class="sc">}</span><span class="ss">, Val F1: </span><span class="sc">{</span>calc_f1(val_predictions, val_true_labels)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb21-65"><a href="#cb21-65" aria-hidden="true" tabindex="-1"></a>            loss_lst <span class="op">=</span> []</span>
<span id="cb21-66"><a href="#cb21-66" aria-hidden="true" tabindex="-1"></a>            val_loss_lst <span class="op">=</span> []</span>
<span id="cb21-67"><a href="#cb21-67" aria-hidden="true" tabindex="-1"></a>            new_model.train()</span>
<span id="cb21-68"><a href="#cb21-68" aria-hidden="true" tabindex="-1"></a>            val_predictions <span class="op">=</span> []</span>
<span id="cb21-69"><a href="#cb21-69" aria-hidden="true" tabindex="-1"></a>            val_true_labels <span class="op">=</span> []</span>
<span id="cb21-70"><a href="#cb21-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-71"><a href="#cb21-71" aria-hidden="true" tabindex="-1"></a>plt.plot(f1_scores, label<span class="op">=</span><span class="st">'Validation F1 score'</span>)</span>
<span id="cb21-72"><a href="#cb21-72" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Validation F1 score for promoter prediction"</span>)</span>
<span id="cb21-73"><a href="#cb21-73" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Batch"</span>)</span>
<span id="cb21-74"><a href="#cb21-74" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"F1 score"</span>)</span>
<span id="cb21-75"><a href="#cb21-75" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb21-76"><a href="#cb21-76" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>With this new model, we want to be able to classify promoter sequences. But, how do we do this when our base model has already been trained on a different task? The answer lies in a process called <strong>fine-tuning</strong>. This is where you take a pre-trained model, and train it again to perform better on a specific task. In this case, we are relying on the fact that our model already understands the intrinsic relationships in genomic sequences well, and are just shifting its goal. Rather than predicting future genomic sequence, we want it to apply its “knowledge” of sequences to predict whether they are promoters or not.</p>
<p>For language models, oftentimes fine-tuning tasks are much simpler than the initial learning process (and we assume that the layers of our model other than the new classification layer are already trained), so we usually use a much smaller learning rate (in this case <span class="math inline">2e^{-5}</span> and fewer training iterations. This is not always true, but it is a good rule of thumb. There are many options you have to optimize fine-tuning, including freezing various layers so that their weights are not updated, using different learning rates for different layers, and much more. But, for the purposes of this notebook, we are simply going to fine-tune the entire model.</p>
<p>Our training loop is very similar here, but even simpler than our initial training:</p>
<ol type="1">
<li>Take a batch from the training dataloader and pass it through the model to get loss</li>
<li>Do the backward pass, and have our optimizer adjust weights according to each parameter’s gradient and the learning rate</li>
<li>Repeat for all batches</li>
<li>Every 100 batches, run all validation samples through the model and calculate an average loss and F1 score (a metric for classification performance) for tracking purposes</li>
</ol>
<p>Now that we’ve trained our model, let’s run the test data through it and see how it performs. While we do so, let’s also look at the attention weights for a specific test sample, and try to get some idea of what our model is doing:</p>
<div id="4587f7b7" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>new_model.<span class="bu">eval</span>()  <span class="co"># Set the model to evaluation mode</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> []</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>true_labels <span class="op">=</span> []</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():  <span class="co"># Disable gradient calculation for inference</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> test_dataloader:</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        input_ids, labels <span class="op">=</span> batch</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> new_model(input_ids)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> outputs[<span class="dv">0</span>]  <span class="co"># Get the logits</span></span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>        predictions.append(torch.argmax(logits).item())</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>        true_labels.append(labels.item())</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">==</span> <span class="dv">3000</span>:</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>            l <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>            fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> np.arange(<span class="dv">2</span>)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> np.arange(<span class="dv">2</span>)</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> x:</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> k <span class="kw">in</span> y:</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>                    axs[j,k].imshow(outputs[<span class="dv">1</span>][<span class="dv">0</span>][l].cpu().detach().numpy(), cmap<span class="op">=</span><span class="st">'gist_stern'</span>, interpolation<span class="op">=</span><span class="st">'nearest'</span>)</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>                    l <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>            fig.set_figwidth(<span class="dv">10</span>)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>            fig.set_figheight(<span class="dv">6</span>)</span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a>            fig.suptitle(<span class="st">"Attention weights of Each Head in Layer 0"</span>)</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>            plt.show()</span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>            l <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>            fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> np.arange(<span class="dv">2</span>)</span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> np.arange(<span class="dv">2</span>)</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> x:</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> k <span class="kw">in</span> y:</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>                    axs[j,k].imshow(torch.mean(outputs[<span class="dv">1</span>][l], dim<span class="op">=</span><span class="dv">0</span>).cpu().detach().numpy(), cmap<span class="op">=</span><span class="st">'gist_stern'</span>, interpolation<span class="op">=</span><span class="st">'nearest'</span>)</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>                    l <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>            fig.set_figwidth(<span class="dv">10</span>)</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>            fig.set_figheight(<span class="dv">6</span>)</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>            fig.suptitle(<span class="st">"Attention Weights of Each Layer Averaged Across Heads"</span>)</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>            plt.show()</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>        i <span class="op">+=</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="259a290d" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="co">#print(len(predictions))</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> np.arange(<span class="bu">len</span>(predictions)) :</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> predictions[i] <span class="op">==</span> true_labels[i]:</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>        correct <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy:</span><span class="sc">{</span><span class="bu">int</span>(correct <span class="op">/</span> <span class="bu">len</span>(predictions) <span class="op">*</span> <span class="dv">100</span>)<span class="sc">:,}</span><span class="ss">%"</span>)</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>true_pos <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>false_pos <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>false_neg <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>true_neg <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> np.arange(<span class="bu">len</span>(predictions)) :</span>
<span id="cb23-13"><a href="#cb23-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> predictions[i] <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> true_labels[i] <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb23-14"><a href="#cb23-14" aria-hidden="true" tabindex="-1"></a>        true_neg <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb23-15"><a href="#cb23-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> predictions[i] <span class="op">==</span> <span class="dv">1</span> <span class="kw">and</span> true_labels[i] <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb23-16"><a href="#cb23-16" aria-hidden="true" tabindex="-1"></a>        false_pos <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb23-17"><a href="#cb23-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> predictions[i] <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> true_labels[i] <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb23-18"><a href="#cb23-18" aria-hidden="true" tabindex="-1"></a>        false_neg <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb23-19"><a href="#cb23-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> predictions[i] <span class="op">==</span> <span class="dv">1</span> <span class="kw">and</span> true_labels[i] <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb23-20"><a href="#cb23-20" aria-hidden="true" tabindex="-1"></a>        true_pos <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb23-21"><a href="#cb23-21" aria-hidden="true" tabindex="-1"></a>precision <span class="op">=</span> true_pos <span class="op">/</span> (true_pos <span class="op">+</span> false_pos)</span>
<span id="cb23-22"><a href="#cb23-22" aria-hidden="true" tabindex="-1"></a>recall <span class="op">=</span> true_pos <span class="op">/</span> (true_pos <span class="op">+</span> false_neg)</span>
<span id="cb23-23"><a href="#cb23-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Precision: </span><span class="sc">{</span><span class="bu">round</span>(precision, <span class="dv">2</span>)<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb23-24"><a href="#cb23-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Recall: </span><span class="sc">{</span><span class="bu">round</span>(recall, <span class="dv">2</span>)<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb23-25"><a href="#cb23-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-26"><a href="#cb23-26" aria-hidden="true" tabindex="-1"></a>F1 <span class="op">=</span> calc_f1(predictions, true_labels)</span>
<span id="cb23-27"><a href="#cb23-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F1 Score: </span><span class="sc">{</span><span class="bu">round</span>(F1, <span class="dv">2</span>)<span class="sc">:,}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Wow, it looks like our model actually performs pretty well! Especially for a model that’s gone through a total of 20 minutes of training on a single GPU. If we expanded our model or trained it over more data it would improve even further, but these are still good results.</p>
<p>Taking a look at the Attention graphs above, we can get some insight into what our model thinks is important for the given sample. The way to read these heatmaps is that the Queries are on the y-axis, and the Keys are on the x-axis. This is all mapped back to the original input, so we essentially have a set of base-by-base relationship heatmaps! For example, a vertical red line indicates that many Query positions saw that Key position as important in the classification task.</p>
<p>The first set is a breakdown of all of the heads of our first attention layer. As I mentioned above, we would expect each head to learn something slightly different about our input, and it appears that this is the case! For example, the second head was very focused on the relationships between the bases directly surrounding the query (which is why we have a bright diagonal line), while the 3rd and 4th heads had much more dispersed attention weights.</p>
<p>This pattern becomes even more apparent as we move to the second set of heatmaps, where we average the attention weights for all heads in each layer (to give an approximate idea of what each full layer learns about the input). At this level, we see some interesting patterns emerge. For example, multiple layers have strong attention weights at the position around 160-170 bp into the sequence, which means that it likely played an important role in deciding this sequence’s classification. Each head and layer learned some unique things about the sequence, so take some time to try and understand what these heatmaps are showing!</p>
<section id="questions-2" class="level4">
<h4 class="anchored" data-anchor-id="questions-2">Questions</h4>
<ol type="1">
<li>Promoters are known for having very characteristic structures of DNA sequence (i.e.&nbsp;TATA boxes as I mentioned above). How could this influence our classification accuracy (i.e.&nbsp;make the classification problem easier or harder)? Why?</li>
<li>You can see that our validation F1 score metric does not uniformly increase across the training process. How can you explain this?</li>
<li>Provide your interpretation of the graphs of the Attention weights above. Are there any locations in the input sequence that appear to be highly important (i.e.&nbsp;have a lot of “attention” on them)?</li>
</ol>
</section>
</section>
<section id="fine-tuning-enhancer-classification" class="level2">
<h2 class="anchored" data-anchor-id="fine-tuning-enhancer-classification">Fine-Tuning (Enhancer Classification)</h2>
<p>Now that we’ve introduced the idea of fine-tuning and have a simple example, let’s move on to a more complicated task. We have another set of data from the <a href="https://github.com/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling.ipynb">Nucleotide Transformer training notebook</a> which is made up of 200-bp sequences that are either labeled as strong enhancers, weak enhancers, or non-enhancers. This is a more complicated task not only because it is multi-class classification, enhancers also generally have less “characteristic” genomic structure compared to promoters. This means that (in theory) our model will have to understand more subtle relationships within each sequence to classify them correctly. This data comes from the below paper:</p>
<p>Geng, Q., Yang, R., &amp; Zhang, L. (2022). A deep learning framework for enhancer prediction using word embedding and sequence generation. Biophysical Chemistry, 286, 106822. https://doi.org/10.1016/j.bpc.2022.106822</p>
<p>To start, we are going to load the data and tokenize it the exact same way we did previously:</p>
<div id="45e5c13d" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the enhancer dataset from the InstaDeep Hugging Face ressources</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>dataset_name <span class="op">=</span> <span class="st">"enhancers_types"</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>train_dataset_enhancers <span class="op">=</span> load_dataset(</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>        <span class="st">"InstaDeepAI/nucleotide_transformer_downstream_tasks"</span>,</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>        dataset_name,</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>        split<span class="op">=</span><span class="st">"train"</span>,</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>        streaming<span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>        trust_remote_code<span class="op">=</span><span class="va">True</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>test_dataset_enhancers <span class="op">=</span> load_dataset(</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>        <span class="st">"InstaDeepAI/nucleotide_transformer_downstream_tasks"</span>,</span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>        dataset_name,</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>        split<span class="op">=</span><span class="st">"test"</span>,</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a>        streaming<span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>        trust_remote_code<span class="op">=</span><span class="va">True</span></span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>num_labels_enhancer <span class="op">=</span> <span class="dv">3</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="dbd1ebf8" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get training data</span></span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>train_sequences_enhancers <span class="op">=</span> train_dataset_enhancers[<span class="st">'sequence'</span>]</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>train_labels_enhancers <span class="op">=</span> train_dataset_enhancers[<span class="st">'label'</span>]</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into a training and a validation dataset</span></span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>train_sequences_enhancers, validation_sequences_enhancers, train_labels_enhancers, validation_labels_enhancers <span class="op">=</span> train_test_split(train_sequences_enhancers,</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>                                                                              train_labels_enhancers, test_size<span class="op">=</span><span class="fl">0.10</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Get test data</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>test_sequences_enhancers <span class="op">=</span> test_dataset_enhancers[<span class="st">'sequence'</span>]</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>test_labels_enhancers <span class="op">=</span> test_dataset_enhancers[<span class="st">'label'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="c6ebecb9" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>ds_train_enhancers <span class="op">=</span> Dataset.from_dict({<span class="st">"data"</span>: train_sequences_enhancers,<span class="st">'labels'</span>:train_labels_enhancers})</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>ds_validation_enhancers <span class="op">=</span> Dataset.from_dict({<span class="st">"data"</span>: validation_sequences_enhancers,<span class="st">'labels'</span>:validation_labels_enhancers})</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>ds_test_enhancers <span class="op">=</span> Dataset.from_dict({<span class="st">"data"</span>: test_sequences_enhancers,<span class="st">'labels'</span>:test_labels_enhancers})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="a6e1ad71" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_function(examples):</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>  min_length <span class="op">=</span> <span class="bu">min</span>(<span class="bu">len</span>(i) <span class="cf">for</span> i <span class="kw">in</span> examples[<span class="st">'data'</span>])</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>  outputs <span class="op">=</span> np.empty((<span class="dv">0</span>, min_length), dtype<span class="op">=</span><span class="st">'int16'</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> example <span class="kw">in</span> examples[<span class="st">"data"</span>]:</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> np.vstack([outputs, encode(example[:min_length])])</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> {</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>      <span class="st">'input_ids'</span>: outputs,</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>      <span class="st">'attention_mask'</span>: [<span class="dv">1</span>] <span class="op">*</span> <span class="bu">len</span>(outputs),</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>tokenized_datasets_train_enhancer <span class="op">=</span> ds_train_enhancers.<span class="bu">map</span>(</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    tokenize_function,</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>    batched<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a>    remove_columns<span class="op">=</span>[<span class="st">'data'</span>],</span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>tokenized_datasets_validation_enhancer <span class="op">=</span> ds_validation_enhancers.<span class="bu">map</span>(</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    tokenize_function,</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    batched<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>    remove_columns<span class="op">=</span>[<span class="st">'data'</span>],</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>tokenized_datasets_test_enhancer <span class="op">=</span> ds_test_enhancers.<span class="bu">map</span>(</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>    tokenize_function,</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>    batched<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>    remove_columns<span class="op">=</span>[<span class="st">'data'</span>],</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> TensorDataset(torch.tensor(tokenized_datasets_train_enhancer[<span class="st">'input_ids'</span>], device<span class="op">=</span>device),</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a>                              torch.tensor(tokenized_datasets_train_enhancer[<span class="st">'labels'</span>], device<span class="op">=</span>device))</span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(train_dataset, shuffle<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=</span><span class="dv">64</span>)</span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>validation_dataset <span class="op">=</span> TensorDataset(torch.tensor(tokenized_datasets_validation_enhancer[<span class="st">'input_ids'</span>], device<span class="op">=</span>device),</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>                              torch.tensor(tokenized_datasets_validation_enhancer[<span class="st">'labels'</span>], device<span class="op">=</span>device))</span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a>validation_dataloader <span class="op">=</span> DataLoader(validation_dataset, shuffle<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> TensorDataset(torch.tensor(tokenized_datasets_test_enhancer[<span class="st">'input_ids'</span>], device<span class="op">=</span>device),</span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a>                              torch.tensor(tokenized_datasets_test_enhancer[<span class="st">'labels'</span>], device<span class="op">=</span>device))</span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a>test_dataloader <span class="op">=</span> DataLoader(test_dataset, shuffle<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, let’s load back in our <strong>original</strong> gLM, i.e.&nbsp;not the one we fine-tuned to classify promoters. This time, we need it to classify on 3 labels rather than 2, so it is simpler to just recreate a new version and fine-tune it from scratch.</p>
<div id="673293d0" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------------------------</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>init_from <span class="op">=</span> <span class="st">'resume'</span> <span class="co"># either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')</span></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>out_dir <span class="op">=</span> <span class="st">'/root/data/out'</span> <span class="co"># ignored if init_from is not 'resume'</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">1337</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="co"># examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.</span></span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>dtype <span class="op">=</span> <span class="st">'bfloat16'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="kw">and</span> torch.cuda.is_bf16_supported() <span class="cf">else</span> <span class="st">'float16'</span> <span class="co"># 'float32' or 'bfloat16' or 'float16'</span></span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a><span class="bu">compile</span> <span class="op">=</span> <span class="va">True</span> <span class="co"># use PyTorch 2.0 to compile the model to be faster</span></span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------------------------</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>torch.cuda.init()</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(seed)</span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>torch.cuda.manual_seed(seed)</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>torch.backends.cuda.matmul.allow_tf32 <span class="op">=</span> <span class="va">True</span> <span class="co"># allow tf32 on matmul</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.allow_tf32 <span class="op">=</span> <span class="va">True</span> <span class="co"># allow tf32 on cudnn</span></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>device_type <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> <span class="st">'cuda'</span> <span class="kw">in</span> device <span class="cf">else</span> <span class="st">'cpu'</span> <span class="co"># for later use in torch.autocast</span></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>ptdtype <span class="op">=</span> {<span class="st">'float32'</span>: torch.float32, <span class="st">'bfloat16'</span>: torch.bfloat16, <span class="st">'float16'</span>: torch.float16}[<span class="st">'float16'</span>]</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>ctx <span class="op">=</span> nullcontext() <span class="cf">if</span> device_type <span class="op">==</span> <span class="st">'cpu'</span> <span class="cf">else</span> torch.amp.autocast(device_type<span class="op">=</span>device_type, dtype<span class="op">=</span>ptdtype)</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a><span class="co"># model</span></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> init_from <span class="op">==</span> <span class="st">'resume'</span>:</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># init from a model saved in a specific directory</span></span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>    ckpt_path <span class="op">=</span> os.path.join(out_dir, <span class="st">'ckpt.pt'</span>)</span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>    checkpoint <span class="op">=</span> torch.load(ckpt_path, map_location<span class="op">=</span>device)</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>    gptconf <span class="op">=</span> GPTConfig(<span class="op">**</span>checkpoint[<span class="st">'model_args'</span>])</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> GPT(gptconf)</span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>    state_dict <span class="op">=</span> checkpoint[<span class="st">'model'</span>]</span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>    unwanted_prefix <span class="op">=</span> <span class="st">'_orig_mod.'</span></span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k,v <span class="kw">in</span> <span class="bu">list</span>(state_dict.items()):</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> k.startswith(unwanted_prefix):</span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>            state_dict[k[<span class="bu">len</span>(unwanted_prefix):]] <span class="op">=</span> state_dict.pop(k)</span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(state_dict)</span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>new_model <span class="op">=</span> ClassificationModel(base_model<span class="op">=</span>model, num_labels<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a>new_model.<span class="bu">eval</span>()</span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a>new_model.to(device)</span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">compile</span>:</span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a>    new_model <span class="op">=</span> torch.<span class="bu">compile</span>(new_model) <span class="co"># requires PyTorch 2.0 (optional)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now, let’s run through the same training loop. You may notice that we have greatly increased the number of epochs we are training for. This is partially due to the fact that the enhancer dataset is much smaller than the promoter dataset (so we need more epochs to get the equivalent number of training batches), but the task is also more complex, so it benefits our model to have more training iterations in general.</p>
<div id="66ddfd24" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(new_model.parameters(), lr<span class="op">=</span><span class="fl">2e-5</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>num_epochs<span class="op">=</span><span class="dv">10</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>total_steps <span class="op">=</span> <span class="bu">len</span>(train_dataloader) <span class="op">*</span> num_epochs</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>val_predictions <span class="op">=</span> []</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>val_true_labels <span class="op">=</span> []</span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a><span class="co">#Training Loop</span></span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>loss_lst <span class="op">=</span> []</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>val_loss_lst <span class="op">=</span> []</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>mcc_scores <span class="op">=</span> []</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>new_model.train()</span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>    i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> train_dataloader:</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>        input_ids, labels <span class="op">=</span> batch</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass</span></span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> new_model(input_ids, labels<span class="op">=</span>labels)</span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> outputs[<span class="dv">1</span>]</span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>        loss_lst.append(loss.item())</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward pass and optimization</span></span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>        i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">99</span>:</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>            new_model.<span class="bu">eval</span>()</span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> val_batch <span class="kw">in</span> validation_dataloader:</span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>                    val_input_ids, val_labels <span class="op">=</span> val_batch</span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>                    optimizer.zero_grad()</span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a>                    val_outputs <span class="op">=</span> new_model(val_input_ids, labels<span class="op">=</span>val_labels)</span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a>                    val_loss <span class="op">=</span> val_outputs[<span class="dv">1</span>]</span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a>                    val_loss_lst.append(val_loss.item())</span>
<span id="cb29-37"><a href="#cb29-37" aria-hidden="true" tabindex="-1"></a>                    val_logits <span class="op">=</span> val_outputs[<span class="dv">0</span>]  <span class="co"># Get the logits</span></span>
<span id="cb29-38"><a href="#cb29-38" aria-hidden="true" tabindex="-1"></a>                    val_predictions.append(torch.argmax(val_logits).item())</span>
<span id="cb29-39"><a href="#cb29-39" aria-hidden="true" tabindex="-1"></a>                    val_true_labels.append(val_labels.item())</span>
<span id="cb29-40"><a href="#cb29-40" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, Batch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>np<span class="sc">.</span>mean(loss_lst)<span class="sc">}</span><span class="ss">, Val Loss: </span><span class="sc">{</span>np<span class="sc">.</span>mean(val_loss_lst)<span class="sc">}</span><span class="ss">, Val MCC: </span><span class="sc">{</span>matthews_corrcoef(val_true_labels, val_predictions)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb29-41"><a href="#cb29-41" aria-hidden="true" tabindex="-1"></a>            mcc_scores.append(matthews_corrcoef(val_true_labels, val_predictions))</span>
<span id="cb29-42"><a href="#cb29-42" aria-hidden="true" tabindex="-1"></a>            loss_lst <span class="op">=</span> []</span>
<span id="cb29-43"><a href="#cb29-43" aria-hidden="true" tabindex="-1"></a>            val_loss_lst <span class="op">=</span> []</span>
<span id="cb29-44"><a href="#cb29-44" aria-hidden="true" tabindex="-1"></a>            new_model.train()</span>
<span id="cb29-45"><a href="#cb29-45" aria-hidden="true" tabindex="-1"></a>            val_predictions <span class="op">=</span> []</span>
<span id="cb29-46"><a href="#cb29-46" aria-hidden="true" tabindex="-1"></a>            val_true_labels <span class="op">=</span> []</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now that we’ve finished training, we can see that our training loss remains significantly higher for this task than our promoter prediction task, as expected. Note that we use a slightly different performance metric here as well (MCC), which gives us a single scalar value for classification performance (whereas using F1 again would not be so simple). In any case, let’s see how this model performs on the test data.</p>
<div id="3528b5f6" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>new_model.<span class="bu">eval</span>()  <span class="co"># Set the model to evaluation mode</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> []</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>true_labels <span class="op">=</span> []</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():  <span class="co"># Disable gradient calculation for inference</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> test_dataloader:</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>        input_ids, labels <span class="op">=</span> batch</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> new_model(input_ids)</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> outputs[<span class="dv">0</span>]  <span class="co"># Get the logits</span></span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>        predictions.append(torch.argmax(logits).item())</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>        true_labels.append(labels.item())</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">==</span> <span class="dv">200</span>:</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>            l <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>            fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> np.arange(<span class="dv">2</span>)</span>
<span id="cb30-18"><a href="#cb30-18" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> np.arange(<span class="dv">2</span>)</span>
<span id="cb30-19"><a href="#cb30-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> x:</span>
<span id="cb30-20"><a href="#cb30-20" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> k <span class="kw">in</span> y:</span>
<span id="cb30-21"><a href="#cb30-21" aria-hidden="true" tabindex="-1"></a>                    axs[j,k].imshow(outputs[<span class="dv">1</span>][<span class="dv">0</span>][l].cpu().detach().numpy(), cmap<span class="op">=</span><span class="st">'gist_stern'</span>, interpolation<span class="op">=</span><span class="st">'nearest'</span>)</span>
<span id="cb30-22"><a href="#cb30-22" aria-hidden="true" tabindex="-1"></a>                    l <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb30-23"><a href="#cb30-23" aria-hidden="true" tabindex="-1"></a>            fig.set_figwidth(<span class="dv">10</span>)</span>
<span id="cb30-24"><a href="#cb30-24" aria-hidden="true" tabindex="-1"></a>            fig.set_figheight(<span class="dv">6</span>)</span>
<span id="cb30-25"><a href="#cb30-25" aria-hidden="true" tabindex="-1"></a>            fig.suptitle(<span class="st">"Attention weights of Each Head in Layer 0"</span>)</span>
<span id="cb30-26"><a href="#cb30-26" aria-hidden="true" tabindex="-1"></a>            plt.show()</span>
<span id="cb30-27"><a href="#cb30-27" aria-hidden="true" tabindex="-1"></a>            l <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb30-28"><a href="#cb30-28" aria-hidden="true" tabindex="-1"></a>            fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb30-29"><a href="#cb30-29" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> np.arange(<span class="dv">2</span>)</span>
<span id="cb30-30"><a href="#cb30-30" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> np.arange(<span class="dv">2</span>)</span>
<span id="cb30-31"><a href="#cb30-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> x:</span>
<span id="cb30-32"><a href="#cb30-32" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> k <span class="kw">in</span> y:</span>
<span id="cb30-33"><a href="#cb30-33" aria-hidden="true" tabindex="-1"></a>                    axs[j,k].imshow(torch.mean(outputs[<span class="dv">1</span>][l], dim<span class="op">=</span><span class="dv">0</span>).cpu().detach().numpy(), cmap<span class="op">=</span><span class="st">'gist_stern'</span>, interpolation<span class="op">=</span><span class="st">'nearest'</span>)</span>
<span id="cb30-34"><a href="#cb30-34" aria-hidden="true" tabindex="-1"></a>                    l <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb30-35"><a href="#cb30-35" aria-hidden="true" tabindex="-1"></a>            fig.set_figwidth(<span class="dv">10</span>)</span>
<span id="cb30-36"><a href="#cb30-36" aria-hidden="true" tabindex="-1"></a>            fig.set_figheight(<span class="dv">6</span>)</span>
<span id="cb30-37"><a href="#cb30-37" aria-hidden="true" tabindex="-1"></a>            fig.suptitle(<span class="st">"Attention Weights of Each Layer Averaged Across Heads"</span>)</span>
<span id="cb30-38"><a href="#cb30-38" aria-hidden="true" tabindex="-1"></a>            plt.show()</span>
<span id="cb30-39"><a href="#cb30-39" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(labels.item())</span>
<span id="cb30-40"><a href="#cb30-40" aria-hidden="true" tabindex="-1"></a>        i <span class="op">+=</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="c5462837" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(predictions))</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> np.arange(<span class="bu">len</span>(predictions)) :</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> predictions[i] <span class="op">==</span> true_labels[i]:</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>        correct <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span><span class="bu">int</span>(correct <span class="op">/</span> <span class="bu">len</span>(predictions) <span class="op">*</span> <span class="dv">100</span>)<span class="sc">:,}</span><span class="ss">%"</span>)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-8"><a href="#cb31-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MCC: </span><span class="sc">{</span><span class="bu">round</span>(matthews_corrcoef(true_labels, predictions), <span class="dv">2</span>)<span class="sc">:,}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Oof, those numbers are a little rough, especially compared to our previous model’s performance on promoter classification. If you look at the attention weight heatmaps, you can also see that the weights are significantly more spread out compared to the promoter model. This could mean many things, and it would take more digging to understand exactly what is happening, but one hypothesis is that our model couldn’t quite understand what exactly to focus on for classification, so we get more “cloudy” attention signal.</p>
<p>In any case, this goes to show that sometimes there are limitations to the complexity that a single model can accurately represent. If we increased the size of our initial gLM and let it run for more training iterations, our performance on this task would likely improve dramatically. Unfortunately, we don’t have the time or computing resources for that in this notebook, but I would encourage all of you to continue to investigate if you are interested!</p>
<section id="questions-3" class="level4">
<h4 class="anchored" data-anchor-id="questions-3">Questions</h4>
<ol type="1">
<li>Why is this enhancer classification problem harder than the promoter classification problem we looked at previously?</li>
<li>Similarly to the question above, provide your interpretation of the graphs of Attention weights.</li>
<li>One characteristic of enhancers is that they tend to have some distance from the gene they influence (much greater than 300 bp). How could we potentially change our original gLM in order to potentially improve our performance in this problem?</li>
</ol>
</section>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>To sum up everything we have gone through in this notebook, here are a few major points:</p>
<ol type="1">
<li>Large language models (LLMs) have become well-known for their popular use (i.e.&nbsp;ChatGPT), but they have strong potential for use as research tools in various fields as well.</li>
<li>Genomic langauge models (gLMs) are a type of LLM that attempt to learn relationships between different of regions of DNA directly from DNA sequence.</li>
<li>In general, gLMs can be created similarly to any other type of language model (usually using the Attention mechanism), however strategies for tokenization can differ due to the simplified vocabulary of DNA sequence.</li>
<li>Once tokenization and embeddings are defined, a gLM can be trained just like any other language model.</li>
<li>gLMs can be fine-tuned to perform specific tasks.</li>
</ol>
<p>The applications of language modeling to functional genomic research is a very new field, and publications of high-performing models are just beginning to emerge. If you are interested in further investigation, we would highly recommend the <a href="https://arxiv.org/abs/2306.15006">DNABERT-2</a> and <a href="https://www.biorxiv.org/content/10.1101/2023.01.11.523679v3.full">Nucleotide Transformer</a> papers. These represent two distinctly different approaches to DNA language modeling, each with their own benefits and downsides.</p>


<!-- -->

</section>

<p>© <a href="https://hakyimlab.org">HakyImLab and Listed Authors</a> - <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 License</a></p></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb32" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Genomic language model training with nanoGPT - qmd version</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> Henry Raeder</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> '2025-04-22'</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="co">  - notebook</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="co">  - gene46100</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> Henry's genomic language model trained on human reference genome with  nanogpt</span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="an">date-modified:</span><span class="co"> last-modified</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="an">draft:</span><span class="co"> false</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="an">eval:</span><span class="co"> false</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> </span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a><span class="co">  kernelspec:</span></span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a><span class="co">    name: "conda-env-nanogpt46100-py"</span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a><span class="co">    language: "python"</span></span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a><span class="co">    display_name: "nanogpt46100"</span></span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>**Acknowledgement**:</span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span>Andrej Karpathy (nanoGPT model framework)</span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span>Dalla-torre et. al. (Nucleotide Transformer workbook)</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a><span class="fu">## Introduction</span></span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>In this notebook, we explore how one can pretrain and fine-tune a basic genomic language model (gLM). As the basis of our model, we use Andrej Karpathy's publicly-available <span class="co">[</span><span class="ot">nanoGPT framework</span><span class="co">](https://github.com/karpathy/nanoGPT)</span> (with some slight edits). This is a flexible implementation of a basic GPT model (based on GPT-2) that can take generic text as input. Here, we showcase how relatively sophisticated models can be trained efficiently, and how these can be fine-tuned to specific prediction tasks using datasets pulled from the <span class="co">[</span><span class="ot">Nucleotide Transformer training notebook</span><span class="co">](https://github.com/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling.ipynb)</span> (direct sources will be cited when datasets are introduced). Generally, the goal of this notebook is to provide hands-on experience in working with language models, and give insight into the fundamentals of attention-based deep learning.</span>
<span id="cb32-27"><a href="#cb32-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-28"><a href="#cb32-28" aria-hidden="true" tabindex="-1"></a><span class="fu">## Steps</span></span>
<span id="cb32-29"><a href="#cb32-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-30"><a href="#cb32-30" aria-hidden="true" tabindex="-1"></a>This notebook demonstrates how to:</span>
<span id="cb32-31"><a href="#cb32-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-32"><a href="#cb32-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-33"><a href="#cb32-33" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span>Tokenize genomic data to be read by a language model</span>
<span id="cb32-34"><a href="#cb32-34" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span>Train and sample from a generative language model</span>
<span id="cb32-35"><a href="#cb32-35" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span>Fine-tune a model with additional training to perform specific tasks</span>
<span id="cb32-36"><a href="#cb32-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-37"><a href="#cb32-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-38"><a href="#cb32-38" aria-hidden="true" tabindex="-1"></a><span class="fu">## Setup</span></span>
<span id="cb32-39"><a href="#cb32-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-40"><a href="#cb32-40" aria-hidden="true" tabindex="-1"></a>First, it is necessary to connect to the GPU. If you are working in Colab, this can be done in the upper right corner by selecting "T4 GPU" as your runtime option.</span>
<span id="cb32-41"><a href="#cb32-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-42"><a href="#cb32-42" aria-hidden="true" tabindex="-1"></a>Next, we need to install a few things that help us build and run the model efficiently.</span>
<span id="cb32-43"><a href="#cb32-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-44"><a href="#cb32-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-45"><a href="#cb32-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-48"><a href="#cb32-48" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-49"><a href="#cb32-49" aria-hidden="true" tabindex="-1"></a><span class="co">#%pip install torch numpy transformers datasets tiktoken wandb tqdm biopython huggingface_hub accelerate</span></span>
<span id="cb32-50"><a href="#cb32-50" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb32-51"><a href="#cb32-51" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pickle</span>
<span id="cb32-52"><a href="#cb32-52" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> requests</span>
<span id="cb32-53"><a href="#cb32-53" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb32-54"><a href="#cb32-54" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> math</span>
<span id="cb32-55"><a href="#cb32-55" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> inspect</span>
<span id="cb32-56"><a href="#cb32-56" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> dataclasses <span class="im">import</span> dataclass</span>
<span id="cb32-57"><a href="#cb32-57" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb32-58"><a href="#cb32-58" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb32-59"><a href="#cb32-59" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb32-60"><a href="#cb32-60" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gc</span>
<span id="cb32-61"><a href="#cb32-61" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb32-62"><a href="#cb32-62" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> contextlib <span class="im">import</span> nullcontext</span>
<span id="cb32-63"><a href="#cb32-63" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.nn.parallel <span class="im">import</span> DistributedDataParallel <span class="im">as</span> DDP</span>
<span id="cb32-64"><a href="#cb32-64" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.distributed <span class="im">import</span> init_process_group, destroy_process_group</span>
<span id="cb32-65"><a href="#cb32-65" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> datasets <span class="im">import</span> load_dataset, Dataset</span>
<span id="cb32-66"><a href="#cb32-66" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torch.utils.data <span class="im">import</span> DataLoader, TensorDataset</span>
<span id="cb32-67"><a href="#cb32-67" aria-hidden="true" tabindex="-1"></a><span class="co">#from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification</span></span>
<span id="cb32-68"><a href="#cb32-68" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> matthews_corrcoef, f1_score</span>
<span id="cb32-69"><a href="#cb32-69" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb32-70"><a href="#cb32-70" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb32-71"><a href="#cb32-71" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-72"><a href="#cb32-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-73"><a href="#cb32-73" aria-hidden="true" tabindex="-1"></a><span class="fu">## Download and Read the Reference Genome</span></span>
<span id="cb32-74"><a href="#cb32-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-75"><a href="#cb32-75" aria-hidden="true" tabindex="-1"></a>For our training, we will be using the GRCh38 human reference genome (credit to the Genome Reference Consortium: https://www.ncbi.nlm.nih.gov/grc). In the next block, we simply create a place to store our genome in the Colab filesystem and download the genome file remotely.</span>
<span id="cb32-76"><a href="#cb32-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-79"><a href="#cb32-79" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-80"><a href="#cb32-80" aria-hidden="true" tabindex="-1"></a>running_in_colab <span class="op">=</span> <span class="va">False</span></span>
<span id="cb32-81"><a href="#cb32-81" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> running_in_colab:</span>
<span id="cb32-82"><a href="#cb32-82" aria-hidden="true" tabindex="-1"></a>  WORKDIR <span class="op">=</span> <span class="st">'/root/data/'</span></span>
<span id="cb32-83"><a href="#cb32-83" aria-hidden="true" tabindex="-1"></a>  <span class="op">!</span>mkdir <span class="op">/</span>root<span class="op">/</span>data</span>
<span id="cb32-84"><a href="#cb32-84" aria-hidden="true" tabindex="-1"></a>  fasta_file <span class="op">=</span> WORKDIR <span class="op">+</span> <span class="st">'genome.fa'</span></span>
<span id="cb32-85"><a href="#cb32-85" aria-hidden="true" tabindex="-1"></a>  <span class="op">!</span>wget <span class="op">-</span>O <span class="op">-</span> https:<span class="op">//</span>hgdownload.soe.ucsc.edu<span class="op">/</span>goldenPath<span class="op">/</span>hg38<span class="op">/</span>bigZips<span class="op">/</span>hg38.fa.gz <span class="op">|</span> gunzip <span class="op">-</span>c <span class="op">&gt;</span> {fasta_file}</span>
<span id="cb32-86"><a href="#cb32-86" aria-hidden="true" tabindex="-1"></a>  input_file_path <span class="op">=</span> os.path.join(WORKDIR, <span class="st">'genome.fa'</span>)</span>
<span id="cb32-87"><a href="#cb32-87" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb32-88"><a href="#cb32-88" aria-hidden="true" tabindex="-1"></a>  WORKDIR <span class="op">=</span> <span class="st">'~/Downloads/'</span></span>
<span id="cb32-89"><a href="#cb32-89" aria-hidden="true" tabindex="-1"></a>  fasta_path <span class="op">=</span> <span class="st">'/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/Reference-Data/ref_sequences/hg38/'</span></span>
<span id="cb32-90"><a href="#cb32-90" aria-hidden="true" tabindex="-1"></a>  fasta_file <span class="op">=</span> fasta_path <span class="op">+</span> <span class="st">'Homo_sapiens_assembly38.fasta'</span></span>
<span id="cb32-91"><a href="#cb32-91" aria-hidden="true" tabindex="-1"></a>  input_file_path <span class="op">=</span> fasta_file</span>
<span id="cb32-92"><a href="#cb32-92" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Properly expand the home directory if using ~</span></span>
<span id="cb32-93"><a href="#cb32-93" aria-hidden="true" tabindex="-1"></a>  WORKDIR <span class="op">=</span> os.path.expanduser(<span class="st">'~/Downloads'</span>)</span>
<span id="cb32-94"><a href="#cb32-94" aria-hidden="true" tabindex="-1"></a>  <span class="co"># Create directory if it doesn't exist</span></span>
<span id="cb32-95"><a href="#cb32-95" aria-hidden="true" tabindex="-1"></a>  os.makedirs(WORKDIR, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-96"><a href="#cb32-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-97"><a href="#cb32-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-98"><a href="#cb32-98" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb32-99"><a href="#cb32-99" aria-hidden="true" tabindex="-1"></a><span class="co">Prepare the hg38 genomic dataset for character-level language modeling.</span></span>
<span id="cb32-100"><a href="#cb32-100" aria-hidden="true" tabindex="-1"></a><span class="co">So instead of encoding with BPE tokens, we just map characters to ints.</span></span>
<span id="cb32-101"><a href="#cb32-101" aria-hidden="true" tabindex="-1"></a><span class="co">Will save train.bin, val.bin containing the ids, and meta.pkl containing the</span></span>
<span id="cb32-102"><a href="#cb32-102" aria-hidden="true" tabindex="-1"></a><span class="co">encoder and decoder and some other related info.</span></span>
<span id="cb32-103"><a href="#cb32-103" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb32-104"><a href="#cb32-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-105"><a href="#cb32-105" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-106"><a href="#cb32-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-107"><a href="#cb32-107" aria-hidden="true" tabindex="-1"></a>Now that we have the genome file saved locally, we have to read it into our environment in order to make it usable. To do this, we just read in the entire file as one long string, which we can then subset and investigate as we wish. For example, we may want to know the total length of our dataset, and all of the unique characters we have:</span>
<span id="cb32-108"><a href="#cb32-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-111"><a href="#cb32-111" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-112"><a href="#cb32-112" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(input_file_path, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb32-113"><a href="#cb32-113" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> f.read()</span>
<span id="cb32-114"><a href="#cb32-114" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"length of dataset in characters: </span><span class="sc">{</span><span class="bu">len</span>(data)<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb32-115"><a href="#cb32-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-116"><a href="#cb32-116" aria-hidden="true" tabindex="-1"></a><span class="co"># get all the unique characters that occur in this text</span></span>
<span id="cb32-117"><a href="#cb32-117" aria-hidden="true" tabindex="-1"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(data)))</span>
<span id="cb32-118"><a href="#cb32-118" aria-hidden="true" tabindex="-1"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(chars)</span>
<span id="cb32-119"><a href="#cb32-119" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"all the unique characters:"</span>, <span class="st">''</span>.join(chars))</span>
<span id="cb32-120"><a href="#cb32-120" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"vocab size: </span><span class="sc">{</span>vocab_size<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb32-121"><a href="#cb32-121" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-122"><a href="#cb32-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-123"><a href="#cb32-123" aria-hidden="true" tabindex="-1"></a>Great! The number of characters we have is approximately the same as the number of nucleotides in the human genome (about 3.2 billion), so it looks like our data imported correctly. However, you may notice that we have more characters than just "A", "T", "C", "G", and "N" as you may expect from genomic data. This is because our text file includes headers for each part of the genome assembly (i.e. chromosomes and fragments).</span>
<span id="cb32-124"><a href="#cb32-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-125"><a href="#cb32-125" aria-hidden="true" tabindex="-1"></a>UPDATE: Daniel tested that performance of downstream task is better when removing non DNA characters. ~~One nice thing about working with a language model is that we don't need to worry about this. Even if the input data is a bit messy, the model is essentially designed to predict the most likely characters based on a general input. Since the headers make up a tiny fraction of the total number of characters, our model will assign a very very low probability to each of the non-ATCGN characters, and they will not influence our outputs.~~</span>
<span id="cb32-126"><a href="#cb32-126" aria-hidden="true" tabindex="-1"></a>To keep the example simple, we will keep using the full genome file with non-DNA characters.</span>
<span id="cb32-127"><a href="#cb32-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-128"><a href="#cb32-128" aria-hidden="true" tabindex="-1"></a><span class="fu">## Mapping Characters to Integers (i.e. Tokenization)</span></span>
<span id="cb32-129"><a href="#cb32-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-130"><a href="#cb32-130" aria-hidden="true" tabindex="-1"></a>Now that we have our text, we need to put it into a format that is more easily readable for our model. Generally, this process is called Tokenization. There are many different methods for this, and each will impact the performance of the model differently (for example byte pair encoding or non-overlapping kmer tokenization used by other gLMs), but for the purpose of this notebook we will use a simple character-level tokenization. This means that we will simply assign a different integer to each unique character, and then transform our input into a long list of integers to be read by our model:</span>
<span id="cb32-131"><a href="#cb32-131" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-134"><a href="#cb32-134" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-135"><a href="#cb32-135" aria-hidden="true" tabindex="-1"></a><span class="co"># create a mapping from characters to integers</span></span>
<span id="cb32-136"><a href="#cb32-136" aria-hidden="true" tabindex="-1"></a>stoi <span class="op">=</span> { ch:i <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }</span>
<span id="cb32-137"><a href="#cb32-137" aria-hidden="true" tabindex="-1"></a>itos <span class="op">=</span> { i:ch <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }</span>
<span id="cb32-138"><a href="#cb32-138" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-139"><a href="#cb32-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-142"><a href="#cb32-142" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-143"><a href="#cb32-143" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> encode(s):</span>
<span id="cb32-144"><a href="#cb32-144" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [stoi[c] <span class="cf">for</span> c <span class="kw">in</span> s] <span class="co"># encoder: take a string, output a list of integers</span></span>
<span id="cb32-145"><a href="#cb32-145" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> decode(l):</span>
<span id="cb32-146"><a href="#cb32-146" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">''</span>.join([itos[i] <span class="cf">for</span> i <span class="kw">in</span> l]) <span class="co"># decoder: take a list of integers, output a string</span></span>
<span id="cb32-147"><a href="#cb32-147" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-148"><a href="#cb32-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-149"><a href="#cb32-149" aria-hidden="true" tabindex="-1"></a>Now that we have our functions defined, it is time to actually do the tokenization. This will take a few minutes, so go ahead and run the block, and see the description of what it is doing below:</span>
<span id="cb32-150"><a href="#cb32-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-153"><a href="#cb32-153" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-154"><a href="#cb32-154" aria-hidden="true" tabindex="-1"></a><span class="co"># create the train and test splits</span></span>
<span id="cb32-155"><a href="#cb32-155" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="bu">len</span>(data)</span>
<span id="cb32-156"><a href="#cb32-156" aria-hidden="true" tabindex="-1"></a>train_data <span class="op">=</span> data[:<span class="bu">int</span>(n<span class="op">*</span><span class="fl">0.9</span>)]</span>
<span id="cb32-157"><a href="#cb32-157" aria-hidden="true" tabindex="-1"></a>val_data <span class="op">=</span> data[<span class="bu">int</span>(n<span class="op">*</span><span class="fl">0.9</span>):]</span>
<span id="cb32-158"><a href="#cb32-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-159"><a href="#cb32-159" aria-hidden="true" tabindex="-1"></a><span class="co">#save memory</span></span>
<span id="cb32-160"><a href="#cb32-160" aria-hidden="true" tabindex="-1"></a><span class="kw">del</span> data</span>
<span id="cb32-161"><a href="#cb32-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-162"><a href="#cb32-162" aria-hidden="true" tabindex="-1"></a><span class="co">#helper function to store/append data to binary files and save RAM</span></span>
<span id="cb32-163"><a href="#cb32-163" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> append_to_binary_file(filename, obj):</span>
<span id="cb32-164"><a href="#cb32-164" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(filename, <span class="st">'ab'</span>) <span class="im">as</span> <span class="bu">file</span>:  <span class="co"># Open in append-binary mode</span></span>
<span id="cb32-165"><a href="#cb32-165" aria-hidden="true" tabindex="-1"></a>        obj.tofile(<span class="bu">file</span>)</span>
<span id="cb32-166"><a href="#cb32-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-167"><a href="#cb32-167" aria-hidden="true" tabindex="-1"></a><span class="co"># encode both to integers</span></span>
<span id="cb32-168"><a href="#cb32-168" aria-hidden="true" tabindex="-1"></a>ntrain<span class="op">=</span><span class="bu">len</span>(train_data)</span>
<span id="cb32-169"><a href="#cb32-169" aria-hidden="true" tabindex="-1"></a>nval <span class="op">=</span> <span class="bu">len</span>(val_data)</span>
<span id="cb32-170"><a href="#cb32-170" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> np.arange(<span class="dv">0</span>,<span class="dv">1</span>,<span class="fl">0.05</span>):</span>
<span id="cb32-171"><a href="#cb32-171" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"Tokenizing: </span><span class="sc">{</span><span class="bu">int</span>(i<span class="op">*</span><span class="dv">100</span>)<span class="sc">:,}</span><span class="ss">% complete"</span>)</span>
<span id="cb32-172"><a href="#cb32-172" aria-hidden="true" tabindex="-1"></a>  train_ids <span class="op">=</span> encode(train_data[<span class="bu">int</span>(ntrain<span class="op">*</span>i):<span class="bu">int</span>(ntrain<span class="op">*</span>(i<span class="op">+</span><span class="fl">0.05</span>))])</span>
<span id="cb32-173"><a href="#cb32-173" aria-hidden="true" tabindex="-1"></a>  val_ids <span class="op">=</span> encode(val_data[<span class="bu">int</span>(nval<span class="op">*</span>i):<span class="bu">int</span>(nval<span class="op">*</span>(i<span class="op">+</span><span class="fl">0.05</span>))])</span>
<span id="cb32-174"><a href="#cb32-174" aria-hidden="true" tabindex="-1"></a>  train_ids <span class="op">=</span> np.array(train_ids, dtype<span class="op">=</span>np.uint16)</span>
<span id="cb32-175"><a href="#cb32-175" aria-hidden="true" tabindex="-1"></a>  val_ids <span class="op">=</span> np.array(val_ids, dtype<span class="op">=</span>np.uint16)</span>
<span id="cb32-176"><a href="#cb32-176" aria-hidden="true" tabindex="-1"></a>  append_to_binary_file(os.path.join(WORKDIR, <span class="st">'train.bin'</span>), train_ids)</span>
<span id="cb32-177"><a href="#cb32-177" aria-hidden="true" tabindex="-1"></a>  append_to_binary_file(os.path.join(WORKDIR, <span class="st">'val.bin'</span>), val_ids)</span>
<span id="cb32-178"><a href="#cb32-178" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"train has </span><span class="sc">{</span>ntrain<span class="sc">:,}</span><span class="ss"> tokens"</span>)</span>
<span id="cb32-179"><a href="#cb32-179" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"val has </span><span class="sc">{</span>nval<span class="sc">:,}</span><span class="ss"> tokens"</span>)</span>
<span id="cb32-180"><a href="#cb32-180" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-181"><a href="#cb32-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-182"><a href="#cb32-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-183"><a href="#cb32-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-184"><a href="#cb32-184" aria-hidden="true" tabindex="-1"></a><span class="ss">1.   </span>First, we are dividing the genome into a training and testing dataset using a simple 90/10 split.</span>
<span id="cb32-185"><a href="#cb32-185" aria-hidden="true" tabindex="-1"></a><span class="ss">2.   </span>Next, we create a helper function that will help us save the integer lists locally. These end up taking up a lot of space in our Python environment, so it is better to save them to the disk and only access them when needed rather than having them sit and take up RAM.</span>
<span id="cb32-186"><a href="#cb32-186" aria-hidden="true" tabindex="-1"></a><span class="ss">3.   </span>Finally, we actually go through and encode/tokenize our data. Since the dataset is so large, we don't have enough memory to do this all at once in Colab, so we encode our data in batches and then push each new batch to our locally-saved files.</span>
<span id="cb32-187"><a href="#cb32-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-188"><a href="#cb32-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-189"><a href="#cb32-189" aria-hidden="true" tabindex="-1"></a>We also want to save some of our metadata (i.e. vocabulary size, and our mappings from characters to integers and back) to pull back in later.</span>
<span id="cb32-190"><a href="#cb32-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-193"><a href="#cb32-193" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-194"><a href="#cb32-194" aria-hidden="true" tabindex="-1"></a><span class="co"># save the meta information as well, to help us encode/decode later</span></span>
<span id="cb32-195"><a href="#cb32-195" aria-hidden="true" tabindex="-1"></a>meta <span class="op">=</span> {</span>
<span id="cb32-196"><a href="#cb32-196" aria-hidden="true" tabindex="-1"></a>    <span class="st">'vocab_size'</span>: vocab_size,</span>
<span id="cb32-197"><a href="#cb32-197" aria-hidden="true" tabindex="-1"></a>    <span class="st">'itos'</span>: itos,</span>
<span id="cb32-198"><a href="#cb32-198" aria-hidden="true" tabindex="-1"></a>    <span class="st">'stoi'</span>: stoi,</span>
<span id="cb32-199"><a href="#cb32-199" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb32-200"><a href="#cb32-200" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(os.path.join(WORKDIR, <span class="st">'meta.pkl'</span>), <span class="st">'wb'</span>) <span class="im">as</span> f:</span>
<span id="cb32-201"><a href="#cb32-201" aria-hidden="true" tabindex="-1"></a>    pickle.dump(meta, f)</span>
<span id="cb32-202"><a href="#cb32-202" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-203"><a href="#cb32-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-206"><a href="#cb32-206" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-207"><a href="#cb32-207" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> running_in_colab:</span>
<span id="cb32-208"><a href="#cb32-208" aria-hidden="true" tabindex="-1"></a>  os.kill(os.getpid(), <span class="dv">9</span>) <span class="co">#If running in a low-memory environment, this will clear our RAM. After this, go run the very first block again!</span></span>
<span id="cb32-209"><a href="#cb32-209" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-210"><a href="#cb32-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-211"><a href="#cb32-211" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Questions:</span></span>
<span id="cb32-212"><a href="#cb32-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-213"><a href="#cb32-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-214"><a href="#cb32-214" aria-hidden="true" tabindex="-1"></a><span class="ss">1.   </span>What does each "token" in our model represent? I.e. a single base, a stretch of a few bases, some functional region, etc.?</span>
<span id="cb32-215"><a href="#cb32-215" aria-hidden="true" tabindex="-1"></a><span class="ss">2.   </span>What is the purpose of tokenization? Why is it necessary to train our model?</span>
<span id="cb32-216"><a href="#cb32-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-217"><a href="#cb32-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-218"><a href="#cb32-218" aria-hidden="true" tabindex="-1"></a><span class="fu">## Creating the Model</span></span>
<span id="cb32-219"><a href="#cb32-219" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-220"><a href="#cb32-220" aria-hidden="true" tabindex="-1"></a>Now that we have our training data ready to go, it's time to actually create the gLM. First, we define the function we are going to use to do the heavy lifting in the attention layers. We will go into more detail on what we mean by attention during training, but if you are interested in what the implementation looks like, see the block below (and be sure to run it either way!)</span>
<span id="cb32-221"><a href="#cb32-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-224"><a href="#cb32-224" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-225"><a href="#cb32-225" aria-hidden="true" tabindex="-1"></a><span class="co">#Defining attention function (copied from Pytorch documentation, just need to save weights correctly)</span></span>
<span id="cb32-226"><a href="#cb32-226" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> scaled_dot_product_attention(query, key, value, attn_mask<span class="op">=</span><span class="va">None</span>, dropout_p<span class="op">=</span><span class="fl">0.0</span>, is_causal<span class="op">=</span><span class="va">False</span>, scale<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb32-227"><a href="#cb32-227" aria-hidden="true" tabindex="-1"></a>    L, S <span class="op">=</span> query.size(<span class="op">-</span><span class="dv">2</span>), key.size(<span class="op">-</span><span class="dv">2</span>)</span>
<span id="cb32-228"><a href="#cb32-228" aria-hidden="true" tabindex="-1"></a>    scale_factor <span class="op">=</span> <span class="dv">1</span> <span class="op">/</span> math.sqrt(query.size(<span class="op">-</span><span class="dv">1</span>)) <span class="cf">if</span> scale <span class="kw">is</span> <span class="va">None</span> <span class="cf">else</span> scale</span>
<span id="cb32-229"><a href="#cb32-229" aria-hidden="true" tabindex="-1"></a>    attn_bias <span class="op">=</span> torch.zeros(L, S, dtype<span class="op">=</span>query.dtype, device<span class="op">=</span>query.device)</span>
<span id="cb32-230"><a href="#cb32-230" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> is_causal:</span>
<span id="cb32-231"><a href="#cb32-231" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> attn_mask <span class="kw">is</span> <span class="va">None</span></span>
<span id="cb32-232"><a href="#cb32-232" aria-hidden="true" tabindex="-1"></a>        temp_mask <span class="op">=</span> torch.ones(L, S, dtype<span class="op">=</span>torch.<span class="bu">bool</span>, device<span class="op">=</span>query.device).tril(diagonal<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb32-233"><a href="#cb32-233" aria-hidden="true" tabindex="-1"></a>        attn_bias.masked_fill_(temp_mask.logical_not(), <span class="bu">float</span>(<span class="st">"-inf"</span>))</span>
<span id="cb32-234"><a href="#cb32-234" aria-hidden="true" tabindex="-1"></a>        attn_bias.to(query.dtype)</span>
<span id="cb32-235"><a href="#cb32-235" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-236"><a href="#cb32-236" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> attn_mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb32-237"><a href="#cb32-237" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> attn_mask.dtype <span class="op">==</span> torch.<span class="bu">bool</span>:</span>
<span id="cb32-238"><a href="#cb32-238" aria-hidden="true" tabindex="-1"></a>            attn_bias.masked_fill_(attn_mask.logical_not(), <span class="bu">float</span>(<span class="st">"-inf"</span>))</span>
<span id="cb32-239"><a href="#cb32-239" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb32-240"><a href="#cb32-240" aria-hidden="true" tabindex="-1"></a>            attn_bias <span class="op">+=</span> attn_mask</span>
<span id="cb32-241"><a href="#cb32-241" aria-hidden="true" tabindex="-1"></a>    attn_weight <span class="op">=</span> query <span class="op">@</span> key.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> scale_factor</span>
<span id="cb32-242"><a href="#cb32-242" aria-hidden="true" tabindex="-1"></a>    attn_weight <span class="op">+=</span> attn_bias</span>
<span id="cb32-243"><a href="#cb32-243" aria-hidden="true" tabindex="-1"></a>    attn_weight <span class="op">=</span> torch.softmax(attn_weight, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb32-244"><a href="#cb32-244" aria-hidden="true" tabindex="-1"></a>    attn_weight <span class="op">=</span> torch.dropout(attn_weight, dropout_p, train<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-245"><a href="#cb32-245" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> attn_weight, attn_weight <span class="op">@</span> value</span>
<span id="cb32-246"><a href="#cb32-246" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-247"><a href="#cb32-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-248"><a href="#cb32-248" aria-hidden="true" tabindex="-1"></a>Now that we have our attention function, it's time to define what our model will look like. This is a lot of code, so we will provide a short summary of what each layer is doing and the order in which they occur. If you are interested in the specifics, feel free to read through the code in detail.</span>
<span id="cb32-249"><a href="#cb32-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-250"><a href="#cb32-250" aria-hidden="true" tabindex="-1"></a>The main model is created as a class simply called **GPT**. Within this model, we have a few different types of layers/elements:</span>
<span id="cb32-251"><a href="#cb32-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-252"><a href="#cb32-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-253"><a href="#cb32-253" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span>**Embedding**: In our model, we project our relatively simple vocabulary (40 characters) into a higher-dimensional space in order to increase the model's capacity to understand more complex relationships between tokens. This is a bit difficult to visualize, but in essence Embedding layers just perform this projection and return the resulting dictionary to use later in our model. For language models, we usually embed both the tokens and the positions of the tokens, for reasons we will discuss later.</span>
<span id="cb32-254"><a href="#cb32-254" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span>**LayerNorm**: As the name suggests, this layer simply applies normalization across the input features. This stabilizes the training, as it ensures each feature has a similar scale.</span>
<span id="cb32-255"><a href="#cb32-255" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span>**Dropout**: Dropout layers randomly zero-out some of the elements in the input tensor. This serves as a form of regularization, and helps avoid overfitting our training data.</span>
<span id="cb32-256"><a href="#cb32-256" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span>**Causal Self-Attention**: The mechanism of this layer will be described in further detail below, but this is the layer where our model learns the relationship between each of the tokens in the input, which is what gives it predictive power to generate what token will come next.</span>
<span id="cb32-257"><a href="#cb32-257" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span>**MLP**: One can think of the MLP class as a mini neural network within our model. It is made up of a linear layer, a GELU activation layer, another linear layer, and finally a dropout layer. In short, the MLP element introduces non-linearity into our model, and helps us transform the output of our attention layer back into something we can pass through attention again, and decode into DNA sequence.</span>
<span id="cb32-258"><a href="#cb32-258" aria-hidden="true" tabindex="-1"></a><span class="ss">*   </span>**Block**: This is not really a layer, it is more a sequence of layers. Specifically, we define a block as LayerNorm → Causal Self-Attention → LayerNorm → MLP</span>
<span id="cb32-259"><a href="#cb32-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-260"><a href="#cb32-260" aria-hidden="true" tabindex="-1"></a>Now that we have these modules defined, the general outline of our GPT model is as below (assume the outputs of each step are passed to the next):</span>
<span id="cb32-261"><a href="#cb32-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-262"><a href="#cb32-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-263"><a href="#cb32-263" aria-hidden="true" tabindex="-1"></a><span class="ss">1.   </span>Token and Position Embedding Layers</span>
<span id="cb32-264"><a href="#cb32-264" aria-hidden="true" tabindex="-1"></a><span class="ss">2.   </span>Dropout layer</span>
<span id="cb32-265"><a href="#cb32-265" aria-hidden="true" tabindex="-1"></a><span class="ss">3.   </span>Blocks (repeated $n$ times, i.e. if we want 6 attention layers we will have 6 Blocks)</span>
<span id="cb32-266"><a href="#cb32-266" aria-hidden="true" tabindex="-1"></a><span class="ss">4.   </span>LayerNorm</span>
<span id="cb32-267"><a href="#cb32-267" aria-hidden="true" tabindex="-1"></a><span class="ss">5.   </span>Linear layer to transform embedded outputs back into our vocab size</span>
<span id="cb32-268"><a href="#cb32-268" aria-hidden="true" tabindex="-1"></a><span class="ss">6.   </span>Loss calculation (if training)</span>
<span id="cb32-269"><a href="#cb32-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-270"><a href="#cb32-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-273"><a href="#cb32-273" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-274"><a href="#cb32-274" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb32-275"><a href="#cb32-275" aria-hidden="true" tabindex="-1"></a><span class="co">Full definition of a GPT Language Model, all of it in this single file.</span></span>
<span id="cb32-276"><a href="#cb32-276" aria-hidden="true" tabindex="-1"></a><span class="co">References:</span></span>
<span id="cb32-277"><a href="#cb32-277" aria-hidden="true" tabindex="-1"></a><span class="co">1) the official GPT-2 TensorFlow implementation released by OpenAI:</span></span>
<span id="cb32-278"><a href="#cb32-278" aria-hidden="true" tabindex="-1"></a><span class="co">https://github.com/openai/gpt-2/blob/master/src/model.py</span></span>
<span id="cb32-279"><a href="#cb32-279" aria-hidden="true" tabindex="-1"></a><span class="co">2) huggingface/transformers PyTorch implementation:</span></span>
<span id="cb32-280"><a href="#cb32-280" aria-hidden="true" tabindex="-1"></a><span class="co">https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py</span></span>
<span id="cb32-281"><a href="#cb32-281" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb32-282"><a href="#cb32-282" aria-hidden="true" tabindex="-1"></a>gc.collect()</span>
<span id="cb32-283"><a href="#cb32-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-284"><a href="#cb32-284" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LayerNorm(nn.Module):</span>
<span id="cb32-285"><a href="#cb32-285" aria-hidden="true" tabindex="-1"></a>    <span class="co">""" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False """</span></span>
<span id="cb32-286"><a href="#cb32-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-287"><a href="#cb32-287" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ndim, bias):</span>
<span id="cb32-288"><a href="#cb32-288" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb32-289"><a href="#cb32-289" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.weight <span class="op">=</span> nn.Parameter(torch.ones(ndim))</span>
<span id="cb32-290"><a href="#cb32-290" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.zeros(ndim)) <span class="cf">if</span> bias <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb32-291"><a href="#cb32-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-292"><a href="#cb32-292" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb32-293"><a href="#cb32-293" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> F.layer_norm(<span class="bu">input</span>, <span class="va">self</span>.weight.shape, <span class="va">self</span>.weight, <span class="va">self</span>.bias, <span class="fl">1e-5</span>)</span>
<span id="cb32-294"><a href="#cb32-294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-295"><a href="#cb32-295" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> CausalSelfAttention(nn.Module):</span>
<span id="cb32-296"><a href="#cb32-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-297"><a href="#cb32-297" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb32-298"><a href="#cb32-298" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb32-299"><a href="#cb32-299" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> config.n_embd <span class="op">%</span> config.n_head <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb32-300"><a href="#cb32-300" aria-hidden="true" tabindex="-1"></a>        <span class="co"># key, query, value projections for all heads, but in a batch</span></span>
<span id="cb32-301"><a href="#cb32-301" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c_attn <span class="op">=</span> nn.Linear(config.n_embd, <span class="dv">3</span> <span class="op">*</span> config.n_embd, bias<span class="op">=</span>config.bias)</span>
<span id="cb32-302"><a href="#cb32-302" aria-hidden="true" tabindex="-1"></a>        <span class="co"># output projection</span></span>
<span id="cb32-303"><a href="#cb32-303" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c_proj <span class="op">=</span> nn.Linear(config.n_embd, config.n_embd, bias<span class="op">=</span>config.bias)</span>
<span id="cb32-304"><a href="#cb32-304" aria-hidden="true" tabindex="-1"></a>        <span class="co"># regularization</span></span>
<span id="cb32-305"><a href="#cb32-305" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn_dropout <span class="op">=</span> nn.Dropout(config.dropout)</span>
<span id="cb32-306"><a href="#cb32-306" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.resid_dropout <span class="op">=</span> nn.Dropout(config.dropout)</span>
<span id="cb32-307"><a href="#cb32-307" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_head <span class="op">=</span> config.n_head</span>
<span id="cb32-308"><a href="#cb32-308" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_embd <span class="op">=</span> config.n_embd</span>
<span id="cb32-309"><a href="#cb32-309" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> config.dropout</span>
<span id="cb32-310"><a href="#cb32-310" aria-hidden="true" tabindex="-1"></a>        <span class="co"># flash attention make GPU go brrrrr but support is only in PyTorch &gt;= 2.0</span></span>
<span id="cb32-311"><a href="#cb32-311" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.flash <span class="op">=</span> <span class="bu">hasattr</span>(torch.nn.functional, <span class="st">'scaled_dot_product_attention'</span>)</span>
<span id="cb32-312"><a href="#cb32-312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-313"><a href="#cb32-313" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb32-314"><a href="#cb32-314" aria-hidden="true" tabindex="-1"></a>        B, T, C <span class="op">=</span> x.size() <span class="co"># batch size, sequence length, embedding dimensionality (n_embd)</span></span>
<span id="cb32-315"><a href="#cb32-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-316"><a href="#cb32-316" aria-hidden="true" tabindex="-1"></a>        <span class="co"># calculate query, key, values for all heads in batch and move head forward to be the batch dim</span></span>
<span id="cb32-317"><a href="#cb32-317" aria-hidden="true" tabindex="-1"></a>        q, k, v  <span class="op">=</span> <span class="va">self</span>.c_attn(x).split(<span class="va">self</span>.n_embd, dim<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb32-318"><a href="#cb32-318" aria-hidden="true" tabindex="-1"></a>        k <span class="op">=</span> k.view(B, T, <span class="va">self</span>.n_head, C <span class="op">//</span> <span class="va">self</span>.n_head).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, nh, T, hs)</span></span>
<span id="cb32-319"><a href="#cb32-319" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> q.view(B, T, <span class="va">self</span>.n_head, C <span class="op">//</span> <span class="va">self</span>.n_head).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, nh, T, hs)</span></span>
<span id="cb32-320"><a href="#cb32-320" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> v.view(B, T, <span class="va">self</span>.n_head, C <span class="op">//</span> <span class="va">self</span>.n_head).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, nh, T, hs)</span></span>
<span id="cb32-321"><a href="#cb32-321" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-322"><a href="#cb32-322" aria-hidden="true" tabindex="-1"></a>        <span class="co"># causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)</span></span>
<span id="cb32-323"><a href="#cb32-323" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.flash:</span>
<span id="cb32-324"><a href="#cb32-324" aria-hidden="true" tabindex="-1"></a>            <span class="co"># efficient attention using Flash Attention CUDA kernels</span></span>
<span id="cb32-325"><a href="#cb32-325" aria-hidden="true" tabindex="-1"></a>            attn_weight, y <span class="op">=</span> scaled_dot_product_attention(q, k, v, attn_mask<span class="op">=</span><span class="va">None</span>, dropout_p<span class="op">=</span><span class="va">self</span>.dropout <span class="cf">if</span> <span class="va">self</span>.training <span class="cf">else</span> <span class="dv">0</span>, is_causal<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-326"><a href="#cb32-326" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> y.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(B, T, C) <span class="co"># re-assemble all head outputs side by side</span></span>
<span id="cb32-327"><a href="#cb32-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-328"><a href="#cb32-328" aria-hidden="true" tabindex="-1"></a>        <span class="co"># output projection</span></span>
<span id="cb32-329"><a href="#cb32-329" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> <span class="va">self</span>.resid_dropout(<span class="va">self</span>.c_proj(y))</span>
<span id="cb32-330"><a href="#cb32-330" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y, torch.mean(attn_weight, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb32-331"><a href="#cb32-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-332"><a href="#cb32-332" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb32-333"><a href="#cb32-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-334"><a href="#cb32-334" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb32-335"><a href="#cb32-335" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb32-336"><a href="#cb32-336" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c_fc    <span class="op">=</span> nn.Linear(config.n_embd, <span class="dv">4</span> <span class="op">*</span> config.n_embd, bias<span class="op">=</span>config.bias)</span>
<span id="cb32-337"><a href="#cb32-337" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.gelu    <span class="op">=</span> nn.GELU()</span>
<span id="cb32-338"><a href="#cb32-338" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c_proj  <span class="op">=</span> nn.Linear(<span class="dv">4</span> <span class="op">*</span> config.n_embd, config.n_embd, bias<span class="op">=</span>config.bias)</span>
<span id="cb32-339"><a href="#cb32-339" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.dropout)</span>
<span id="cb32-340"><a href="#cb32-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-341"><a href="#cb32-341" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb32-342"><a href="#cb32-342" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.c_fc(x)</span>
<span id="cb32-343"><a href="#cb32-343" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.gelu(x)</span>
<span id="cb32-344"><a href="#cb32-344" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.c_proj(x)</span>
<span id="cb32-345"><a href="#cb32-345" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb32-346"><a href="#cb32-346" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb32-347"><a href="#cb32-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-348"><a href="#cb32-348" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb32-349"><a href="#cb32-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-350"><a href="#cb32-350" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb32-351"><a href="#cb32-351" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb32-352"><a href="#cb32-352" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln_1 <span class="op">=</span> LayerNorm(config.n_embd, bias<span class="op">=</span>config.bias)</span>
<span id="cb32-353"><a href="#cb32-353" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.attn <span class="op">=</span> CausalSelfAttention(config)</span>
<span id="cb32-354"><a href="#cb32-354" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ln_2 <span class="op">=</span> LayerNorm(config.n_embd, bias<span class="op">=</span>config.bias)</span>
<span id="cb32-355"><a href="#cb32-355" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.mlp <span class="op">=</span> MLP(config)</span>
<span id="cb32-356"><a href="#cb32-356" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-357"><a href="#cb32-357" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb32-358"><a href="#cb32-358" aria-hidden="true" tabindex="-1"></a>        val, attn_weight <span class="op">=</span> <span class="va">self</span>.attn(<span class="va">self</span>.ln_1(x))</span>
<span id="cb32-359"><a href="#cb32-359" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> val</span>
<span id="cb32-360"><a href="#cb32-360" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mlp(<span class="va">self</span>.ln_2(x))</span>
<span id="cb32-361"><a href="#cb32-361" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x, attn_weight</span>
<span id="cb32-362"><a href="#cb32-362" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-363"><a href="#cb32-363" aria-hidden="true" tabindex="-1"></a><span class="at">@dataclass</span></span>
<span id="cb32-364"><a href="#cb32-364" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GPTConfig: <span class="co">#Default values for configuration, originally designed to match GPT-2, will override for training</span></span>
<span id="cb32-365"><a href="#cb32-365" aria-hidden="true" tabindex="-1"></a>    block_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1024</span></span>
<span id="cb32-366"><a href="#cb32-366" aria-hidden="true" tabindex="-1"></a>    vocab_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">50304</span> <span class="co"># GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency</span></span>
<span id="cb32-367"><a href="#cb32-367" aria-hidden="true" tabindex="-1"></a>    n_layer: <span class="bu">int</span> <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb32-368"><a href="#cb32-368" aria-hidden="true" tabindex="-1"></a>    n_head: <span class="bu">int</span> <span class="op">=</span> <span class="dv">12</span></span>
<span id="cb32-369"><a href="#cb32-369" aria-hidden="true" tabindex="-1"></a>    n_embd: <span class="bu">int</span> <span class="op">=</span> <span class="dv">768</span></span>
<span id="cb32-370"><a href="#cb32-370" aria-hidden="true" tabindex="-1"></a>    dropout: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb32-371"><a href="#cb32-371" aria-hidden="true" tabindex="-1"></a>    bias: <span class="bu">bool</span> <span class="op">=</span> <span class="va">True</span> <span class="co"># True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster</span></span>
<span id="cb32-372"><a href="#cb32-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-373"><a href="#cb32-373" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GPT(nn.Module):</span>
<span id="cb32-374"><a href="#cb32-374" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-375"><a href="#cb32-375" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config):</span>
<span id="cb32-376"><a href="#cb32-376" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb32-377"><a href="#cb32-377" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> config.vocab_size <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span>
<span id="cb32-378"><a href="#cb32-378" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> config.block_size <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span>
<span id="cb32-379"><a href="#cb32-379" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.config <span class="op">=</span> config</span>
<span id="cb32-380"><a href="#cb32-380" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-381"><a href="#cb32-381" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transformer <span class="op">=</span> nn.ModuleDict(<span class="bu">dict</span>(</span>
<span id="cb32-382"><a href="#cb32-382" aria-hidden="true" tabindex="-1"></a>            wte <span class="op">=</span> nn.Embedding(config.vocab_size, config.n_embd),</span>
<span id="cb32-383"><a href="#cb32-383" aria-hidden="true" tabindex="-1"></a>            wpe <span class="op">=</span> nn.Embedding(config.block_size, config.n_embd),</span>
<span id="cb32-384"><a href="#cb32-384" aria-hidden="true" tabindex="-1"></a>            drop <span class="op">=</span> nn.Dropout(config.dropout),</span>
<span id="cb32-385"><a href="#cb32-385" aria-hidden="true" tabindex="-1"></a>            h <span class="op">=</span> nn.ModuleList([Block(config) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.n_layer)]),</span>
<span id="cb32-386"><a href="#cb32-386" aria-hidden="true" tabindex="-1"></a>            ln_f <span class="op">=</span> LayerNorm(config.n_embd, bias<span class="op">=</span>config.bias),</span>
<span id="cb32-387"><a href="#cb32-387" aria-hidden="true" tabindex="-1"></a>        ))</span>
<span id="cb32-388"><a href="#cb32-388" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lm_head <span class="op">=</span> nn.Linear(config.n_embd, config.vocab_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb32-389"><a href="#cb32-389" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transformer.wte.weight <span class="op">=</span> <span class="va">self</span>.lm_head.weight <span class="co"># https://paperswithcode.com/method/weight-tying</span></span>
<span id="cb32-390"><a href="#cb32-390" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-391"><a href="#cb32-391" aria-hidden="true" tabindex="-1"></a>        <span class="co"># init all weights</span></span>
<span id="cb32-392"><a href="#cb32-392" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.<span class="bu">apply</span>(<span class="va">self</span>._init_weights)</span>
<span id="cb32-393"><a href="#cb32-393" aria-hidden="true" tabindex="-1"></a>        <span class="co"># apply special scaled init to the residual projections, per GPT-2 paper</span></span>
<span id="cb32-394"><a href="#cb32-394" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> pn, p <span class="kw">in</span> <span class="va">self</span>.named_parameters():</span>
<span id="cb32-395"><a href="#cb32-395" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> pn.endswith(<span class="st">'c_proj.weight'</span>):</span>
<span id="cb32-396"><a href="#cb32-396" aria-hidden="true" tabindex="-1"></a>                torch.nn.init.normal_(p, mean<span class="op">=</span><span class="fl">0.0</span>, std<span class="op">=</span><span class="fl">0.02</span><span class="op">/</span>math.sqrt(<span class="dv">2</span> <span class="op">*</span> config.n_layer))</span>
<span id="cb32-397"><a href="#cb32-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-398"><a href="#cb32-398" aria-hidden="true" tabindex="-1"></a>        <span class="co"># report number of parameters</span></span>
<span id="cb32-399"><a href="#cb32-399" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"number of parameters: </span><span class="sc">%.2f</span><span class="st">M"</span> <span class="op">%</span> (<span class="va">self</span>.get_num_params()<span class="op">/</span><span class="fl">1e6</span>,))</span>
<span id="cb32-400"><a href="#cb32-400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-401"><a href="#cb32-401" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> get_num_params(<span class="va">self</span>, non_embedding<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb32-402"><a href="#cb32-402" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb32-403"><a href="#cb32-403" aria-hidden="true" tabindex="-1"></a><span class="co">        Return the number of parameters in the model.</span></span>
<span id="cb32-404"><a href="#cb32-404" aria-hidden="true" tabindex="-1"></a><span class="co">        For non-embedding count (default), the position embeddings get subtracted.</span></span>
<span id="cb32-405"><a href="#cb32-405" aria-hidden="true" tabindex="-1"></a><span class="co">        The token embeddings would too, except due to the parameter sharing these</span></span>
<span id="cb32-406"><a href="#cb32-406" aria-hidden="true" tabindex="-1"></a><span class="co">        params are actually used as weights in the final layer, so we include them.</span></span>
<span id="cb32-407"><a href="#cb32-407" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb32-408"><a href="#cb32-408" aria-hidden="true" tabindex="-1"></a>        n_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.parameters())</span>
<span id="cb32-409"><a href="#cb32-409" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> non_embedding:</span>
<span id="cb32-410"><a href="#cb32-410" aria-hidden="true" tabindex="-1"></a>            n_params <span class="op">-=</span> <span class="va">self</span>.transformer.wpe.weight.numel()</span>
<span id="cb32-411"><a href="#cb32-411" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> n_params</span>
<span id="cb32-412"><a href="#cb32-412" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-413"><a href="#cb32-413" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _init_weights(<span class="va">self</span>, module):</span>
<span id="cb32-414"><a href="#cb32-414" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(module, nn.Linear):</span>
<span id="cb32-415"><a href="#cb32-415" aria-hidden="true" tabindex="-1"></a>            torch.nn.init.normal_(module.weight, mean<span class="op">=</span><span class="fl">0.0</span>, std<span class="op">=</span><span class="fl">0.02</span>)</span>
<span id="cb32-416"><a href="#cb32-416" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> module.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb32-417"><a href="#cb32-417" aria-hidden="true" tabindex="-1"></a>                torch.nn.init.zeros_(module.bias)</span>
<span id="cb32-418"><a href="#cb32-418" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> <span class="bu">isinstance</span>(module, nn.Embedding):</span>
<span id="cb32-419"><a href="#cb32-419" aria-hidden="true" tabindex="-1"></a>            torch.nn.init.normal_(module.weight, mean<span class="op">=</span><span class="fl">0.0</span>, std<span class="op">=</span><span class="fl">0.02</span>)</span>
<span id="cb32-420"><a href="#cb32-420" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-421"><a href="#cb32-421" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb32-422"><a href="#cb32-422" aria-hidden="true" tabindex="-1"></a>        <span class="co">#device = idx.device</span></span>
<span id="cb32-423"><a href="#cb32-423" aria-hidden="true" tabindex="-1"></a>        b, t <span class="op">=</span> idx.size()</span>
<span id="cb32-424"><a href="#cb32-424" aria-hidden="true" tabindex="-1"></a>        weight_lst <span class="op">=</span> []</span>
<span id="cb32-425"><a href="#cb32-425" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> t <span class="op">&lt;=</span> <span class="va">self</span>.config.block_size, <span class="ss">f"Cannot forward sequence of length </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">, block size is only </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>config<span class="sc">.</span>block_size<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb32-426"><a href="#cb32-426" aria-hidden="true" tabindex="-1"></a>        pos <span class="op">=</span> torch.arange(<span class="dv">0</span>, t, dtype<span class="op">=</span>torch.<span class="bu">long</span>, device<span class="op">=</span>device) <span class="co"># shape (t)</span></span>
<span id="cb32-427"><a href="#cb32-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-428"><a href="#cb32-428" aria-hidden="true" tabindex="-1"></a>        <span class="co"># forward the GPT model itself</span></span>
<span id="cb32-429"><a href="#cb32-429" aria-hidden="true" tabindex="-1"></a>        tok_emb <span class="op">=</span> <span class="va">self</span>.transformer.wte(idx) <span class="co"># token embeddings of shape (b, t, n_embd)</span></span>
<span id="cb32-430"><a href="#cb32-430" aria-hidden="true" tabindex="-1"></a>        pos_emb <span class="op">=</span> <span class="va">self</span>.transformer.wpe(pos) <span class="co"># position embeddings of shape (t, n_embd)</span></span>
<span id="cb32-431"><a href="#cb32-431" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.transformer.drop(tok_emb <span class="op">+</span> pos_emb)</span>
<span id="cb32-432"><a href="#cb32-432" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.transformer.h:</span>
<span id="cb32-433"><a href="#cb32-433" aria-hidden="true" tabindex="-1"></a>            x, attn_weight <span class="op">=</span> block(x)</span>
<span id="cb32-434"><a href="#cb32-434" aria-hidden="true" tabindex="-1"></a>            weight_lst.append(attn_weight)</span>
<span id="cb32-435"><a href="#cb32-435" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.transformer.ln_f(x)</span>
<span id="cb32-436"><a href="#cb32-436" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-437"><a href="#cb32-437" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb32-438"><a href="#cb32-438" aria-hidden="true" tabindex="-1"></a>            <span class="co"># if we are given some desired targets also calculate the loss</span></span>
<span id="cb32-439"><a href="#cb32-439" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> <span class="va">self</span>.lm_head(x)</span>
<span id="cb32-440"><a href="#cb32-440" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> F.cross_entropy(logits.view(<span class="op">-</span><span class="dv">1</span>, logits.size(<span class="op">-</span><span class="dv">1</span>)), targets.view(<span class="op">-</span><span class="dv">1</span>), ignore_index<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb32-441"><a href="#cb32-441" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb32-442"><a href="#cb32-442" aria-hidden="true" tabindex="-1"></a>            <span class="co"># inference-time mini-optimization: only forward the lm_head on the very last position</span></span>
<span id="cb32-443"><a href="#cb32-443" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> <span class="va">self</span>.lm_head(x[:, [<span class="op">-</span><span class="dv">1</span>], :]) <span class="co"># note: using list [-1] to preserve the time dim</span></span>
<span id="cb32-444"><a href="#cb32-444" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb32-445"><a href="#cb32-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-446"><a href="#cb32-446" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> logits, loss, weight_lst</span>
<span id="cb32-447"><a href="#cb32-447" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-448"><a href="#cb32-448" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> crop_block_size(<span class="va">self</span>, block_size):</span>
<span id="cb32-449"><a href="#cb32-449" aria-hidden="true" tabindex="-1"></a>        <span class="co"># model surgery to decrease the block size if necessary</span></span>
<span id="cb32-450"><a href="#cb32-450" aria-hidden="true" tabindex="-1"></a>        <span class="co"># e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)</span></span>
<span id="cb32-451"><a href="#cb32-451" aria-hidden="true" tabindex="-1"></a>        <span class="co"># but want to use a smaller block size for some smaller, simpler model</span></span>
<span id="cb32-452"><a href="#cb32-452" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> block_size <span class="op">&lt;=</span> <span class="va">self</span>.config.block_size</span>
<span id="cb32-453"><a href="#cb32-453" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.config.block_size <span class="op">=</span> block_size</span>
<span id="cb32-454"><a href="#cb32-454" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.transformer.wpe.weight <span class="op">=</span> nn.Parameter(<span class="va">self</span>.transformer.wpe.weight[:block_size])</span>
<span id="cb32-455"><a href="#cb32-455" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.transformer.h:</span>
<span id="cb32-456"><a href="#cb32-456" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">hasattr</span>(block.attn, <span class="st">'bias'</span>):</span>
<span id="cb32-457"><a href="#cb32-457" aria-hidden="true" tabindex="-1"></a>                block.attn.bias <span class="op">=</span> block.attn.bias[:,:,:block_size,:block_size]</span>
<span id="cb32-458"><a href="#cb32-458" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-459"><a href="#cb32-459" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> configure_optimizers(<span class="va">self</span>, weight_decay, learning_rate, betas, device_type):</span>
<span id="cb32-460"><a href="#cb32-460" aria-hidden="true" tabindex="-1"></a>        <span class="co"># start with all of the candidate parameters</span></span>
<span id="cb32-461"><a href="#cb32-461" aria-hidden="true" tabindex="-1"></a>        param_dict <span class="op">=</span> {pn: p <span class="cf">for</span> pn, p <span class="kw">in</span> <span class="va">self</span>.named_parameters()}</span>
<span id="cb32-462"><a href="#cb32-462" aria-hidden="true" tabindex="-1"></a>        <span class="co"># filter out those that do not require grad</span></span>
<span id="cb32-463"><a href="#cb32-463" aria-hidden="true" tabindex="-1"></a>        param_dict <span class="op">=</span> {pn: p <span class="cf">for</span> pn, p <span class="kw">in</span> param_dict.items() <span class="cf">if</span> p.requires_grad}</span>
<span id="cb32-464"><a href="#cb32-464" aria-hidden="true" tabindex="-1"></a>        <span class="co"># create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.</span></span>
<span id="cb32-465"><a href="#cb32-465" aria-hidden="true" tabindex="-1"></a>        <span class="co"># i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.</span></span>
<span id="cb32-466"><a href="#cb32-466" aria-hidden="true" tabindex="-1"></a>        decay_params <span class="op">=</span> [p <span class="cf">for</span> n, p <span class="kw">in</span> param_dict.items() <span class="cf">if</span> p.dim() <span class="op">&gt;=</span> <span class="dv">2</span>]</span>
<span id="cb32-467"><a href="#cb32-467" aria-hidden="true" tabindex="-1"></a>        nodecay_params <span class="op">=</span> [p <span class="cf">for</span> n, p <span class="kw">in</span> param_dict.items() <span class="cf">if</span> p.dim() <span class="op">&lt;</span> <span class="dv">2</span>]</span>
<span id="cb32-468"><a href="#cb32-468" aria-hidden="true" tabindex="-1"></a>        optim_groups <span class="op">=</span> [</span>
<span id="cb32-469"><a href="#cb32-469" aria-hidden="true" tabindex="-1"></a>            {<span class="st">'params'</span>: decay_params, <span class="st">'weight_decay'</span>: weight_decay},</span>
<span id="cb32-470"><a href="#cb32-470" aria-hidden="true" tabindex="-1"></a>            {<span class="st">'params'</span>: nodecay_params, <span class="st">'weight_decay'</span>: <span class="fl">0.0</span>}</span>
<span id="cb32-471"><a href="#cb32-471" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb32-472"><a href="#cb32-472" aria-hidden="true" tabindex="-1"></a>        num_decay_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> decay_params)</span>
<span id="cb32-473"><a href="#cb32-473" aria-hidden="true" tabindex="-1"></a>        num_nodecay_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> nodecay_params)</span>
<span id="cb32-474"><a href="#cb32-474" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"num decayed parameter tensors: </span><span class="sc">{</span><span class="bu">len</span>(decay_params)<span class="sc">}</span><span class="ss">, with </span><span class="sc">{</span>num_decay_params<span class="sc">:,}</span><span class="ss"> parameters"</span>)</span>
<span id="cb32-475"><a href="#cb32-475" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"num non-decayed parameter tensors: </span><span class="sc">{</span><span class="bu">len</span>(nodecay_params)<span class="sc">}</span><span class="ss">, with </span><span class="sc">{</span>num_nodecay_params<span class="sc">:,}</span><span class="ss"> parameters"</span>)</span>
<span id="cb32-476"><a href="#cb32-476" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Create AdamW optimizer and use the fused version if it is available</span></span>
<span id="cb32-477"><a href="#cb32-477" aria-hidden="true" tabindex="-1"></a>        fused_available <span class="op">=</span> <span class="st">'fused'</span> <span class="kw">in</span> inspect.signature(torch.optim.AdamW).parameters</span>
<span id="cb32-478"><a href="#cb32-478" aria-hidden="true" tabindex="-1"></a>        use_fused <span class="op">=</span> fused_available <span class="kw">and</span> device_type <span class="op">==</span> <span class="st">'cuda'</span></span>
<span id="cb32-479"><a href="#cb32-479" aria-hidden="true" tabindex="-1"></a>        extra_args <span class="op">=</span> <span class="bu">dict</span>(fused<span class="op">=</span><span class="va">True</span>) <span class="cf">if</span> use_fused <span class="cf">else</span> <span class="bu">dict</span>()</span>
<span id="cb32-480"><a href="#cb32-480" aria-hidden="true" tabindex="-1"></a>        optimizer <span class="op">=</span> torch.optim.AdamW(optim_groups, lr<span class="op">=</span>learning_rate, betas<span class="op">=</span>betas, <span class="op">**</span>extra_args)</span>
<span id="cb32-481"><a href="#cb32-481" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"using fused AdamW: </span><span class="sc">{</span>use_fused<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-482"><a href="#cb32-482" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-483"><a href="#cb32-483" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> optimizer</span>
<span id="cb32-484"><a href="#cb32-484" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-485"><a href="#cb32-485" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> estimate_mfu(<span class="va">self</span>, fwdbwd_per_iter, dt):</span>
<span id="cb32-486"><a href="#cb32-486" aria-hidden="true" tabindex="-1"></a>        <span class="co">""" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS """</span></span>
<span id="cb32-487"><a href="#cb32-487" aria-hidden="true" tabindex="-1"></a>        <span class="co"># first estimate the number of flops we do per iteration.</span></span>
<span id="cb32-488"><a href="#cb32-488" aria-hidden="true" tabindex="-1"></a>        <span class="co"># see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311</span></span>
<span id="cb32-489"><a href="#cb32-489" aria-hidden="true" tabindex="-1"></a>        N <span class="op">=</span> <span class="va">self</span>.get_num_params()</span>
<span id="cb32-490"><a href="#cb32-490" aria-hidden="true" tabindex="-1"></a>        cfg <span class="op">=</span> <span class="va">self</span>.config</span>
<span id="cb32-491"><a href="#cb32-491" aria-hidden="true" tabindex="-1"></a>        L, H, Q, T <span class="op">=</span> cfg.n_layer, cfg.n_head, cfg.n_embd<span class="op">//</span>cfg.n_head, cfg.block_size</span>
<span id="cb32-492"><a href="#cb32-492" aria-hidden="true" tabindex="-1"></a>        flops_per_token <span class="op">=</span> <span class="dv">6</span><span class="op">*</span>N <span class="op">+</span> <span class="dv">12</span><span class="op">*</span>L<span class="op">*</span>H<span class="op">*</span>Q<span class="op">*</span>T</span>
<span id="cb32-493"><a href="#cb32-493" aria-hidden="true" tabindex="-1"></a>        flops_per_fwdbwd <span class="op">=</span> flops_per_token <span class="op">*</span> T</span>
<span id="cb32-494"><a href="#cb32-494" aria-hidden="true" tabindex="-1"></a>        flops_per_iter <span class="op">=</span> flops_per_fwdbwd <span class="op">*</span> fwdbwd_per_iter</span>
<span id="cb32-495"><a href="#cb32-495" aria-hidden="true" tabindex="-1"></a>        <span class="co"># express our flops throughput as ratio of A100 bfloat16 peak flops</span></span>
<span id="cb32-496"><a href="#cb32-496" aria-hidden="true" tabindex="-1"></a>        flops_achieved <span class="op">=</span> flops_per_iter <span class="op">*</span> (<span class="fl">1.0</span><span class="op">/</span>dt) <span class="co"># per second</span></span>
<span id="cb32-497"><a href="#cb32-497" aria-hidden="true" tabindex="-1"></a>        flops_promised <span class="op">=</span> <span class="fl">312e12</span> <span class="co"># A100 GPU bfloat16 peak flops is 312 TFLOPS</span></span>
<span id="cb32-498"><a href="#cb32-498" aria-hidden="true" tabindex="-1"></a>        mfu <span class="op">=</span> flops_achieved <span class="op">/</span> flops_promised</span>
<span id="cb32-499"><a href="#cb32-499" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> mfu</span>
<span id="cb32-500"><a href="#cb32-500" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-501"><a href="#cb32-501" aria-hidden="true" tabindex="-1"></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb32-502"><a href="#cb32-502" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens, temperature<span class="op">=</span><span class="fl">1.0</span>, top_k<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb32-503"><a href="#cb32-503" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""</span></span>
<span id="cb32-504"><a href="#cb32-504" aria-hidden="true" tabindex="-1"></a><span class="co">        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete</span></span>
<span id="cb32-505"><a href="#cb32-505" aria-hidden="true" tabindex="-1"></a><span class="co">        the sequence max_new_tokens times, feeding the predictions back into the model each time.</span></span>
<span id="cb32-506"><a href="#cb32-506" aria-hidden="true" tabindex="-1"></a><span class="co">        Most likely you'll want to make sure to be in model.eval() mode of operation for this.</span></span>
<span id="cb32-507"><a href="#cb32-507" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb32-508"><a href="#cb32-508" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb32-509"><a href="#cb32-509" aria-hidden="true" tabindex="-1"></a>            <span class="co"># if the sequence context is growing too long we must crop it at block_size</span></span>
<span id="cb32-510"><a href="#cb32-510" aria-hidden="true" tabindex="-1"></a>            idx_cond <span class="op">=</span> idx <span class="cf">if</span> idx.size(<span class="dv">1</span>) <span class="op">&lt;=</span> <span class="va">self</span>.config.block_size <span class="cf">else</span> idx[:, <span class="op">-</span><span class="va">self</span>.config.block_size:]</span>
<span id="cb32-511"><a href="#cb32-511" aria-hidden="true" tabindex="-1"></a>            <span class="co"># forward the model to get the logits for the index in the sequence</span></span>
<span id="cb32-512"><a href="#cb32-512" aria-hidden="true" tabindex="-1"></a>            logits, _ <span class="op">=</span> <span class="va">self</span>(idx_cond)[:<span class="dv">2</span>]</span>
<span id="cb32-513"><a href="#cb32-513" aria-hidden="true" tabindex="-1"></a>            <span class="co"># pluck the logits at the final step and scale by desired temperature</span></span>
<span id="cb32-514"><a href="#cb32-514" aria-hidden="true" tabindex="-1"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="op">/</span> temperature</span>
<span id="cb32-515"><a href="#cb32-515" aria-hidden="true" tabindex="-1"></a>            <span class="co"># optionally crop the logits to only the top k options</span></span>
<span id="cb32-516"><a href="#cb32-516" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> top_k <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb32-517"><a href="#cb32-517" aria-hidden="true" tabindex="-1"></a>                v, _ <span class="op">=</span> torch.topk(logits, <span class="bu">min</span>(top_k, logits.size(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb32-518"><a href="#cb32-518" aria-hidden="true" tabindex="-1"></a>                logits[logits <span class="op">&lt;</span> v[:, [<span class="op">-</span><span class="dv">1</span>]]] <span class="op">=</span> <span class="op">-</span><span class="bu">float</span>(<span class="st">'Inf'</span>)</span>
<span id="cb32-519"><a href="#cb32-519" aria-hidden="true" tabindex="-1"></a>            <span class="co"># apply softmax to convert logits to (normalized) probabilities</span></span>
<span id="cb32-520"><a href="#cb32-520" aria-hidden="true" tabindex="-1"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb32-521"><a href="#cb32-521" aria-hidden="true" tabindex="-1"></a>            <span class="co"># sample from the distribution</span></span>
<span id="cb32-522"><a href="#cb32-522" aria-hidden="true" tabindex="-1"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb32-523"><a href="#cb32-523" aria-hidden="true" tabindex="-1"></a>            <span class="co"># append sampled index to the running sequence and continue</span></span>
<span id="cb32-524"><a href="#cb32-524" aria-hidden="true" tabindex="-1"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb32-525"><a href="#cb32-525" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-526"><a href="#cb32-526" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> idx</span>
<span id="cb32-527"><a href="#cb32-527" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-528"><a href="#cb32-528" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-529"><a href="#cb32-529" aria-hidden="true" tabindex="-1"></a><span class="fu">## Training</span></span>
<span id="cb32-530"><a href="#cb32-530" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-531"><a href="#cb32-531" aria-hidden="true" tabindex="-1"></a>Now that our model constructors are ready, it's time to start training! First, see below for the configuration settings we are using for the model. Most of these are not important for the purposes of this notebook, but just note that we are creating a model with 4 attention layers with 4 heads each (more on that below), our embedding size is 384 (so our vocabulary gets projected into 384-dimensional space), and our block size/context size is 300, so our model will look at 300 bp of sequence as each input. Training will take a while (about 5-10 minutes), so run the code block below and go to the end for a description of the Attention mechanism and a rundown of what our training loop does.</span>
<span id="cb32-532"><a href="#cb32-532" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-535"><a href="#cb32-535" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-536"><a href="#cb32-536" aria-hidden="true" tabindex="-1"></a><span class="co">#| colab: {base_uri: https://localhost:8080/}</span></span>
<span id="cb32-537"><a href="#cb32-537" aria-hidden="true" tabindex="-1"></a><span class="co">#@title Training Time!</span></span>
<span id="cb32-538"><a href="#cb32-538" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-539"><a href="#cb32-539" aria-hidden="true" tabindex="-1"></a>gc.collect()</span>
<span id="cb32-540"><a href="#cb32-540" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------------------------</span></span>
<span id="cb32-541"><a href="#cb32-541" aria-hidden="true" tabindex="-1"></a><span class="co"># default config values designed to train a gpt on hg38 reference genome</span></span>
<span id="cb32-542"><a href="#cb32-542" aria-hidden="true" tabindex="-1"></a><span class="co"># I/O</span></span>
<span id="cb32-543"><a href="#cb32-543" aria-hidden="true" tabindex="-1"></a>out_dir <span class="op">=</span> WORKDIR <span class="op">+</span> <span class="st">'out'</span></span>
<span id="cb32-544"><a href="#cb32-544" aria-hidden="true" tabindex="-1"></a>eval_interval <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb32-545"><a href="#cb32-545" aria-hidden="true" tabindex="-1"></a>log_interval <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb32-546"><a href="#cb32-546" aria-hidden="true" tabindex="-1"></a>eval_iters <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb32-547"><a href="#cb32-547" aria-hidden="true" tabindex="-1"></a>eval_only <span class="op">=</span> <span class="va">False</span> <span class="co"># if True, script exits right after the first eval</span></span>
<span id="cb32-548"><a href="#cb32-548" aria-hidden="true" tabindex="-1"></a>always_save_checkpoint <span class="op">=</span> <span class="va">False</span> <span class="co"># if True, always save a checkpoint after each eval</span></span>
<span id="cb32-549"><a href="#cb32-549" aria-hidden="true" tabindex="-1"></a>init_from <span class="op">=</span> <span class="st">'scratch'</span> <span class="co"># 'scratch' or 'resume' or 'gpt2*'</span></span>
<span id="cb32-550"><a href="#cb32-550" aria-hidden="true" tabindex="-1"></a><span class="co"># wandb logging</span></span>
<span id="cb32-551"><a href="#cb32-551" aria-hidden="true" tabindex="-1"></a>wandb_log <span class="op">=</span> <span class="va">False</span> <span class="co"># disabled by default</span></span>
<span id="cb32-552"><a href="#cb32-552" aria-hidden="true" tabindex="-1"></a>wandb_project <span class="op">=</span> <span class="st">'genome_char'</span></span>
<span id="cb32-553"><a href="#cb32-553" aria-hidden="true" tabindex="-1"></a>wandb_run_name <span class="op">=</span> <span class="st">'mini-gpt'</span> <span class="co"># 'run' + str(time.time())</span></span>
<span id="cb32-554"><a href="#cb32-554" aria-hidden="true" tabindex="-1"></a><span class="co"># data</span></span>
<span id="cb32-555"><a href="#cb32-555" aria-hidden="true" tabindex="-1"></a>dataset <span class="op">=</span> <span class="st">'genome_char'</span></span>
<span id="cb32-556"><a href="#cb32-556" aria-hidden="true" tabindex="-1"></a>gradient_accumulation_steps <span class="op">=</span> <span class="dv">1</span> <span class="co"># used to simulate larger batch sizes</span></span>
<span id="cb32-557"><a href="#cb32-557" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">64</span> <span class="co"># if gradient_accumulation_steps &gt; 1, this is the micro-batch size</span></span>
<span id="cb32-558"><a href="#cb32-558" aria-hidden="true" tabindex="-1"></a>block_size <span class="op">=</span> <span class="dv">300</span></span>
<span id="cb32-559"><a href="#cb32-559" aria-hidden="true" tabindex="-1"></a><span class="co"># model</span></span>
<span id="cb32-560"><a href="#cb32-560" aria-hidden="true" tabindex="-1"></a>n_layer <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb32-561"><a href="#cb32-561" aria-hidden="true" tabindex="-1"></a>n_head <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb32-562"><a href="#cb32-562" aria-hidden="true" tabindex="-1"></a>n_embd <span class="op">=</span> <span class="dv">384</span></span>
<span id="cb32-563"><a href="#cb32-563" aria-hidden="true" tabindex="-1"></a>dropout <span class="op">=</span> <span class="fl">0.2</span> <span class="co"># for pretraining 0 is good, for finetuning try 0.1+</span></span>
<span id="cb32-564"><a href="#cb32-564" aria-hidden="true" tabindex="-1"></a>bias <span class="op">=</span> <span class="va">False</span> <span class="co"># do we use bias inside LayerNorm and Linear layers?</span></span>
<span id="cb32-565"><a href="#cb32-565" aria-hidden="true" tabindex="-1"></a><span class="co"># adamw optimizer</span></span>
<span id="cb32-566"><a href="#cb32-566" aria-hidden="true" tabindex="-1"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span> <span class="co"># max learning rate</span></span>
<span id="cb32-567"><a href="#cb32-567" aria-hidden="true" tabindex="-1"></a>max_iters <span class="op">=</span> <span class="dv">4000</span> <span class="co"># total number of training iterations</span></span>
<span id="cb32-568"><a href="#cb32-568" aria-hidden="true" tabindex="-1"></a>weight_decay <span class="op">=</span> <span class="fl">1e-1</span></span>
<span id="cb32-569"><a href="#cb32-569" aria-hidden="true" tabindex="-1"></a>beta1 <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb32-570"><a href="#cb32-570" aria-hidden="true" tabindex="-1"></a>beta2 <span class="op">=</span> <span class="fl">0.99</span></span>
<span id="cb32-571"><a href="#cb32-571" aria-hidden="true" tabindex="-1"></a>grad_clip <span class="op">=</span> <span class="fl">1.0</span> <span class="co"># clip gradients at this value, or disable if == 0.0</span></span>
<span id="cb32-572"><a href="#cb32-572" aria-hidden="true" tabindex="-1"></a><span class="co"># learning rate decay settings</span></span>
<span id="cb32-573"><a href="#cb32-573" aria-hidden="true" tabindex="-1"></a>decay_lr <span class="op">=</span> <span class="va">True</span> <span class="co"># whether to decay the learning rate</span></span>
<span id="cb32-574"><a href="#cb32-574" aria-hidden="true" tabindex="-1"></a>warmup_iters <span class="op">=</span> <span class="dv">100</span> <span class="co"># how many steps to warm up for</span></span>
<span id="cb32-575"><a href="#cb32-575" aria-hidden="true" tabindex="-1"></a>lr_decay_iters <span class="op">=</span> <span class="dv">4000</span> <span class="co"># should be ~= max_iters per Chinchilla</span></span>
<span id="cb32-576"><a href="#cb32-576" aria-hidden="true" tabindex="-1"></a>min_lr <span class="op">=</span> <span class="fl">1e-4</span> <span class="co"># minimum learning rate, should be ~= learning_rate/10 per Chinchilla</span></span>
<span id="cb32-577"><a href="#cb32-577" aria-hidden="true" tabindex="-1"></a><span class="co"># DDP settings</span></span>
<span id="cb32-578"><a href="#cb32-578" aria-hidden="true" tabindex="-1"></a><span class="co">#backend = 'nccl' # 'nccl', 'gloo', etc.</span></span>
<span id="cb32-579"><a href="#cb32-579" aria-hidden="true" tabindex="-1"></a><span class="co"># system</span></span>
<span id="cb32-580"><a href="#cb32-580" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="co"># examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks</span></span>
<span id="cb32-581"><a href="#cb32-581" aria-hidden="true" tabindex="-1"></a>dtype <span class="op">=</span> <span class="st">'float16'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="kw">and</span> torch.cuda.is_bf16_supported() <span class="cf">else</span> <span class="st">'float16'</span> <span class="co"># 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler</span></span>
<span id="cb32-582"><a href="#cb32-582" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.cuda.is_available() <span class="kw">and</span> device <span class="op">==</span> <span class="st">'cuda'</span>:</span>
<span id="cb32-583"><a href="#cb32-583" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'cuda'</span>)</span>
<span id="cb32-584"><a href="#cb32-584" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb32-585"><a href="#cb32-585" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'CPU'</span>)</span>
<span id="cb32-586"><a href="#cb32-586" aria-hidden="true" tabindex="-1"></a><span class="bu">compile</span> <span class="op">=</span> <span class="va">True</span> <span class="co"># use PyTorch 2.0 to compile the model to be faster</span></span>
<span id="cb32-587"><a href="#cb32-587" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------------------------</span></span>
<span id="cb32-588"><a href="#cb32-588" aria-hidden="true" tabindex="-1"></a>config_keys <span class="op">=</span> [k <span class="cf">for</span> k,v <span class="kw">in</span> <span class="bu">globals</span>().items() <span class="cf">if</span> <span class="kw">not</span> k.startswith(<span class="st">'_'</span>) <span class="kw">and</span> <span class="bu">isinstance</span>(v, (<span class="bu">int</span>, <span class="bu">float</span>, <span class="bu">bool</span>, <span class="bu">str</span>))]</span>
<span id="cb32-589"><a href="#cb32-589" aria-hidden="true" tabindex="-1"></a><span class="co">#exec(open('configurator.py').read()) # overrides from command line or config file</span></span>
<span id="cb32-590"><a href="#cb32-590" aria-hidden="true" tabindex="-1"></a>config <span class="op">=</span> {k: <span class="bu">globals</span>()[k] <span class="cf">for</span> k <span class="kw">in</span> config_keys} <span class="co"># will be useful for logging</span></span>
<span id="cb32-591"><a href="#cb32-591" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------------------------</span></span>
<span id="cb32-592"><a href="#cb32-592" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-593"><a href="#cb32-593" aria-hidden="true" tabindex="-1"></a><span class="co"># various inits, derived attributes, I/O setup</span></span>
<span id="cb32-594"><a href="#cb32-594" aria-hidden="true" tabindex="-1"></a>master_process <span class="op">=</span> <span class="va">True</span></span>
<span id="cb32-595"><a href="#cb32-595" aria-hidden="true" tabindex="-1"></a>seed_offset <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-596"><a href="#cb32-596" aria-hidden="true" tabindex="-1"></a>ddp_world_size <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb32-597"><a href="#cb32-597" aria-hidden="true" tabindex="-1"></a>tokens_per_iter <span class="op">=</span> gradient_accumulation_steps <span class="op">*</span> ddp_world_size <span class="op">*</span> batch_size <span class="op">*</span> block_size</span>
<span id="cb32-598"><a href="#cb32-598" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"tokens per iteration will be: </span><span class="sc">{</span>tokens_per_iter<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb32-599"><a href="#cb32-599" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-600"><a href="#cb32-600" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> master_process:</span>
<span id="cb32-601"><a href="#cb32-601" aria-hidden="true" tabindex="-1"></a>    os.makedirs(out_dir, exist_ok<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-602"><a href="#cb32-602" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(<span class="dv">1337</span> <span class="op">+</span> seed_offset)</span>
<span id="cb32-603"><a href="#cb32-603" aria-hidden="true" tabindex="-1"></a>torch.backends.cuda.matmul.allow_tf32 <span class="op">=</span> <span class="va">True</span> <span class="co"># allow tf32 on matmul</span></span>
<span id="cb32-604"><a href="#cb32-604" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.allow_tf32 <span class="op">=</span> <span class="va">True</span> <span class="co"># allow tf32 on cudnn</span></span>
<span id="cb32-605"><a href="#cb32-605" aria-hidden="true" tabindex="-1"></a>device_type <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> <span class="st">'cuda'</span> <span class="kw">in</span> device <span class="cf">else</span> <span class="st">'cpu'</span> <span class="co"># for later use in torch.autocast</span></span>
<span id="cb32-606"><a href="#cb32-606" aria-hidden="true" tabindex="-1"></a><span class="co"># note: float16 data type will automatically use a GradScaler</span></span>
<span id="cb32-607"><a href="#cb32-607" aria-hidden="true" tabindex="-1"></a>ptdtype <span class="op">=</span> {<span class="st">'float32'</span>: torch.float32, <span class="st">'bfloat16'</span>: torch.bfloat16, <span class="st">'float16'</span>: torch.float16}[dtype]</span>
<span id="cb32-608"><a href="#cb32-608" aria-hidden="true" tabindex="-1"></a>ctx <span class="op">=</span> nullcontext() <span class="cf">if</span> device_type <span class="op">==</span> <span class="st">'cpu'</span> <span class="cf">else</span> torch.amp.autocast(device_type<span class="op">=</span>device_type, dtype<span class="op">=</span>ptdtype)</span>
<span id="cb32-609"><a href="#cb32-609" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-610"><a href="#cb32-610" aria-hidden="true" tabindex="-1"></a><span class="co"># poor man's data loader</span></span>
<span id="cb32-611"><a href="#cb32-611" aria-hidden="true" tabindex="-1"></a>data_dir <span class="op">=</span> <span class="st">'/root/data'</span></span>
<span id="cb32-612"><a href="#cb32-612" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_batch(split):</span>
<span id="cb32-613"><a href="#cb32-613" aria-hidden="true" tabindex="-1"></a>    <span class="co"># We recreate np.memmap every batch to avoid a memory leak, as per</span></span>
<span id="cb32-614"><a href="#cb32-614" aria-hidden="true" tabindex="-1"></a>    <span class="co"># https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122</span></span>
<span id="cb32-615"><a href="#cb32-615" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> split <span class="op">==</span> <span class="st">'train'</span>:</span>
<span id="cb32-616"><a href="#cb32-616" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> np.memmap(os.path.join(data_dir, <span class="st">'train.bin'</span>), dtype<span class="op">=</span>np.uint16, mode<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb32-617"><a href="#cb32-617" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb32-618"><a href="#cb32-618" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> np.memmap(os.path.join(data_dir, <span class="st">'val.bin'</span>), dtype<span class="op">=</span>np.uint16, mode<span class="op">=</span><span class="st">'r'</span>)</span>
<span id="cb32-619"><a href="#cb32-619" aria-hidden="true" tabindex="-1"></a>    ix <span class="op">=</span> torch.randint(<span class="bu">len</span>(data) <span class="op">-</span> block_size, (batch_size,))</span>
<span id="cb32-620"><a href="#cb32-620" aria-hidden="true" tabindex="-1"></a>    x <span class="op">=</span> torch.stack([torch.from_numpy((data[i:i<span class="op">+</span>block_size]).astype(np.int64)) <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb32-621"><a href="#cb32-621" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> torch.stack([torch.from_numpy((data[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span><span class="dv">1</span><span class="op">+</span>block_size]).astype(np.int64)) <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb32-622"><a href="#cb32-622" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> device_type <span class="op">==</span> <span class="st">'cuda'</span>:</span>
<span id="cb32-623"><a href="#cb32-623" aria-hidden="true" tabindex="-1"></a>        <span class="co"># pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)</span></span>
<span id="cb32-624"><a href="#cb32-624" aria-hidden="true" tabindex="-1"></a>        x, y <span class="op">=</span> x.pin_memory().to(device, non_blocking<span class="op">=</span><span class="va">True</span>), y.pin_memory().to(device, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-625"><a href="#cb32-625" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb32-626"><a href="#cb32-626" aria-hidden="true" tabindex="-1"></a>        x, y <span class="op">=</span> x.to(device), y.to(device)</span>
<span id="cb32-627"><a href="#cb32-627" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x, y</span>
<span id="cb32-628"><a href="#cb32-628" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-629"><a href="#cb32-629" aria-hidden="true" tabindex="-1"></a><span class="co"># init these up here, can override if init_from='resume' (i.e. from a checkpoint)</span></span>
<span id="cb32-630"><a href="#cb32-630" aria-hidden="true" tabindex="-1"></a>iter_num <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-631"><a href="#cb32-631" aria-hidden="true" tabindex="-1"></a>best_val_loss <span class="op">=</span> <span class="fl">1e9</span></span>
<span id="cb32-632"><a href="#cb32-632" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-633"><a href="#cb32-633" aria-hidden="true" tabindex="-1"></a><span class="co"># attempt to derive vocab_size from the dataset</span></span>
<span id="cb32-634"><a href="#cb32-634" aria-hidden="true" tabindex="-1"></a>meta_path <span class="op">=</span> os.path.join(data_dir, <span class="st">'meta.pkl'</span>)</span>
<span id="cb32-635"><a href="#cb32-635" aria-hidden="true" tabindex="-1"></a>meta_vocab_size <span class="op">=</span> <span class="va">None</span></span>
<span id="cb32-636"><a href="#cb32-636" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> os.path.exists(meta_path):</span>
<span id="cb32-637"><a href="#cb32-637" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(meta_path, <span class="st">'rb'</span>) <span class="im">as</span> f:</span>
<span id="cb32-638"><a href="#cb32-638" aria-hidden="true" tabindex="-1"></a>        meta <span class="op">=</span> pickle.load(f)</span>
<span id="cb32-639"><a href="#cb32-639" aria-hidden="true" tabindex="-1"></a>    meta_vocab_size <span class="op">=</span> meta[<span class="st">'vocab_size'</span>]</span>
<span id="cb32-640"><a href="#cb32-640" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"found vocab_size = </span><span class="sc">{</span>meta_vocab_size<span class="sc">}</span><span class="ss"> (inside </span><span class="sc">{</span>meta_path<span class="sc">}</span><span class="ss">)"</span>)</span>
<span id="cb32-641"><a href="#cb32-641" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-642"><a href="#cb32-642" aria-hidden="true" tabindex="-1"></a><span class="co"># model init</span></span>
<span id="cb32-643"><a href="#cb32-643" aria-hidden="true" tabindex="-1"></a>model_args <span class="op">=</span> <span class="bu">dict</span>(n_layer<span class="op">=</span>n_layer, n_head<span class="op">=</span>n_head, n_embd<span class="op">=</span>n_embd, block_size<span class="op">=</span>block_size,</span>
<span id="cb32-644"><a href="#cb32-644" aria-hidden="true" tabindex="-1"></a>                  bias<span class="op">=</span>bias, vocab_size<span class="op">=</span><span class="va">None</span>, dropout<span class="op">=</span>dropout) <span class="co"># start with model_args from command line</span></span>
<span id="cb32-645"><a href="#cb32-645" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> init_from <span class="op">==</span> <span class="st">'scratch'</span>:</span>
<span id="cb32-646"><a href="#cb32-646" aria-hidden="true" tabindex="-1"></a>    <span class="co"># init a new model from scratch</span></span>
<span id="cb32-647"><a href="#cb32-647" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Initializing a new model from scratch"</span>)</span>
<span id="cb32-648"><a href="#cb32-648" aria-hidden="true" tabindex="-1"></a>    <span class="co"># determine the vocab size we'll use for from-scratch training</span></span>
<span id="cb32-649"><a href="#cb32-649" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> meta_vocab_size <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb32-650"><a href="#cb32-650" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)"</span>)</span>
<span id="cb32-651"><a href="#cb32-651" aria-hidden="true" tabindex="-1"></a>    model_args[<span class="st">'vocab_size'</span>] <span class="op">=</span> meta_vocab_size <span class="cf">if</span> meta_vocab_size <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> <span class="dv">50304</span></span>
<span id="cb32-652"><a href="#cb32-652" aria-hidden="true" tabindex="-1"></a>    gptconf <span class="op">=</span> GPTConfig(<span class="op">**</span>model_args)</span>
<span id="cb32-653"><a href="#cb32-653" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> GPT(gptconf)</span>
<span id="cb32-654"><a href="#cb32-654" aria-hidden="true" tabindex="-1"></a><span class="co"># crop down the model block size if desired, using model surgery</span></span>
<span id="cb32-655"><a href="#cb32-655" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> block_size <span class="op">&lt;</span> model.config.block_size:</span>
<span id="cb32-656"><a href="#cb32-656" aria-hidden="true" tabindex="-1"></a>    model.crop_block_size(block_size)</span>
<span id="cb32-657"><a href="#cb32-657" aria-hidden="true" tabindex="-1"></a>    model_args[<span class="st">'block_size'</span>] <span class="op">=</span> block_size <span class="co"># so that the checkpoint will have the right value</span></span>
<span id="cb32-658"><a href="#cb32-658" aria-hidden="true" tabindex="-1"></a>model.to(device)</span>
<span id="cb32-659"><a href="#cb32-659" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-660"><a href="#cb32-660" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize a GradScaler. If enabled=False scaler is a no-op</span></span>
<span id="cb32-661"><a href="#cb32-661" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> torch.cuda.amp.GradScaler(enabled<span class="op">=</span>(dtype <span class="op">==</span> <span class="st">'float16'</span>))</span>
<span id="cb32-662"><a href="#cb32-662" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-663"><a href="#cb32-663" aria-hidden="true" tabindex="-1"></a><span class="co"># optimizer</span></span>
<span id="cb32-664"><a href="#cb32-664" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)</span>
<span id="cb32-665"><a href="#cb32-665" aria-hidden="true" tabindex="-1"></a><span class="co">#if init_from == 'resume':</span></span>
<span id="cb32-666"><a href="#cb32-666" aria-hidden="true" tabindex="-1"></a><span class="co">#    optimizer.load_state_dict(checkpoint['optimizer'])</span></span>
<span id="cb32-667"><a href="#cb32-667" aria-hidden="true" tabindex="-1"></a>checkpoint <span class="op">=</span> <span class="va">None</span> <span class="co"># free up memory</span></span>
<span id="cb32-668"><a href="#cb32-668" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-669"><a href="#cb32-669" aria-hidden="true" tabindex="-1"></a><span class="co"># compile the model</span></span>
<span id="cb32-670"><a href="#cb32-670" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">compile</span>:</span>
<span id="cb32-671"><a href="#cb32-671" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"compiling the model... (takes a ~minute)"</span>)</span>
<span id="cb32-672"><a href="#cb32-672" aria-hidden="true" tabindex="-1"></a>    unoptimized_model <span class="op">=</span> model</span>
<span id="cb32-673"><a href="#cb32-673" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> torch.<span class="bu">compile</span>(model) <span class="co"># requires PyTorch 2.0</span></span>
<span id="cb32-674"><a href="#cb32-674" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-675"><a href="#cb32-675" aria-hidden="true" tabindex="-1"></a><span class="co"># helps estimate an arbitrarily accurate loss over either split using many batches</span></span>
<span id="cb32-676"><a href="#cb32-676" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb32-677"><a href="#cb32-677" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> estimate_loss():</span>
<span id="cb32-678"><a href="#cb32-678" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> {}</span>
<span id="cb32-679"><a href="#cb32-679" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb32-680"><a href="#cb32-680" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> split <span class="kw">in</span> [<span class="st">'train'</span>, <span class="st">'val'</span>]:</span>
<span id="cb32-681"><a href="#cb32-681" aria-hidden="true" tabindex="-1"></a>        losses <span class="op">=</span> torch.zeros(eval_iters)</span>
<span id="cb32-682"><a href="#cb32-682" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(eval_iters):</span>
<span id="cb32-683"><a href="#cb32-683" aria-hidden="true" tabindex="-1"></a>            X, Y <span class="op">=</span> get_batch(split)</span>
<span id="cb32-684"><a href="#cb32-684" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> ctx:</span>
<span id="cb32-685"><a href="#cb32-685" aria-hidden="true" tabindex="-1"></a>                logits, loss <span class="op">=</span> model(X, Y)[:<span class="dv">2</span>]</span>
<span id="cb32-686"><a href="#cb32-686" aria-hidden="true" tabindex="-1"></a>            losses[k] <span class="op">=</span> loss.item()</span>
<span id="cb32-687"><a href="#cb32-687" aria-hidden="true" tabindex="-1"></a>        out[split] <span class="op">=</span> losses.mean()</span>
<span id="cb32-688"><a href="#cb32-688" aria-hidden="true" tabindex="-1"></a>    model.train()</span>
<span id="cb32-689"><a href="#cb32-689" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out</span>
<span id="cb32-690"><a href="#cb32-690" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-691"><a href="#cb32-691" aria-hidden="true" tabindex="-1"></a><span class="co"># learning rate decay scheduler (cosine with warmup)</span></span>
<span id="cb32-692"><a href="#cb32-692" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_lr(it):</span>
<span id="cb32-693"><a href="#cb32-693" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1) linear warmup for warmup_iters steps</span></span>
<span id="cb32-694"><a href="#cb32-694" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> it <span class="op">&lt;</span> warmup_iters:</span>
<span id="cb32-695"><a href="#cb32-695" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> learning_rate <span class="op">*</span> it <span class="op">/</span> warmup_iters</span>
<span id="cb32-696"><a href="#cb32-696" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 2) if it &gt; lr_decay_iters, return min learning rate</span></span>
<span id="cb32-697"><a href="#cb32-697" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> it <span class="op">&gt;</span> lr_decay_iters:</span>
<span id="cb32-698"><a href="#cb32-698" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> min_lr</span>
<span id="cb32-699"><a href="#cb32-699" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 3) in between, use cosine decay down to min learning rate</span></span>
<span id="cb32-700"><a href="#cb32-700" aria-hidden="true" tabindex="-1"></a>    decay_ratio <span class="op">=</span> (it <span class="op">-</span> warmup_iters) <span class="op">/</span> (lr_decay_iters <span class="op">-</span> warmup_iters)</span>
<span id="cb32-701"><a href="#cb32-701" aria-hidden="true" tabindex="-1"></a>    <span class="cf">assert</span> <span class="dv">0</span> <span class="op">&lt;=</span> decay_ratio <span class="op">&lt;=</span> <span class="dv">1</span></span>
<span id="cb32-702"><a href="#cb32-702" aria-hidden="true" tabindex="-1"></a>    coeff <span class="op">=</span> <span class="fl">0.5</span> <span class="op">*</span> (<span class="fl">1.0</span> <span class="op">+</span> math.cos(math.pi <span class="op">*</span> decay_ratio)) <span class="co"># coeff ranges 0..1</span></span>
<span id="cb32-703"><a href="#cb32-703" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> min_lr <span class="op">+</span> coeff <span class="op">*</span> (learning_rate <span class="op">-</span> min_lr)</span>
<span id="cb32-704"><a href="#cb32-704" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-705"><a href="#cb32-705" aria-hidden="true" tabindex="-1"></a><span class="co"># logging</span></span>
<span id="cb32-706"><a href="#cb32-706" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> wandb_log <span class="kw">and</span> master_process:</span>
<span id="cb32-707"><a href="#cb32-707" aria-hidden="true" tabindex="-1"></a>    <span class="im">import</span> wandb</span>
<span id="cb32-708"><a href="#cb32-708" aria-hidden="true" tabindex="-1"></a>    wandb.init(project<span class="op">=</span>wandb_project, name<span class="op">=</span>wandb_run_name, config<span class="op">=</span>config)</span>
<span id="cb32-709"><a href="#cb32-709" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-710"><a href="#cb32-710" aria-hidden="true" tabindex="-1"></a><span class="co"># training loop</span></span>
<span id="cb32-711"><a href="#cb32-711" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> get_batch(<span class="st">'train'</span>) <span class="co"># fetch the very first batch</span></span>
<span id="cb32-712"><a href="#cb32-712" aria-hidden="true" tabindex="-1"></a>t0 <span class="op">=</span> time.time()</span>
<span id="cb32-713"><a href="#cb32-713" aria-hidden="true" tabindex="-1"></a>local_iter_num <span class="op">=</span> <span class="dv">0</span> <span class="co"># number of iterations in the lifetime of this process</span></span>
<span id="cb32-714"><a href="#cb32-714" aria-hidden="true" tabindex="-1"></a>raw_model <span class="op">=</span> model</span>
<span id="cb32-715"><a href="#cb32-715" aria-hidden="true" tabindex="-1"></a>running_mfu <span class="op">=</span> <span class="op">-</span><span class="fl">1.0</span></span>
<span id="cb32-716"><a href="#cb32-716" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb32-717"><a href="#cb32-717" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-718"><a href="#cb32-718" aria-hidden="true" tabindex="-1"></a>    <span class="co"># determine and set the learning rate for this iteration</span></span>
<span id="cb32-719"><a href="#cb32-719" aria-hidden="true" tabindex="-1"></a>    lr <span class="op">=</span> get_lr(iter_num) <span class="cf">if</span> decay_lr <span class="cf">else</span> learning_rate</span>
<span id="cb32-720"><a href="#cb32-720" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> param_group <span class="kw">in</span> optimizer.param_groups:</span>
<span id="cb32-721"><a href="#cb32-721" aria-hidden="true" tabindex="-1"></a>        param_group[<span class="st">'lr'</span>] <span class="op">=</span> lr</span>
<span id="cb32-722"><a href="#cb32-722" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-723"><a href="#cb32-723" aria-hidden="true" tabindex="-1"></a>    <span class="co"># evaluate the loss on train/val sets and write checkpoints</span></span>
<span id="cb32-724"><a href="#cb32-724" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> iter_num <span class="op">%</span> eval_interval <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> master_process:</span>
<span id="cb32-725"><a href="#cb32-725" aria-hidden="true" tabindex="-1"></a>        losses <span class="op">=</span> estimate_loss()</span>
<span id="cb32-726"><a href="#cb32-726" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"step </span><span class="sc">{</span>iter_num<span class="sc">}</span><span class="ss">: train loss </span><span class="sc">{</span>losses[<span class="st">'train'</span>]<span class="sc">:.4f}</span><span class="ss">, val loss </span><span class="sc">{</span>losses[<span class="st">'val'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb32-727"><a href="#cb32-727" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> wandb_log:</span>
<span id="cb32-728"><a href="#cb32-728" aria-hidden="true" tabindex="-1"></a>            wandb.log({</span>
<span id="cb32-729"><a href="#cb32-729" aria-hidden="true" tabindex="-1"></a>                <span class="st">"iter"</span>: iter_num,</span>
<span id="cb32-730"><a href="#cb32-730" aria-hidden="true" tabindex="-1"></a>                <span class="st">"train/loss"</span>: losses[<span class="st">'train'</span>],</span>
<span id="cb32-731"><a href="#cb32-731" aria-hidden="true" tabindex="-1"></a>                <span class="st">"val/loss"</span>: losses[<span class="st">'val'</span>],</span>
<span id="cb32-732"><a href="#cb32-732" aria-hidden="true" tabindex="-1"></a>                <span class="st">"lr"</span>: lr,</span>
<span id="cb32-733"><a href="#cb32-733" aria-hidden="true" tabindex="-1"></a>                <span class="st">"mfu"</span>: running_mfu<span class="op">*</span><span class="dv">100</span>, <span class="co"># convert to percentage</span></span>
<span id="cb32-734"><a href="#cb32-734" aria-hidden="true" tabindex="-1"></a>            })</span>
<span id="cb32-735"><a href="#cb32-735" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> losses[<span class="st">'val'</span>] <span class="op">&lt;</span> best_val_loss <span class="kw">or</span> always_save_checkpoint:</span>
<span id="cb32-736"><a href="#cb32-736" aria-hidden="true" tabindex="-1"></a>            best_val_loss <span class="op">=</span> losses[<span class="st">'val'</span>]</span>
<span id="cb32-737"><a href="#cb32-737" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> iter_num <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb32-738"><a href="#cb32-738" aria-hidden="true" tabindex="-1"></a>                checkpoint <span class="op">=</span> {</span>
<span id="cb32-739"><a href="#cb32-739" aria-hidden="true" tabindex="-1"></a>                    <span class="st">'model'</span>: raw_model.state_dict(),</span>
<span id="cb32-740"><a href="#cb32-740" aria-hidden="true" tabindex="-1"></a>                    <span class="st">'optimizer'</span>: optimizer.state_dict(),</span>
<span id="cb32-741"><a href="#cb32-741" aria-hidden="true" tabindex="-1"></a>                    <span class="st">'model_args'</span>: model_args,</span>
<span id="cb32-742"><a href="#cb32-742" aria-hidden="true" tabindex="-1"></a>                    <span class="st">'iter_num'</span>: iter_num,</span>
<span id="cb32-743"><a href="#cb32-743" aria-hidden="true" tabindex="-1"></a>                    <span class="st">'best_val_loss'</span>: best_val_loss,</span>
<span id="cb32-744"><a href="#cb32-744" aria-hidden="true" tabindex="-1"></a>                    <span class="st">'config'</span>: config,</span>
<span id="cb32-745"><a href="#cb32-745" aria-hidden="true" tabindex="-1"></a>                }</span>
<span id="cb32-746"><a href="#cb32-746" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="ss">f"saving checkpoint to </span><span class="sc">{</span>out_dir<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-747"><a href="#cb32-747" aria-hidden="true" tabindex="-1"></a>                torch.save(checkpoint, os.path.join(out_dir, <span class="st">'ckpt.pt'</span>))</span>
<span id="cb32-748"><a href="#cb32-748" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> iter_num <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> eval_only:</span>
<span id="cb32-749"><a href="#cb32-749" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb32-750"><a href="#cb32-750" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-751"><a href="#cb32-751" aria-hidden="true" tabindex="-1"></a>    <span class="co"># forward backward update, with optional gradient accumulation to simulate larger batch size</span></span>
<span id="cb32-752"><a href="#cb32-752" aria-hidden="true" tabindex="-1"></a>    <span class="co"># and using the GradScaler if data type is float16</span></span>
<span id="cb32-753"><a href="#cb32-753" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> micro_step <span class="kw">in</span> <span class="bu">range</span>(gradient_accumulation_steps):</span>
<span id="cb32-754"><a href="#cb32-754" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> ctx:</span>
<span id="cb32-755"><a href="#cb32-755" aria-hidden="true" tabindex="-1"></a>            logits, loss <span class="op">=</span> model(X, Y)[:<span class="dv">2</span>]</span>
<span id="cb32-756"><a href="#cb32-756" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> loss <span class="op">/</span> gradient_accumulation_steps <span class="co"># scale the loss to account for gradient accumulation</span></span>
<span id="cb32-757"><a href="#cb32-757" aria-hidden="true" tabindex="-1"></a>        <span class="co"># immediately async prefetch next batch while model is doing the forward pass on the GPU</span></span>
<span id="cb32-758"><a href="#cb32-758" aria-hidden="true" tabindex="-1"></a>        X, Y <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb32-759"><a href="#cb32-759" aria-hidden="true" tabindex="-1"></a>        <span class="co"># backward pass, with gradient scaling if training in fp16</span></span>
<span id="cb32-760"><a href="#cb32-760" aria-hidden="true" tabindex="-1"></a>        scaler.scale(loss).backward()</span>
<span id="cb32-761"><a href="#cb32-761" aria-hidden="true" tabindex="-1"></a>    <span class="co"># clip the gradient</span></span>
<span id="cb32-762"><a href="#cb32-762" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> grad_clip <span class="op">!=</span> <span class="fl">0.0</span>:</span>
<span id="cb32-763"><a href="#cb32-763" aria-hidden="true" tabindex="-1"></a>        scaler.unscale_(optimizer)</span>
<span id="cb32-764"><a href="#cb32-764" aria-hidden="true" tabindex="-1"></a>        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)</span>
<span id="cb32-765"><a href="#cb32-765" aria-hidden="true" tabindex="-1"></a>    <span class="co"># step the optimizer and scaler if training in fp16</span></span>
<span id="cb32-766"><a href="#cb32-766" aria-hidden="true" tabindex="-1"></a>    scaler.step(optimizer)</span>
<span id="cb32-767"><a href="#cb32-767" aria-hidden="true" tabindex="-1"></a>    scaler.update()</span>
<span id="cb32-768"><a href="#cb32-768" aria-hidden="true" tabindex="-1"></a>    <span class="co"># flush the gradients as soon as we can, no need for this memory anymore</span></span>
<span id="cb32-769"><a href="#cb32-769" aria-hidden="true" tabindex="-1"></a>    optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb32-770"><a href="#cb32-770" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-771"><a href="#cb32-771" aria-hidden="true" tabindex="-1"></a>    <span class="co"># timing and logging</span></span>
<span id="cb32-772"><a href="#cb32-772" aria-hidden="true" tabindex="-1"></a>    t1 <span class="op">=</span> time.time()</span>
<span id="cb32-773"><a href="#cb32-773" aria-hidden="true" tabindex="-1"></a>    dt <span class="op">=</span> t1 <span class="op">-</span> t0</span>
<span id="cb32-774"><a href="#cb32-774" aria-hidden="true" tabindex="-1"></a>    t0 <span class="op">=</span> t1</span>
<span id="cb32-775"><a href="#cb32-775" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> iter_num <span class="op">%</span> log_interval <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> master_process:</span>
<span id="cb32-776"><a href="#cb32-776" aria-hidden="true" tabindex="-1"></a>        <span class="co"># get loss as float. note: this is a CPU-GPU sync point</span></span>
<span id="cb32-777"><a href="#cb32-777" aria-hidden="true" tabindex="-1"></a>        <span class="co"># scale up to undo the division above, approximating the true total loss (exact would have been a sum)</span></span>
<span id="cb32-778"><a href="#cb32-778" aria-hidden="true" tabindex="-1"></a>        lossf <span class="op">=</span> loss.item() <span class="op">*</span> gradient_accumulation_steps</span>
<span id="cb32-779"><a href="#cb32-779" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> local_iter_num <span class="op">&gt;=</span> <span class="dv">5</span>: <span class="co"># let the training loop settle a bit</span></span>
<span id="cb32-780"><a href="#cb32-780" aria-hidden="true" tabindex="-1"></a>            mfu <span class="op">=</span> raw_model.estimate_mfu(batch_size <span class="op">*</span> gradient_accumulation_steps, dt)</span>
<span id="cb32-781"><a href="#cb32-781" aria-hidden="true" tabindex="-1"></a>            running_mfu <span class="op">=</span> mfu <span class="cf">if</span> running_mfu <span class="op">==</span> <span class="op">-</span><span class="fl">1.0</span> <span class="cf">else</span> <span class="fl">0.9</span><span class="op">*</span>running_mfu <span class="op">+</span> <span class="fl">0.1</span><span class="op">*</span>mfu</span>
<span id="cb32-782"><a href="#cb32-782" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"iter </span><span class="sc">{</span>iter_num<span class="sc">}</span><span class="ss">: loss </span><span class="sc">{</span>lossf<span class="sc">:.4f}</span><span class="ss">, time </span><span class="sc">{</span>dt<span class="op">*</span><span class="dv">1000</span><span class="sc">:.2f}</span><span class="ss">ms, mfu </span><span class="sc">{</span>running_mfu<span class="op">*</span><span class="dv">100</span><span class="sc">:.2f}</span><span class="ss">%"</span>)</span>
<span id="cb32-783"><a href="#cb32-783" aria-hidden="true" tabindex="-1"></a>    iter_num <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-784"><a href="#cb32-784" aria-hidden="true" tabindex="-1"></a>    local_iter_num <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-785"><a href="#cb32-785" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-786"><a href="#cb32-786" aria-hidden="true" tabindex="-1"></a>    <span class="co"># termination conditions</span></span>
<span id="cb32-787"><a href="#cb32-787" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> iter_num <span class="op">&gt;</span> max_iters:</span>
<span id="cb32-788"><a href="#cb32-788" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb32-789"><a href="#cb32-789" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-790"><a href="#cb32-790" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-791"><a href="#cb32-791" aria-hidden="true" tabindex="-1"></a>Now that we've started training, let's talk about Attention. The first paper which outlines the Transformer neural network architecture (which relies heavily on the Attention mechanism and is the basis for most large language models today) is <span class="co">[</span><span class="ot">Attention is All you Need</span><span class="co">](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)</span>. This paper describes the application of Attention in a machine translation model, but the underlying ideas can be applied across any model that processes sequences of text.</span>
<span id="cb32-792"><a href="#cb32-792" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-793"><a href="#cb32-793" aria-hidden="true" tabindex="-1"></a>At its core, the Attention mechanism allows us to analyze the pairwise relationship between every token in a given sequence. So, in a very simple example, if you're looking at the sentence "The cow jumped over the moon", and you are tokenizing by word, you may expect there to be a strong relationship between "jumped" and "over" (as one tends to jump over things), but not between the two iterations of "the". Below, I describe the actual mechanism behind this in plain language. It will be an oversimplification, but hopefully it can provide some context into what our model is doing.</span>
<span id="cb32-794"><a href="#cb32-794" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-795"><a href="#cb32-795" aria-hidden="true" tabindex="-1"></a>The way we model this using Attention is via Query (Q), Key (K), and Value (V) matrices. In short, the input into the Attention layer goes through three separate matrix multiplication processes with different weights to create Q, K, and V. The idea behind these matrices is that Q represents each position/token in the sequence, and what information each of those positions is "looking for". K represents the information that each position contains, and therefore the actual Attention Score is obtained by doing matrix multiplication on Q and K. If two positions are a good match, they will have a higher attention score. The authors of the <span class="co">[</span><span class="ot">Enformer</span><span class="co">](https://www.nature.com/articles/s41592-021-01252-x)</span> paper provide a good, simplified visualization of this, where darker colors represent a stronger relationship in the Key vector to the given Q position:</span>
<span id="cb32-796"><a href="#cb32-796" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-797"><a href="#cb32-797" aria-hidden="true" tabindex="-1"></a><span class="al">![query-key-value](/post/images/key-query.png)</span></span>
<span id="cb32-798"><a href="#cb32-798" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-799"><a href="#cb32-799" aria-hidden="true" tabindex="-1"></a>Once we have our Attention score matrix, we then multiply this by the V matrix, which represents the actual information of each token. This puts the Attention scores back into the context of the original token embeddings, which can then be passed to future layers.</span>
<span id="cb32-800"><a href="#cb32-800" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-801"><a href="#cb32-801" aria-hidden="true" tabindex="-1"></a>One thing to note is that this calculation is often done in parallel, across multiple "heads" within a single layer. The idea behind this is that each head can learn different weights, and therefore multiple types of relationships between tokens can be represented within the same model. The <span class="co">[</span><span class="ot">Attention is All you Need</span><span class="co">](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)</span> paper provides a good diagram of the Attention process, and what multi-head attention looks like (note that Scaled Dot-Produce Attention is essentially the process we just described):</span>
<span id="cb32-802"><a href="#cb32-802" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-803"><a href="#cb32-803" aria-hidden="true" tabindex="-1"></a><span class="al">![Attention_Diagram](/post/images/scale-dot-attention-multihead.png)</span></span>
<span id="cb32-804"><a href="#cb32-804" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-805"><a href="#cb32-805" aria-hidden="true" tabindex="-1"></a>Now that we've defined the Attention mechanism, our training loop is actually pretty straightforward. See below for the steps:</span>
<span id="cb32-806"><a href="#cb32-806" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-807"><a href="#cb32-807" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-808"><a href="#cb32-808" aria-hidden="true" tabindex="-1"></a><span class="ss">1.   </span>Generate a batch from our training data: This consists of sampling a 300 bp sequence out of the genome as our "data", then the same 300 bp but shifted over by 1 as our "labels". Our batch size is 64, so a single batch will be 64 of these pairs of sequences</span>
<span id="cb32-809"><a href="#cb32-809" aria-hidden="true" tabindex="-1"></a><span class="ss">2.   </span>Plug the batch into our model to calculate loss. Our model is "causal", so it tries to predict the next token based on all previous tokens (i.e. data<span class="co">[</span><span class="ot">0</span><span class="co">]</span> is used to predict label<span class="co">[</span><span class="ot">0</span><span class="co">]</span>, data<span class="co">[</span><span class="ot">0:1</span><span class="co">]</span> is used to predict label<span class="co">[</span><span class="ot">1</span><span class="co">]</span>, data<span class="co">[</span><span class="ot">0:2</span><span class="co">]</span> is used to predict label<span class="co">[</span><span class="ot">2</span><span class="co">]</span>, etc.)</span>
<span id="cb32-810"><a href="#cb32-810" aria-hidden="true" tabindex="-1"></a><span class="ss">3.   </span>Once we have calculated the loss (using Cross Entropy Loss), we begin the backward pass and take a step based on the gradients of all of our parameters.</span>
<span id="cb32-811"><a href="#cb32-811" aria-hidden="true" tabindex="-1"></a><span class="ss">4.   </span>Grab another batch and repeat</span>
<span id="cb32-812"><a href="#cb32-812" aria-hidden="true" tabindex="-1"></a><span class="ss">5.   </span>Once we hit the evaluation interval (1000 batches), we pause our model training, and evaluate the average loss over 100 additional batches of the training and validation sets, to get a checkpoint of our model performance</span>
<span id="cb32-813"><a href="#cb32-813" aria-hidden="true" tabindex="-1"></a><span class="ss">6.   </span>If the performance on the validation data is the best we've seen yet, we save this version of the model as a checkpoint to use later.</span>
<span id="cb32-814"><a href="#cb32-814" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-815"><a href="#cb32-815" aria-hidden="true" tabindex="-1"></a>Obviously there is quite a bit more code than just these steps above, but this summary covers all of the functionally important parts. Much of the extra code above is related to making the model run efficiently, keeping track of how long it has been running, ensuring we are allocating GPU/CPU memory correctly, etc.</span>
<span id="cb32-816"><a href="#cb32-816" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-817"><a href="#cb32-817" aria-hidden="true" tabindex="-1"></a><span class="fu">## Sampling from the model</span></span>
<span id="cb32-818"><a href="#cb32-818" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-819"><a href="#cb32-819" aria-hidden="true" tabindex="-1"></a>Great! Now that we have a trained model, let's see what it can generate it. The code block below simply pulls out the highest-performing version of the model we just trained, and feeds it a newline character to see what it will generate:</span>
<span id="cb32-820"><a href="#cb32-820" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-821"><a href="#cb32-821" aria-hidden="true" tabindex="-1"></a>Sampling from the model</span>
<span id="cb32-822"><a href="#cb32-822" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-825"><a href="#cb32-825" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-826"><a href="#cb32-826" aria-hidden="true" tabindex="-1"></a><span class="co">#| colab: {base_uri: https://localhost:8080/}</span></span>
<span id="cb32-827"><a href="#cb32-827" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb32-828"><a href="#cb32-828" aria-hidden="true" tabindex="-1"></a><span class="co">Sample from a trained model</span></span>
<span id="cb32-829"><a href="#cb32-829" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb32-830"><a href="#cb32-830" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-831"><a href="#cb32-831" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------------------------</span></span>
<span id="cb32-832"><a href="#cb32-832" aria-hidden="true" tabindex="-1"></a>init_from <span class="op">=</span> <span class="st">'resume'</span> <span class="co"># either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')</span></span>
<span id="cb32-833"><a href="#cb32-833" aria-hidden="true" tabindex="-1"></a>out_dir <span class="op">=</span> <span class="st">'/root/data/out'</span> <span class="co"># ignored if init_from is not 'resume'</span></span>
<span id="cb32-834"><a href="#cb32-834" aria-hidden="true" tabindex="-1"></a>start <span class="op">=</span> <span class="st">"</span><span class="ch">\n</span><span class="st">"</span> <span class="co"># or "&lt;|endoftext|&gt;" or etc. Can also specify a file, use as: "FILE:prompt.txt"</span></span>
<span id="cb32-835"><a href="#cb32-835" aria-hidden="true" tabindex="-1"></a>num_samples <span class="op">=</span> <span class="dv">5</span> <span class="co"># number of samples to draw</span></span>
<span id="cb32-836"><a href="#cb32-836" aria-hidden="true" tabindex="-1"></a>max_new_tokens <span class="op">=</span> <span class="dv">250</span> <span class="co"># number of tokens generated in each sample</span></span>
<span id="cb32-837"><a href="#cb32-837" aria-hidden="true" tabindex="-1"></a>temperature <span class="op">=</span> <span class="fl">0.8</span> <span class="co"># 1.0 = no change, &lt; 1.0 = less random, &gt; 1.0 = more random, in predictions</span></span>
<span id="cb32-838"><a href="#cb32-838" aria-hidden="true" tabindex="-1"></a>top_k <span class="op">=</span> <span class="dv">200</span> <span class="co"># retain only the top_k most likely tokens, clamp others to have 0 probability</span></span>
<span id="cb32-839"><a href="#cb32-839" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">1337</span></span>
<span id="cb32-840"><a href="#cb32-840" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="co"># examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.</span></span>
<span id="cb32-841"><a href="#cb32-841" aria-hidden="true" tabindex="-1"></a>dtype <span class="op">=</span> <span class="st">'bfloat16'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="kw">and</span> torch.cuda.is_bf16_supported() <span class="cf">else</span> <span class="st">'float16'</span> <span class="co"># 'float32' or 'bfloat16' or 'float16'</span></span>
<span id="cb32-842"><a href="#cb32-842" aria-hidden="true" tabindex="-1"></a><span class="bu">compile</span> <span class="op">=</span> <span class="va">True</span> <span class="co"># use PyTorch 2.0 to compile the model to be faster</span></span>
<span id="cb32-843"><a href="#cb32-843" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------------------------</span></span>
<span id="cb32-844"><a href="#cb32-844" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-845"><a href="#cb32-845" aria-hidden="true" tabindex="-1"></a>torch.cuda.init()</span>
<span id="cb32-846"><a href="#cb32-846" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(seed)</span>
<span id="cb32-847"><a href="#cb32-847" aria-hidden="true" tabindex="-1"></a>torch.cuda.manual_seed(seed)</span>
<span id="cb32-848"><a href="#cb32-848" aria-hidden="true" tabindex="-1"></a>torch.backends.cuda.matmul.allow_tf32 <span class="op">=</span> <span class="va">True</span> <span class="co"># allow tf32 on matmul</span></span>
<span id="cb32-849"><a href="#cb32-849" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.allow_tf32 <span class="op">=</span> <span class="va">True</span> <span class="co"># allow tf32 on cudnn</span></span>
<span id="cb32-850"><a href="#cb32-850" aria-hidden="true" tabindex="-1"></a>device_type <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> <span class="st">'cuda'</span> <span class="kw">in</span> device <span class="cf">else</span> <span class="st">'cpu'</span> <span class="co"># for later use in torch.autocast</span></span>
<span id="cb32-851"><a href="#cb32-851" aria-hidden="true" tabindex="-1"></a>ptdtype <span class="op">=</span> {<span class="st">'float32'</span>: torch.float32, <span class="st">'bfloat16'</span>: torch.bfloat16, <span class="st">'float16'</span>: torch.float16}[<span class="st">'float16'</span>]</span>
<span id="cb32-852"><a href="#cb32-852" aria-hidden="true" tabindex="-1"></a>ctx <span class="op">=</span> nullcontext() <span class="cf">if</span> device_type <span class="op">==</span> <span class="st">'cpu'</span> <span class="cf">else</span> torch.amp.autocast(device_type<span class="op">=</span>device_type, dtype<span class="op">=</span>ptdtype)</span>
<span id="cb32-853"><a href="#cb32-853" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-854"><a href="#cb32-854" aria-hidden="true" tabindex="-1"></a><span class="co"># model</span></span>
<span id="cb32-855"><a href="#cb32-855" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> init_from <span class="op">==</span> <span class="st">'resume'</span>:</span>
<span id="cb32-856"><a href="#cb32-856" aria-hidden="true" tabindex="-1"></a>    <span class="co"># init from a model saved in a specific directory</span></span>
<span id="cb32-857"><a href="#cb32-857" aria-hidden="true" tabindex="-1"></a>    ckpt_path <span class="op">=</span> os.path.join(out_dir, <span class="st">'ckpt.pt'</span>)</span>
<span id="cb32-858"><a href="#cb32-858" aria-hidden="true" tabindex="-1"></a>    checkpoint <span class="op">=</span> torch.load(ckpt_path, map_location<span class="op">=</span>device)</span>
<span id="cb32-859"><a href="#cb32-859" aria-hidden="true" tabindex="-1"></a>    gptconf <span class="op">=</span> GPTConfig(<span class="op">**</span>checkpoint[<span class="st">'model_args'</span>])</span>
<span id="cb32-860"><a href="#cb32-860" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> GPT(gptconf)</span>
<span id="cb32-861"><a href="#cb32-861" aria-hidden="true" tabindex="-1"></a>    state_dict <span class="op">=</span> checkpoint[<span class="st">'model'</span>]</span>
<span id="cb32-862"><a href="#cb32-862" aria-hidden="true" tabindex="-1"></a>    unwanted_prefix <span class="op">=</span> <span class="st">'_orig_mod.'</span></span>
<span id="cb32-863"><a href="#cb32-863" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k,v <span class="kw">in</span> <span class="bu">list</span>(state_dict.items()):</span>
<span id="cb32-864"><a href="#cb32-864" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> k.startswith(unwanted_prefix):</span>
<span id="cb32-865"><a href="#cb32-865" aria-hidden="true" tabindex="-1"></a>            state_dict[k[<span class="bu">len</span>(unwanted_prefix):]] <span class="op">=</span> state_dict.pop(k)</span>
<span id="cb32-866"><a href="#cb32-866" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(state_dict)</span>
<span id="cb32-867"><a href="#cb32-867" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-868"><a href="#cb32-868" aria-hidden="true" tabindex="-1"></a>model.<span class="bu">eval</span>()</span>
<span id="cb32-869"><a href="#cb32-869" aria-hidden="true" tabindex="-1"></a>model.to(device)</span>
<span id="cb32-870"><a href="#cb32-870" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">compile</span>:</span>
<span id="cb32-871"><a href="#cb32-871" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> torch.<span class="bu">compile</span>(model) <span class="co"># requires PyTorch 2.0 (optional)</span></span>
<span id="cb32-872"><a href="#cb32-872" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-873"><a href="#cb32-873" aria-hidden="true" tabindex="-1"></a><span class="co"># look for the meta pickle in case it is available in the dataset folder</span></span>
<span id="cb32-874"><a href="#cb32-874" aria-hidden="true" tabindex="-1"></a>load_meta <span class="op">=</span> <span class="va">False</span></span>
<span id="cb32-875"><a href="#cb32-875" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> init_from <span class="op">==</span> <span class="st">'resume'</span> <span class="kw">and</span> <span class="st">'config'</span> <span class="kw">in</span> checkpoint <span class="kw">and</span> <span class="st">'dataset'</span> <span class="kw">in</span> checkpoint[<span class="st">'config'</span>]: <span class="co"># older checkpoints might not have these...</span></span>
<span id="cb32-876"><a href="#cb32-876" aria-hidden="true" tabindex="-1"></a>    meta_path <span class="op">=</span> os.path.join(<span class="st">'/root/data'</span>, <span class="st">'meta.pkl'</span>)</span>
<span id="cb32-877"><a href="#cb32-877" aria-hidden="true" tabindex="-1"></a>    load_meta <span class="op">=</span> os.path.exists(meta_path)</span>
<span id="cb32-878"><a href="#cb32-878" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> load_meta:</span>
<span id="cb32-879"><a href="#cb32-879" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Loading meta from </span><span class="sc">{</span>meta_path<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb32-880"><a href="#cb32-880" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(meta_path, <span class="st">'rb'</span>) <span class="im">as</span> f:</span>
<span id="cb32-881"><a href="#cb32-881" aria-hidden="true" tabindex="-1"></a>        meta <span class="op">=</span> pickle.load(f)</span>
<span id="cb32-882"><a href="#cb32-882" aria-hidden="true" tabindex="-1"></a>    <span class="co"># </span><span class="al">TODO</span><span class="co"> want to make this more general to arbitrary encoder/decoder schemes</span></span>
<span id="cb32-883"><a href="#cb32-883" aria-hidden="true" tabindex="-1"></a>    stoi, itos <span class="op">=</span> meta[<span class="st">'stoi'</span>], meta[<span class="st">'itos'</span>]</span>
<span id="cb32-884"><a href="#cb32-884" aria-hidden="true" tabindex="-1"></a>    encode <span class="op">=</span> <span class="kw">lambda</span> s: [stoi[c] <span class="cf">for</span> c <span class="kw">in</span> s]</span>
<span id="cb32-885"><a href="#cb32-885" aria-hidden="true" tabindex="-1"></a>    decode <span class="op">=</span> <span class="kw">lambda</span> l: <span class="st">''</span>.join([itos[i] <span class="cf">for</span> i <span class="kw">in</span> l])</span>
<span id="cb32-886"><a href="#cb32-886" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-887"><a href="#cb32-887" aria-hidden="true" tabindex="-1"></a><span class="co"># encode the beginning of the prompt</span></span>
<span id="cb32-888"><a href="#cb32-888" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> start.startswith(<span class="st">'FILE:'</span>):</span>
<span id="cb32-889"><a href="#cb32-889" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(start[<span class="dv">5</span>:], <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> f:</span>
<span id="cb32-890"><a href="#cb32-890" aria-hidden="true" tabindex="-1"></a>        start <span class="op">=</span> f.read()</span>
<span id="cb32-891"><a href="#cb32-891" aria-hidden="true" tabindex="-1"></a>start_ids <span class="op">=</span> encode(start)</span>
<span id="cb32-892"><a href="#cb32-892" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> (torch.tensor(start_ids, dtype<span class="op">=</span>torch.<span class="bu">long</span>, device<span class="op">=</span>device)[<span class="va">None</span>, ...])</span>
<span id="cb32-893"><a href="#cb32-893" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-894"><a href="#cb32-894" aria-hidden="true" tabindex="-1"></a><span class="co"># run generation</span></span>
<span id="cb32-895"><a href="#cb32-895" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb32-896"><a href="#cb32-896" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> ctx:</span>
<span id="cb32-897"><a href="#cb32-897" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(num_samples):</span>
<span id="cb32-898"><a href="#cb32-898" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> model.generate(x, max_new_tokens, temperature<span class="op">=</span>temperature, top_k<span class="op">=</span>top_k)</span>
<span id="cb32-899"><a href="#cb32-899" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(decode(y[<span class="dv">0</span>].tolist()))</span>
<span id="cb32-900"><a href="#cb32-900" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">'---------------'</span>)</span>
<span id="cb32-901"><a href="#cb32-901" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-902"><a href="#cb32-902" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-903"><a href="#cb32-903" aria-hidden="true" tabindex="-1"></a>And voila, it looks like our model is generating DNA sequence! But, that begs the question, how do we evaluate whether our model is actually performing well? With natural language, it's pretty easy to tell whether an output makes sense or not, but it is not so simple with DNA. Therefore, let's talk about the process of fine-tuning our model for more specific tasks.</span>
<span id="cb32-904"><a href="#cb32-904" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-905"><a href="#cb32-905" aria-hidden="true" tabindex="-1"></a>**Note for those interested**: You may also notice some uppercase and lowercase sequences. This is expected, as in our original file any soft-masked bases (i.e. lower-confidence segments) are lowercase rather than uppercase. The fact that our model is not mixing lower and uppercase bases is a good sign, as these should not be mixed based on sequencing assembly techniques.</span>
<span id="cb32-906"><a href="#cb32-906" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-907"><a href="#cb32-907" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Questions</span></span>
<span id="cb32-908"><a href="#cb32-908" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-909"><a href="#cb32-909" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-910"><a href="#cb32-910" aria-hidden="true" tabindex="-1"></a><span class="ss">1.   </span>As mentioned above, our model is using "causal" self-attention, meaning that it is only paying attention to the ~300 tokens/base pairs that come before the current one. Does this align with how DNA interacts in a biological sense? Why or why not?</span>
<span id="cb32-911"><a href="#cb32-911" aria-hidden="true" tabindex="-1"></a><span class="ss">2.   </span>What effect do you think adding more attention layers/heads to our model would have? What about training for more epochs? Are there any downsides to expanding our model in this way?</span>
<span id="cb32-912"><a href="#cb32-912" aria-hidden="true" tabindex="-1"></a><span class="ss">3.   </span>What is the difference between the Query, Key, and Value matrices?</span>
<span id="cb32-913"><a href="#cb32-913" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-914"><a href="#cb32-914" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-915"><a href="#cb32-915" aria-hidden="true" tabindex="-1"></a><span class="fu">## Fine-Tuning (Promoter Classification)</span></span>
<span id="cb32-916"><a href="#cb32-916" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-917"><a href="#cb32-917" aria-hidden="true" tabindex="-1"></a>Now that we have our gLM up and running, let's try applying it to two different problems. First, let's look at a promoter classification problem pulled from the <span class="co">[</span><span class="ot">Nucleotide Transformer training notebook</span><span class="co">](https://github.com/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling.ipynb)</span>. In short, this dataset is made up of 300 bp sequences, each labelled as 1 for promoter or 0 for non-promoter. Let's try to train our language model to separate these classes!</span>
<span id="cb32-918"><a href="#cb32-918" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-919"><a href="#cb32-919" aria-hidden="true" tabindex="-1"></a>Note: This data was pulled from the <span class="co">[</span><span class="ot">DeePromoter</span><span class="co">](https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2019.00286/full)</span> paper, as cited below:</span>
<span id="cb32-920"><a href="#cb32-920" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-921"><a href="#cb32-921" aria-hidden="true" tabindex="-1"></a>Oubounyt, M., Louadi, Z., Tayara, H., &amp; Chong, K. T. (2019). DeePromoter: Robust Promoter Predictor Using Deep Learning. Frontiers in Genetics, 10. https://doi.org/10.3389/fgene.2019.00286</span>
<span id="cb32-922"><a href="#cb32-922" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-923"><a href="#cb32-923" aria-hidden="true" tabindex="-1"></a>First, let's actually load in the data:</span>
<span id="cb32-924"><a href="#cb32-924" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-927"><a href="#cb32-927" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-928"><a href="#cb32-928" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the promoter dataset from the InstaDeep Hugging Face resources</span></span>
<span id="cb32-929"><a href="#cb32-929" aria-hidden="true" tabindex="-1"></a>dataset_name <span class="op">=</span> <span class="st">"promoter_all"</span></span>
<span id="cb32-930"><a href="#cb32-930" aria-hidden="true" tabindex="-1"></a>train_dataset_promoter <span class="op">=</span> load_dataset(</span>
<span id="cb32-931"><a href="#cb32-931" aria-hidden="true" tabindex="-1"></a>        <span class="st">"InstaDeepAI/nucleotide_transformer_downstream_tasks"</span>,</span>
<span id="cb32-932"><a href="#cb32-932" aria-hidden="true" tabindex="-1"></a>        dataset_name,</span>
<span id="cb32-933"><a href="#cb32-933" aria-hidden="true" tabindex="-1"></a>        split<span class="op">=</span><span class="st">"train"</span>,</span>
<span id="cb32-934"><a href="#cb32-934" aria-hidden="true" tabindex="-1"></a>        streaming<span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb32-935"><a href="#cb32-935" aria-hidden="true" tabindex="-1"></a>        trust_remote_code<span class="op">=</span><span class="va">True</span></span>
<span id="cb32-936"><a href="#cb32-936" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb32-937"><a href="#cb32-937" aria-hidden="true" tabindex="-1"></a>test_dataset_promoter <span class="op">=</span> load_dataset(</span>
<span id="cb32-938"><a href="#cb32-938" aria-hidden="true" tabindex="-1"></a>        <span class="st">"InstaDeepAI/nucleotide_transformer_downstream_tasks"</span>,</span>
<span id="cb32-939"><a href="#cb32-939" aria-hidden="true" tabindex="-1"></a>        dataset_name,</span>
<span id="cb32-940"><a href="#cb32-940" aria-hidden="true" tabindex="-1"></a>        split<span class="op">=</span><span class="st">"test"</span>,</span>
<span id="cb32-941"><a href="#cb32-941" aria-hidden="true" tabindex="-1"></a>        streaming<span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb32-942"><a href="#cb32-942" aria-hidden="true" tabindex="-1"></a>        trust_remote_code<span class="op">=</span><span class="va">True</span></span>
<span id="cb32-943"><a href="#cb32-943" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb32-944"><a href="#cb32-944" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-945"><a href="#cb32-945" aria-hidden="true" tabindex="-1"></a>num_labels_promoter <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb32-946"><a href="#cb32-946" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-947"><a href="#cb32-947" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-948"><a href="#cb32-948" aria-hidden="true" tabindex="-1"></a>Next, let's separate out the labels from the actual DNA sequences, and separate 5% of our training data to use as a validation set (the test set is already separated).</span>
<span id="cb32-949"><a href="#cb32-949" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-952"><a href="#cb32-952" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-953"><a href="#cb32-953" aria-hidden="true" tabindex="-1"></a><span class="co"># Get training data</span></span>
<span id="cb32-954"><a href="#cb32-954" aria-hidden="true" tabindex="-1"></a>train_sequences_promoter <span class="op">=</span> train_dataset_promoter[<span class="st">'sequence'</span>]</span>
<span id="cb32-955"><a href="#cb32-955" aria-hidden="true" tabindex="-1"></a>train_labels_promoter <span class="op">=</span> train_dataset_promoter[<span class="st">'label'</span>]</span>
<span id="cb32-956"><a href="#cb32-956" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-957"><a href="#cb32-957" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into a training and a validation dataset</span></span>
<span id="cb32-958"><a href="#cb32-958" aria-hidden="true" tabindex="-1"></a>train_sequences_promoter, validation_sequences_promoter, train_labels_promoter, validation_labels_promoter <span class="op">=</span> train_test_split(train_sequences_promoter,</span>
<span id="cb32-959"><a href="#cb32-959" aria-hidden="true" tabindex="-1"></a>                                                                              train_labels_promoter, test_size<span class="op">=</span><span class="fl">0.05</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb32-960"><a href="#cb32-960" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-961"><a href="#cb32-961" aria-hidden="true" tabindex="-1"></a><span class="co"># Get test data</span></span>
<span id="cb32-962"><a href="#cb32-962" aria-hidden="true" tabindex="-1"></a>test_sequences_promoter <span class="op">=</span> test_dataset_promoter[<span class="st">'sequence'</span>]</span>
<span id="cb32-963"><a href="#cb32-963" aria-hidden="true" tabindex="-1"></a>test_labels_promoter <span class="op">=</span> test_dataset_promoter[<span class="st">'label'</span>]</span>
<span id="cb32-964"><a href="#cb32-964" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-965"><a href="#cb32-965" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-966"><a href="#cb32-966" aria-hidden="true" tabindex="-1"></a>Now, let's take a quick look at what one of these examples looks like:</span>
<span id="cb32-967"><a href="#cb32-967" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-970"><a href="#cb32-970" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-971"><a href="#cb32-971" aria-hidden="true" tabindex="-1"></a><span class="co">#| colab: {base_uri: https://localhost:8080/}</span></span>
<span id="cb32-972"><a href="#cb32-972" aria-hidden="true" tabindex="-1"></a>idx_sequence <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb32-973"><a href="#cb32-973" aria-hidden="true" tabindex="-1"></a>sequence, label <span class="op">=</span> train_sequences_promoter[idx_sequence], train_labels_promoter[idx_sequence]</span>
<span id="cb32-974"><a href="#cb32-974" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"The DNA sequence is </span><span class="sc">{</span>sequence<span class="sc">}</span><span class="ss">."</span>)</span>
<span id="cb32-975"><a href="#cb32-975" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Its associated label is label </span><span class="sc">{</span>label<span class="sc">}</span><span class="ss">."</span>)</span>
<span id="cb32-976"><a href="#cb32-976" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-977"><a href="#cb32-977" aria-hidden="true" tabindex="-1"></a>idx_TATA <span class="op">=</span> sequence.find(<span class="st">"TATA"</span>)</span>
<span id="cb32-978"><a href="#cb32-978" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-979"><a href="#cb32-979" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"This promoter is a TATA promoter, as the TATA motif is present at the </span><span class="sc">{</span>idx_TATA<span class="sc">}</span><span class="ss">st nucleotide."</span>)</span>
<span id="cb32-980"><a href="#cb32-980" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-981"><a href="#cb32-981" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-982"><a href="#cb32-982" aria-hidden="true" tabindex="-1"></a>Now that we have all of our datasets, it is time to tokenize them using the same mapping that we created for our language model. first, we load them into a Dataset object so we can work with them more easily:</span>
<span id="cb32-983"><a href="#cb32-983" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-986"><a href="#cb32-986" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-987"><a href="#cb32-987" aria-hidden="true" tabindex="-1"></a><span class="co"># Promoter dataset</span></span>
<span id="cb32-988"><a href="#cb32-988" aria-hidden="true" tabindex="-1"></a>ds_train_promoter <span class="op">=</span> Dataset.from_dict({<span class="st">"data"</span>: train_sequences_promoter,<span class="st">'labels'</span>:train_labels_promoter})</span>
<span id="cb32-989"><a href="#cb32-989" aria-hidden="true" tabindex="-1"></a>ds_validation_promoter <span class="op">=</span> Dataset.from_dict({<span class="st">"data"</span>: validation_sequences_promoter,<span class="st">'labels'</span>:validation_labels_promoter})</span>
<span id="cb32-990"><a href="#cb32-990" aria-hidden="true" tabindex="-1"></a>ds_test_promoter <span class="op">=</span> Dataset.from_dict({<span class="st">"data"</span>: test_sequences_promoter,<span class="st">'labels'</span>:test_labels_promoter})</span>
<span id="cb32-991"><a href="#cb32-991" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-992"><a href="#cb32-992" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-993"><a href="#cb32-993" aria-hidden="true" tabindex="-1"></a>Next, we do the actual tokenization. However, remember that our previous encoding function could only handle a single example at a time (since our initial data file was one long string). We define a new tokenizing function below to use the same process, but apply it to multiple separate sequences:</span>
<span id="cb32-994"><a href="#cb32-994" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-997"><a href="#cb32-997" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-998"><a href="#cb32-998" aria-hidden="true" tabindex="-1"></a><span class="co">#| colab: {base_uri: https://localhost:8080/, height: 113, referenced_widgets: [2a9b870265654322be2a63d4ff812dfe, 5e9c130c410646e4813b40a6fb3597b1, 5f3b654fc40f4b0b8d83568dea1239cd, 4bbba4b5bd8b48e891c7bdae396e7488, 5a8f1f3fa60142db9fb87f978df994b7, 32d80445eaeb4156a24e26ba193fb2f1, 373ac7e2f3ca40279710de1eb5020a48, e1fe97f77a924e9dbd5f4224cbe18684, 77d274c224a94518b26812ce062effe9, 69dc1b78f0c94df98d9fbd35c8bc948a, 0000abf1fcc24784856b4540222bff6d, 3225536be36741de948c748768aae884, eb29624d75844a9780c3012ca7ba2dc9, 613931c3d625432da145ce3cde5a7a58, c62c16210fc743ae8842c1cf9118f0ad, 755a2822c8e740538001ae12f516a41d, 81f5d6970fa44ff8a2cf8f2a98bd47be, 926fd6673e154dc2b3ff34f09bd1a81a, a64a0f215cf54446a64c38add5358ae9, d0d29ef4832f4c51b63490dcd0b7321a, dc37b9e04c8c480a96cdb01f3f543a37, 951732f504604f8baf480d7f1760fbd1, 8827a52655f34c20b1ad6793dfcfd7dc, e023f192b56545d9b41fdf65337ea505, 5d6f089e0b8849dd95af0bcf458919cf, 54789985ef0a45629aacd7a334abb4dd, fb43d8ccf9514db58062d592cba297c6, 63c6af1ca8104840b0691acd20cdf04c, a085653b4546498cbf8df67855e8a318, 3632c4a3e47a421d994625d8a169fd7c, 6ab83fc6c9604a849673eec027066ccd, ba58db5153cf4045a855f795d08f7e74, 3e778489ec2e4fca8c7322b636c72d45]}</span></span>
<span id="cb32-999"><a href="#cb32-999" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_function(examples):</span>
<span id="cb32-1000"><a href="#cb32-1000" aria-hidden="true" tabindex="-1"></a>  outputs <span class="op">=</span> np.empty((<span class="dv">0</span>, <span class="dv">300</span>), dtype<span class="op">=</span><span class="st">'int16'</span>)</span>
<span id="cb32-1001"><a href="#cb32-1001" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> example <span class="kw">in</span> examples[<span class="st">"data"</span>]:</span>
<span id="cb32-1002"><a href="#cb32-1002" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> np.vstack([outputs, encode(example)])</span>
<span id="cb32-1003"><a href="#cb32-1003" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> {</span>
<span id="cb32-1004"><a href="#cb32-1004" aria-hidden="true" tabindex="-1"></a>      <span class="st">'input_ids'</span>: outputs,</span>
<span id="cb32-1005"><a href="#cb32-1005" aria-hidden="true" tabindex="-1"></a>      <span class="st">'attention_mask'</span>: [<span class="dv">1</span>] <span class="op">*</span> <span class="bu">len</span>(outputs),</span>
<span id="cb32-1006"><a href="#cb32-1006" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb32-1007"><a href="#cb32-1007" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1008"><a href="#cb32-1008" aria-hidden="true" tabindex="-1"></a>tokenized_datasets_train_promoter <span class="op">=</span> ds_train_promoter.<span class="bu">map</span>(</span>
<span id="cb32-1009"><a href="#cb32-1009" aria-hidden="true" tabindex="-1"></a>    tokenize_function,</span>
<span id="cb32-1010"><a href="#cb32-1010" aria-hidden="true" tabindex="-1"></a>    batched<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb32-1011"><a href="#cb32-1011" aria-hidden="true" tabindex="-1"></a>    remove_columns<span class="op">=</span>[<span class="st">'data'</span>],</span>
<span id="cb32-1012"><a href="#cb32-1012" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb32-1013"><a href="#cb32-1013" aria-hidden="true" tabindex="-1"></a>tokenized_datasets_validation_promoter <span class="op">=</span> ds_validation_promoter.<span class="bu">map</span>(</span>
<span id="cb32-1014"><a href="#cb32-1014" aria-hidden="true" tabindex="-1"></a>    tokenize_function,</span>
<span id="cb32-1015"><a href="#cb32-1015" aria-hidden="true" tabindex="-1"></a>    batched<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb32-1016"><a href="#cb32-1016" aria-hidden="true" tabindex="-1"></a>    remove_columns<span class="op">=</span>[<span class="st">'data'</span>],</span>
<span id="cb32-1017"><a href="#cb32-1017" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb32-1018"><a href="#cb32-1018" aria-hidden="true" tabindex="-1"></a>tokenized_datasets_test_promoter <span class="op">=</span> ds_test_promoter.<span class="bu">map</span>(</span>
<span id="cb32-1019"><a href="#cb32-1019" aria-hidden="true" tabindex="-1"></a>    tokenize_function,</span>
<span id="cb32-1020"><a href="#cb32-1020" aria-hidden="true" tabindex="-1"></a>    batched<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb32-1021"><a href="#cb32-1021" aria-hidden="true" tabindex="-1"></a>    remove_columns<span class="op">=</span>[<span class="st">'data'</span>],</span>
<span id="cb32-1022"><a href="#cb32-1022" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb32-1023"><a href="#cb32-1023" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-1024"><a href="#cb32-1024" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1025"><a href="#cb32-1025" aria-hidden="true" tabindex="-1"></a>Now that we've tokenized the data, let's load it into Tensor format (which Pytorch uses in its models). Then, we can create a DataLoader, which will automatically create batches of our data and put them into a format which our model can read easily:</span>
<span id="cb32-1026"><a href="#cb32-1026" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1029"><a href="#cb32-1029" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-1030"><a href="#cb32-1030" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> TensorDataset(torch.tensor(tokenized_datasets_train_promoter[<span class="st">'input_ids'</span>], device<span class="op">=</span>device),</span>
<span id="cb32-1031"><a href="#cb32-1031" aria-hidden="true" tabindex="-1"></a>                              torch.tensor(tokenized_datasets_train_promoter[<span class="st">'labels'</span>], device<span class="op">=</span>device))</span>
<span id="cb32-1032"><a href="#cb32-1032" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(train_dataset, shuffle<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=</span><span class="dv">64</span>)</span>
<span id="cb32-1033"><a href="#cb32-1033" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1034"><a href="#cb32-1034" aria-hidden="true" tabindex="-1"></a>validation_dataset <span class="op">=</span> TensorDataset(torch.tensor(tokenized_datasets_validation_promoter[<span class="st">'input_ids'</span>], device<span class="op">=</span>device),</span>
<span id="cb32-1035"><a href="#cb32-1035" aria-hidden="true" tabindex="-1"></a>                              torch.tensor(tokenized_datasets_validation_promoter[<span class="st">'labels'</span>], device<span class="op">=</span>device))</span>
<span id="cb32-1036"><a href="#cb32-1036" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1037"><a href="#cb32-1037" aria-hidden="true" tabindex="-1"></a>validation_dataloader <span class="op">=</span> DataLoader(validation_dataset, shuffle<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=</span><span class="dv">64</span>)</span>
<span id="cb32-1038"><a href="#cb32-1038" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1039"><a href="#cb32-1039" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> TensorDataset(torch.tensor(tokenized_datasets_test_promoter[<span class="st">'input_ids'</span>], device<span class="op">=</span>device),</span>
<span id="cb32-1040"><a href="#cb32-1040" aria-hidden="true" tabindex="-1"></a>                              torch.tensor(tokenized_datasets_test_promoter[<span class="st">'labels'</span>], device<span class="op">=</span>device))</span>
<span id="cb32-1041"><a href="#cb32-1041" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1042"><a href="#cb32-1042" aria-hidden="true" tabindex="-1"></a>test_dataloader <span class="op">=</span> DataLoader(test_dataset, shuffle<span class="op">=</span><span class="va">False</span>, batch_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb32-1043"><a href="#cb32-1043" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-1044"><a href="#cb32-1044" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1045"><a href="#cb32-1045" aria-hidden="true" tabindex="-1"></a>In the last step before training, we have to actually define a slightly new type of model. Essentially all we need to do here is add one more linear layer to our old model, which takes the original output and projects it into a two-dimensional output. This means that the "new" model will output the probability of the example belonging to each class. To do this, we just create a new model which pulls in our language model and adds the new layer on top:</span>
<span id="cb32-1046"><a href="#cb32-1046" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1049"><a href="#cb32-1049" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-1050"><a href="#cb32-1050" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ClassificationModel(nn.Module):</span>
<span id="cb32-1051"><a href="#cb32-1051" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, base_model, num_labels):</span>
<span id="cb32-1052"><a href="#cb32-1052" aria-hidden="true" tabindex="-1"></a>    <span class="bu">super</span>(ClassificationModel, <span class="va">self</span>).<span class="fu">__init__</span>()</span>
<span id="cb32-1053"><a href="#cb32-1053" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.orig_model <span class="op">=</span> base_model</span>
<span id="cb32-1054"><a href="#cb32-1054" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.classifier <span class="op">=</span> nn.Linear(base_model.config.vocab_size, num_labels)</span>
<span id="cb32-1055"><a href="#cb32-1055" aria-hidden="true" tabindex="-1"></a>    <span class="va">self</span>.num_labels <span class="op">=</span> num_labels</span>
<span id="cb32-1056"><a href="#cb32-1056" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1057"><a href="#cb32-1057" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> forward(<span class="va">self</span>, input_ids, labels<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb32-1058"><a href="#cb32-1058" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> <span class="va">self</span>.orig_model(input_ids)</span>
<span id="cb32-1059"><a href="#cb32-1059" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> <span class="va">self</span>.classifier(outputs[<span class="dv">0</span>][:, <span class="dv">0</span>, :])</span>
<span id="cb32-1060"><a href="#cb32-1060" aria-hidden="true" tabindex="-1"></a>    attn_weights <span class="op">=</span> outputs[<span class="dv">2</span>]</span>
<span id="cb32-1061"><a href="#cb32-1061" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1062"><a href="#cb32-1062" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb32-1063"><a href="#cb32-1063" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> labels <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb32-1064"><a href="#cb32-1064" aria-hidden="true" tabindex="-1"></a>      loss_fct <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb32-1065"><a href="#cb32-1065" aria-hidden="true" tabindex="-1"></a>      loss <span class="op">=</span> loss_fct(logits.view(<span class="op">-</span><span class="dv">1</span>, <span class="va">self</span>.num_labels), labels.view(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb32-1066"><a href="#cb32-1066" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (logits, loss, attn_weights) <span class="cf">if</span> loss <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span> <span class="cf">else</span> (logits, attn_weights)</span>
<span id="cb32-1067"><a href="#cb32-1067" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-1068"><a href="#cb32-1068" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1069"><a href="#cb32-1069" aria-hidden="true" tabindex="-1"></a>Now that we have our new model defined, let's pull in the model we trained previously, and use it to create this new 2-label classification model:</span>
<span id="cb32-1070"><a href="#cb32-1070" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1073"><a href="#cb32-1073" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-1074"><a href="#cb32-1074" aria-hidden="true" tabindex="-1"></a><span class="co">#| colab: {base_uri: https://localhost:8080/}</span></span>
<span id="cb32-1075"><a href="#cb32-1075" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------------------------</span></span>
<span id="cb32-1076"><a href="#cb32-1076" aria-hidden="true" tabindex="-1"></a>init_from <span class="op">=</span> <span class="st">'resume'</span> <span class="co"># either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')</span></span>
<span id="cb32-1077"><a href="#cb32-1077" aria-hidden="true" tabindex="-1"></a>out_dir <span class="op">=</span> <span class="st">'/root/data/out'</span> <span class="co"># ignored if init_from is not 'resume'</span></span>
<span id="cb32-1078"><a href="#cb32-1078" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">1337</span></span>
<span id="cb32-1079"><a href="#cb32-1079" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="co"># examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.</span></span>
<span id="cb32-1080"><a href="#cb32-1080" aria-hidden="true" tabindex="-1"></a>dtype <span class="op">=</span> <span class="st">'bfloat16'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="kw">and</span> torch.cuda.is_bf16_supported() <span class="cf">else</span> <span class="st">'float16'</span> <span class="co"># 'float32' or 'bfloat16' or 'float16'</span></span>
<span id="cb32-1081"><a href="#cb32-1081" aria-hidden="true" tabindex="-1"></a><span class="bu">compile</span> <span class="op">=</span> <span class="va">True</span> <span class="co"># use PyTorch 2.0 to compile the model to be faster</span></span>
<span id="cb32-1082"><a href="#cb32-1082" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------------------------</span></span>
<span id="cb32-1083"><a href="#cb32-1083" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1084"><a href="#cb32-1084" aria-hidden="true" tabindex="-1"></a>torch.cuda.init()</span>
<span id="cb32-1085"><a href="#cb32-1085" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(seed)</span>
<span id="cb32-1086"><a href="#cb32-1086" aria-hidden="true" tabindex="-1"></a>torch.cuda.manual_seed(seed)</span>
<span id="cb32-1087"><a href="#cb32-1087" aria-hidden="true" tabindex="-1"></a>torch.backends.cuda.matmul.allow_tf32 <span class="op">=</span> <span class="va">True</span> <span class="co"># allow tf32 on matmul</span></span>
<span id="cb32-1088"><a href="#cb32-1088" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.allow_tf32 <span class="op">=</span> <span class="va">True</span> <span class="co"># allow tf32 on cudnn</span></span>
<span id="cb32-1089"><a href="#cb32-1089" aria-hidden="true" tabindex="-1"></a>device_type <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> <span class="st">'cuda'</span> <span class="kw">in</span> device <span class="cf">else</span> <span class="st">'cpu'</span> <span class="co"># for later use in torch.autocast</span></span>
<span id="cb32-1090"><a href="#cb32-1090" aria-hidden="true" tabindex="-1"></a>ptdtype <span class="op">=</span> {<span class="st">'float32'</span>: torch.float32, <span class="st">'bfloat16'</span>: torch.bfloat16, <span class="st">'float16'</span>: torch.float16}[<span class="st">'float16'</span>]</span>
<span id="cb32-1091"><a href="#cb32-1091" aria-hidden="true" tabindex="-1"></a>ctx <span class="op">=</span> nullcontext() <span class="cf">if</span> device_type <span class="op">==</span> <span class="st">'cpu'</span> <span class="cf">else</span> torch.amp.autocast(device_type<span class="op">=</span>device_type, dtype<span class="op">=</span>ptdtype)</span>
<span id="cb32-1092"><a href="#cb32-1092" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1093"><a href="#cb32-1093" aria-hidden="true" tabindex="-1"></a><span class="co"># model</span></span>
<span id="cb32-1094"><a href="#cb32-1094" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> init_from <span class="op">==</span> <span class="st">'resume'</span>:</span>
<span id="cb32-1095"><a href="#cb32-1095" aria-hidden="true" tabindex="-1"></a>    <span class="co"># init from a model saved in a specific directory</span></span>
<span id="cb32-1096"><a href="#cb32-1096" aria-hidden="true" tabindex="-1"></a>    ckpt_path <span class="op">=</span> os.path.join(out_dir, <span class="st">'ckpt.pt'</span>)</span>
<span id="cb32-1097"><a href="#cb32-1097" aria-hidden="true" tabindex="-1"></a>    checkpoint <span class="op">=</span> torch.load(ckpt_path, map_location<span class="op">=</span>device)</span>
<span id="cb32-1098"><a href="#cb32-1098" aria-hidden="true" tabindex="-1"></a>    gptconf <span class="op">=</span> GPTConfig(<span class="op">**</span>checkpoint[<span class="st">'model_args'</span>])</span>
<span id="cb32-1099"><a href="#cb32-1099" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> GPT(gptconf)</span>
<span id="cb32-1100"><a href="#cb32-1100" aria-hidden="true" tabindex="-1"></a>    state_dict <span class="op">=</span> checkpoint[<span class="st">'model'</span>]</span>
<span id="cb32-1101"><a href="#cb32-1101" aria-hidden="true" tabindex="-1"></a>    unwanted_prefix <span class="op">=</span> <span class="st">'_orig_mod.'</span></span>
<span id="cb32-1102"><a href="#cb32-1102" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k,v <span class="kw">in</span> <span class="bu">list</span>(state_dict.items()):</span>
<span id="cb32-1103"><a href="#cb32-1103" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> k.startswith(unwanted_prefix):</span>
<span id="cb32-1104"><a href="#cb32-1104" aria-hidden="true" tabindex="-1"></a>            state_dict[k[<span class="bu">len</span>(unwanted_prefix):]] <span class="op">=</span> state_dict.pop(k)</span>
<span id="cb32-1105"><a href="#cb32-1105" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(state_dict)</span>
<span id="cb32-1106"><a href="#cb32-1106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1107"><a href="#cb32-1107" aria-hidden="true" tabindex="-1"></a>new_model <span class="op">=</span> ClassificationModel(base_model<span class="op">=</span>model, num_labels<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb32-1108"><a href="#cb32-1108" aria-hidden="true" tabindex="-1"></a>new_model.<span class="bu">eval</span>()</span>
<span id="cb32-1109"><a href="#cb32-1109" aria-hidden="true" tabindex="-1"></a>new_model.to(device)</span>
<span id="cb32-1110"><a href="#cb32-1110" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">compile</span>:</span>
<span id="cb32-1111"><a href="#cb32-1111" aria-hidden="true" tabindex="-1"></a>    new_model <span class="op">=</span> torch.<span class="bu">compile</span>(new_model) <span class="co"># requires PyTorch 2.0 (optional)</span></span>
<span id="cb32-1112"><a href="#cb32-1112" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-1113"><a href="#cb32-1113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1114"><a href="#cb32-1114" aria-hidden="true" tabindex="-1"></a>Now, go ahead and run the next block of code to train our new model. Again, this will take about 10 minutes, so go to the next paragraph for a description of what our training is actually doing.</span>
<span id="cb32-1115"><a href="#cb32-1115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1118"><a href="#cb32-1118" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-1119"><a href="#cb32-1119" aria-hidden="true" tabindex="-1"></a><span class="co">#| colab: {base_uri: https://localhost:8080/, height: 715}</span></span>
<span id="cb32-1120"><a href="#cb32-1120" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(new_model.parameters(), lr<span class="op">=</span><span class="fl">2e-5</span>)</span>
<span id="cb32-1121"><a href="#cb32-1121" aria-hidden="true" tabindex="-1"></a>num_epochs<span class="op">=</span><span class="dv">2</span></span>
<span id="cb32-1122"><a href="#cb32-1122" aria-hidden="true" tabindex="-1"></a>total_steps <span class="op">=</span> <span class="bu">len</span>(train_dataloader) <span class="op">*</span> num_epochs</span>
<span id="cb32-1123"><a href="#cb32-1123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1124"><a href="#cb32-1124" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> calc_f1(predictions, true_labels):</span>
<span id="cb32-1125"><a href="#cb32-1125" aria-hidden="true" tabindex="-1"></a>    true_pos <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-1126"><a href="#cb32-1126" aria-hidden="true" tabindex="-1"></a>    false_pos <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-1127"><a href="#cb32-1127" aria-hidden="true" tabindex="-1"></a>    false_neg <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-1128"><a href="#cb32-1128" aria-hidden="true" tabindex="-1"></a>    true_neg <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-1129"><a href="#cb32-1129" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> np.arange(<span class="bu">len</span>(predictions)) :</span>
<span id="cb32-1130"><a href="#cb32-1130" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> predictions[i] <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> true_labels[i] <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb32-1131"><a href="#cb32-1131" aria-hidden="true" tabindex="-1"></a>            true_neg <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-1132"><a href="#cb32-1132" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> predictions[i] <span class="op">==</span> <span class="dv">1</span> <span class="kw">and</span> true_labels[i] <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb32-1133"><a href="#cb32-1133" aria-hidden="true" tabindex="-1"></a>            false_pos <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-1134"><a href="#cb32-1134" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> predictions[i] <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> true_labels[i] <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb32-1135"><a href="#cb32-1135" aria-hidden="true" tabindex="-1"></a>            false_neg <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-1136"><a href="#cb32-1136" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> predictions[i] <span class="op">==</span> <span class="dv">1</span> <span class="kw">and</span> true_labels[i] <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb32-1137"><a href="#cb32-1137" aria-hidden="true" tabindex="-1"></a>            true_pos <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-1138"><a href="#cb32-1138" aria-hidden="true" tabindex="-1"></a>    precision <span class="op">=</span> true_pos <span class="op">/</span> (true_pos <span class="op">+</span> false_pos) <span class="cf">if</span> true_pos <span class="op">+</span> false_pos <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb32-1139"><a href="#cb32-1139" aria-hidden="true" tabindex="-1"></a>    recall <span class="op">=</span> true_pos <span class="op">/</span> (true_pos <span class="op">+</span> false_neg)</span>
<span id="cb32-1140"><a href="#cb32-1140" aria-hidden="true" tabindex="-1"></a>    F1 <span class="op">=</span> (<span class="dv">2</span><span class="op">*</span>precision<span class="op">*</span>recall <span class="op">/</span> (precision <span class="op">+</span> recall))</span>
<span id="cb32-1141"><a href="#cb32-1141" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> F1</span>
<span id="cb32-1142"><a href="#cb32-1142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1143"><a href="#cb32-1143" aria-hidden="true" tabindex="-1"></a>val_predictions <span class="op">=</span> []</span>
<span id="cb32-1144"><a href="#cb32-1144" aria-hidden="true" tabindex="-1"></a>val_true_labels <span class="op">=</span> []</span>
<span id="cb32-1145"><a href="#cb32-1145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1146"><a href="#cb32-1146" aria-hidden="true" tabindex="-1"></a><span class="co">#Training Loop</span></span>
<span id="cb32-1147"><a href="#cb32-1147" aria-hidden="true" tabindex="-1"></a>loss_lst <span class="op">=</span> []</span>
<span id="cb32-1148"><a href="#cb32-1148" aria-hidden="true" tabindex="-1"></a>val_loss_lst <span class="op">=</span> []</span>
<span id="cb32-1149"><a href="#cb32-1149" aria-hidden="true" tabindex="-1"></a>f1_scores <span class="op">=</span> []</span>
<span id="cb32-1150"><a href="#cb32-1150" aria-hidden="true" tabindex="-1"></a>new_model.train()</span>
<span id="cb32-1151"><a href="#cb32-1151" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb32-1152"><a href="#cb32-1152" aria-hidden="true" tabindex="-1"></a>    i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-1153"><a href="#cb32-1153" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> train_dataloader:</span>
<span id="cb32-1154"><a href="#cb32-1154" aria-hidden="true" tabindex="-1"></a>        input_ids, labels <span class="op">=</span> batch</span>
<span id="cb32-1155"><a href="#cb32-1155" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb32-1156"><a href="#cb32-1156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1157"><a href="#cb32-1157" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass</span></span>
<span id="cb32-1158"><a href="#cb32-1158" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> new_model(input_ids, labels<span class="op">=</span>labels)</span>
<span id="cb32-1159"><a href="#cb32-1159" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> outputs[<span class="dv">1</span>]</span>
<span id="cb32-1160"><a href="#cb32-1160" aria-hidden="true" tabindex="-1"></a>        loss_lst.append(loss.item())</span>
<span id="cb32-1161"><a href="#cb32-1161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1162"><a href="#cb32-1162" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward pass and optimization</span></span>
<span id="cb32-1163"><a href="#cb32-1163" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb32-1164"><a href="#cb32-1164" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb32-1165"><a href="#cb32-1165" aria-hidden="true" tabindex="-1"></a>        i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-1166"><a href="#cb32-1166" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">99</span>:</span>
<span id="cb32-1167"><a href="#cb32-1167" aria-hidden="true" tabindex="-1"></a>            new_model.<span class="bu">eval</span>()</span>
<span id="cb32-1168"><a href="#cb32-1168" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb32-1169"><a href="#cb32-1169" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> val_batch <span class="kw">in</span> validation_dataloader:</span>
<span id="cb32-1170"><a href="#cb32-1170" aria-hidden="true" tabindex="-1"></a>                    val_input_ids, val_labels <span class="op">=</span> val_batch</span>
<span id="cb32-1171"><a href="#cb32-1171" aria-hidden="true" tabindex="-1"></a>                    optimizer.zero_grad()</span>
<span id="cb32-1172"><a href="#cb32-1172" aria-hidden="true" tabindex="-1"></a>                    val_outputs <span class="op">=</span> new_model(val_input_ids, labels<span class="op">=</span>val_labels)</span>
<span id="cb32-1173"><a href="#cb32-1173" aria-hidden="true" tabindex="-1"></a>                    val_loss <span class="op">=</span> val_outputs[<span class="dv">1</span>]</span>
<span id="cb32-1174"><a href="#cb32-1174" aria-hidden="true" tabindex="-1"></a>                    val_loss_lst.append(val_loss.item())</span>
<span id="cb32-1175"><a href="#cb32-1175" aria-hidden="true" tabindex="-1"></a>                    val_logits <span class="op">=</span> val_outputs[<span class="dv">0</span>]  <span class="co"># Get the logits</span></span>
<span id="cb32-1176"><a href="#cb32-1176" aria-hidden="true" tabindex="-1"></a>                    val_predictions.append(val_logits)</span>
<span id="cb32-1177"><a href="#cb32-1177" aria-hidden="true" tabindex="-1"></a>                    val_true_labels.append(val_labels)</span>
<span id="cb32-1178"><a href="#cb32-1178" aria-hidden="true" tabindex="-1"></a>                val_predictions <span class="op">=</span> torch.cat(val_predictions, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb32-1179"><a href="#cb32-1179" aria-hidden="true" tabindex="-1"></a>                val_true_labels <span class="op">=</span> torch.cat(val_true_labels, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb32-1180"><a href="#cb32-1180" aria-hidden="true" tabindex="-1"></a>                val_predictions <span class="op">=</span> torch.argmax(val_predictions, dim<span class="op">=</span><span class="dv">1</span>).tolist()</span>
<span id="cb32-1181"><a href="#cb32-1181" aria-hidden="true" tabindex="-1"></a>                val_true_labels <span class="op">=</span> val_true_labels.tolist()</span>
<span id="cb32-1182"><a href="#cb32-1182" aria-hidden="true" tabindex="-1"></a>                f1_scores.append(calc_f1(val_predictions, val_true_labels))</span>
<span id="cb32-1183"><a href="#cb32-1183" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, Batch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>np<span class="sc">.</span>mean(loss_lst)<span class="sc">}</span><span class="ss">, Val Loss: </span><span class="sc">{</span>np<span class="sc">.</span>mean(val_loss_lst)<span class="sc">}</span><span class="ss">, Val F1: </span><span class="sc">{</span>calc_f1(val_predictions, val_true_labels)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-1184"><a href="#cb32-1184" aria-hidden="true" tabindex="-1"></a>            loss_lst <span class="op">=</span> []</span>
<span id="cb32-1185"><a href="#cb32-1185" aria-hidden="true" tabindex="-1"></a>            val_loss_lst <span class="op">=</span> []</span>
<span id="cb32-1186"><a href="#cb32-1186" aria-hidden="true" tabindex="-1"></a>            new_model.train()</span>
<span id="cb32-1187"><a href="#cb32-1187" aria-hidden="true" tabindex="-1"></a>            val_predictions <span class="op">=</span> []</span>
<span id="cb32-1188"><a href="#cb32-1188" aria-hidden="true" tabindex="-1"></a>            val_true_labels <span class="op">=</span> []</span>
<span id="cb32-1189"><a href="#cb32-1189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1190"><a href="#cb32-1190" aria-hidden="true" tabindex="-1"></a>plt.plot(f1_scores, label<span class="op">=</span><span class="st">'Validation F1 score'</span>)</span>
<span id="cb32-1191"><a href="#cb32-1191" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Validation F1 score for promoter prediction"</span>)</span>
<span id="cb32-1192"><a href="#cb32-1192" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Batch"</span>)</span>
<span id="cb32-1193"><a href="#cb32-1193" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"F1 score"</span>)</span>
<span id="cb32-1194"><a href="#cb32-1194" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb32-1195"><a href="#cb32-1195" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb32-1196"><a href="#cb32-1196" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-1197"><a href="#cb32-1197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1198"><a href="#cb32-1198" aria-hidden="true" tabindex="-1"></a>With this new model, we want to be able to classify promoter sequences. But, how do we do this when our base model has already been trained on a different task? The answer lies in a process called **fine-tuning**. This is where you take a pre-trained model, and train it again to perform better on a specific task. In this case, we are relying on the fact that our model already understands the intrinsic relationships in genomic sequences well, and are just shifting its goal. Rather than predicting future genomic sequence, we want it to apply its "knowledge" of sequences to predict whether they are promoters or not.</span>
<span id="cb32-1199"><a href="#cb32-1199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1200"><a href="#cb32-1200" aria-hidden="true" tabindex="-1"></a>For language models, oftentimes fine-tuning tasks are much simpler than the initial learning process (and we assume that the layers of our model other than the new classification layer are already trained), so we usually use a much smaller learning rate (in this case $2e^{-5}$ and fewer training iterations. This is not always true, but it is a good rule of thumb. There are many options you have to optimize fine-tuning, including freezing various layers so that their weights are not updated, using different learning rates for different layers, and much more. But, for the purposes of this notebook, we are simply going to fine-tune the entire model.</span>
<span id="cb32-1201"><a href="#cb32-1201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1202"><a href="#cb32-1202" aria-hidden="true" tabindex="-1"></a>Our training loop is very similar here, but even simpler than our initial training:</span>
<span id="cb32-1203"><a href="#cb32-1203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1204"><a href="#cb32-1204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1205"><a href="#cb32-1205" aria-hidden="true" tabindex="-1"></a><span class="ss">1.   </span>Take a batch from the training dataloader and pass it through the model to get loss</span>
<span id="cb32-1206"><a href="#cb32-1206" aria-hidden="true" tabindex="-1"></a><span class="ss">2.   </span>Do the backward pass, and have our optimizer adjust weights according to each parameter's gradient and the learning rate</span>
<span id="cb32-1207"><a href="#cb32-1207" aria-hidden="true" tabindex="-1"></a><span class="ss">3.   </span>Repeat for all batches</span>
<span id="cb32-1208"><a href="#cb32-1208" aria-hidden="true" tabindex="-1"></a><span class="ss">4.   </span>Every 100 batches, run all validation samples through the model and calculate an average loss and F1 score (a metric for classification performance) for tracking purposes</span>
<span id="cb32-1209"><a href="#cb32-1209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1210"><a href="#cb32-1210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1211"><a href="#cb32-1211" aria-hidden="true" tabindex="-1"></a>Now that we've trained our model, let's run the test data through it and see how it performs. While we do so, let's also look at the attention weights for a specific test sample, and try to get some idea of what our model is doing:</span>
<span id="cb32-1212"><a href="#cb32-1212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1215"><a href="#cb32-1215" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-1216"><a href="#cb32-1216" aria-hidden="true" tabindex="-1"></a><span class="co">#| colab: {base_uri: https://localhost:8080/, height: 1000}</span></span>
<span id="cb32-1217"><a href="#cb32-1217" aria-hidden="true" tabindex="-1"></a>new_model.<span class="bu">eval</span>()  <span class="co"># Set the model to evaluation mode</span></span>
<span id="cb32-1218"><a href="#cb32-1218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1219"><a href="#cb32-1219" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> []</span>
<span id="cb32-1220"><a href="#cb32-1220" aria-hidden="true" tabindex="-1"></a>true_labels <span class="op">=</span> []</span>
<span id="cb32-1221"><a href="#cb32-1221" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-1222"><a href="#cb32-1222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1223"><a href="#cb32-1223" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():  <span class="co"># Disable gradient calculation for inference</span></span>
<span id="cb32-1224"><a href="#cb32-1224" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> test_dataloader:</span>
<span id="cb32-1225"><a href="#cb32-1225" aria-hidden="true" tabindex="-1"></a>        input_ids, labels <span class="op">=</span> batch</span>
<span id="cb32-1226"><a href="#cb32-1226" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> new_model(input_ids)</span>
<span id="cb32-1227"><a href="#cb32-1227" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> outputs[<span class="dv">0</span>]  <span class="co"># Get the logits</span></span>
<span id="cb32-1228"><a href="#cb32-1228" aria-hidden="true" tabindex="-1"></a>        predictions.append(torch.argmax(logits).item())</span>
<span id="cb32-1229"><a href="#cb32-1229" aria-hidden="true" tabindex="-1"></a>        true_labels.append(labels.item())</span>
<span id="cb32-1230"><a href="#cb32-1230" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">==</span> <span class="dv">3000</span>:</span>
<span id="cb32-1231"><a href="#cb32-1231" aria-hidden="true" tabindex="-1"></a>            l <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-1232"><a href="#cb32-1232" aria-hidden="true" tabindex="-1"></a>            fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb32-1233"><a href="#cb32-1233" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> np.arange(<span class="dv">2</span>)</span>
<span id="cb32-1234"><a href="#cb32-1234" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> np.arange(<span class="dv">2</span>)</span>
<span id="cb32-1235"><a href="#cb32-1235" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> x:</span>
<span id="cb32-1236"><a href="#cb32-1236" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> k <span class="kw">in</span> y:</span>
<span id="cb32-1237"><a href="#cb32-1237" aria-hidden="true" tabindex="-1"></a>                    axs[j,k].imshow(outputs[<span class="dv">1</span>][<span class="dv">0</span>][l].cpu().detach().numpy(), cmap<span class="op">=</span><span class="st">'gist_stern'</span>, interpolation<span class="op">=</span><span class="st">'nearest'</span>)</span>
<span id="cb32-1238"><a href="#cb32-1238" aria-hidden="true" tabindex="-1"></a>                    l <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-1239"><a href="#cb32-1239" aria-hidden="true" tabindex="-1"></a>            fig.set_figwidth(<span class="dv">10</span>)</span>
<span id="cb32-1240"><a href="#cb32-1240" aria-hidden="true" tabindex="-1"></a>            fig.set_figheight(<span class="dv">6</span>)</span>
<span id="cb32-1241"><a href="#cb32-1241" aria-hidden="true" tabindex="-1"></a>            fig.suptitle(<span class="st">"Attention weights of Each Head in Layer 0"</span>)</span>
<span id="cb32-1242"><a href="#cb32-1242" aria-hidden="true" tabindex="-1"></a>            plt.show()</span>
<span id="cb32-1243"><a href="#cb32-1243" aria-hidden="true" tabindex="-1"></a>            l <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-1244"><a href="#cb32-1244" aria-hidden="true" tabindex="-1"></a>            fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb32-1245"><a href="#cb32-1245" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> np.arange(<span class="dv">2</span>)</span>
<span id="cb32-1246"><a href="#cb32-1246" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> np.arange(<span class="dv">2</span>)</span>
<span id="cb32-1247"><a href="#cb32-1247" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> x:</span>
<span id="cb32-1248"><a href="#cb32-1248" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> k <span class="kw">in</span> y:</span>
<span id="cb32-1249"><a href="#cb32-1249" aria-hidden="true" tabindex="-1"></a>                    axs[j,k].imshow(torch.mean(outputs[<span class="dv">1</span>][l], dim<span class="op">=</span><span class="dv">0</span>).cpu().detach().numpy(), cmap<span class="op">=</span><span class="st">'gist_stern'</span>, interpolation<span class="op">=</span><span class="st">'nearest'</span>)</span>
<span id="cb32-1250"><a href="#cb32-1250" aria-hidden="true" tabindex="-1"></a>                    l <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-1251"><a href="#cb32-1251" aria-hidden="true" tabindex="-1"></a>            fig.set_figwidth(<span class="dv">10</span>)</span>
<span id="cb32-1252"><a href="#cb32-1252" aria-hidden="true" tabindex="-1"></a>            fig.set_figheight(<span class="dv">6</span>)</span>
<span id="cb32-1253"><a href="#cb32-1253" aria-hidden="true" tabindex="-1"></a>            fig.suptitle(<span class="st">"Attention Weights of Each Layer Averaged Across Heads"</span>)</span>
<span id="cb32-1254"><a href="#cb32-1254" aria-hidden="true" tabindex="-1"></a>            plt.show()</span>
<span id="cb32-1255"><a href="#cb32-1255" aria-hidden="true" tabindex="-1"></a>        i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-1256"><a href="#cb32-1256" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-1257"><a href="#cb32-1257" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1260"><a href="#cb32-1260" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-1261"><a href="#cb32-1261" aria-hidden="true" tabindex="-1"></a><span class="co">#| colab: {base_uri: https://localhost:8080/}</span></span>
<span id="cb32-1262"><a href="#cb32-1262" aria-hidden="true" tabindex="-1"></a>correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-1263"><a href="#cb32-1263" aria-hidden="true" tabindex="-1"></a><span class="co">#print(len(predictions))</span></span>
<span id="cb32-1264"><a href="#cb32-1264" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> np.arange(<span class="bu">len</span>(predictions)) :</span>
<span id="cb32-1265"><a href="#cb32-1265" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> predictions[i] <span class="op">==</span> true_labels[i]:</span>
<span id="cb32-1266"><a href="#cb32-1266" aria-hidden="true" tabindex="-1"></a>        correct <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-1267"><a href="#cb32-1267" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy:</span><span class="sc">{</span><span class="bu">int</span>(correct <span class="op">/</span> <span class="bu">len</span>(predictions) <span class="op">*</span> <span class="dv">100</span>)<span class="sc">:,}</span><span class="ss">%"</span>)</span>
<span id="cb32-1268"><a href="#cb32-1268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1269"><a href="#cb32-1269" aria-hidden="true" tabindex="-1"></a>true_pos <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-1270"><a href="#cb32-1270" aria-hidden="true" tabindex="-1"></a>false_pos <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-1271"><a href="#cb32-1271" aria-hidden="true" tabindex="-1"></a>false_neg <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-1272"><a href="#cb32-1272" aria-hidden="true" tabindex="-1"></a>true_neg <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-1273"><a href="#cb32-1273" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> np.arange(<span class="bu">len</span>(predictions)) :</span>
<span id="cb32-1274"><a href="#cb32-1274" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> predictions[i] <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> true_labels[i] <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb32-1275"><a href="#cb32-1275" aria-hidden="true" tabindex="-1"></a>        true_neg <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-1276"><a href="#cb32-1276" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> predictions[i] <span class="op">==</span> <span class="dv">1</span> <span class="kw">and</span> true_labels[i] <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb32-1277"><a href="#cb32-1277" aria-hidden="true" tabindex="-1"></a>        false_pos <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-1278"><a href="#cb32-1278" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> predictions[i] <span class="op">==</span> <span class="dv">0</span> <span class="kw">and</span> true_labels[i] <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb32-1279"><a href="#cb32-1279" aria-hidden="true" tabindex="-1"></a>        false_neg <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-1280"><a href="#cb32-1280" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> predictions[i] <span class="op">==</span> <span class="dv">1</span> <span class="kw">and</span> true_labels[i] <span class="op">==</span> <span class="dv">1</span>:</span>
<span id="cb32-1281"><a href="#cb32-1281" aria-hidden="true" tabindex="-1"></a>        true_pos <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-1282"><a href="#cb32-1282" aria-hidden="true" tabindex="-1"></a>precision <span class="op">=</span> true_pos <span class="op">/</span> (true_pos <span class="op">+</span> false_pos)</span>
<span id="cb32-1283"><a href="#cb32-1283" aria-hidden="true" tabindex="-1"></a>recall <span class="op">=</span> true_pos <span class="op">/</span> (true_pos <span class="op">+</span> false_neg)</span>
<span id="cb32-1284"><a href="#cb32-1284" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Precision: </span><span class="sc">{</span><span class="bu">round</span>(precision, <span class="dv">2</span>)<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb32-1285"><a href="#cb32-1285" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Recall: </span><span class="sc">{</span><span class="bu">round</span>(recall, <span class="dv">2</span>)<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb32-1286"><a href="#cb32-1286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1287"><a href="#cb32-1287" aria-hidden="true" tabindex="-1"></a>F1 <span class="op">=</span> calc_f1(predictions, true_labels)</span>
<span id="cb32-1288"><a href="#cb32-1288" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"F1 Score: </span><span class="sc">{</span><span class="bu">round</span>(F1, <span class="dv">2</span>)<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb32-1289"><a href="#cb32-1289" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-1290"><a href="#cb32-1290" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1291"><a href="#cb32-1291" aria-hidden="true" tabindex="-1"></a>Wow, it looks like our model actually performs pretty well! Especially for a model that's gone through a total of 20 minutes of training on a single GPU. If we expanded our model or trained it over more data it would improve even further, but these are still good results.</span>
<span id="cb32-1292"><a href="#cb32-1292" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1293"><a href="#cb32-1293" aria-hidden="true" tabindex="-1"></a>Taking a look at the Attention graphs above, we can get some insight into what our model thinks is important for the given sample. The way to read these heatmaps is that the Queries are on the y-axis, and the Keys are on the x-axis. This is all mapped back to the original input, so we essentially have a set of base-by-base relationship heatmaps! For example, a vertical red line indicates that many Query positions saw that Key position as important in the classification task.</span>
<span id="cb32-1294"><a href="#cb32-1294" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1295"><a href="#cb32-1295" aria-hidden="true" tabindex="-1"></a>The first set is a breakdown of all of the heads of our first attention layer. As I mentioned above, we would expect each head to learn something slightly different about our input, and it appears that this is the case! For example, the second head was very focused on the relationships between the bases directly surrounding the query (which is why we have a bright diagonal line), while the 3rd and 4th heads had much more dispersed attention weights.</span>
<span id="cb32-1296"><a href="#cb32-1296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1297"><a href="#cb32-1297" aria-hidden="true" tabindex="-1"></a>This pattern becomes even more apparent as we move to the second set of heatmaps, where we average the attention weights for all heads in each layer (to give an approximate idea of what each full layer learns about the input). At this level, we see some interesting patterns emerge. For example, multiple layers have strong attention weights at the position around 160-170 bp into the sequence, which means that it likely played an important role in deciding this sequence's classification. Each head and layer learned some unique things about the sequence, so take some time to try and understand what these heatmaps are showing!</span>
<span id="cb32-1298"><a href="#cb32-1298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1299"><a href="#cb32-1299" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Questions</span></span>
<span id="cb32-1300"><a href="#cb32-1300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1301"><a href="#cb32-1301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1302"><a href="#cb32-1302" aria-hidden="true" tabindex="-1"></a><span class="ss">1.   </span>Promoters are known for having very characteristic structures of DNA sequence (i.e. TATA boxes as I mentioned above). How could this influence our classification accuracy (i.e. make the classification problem easier or harder)? Why?</span>
<span id="cb32-1303"><a href="#cb32-1303" aria-hidden="true" tabindex="-1"></a><span class="ss">2.   </span>You can see that our validation F1 score metric does not uniformly increase across the training process. How can you explain this?</span>
<span id="cb32-1304"><a href="#cb32-1304" aria-hidden="true" tabindex="-1"></a><span class="ss">3.   </span>Provide your interpretation of the graphs of the Attention weights above. Are there any locations in the input sequence that appear to be highly important (i.e. have a lot of "attention" on them)?</span>
<span id="cb32-1305"><a href="#cb32-1305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1306"><a href="#cb32-1306" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1307"><a href="#cb32-1307" aria-hidden="true" tabindex="-1"></a><span class="fu">## Fine-Tuning (Enhancer Classification)</span></span>
<span id="cb32-1308"><a href="#cb32-1308" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1309"><a href="#cb32-1309" aria-hidden="true" tabindex="-1"></a>Now that we've introduced the idea of fine-tuning and have a simple example, let's move on to a more complicated task. We have another set of data from the <span class="co">[</span><span class="ot">Nucleotide Transformer training notebook</span><span class="co">](https://github.com/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling.ipynb)</span> which is made up of 200-bp sequences that are either labeled as strong enhancers, weak enhancers, or non-enhancers. This is a more complicated task not only because it is multi-class classification, enhancers also generally have less "characteristic" genomic structure compared to promoters. This means that (in theory) our model will have to understand more subtle relationships within each sequence to classify them correctly. This data comes from the below paper:</span>
<span id="cb32-1310"><a href="#cb32-1310" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1311"><a href="#cb32-1311" aria-hidden="true" tabindex="-1"></a>Geng, Q., Yang, R., &amp; Zhang, L. (2022). A deep learning framework for enhancer prediction using word embedding and sequence generation. Biophysical Chemistry, 286, 106822. https://doi.org/10.1016/j.bpc.2022.106822</span>
<span id="cb32-1312"><a href="#cb32-1312" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1313"><a href="#cb32-1313" aria-hidden="true" tabindex="-1"></a>To start, we are going to load the data and tokenize it the exact same way we did previously:</span>
<span id="cb32-1314"><a href="#cb32-1314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1317"><a href="#cb32-1317" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-1318"><a href="#cb32-1318" aria-hidden="true" tabindex="-1"></a><span class="co"># Load the enhancer dataset from the InstaDeep Hugging Face ressources</span></span>
<span id="cb32-1319"><a href="#cb32-1319" aria-hidden="true" tabindex="-1"></a>dataset_name <span class="op">=</span> <span class="st">"enhancers_types"</span></span>
<span id="cb32-1320"><a href="#cb32-1320" aria-hidden="true" tabindex="-1"></a>train_dataset_enhancers <span class="op">=</span> load_dataset(</span>
<span id="cb32-1321"><a href="#cb32-1321" aria-hidden="true" tabindex="-1"></a>        <span class="st">"InstaDeepAI/nucleotide_transformer_downstream_tasks"</span>,</span>
<span id="cb32-1322"><a href="#cb32-1322" aria-hidden="true" tabindex="-1"></a>        dataset_name,</span>
<span id="cb32-1323"><a href="#cb32-1323" aria-hidden="true" tabindex="-1"></a>        split<span class="op">=</span><span class="st">"train"</span>,</span>
<span id="cb32-1324"><a href="#cb32-1324" aria-hidden="true" tabindex="-1"></a>        streaming<span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb32-1325"><a href="#cb32-1325" aria-hidden="true" tabindex="-1"></a>        trust_remote_code<span class="op">=</span><span class="va">True</span></span>
<span id="cb32-1326"><a href="#cb32-1326" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb32-1327"><a href="#cb32-1327" aria-hidden="true" tabindex="-1"></a>test_dataset_enhancers <span class="op">=</span> load_dataset(</span>
<span id="cb32-1328"><a href="#cb32-1328" aria-hidden="true" tabindex="-1"></a>        <span class="st">"InstaDeepAI/nucleotide_transformer_downstream_tasks"</span>,</span>
<span id="cb32-1329"><a href="#cb32-1329" aria-hidden="true" tabindex="-1"></a>        dataset_name,</span>
<span id="cb32-1330"><a href="#cb32-1330" aria-hidden="true" tabindex="-1"></a>        split<span class="op">=</span><span class="st">"test"</span>,</span>
<span id="cb32-1331"><a href="#cb32-1331" aria-hidden="true" tabindex="-1"></a>        streaming<span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb32-1332"><a href="#cb32-1332" aria-hidden="true" tabindex="-1"></a>        trust_remote_code<span class="op">=</span><span class="va">True</span></span>
<span id="cb32-1333"><a href="#cb32-1333" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb32-1334"><a href="#cb32-1334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1335"><a href="#cb32-1335" aria-hidden="true" tabindex="-1"></a>num_labels_enhancer <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb32-1336"><a href="#cb32-1336" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-1337"><a href="#cb32-1337" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1340"><a href="#cb32-1340" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-1341"><a href="#cb32-1341" aria-hidden="true" tabindex="-1"></a><span class="co"># Get training data</span></span>
<span id="cb32-1342"><a href="#cb32-1342" aria-hidden="true" tabindex="-1"></a>train_sequences_enhancers <span class="op">=</span> train_dataset_enhancers[<span class="st">'sequence'</span>]</span>
<span id="cb32-1343"><a href="#cb32-1343" aria-hidden="true" tabindex="-1"></a>train_labels_enhancers <span class="op">=</span> train_dataset_enhancers[<span class="st">'label'</span>]</span>
<span id="cb32-1344"><a href="#cb32-1344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1345"><a href="#cb32-1345" aria-hidden="true" tabindex="-1"></a><span class="co"># Split the dataset into a training and a validation dataset</span></span>
<span id="cb32-1346"><a href="#cb32-1346" aria-hidden="true" tabindex="-1"></a>train_sequences_enhancers, validation_sequences_enhancers, train_labels_enhancers, validation_labels_enhancers <span class="op">=</span> train_test_split(train_sequences_enhancers,</span>
<span id="cb32-1347"><a href="#cb32-1347" aria-hidden="true" tabindex="-1"></a>                                                                              train_labels_enhancers, test_size<span class="op">=</span><span class="fl">0.10</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb32-1348"><a href="#cb32-1348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1349"><a href="#cb32-1349" aria-hidden="true" tabindex="-1"></a><span class="co"># Get test data</span></span>
<span id="cb32-1350"><a href="#cb32-1350" aria-hidden="true" tabindex="-1"></a>test_sequences_enhancers <span class="op">=</span> test_dataset_enhancers[<span class="st">'sequence'</span>]</span>
<span id="cb32-1351"><a href="#cb32-1351" aria-hidden="true" tabindex="-1"></a>test_labels_enhancers <span class="op">=</span> test_dataset_enhancers[<span class="st">'label'</span>]</span>
<span id="cb32-1352"><a href="#cb32-1352" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-1353"><a href="#cb32-1353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1356"><a href="#cb32-1356" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-1357"><a href="#cb32-1357" aria-hidden="true" tabindex="-1"></a>ds_train_enhancers <span class="op">=</span> Dataset.from_dict({<span class="st">"data"</span>: train_sequences_enhancers,<span class="st">'labels'</span>:train_labels_enhancers})</span>
<span id="cb32-1358"><a href="#cb32-1358" aria-hidden="true" tabindex="-1"></a>ds_validation_enhancers <span class="op">=</span> Dataset.from_dict({<span class="st">"data"</span>: validation_sequences_enhancers,<span class="st">'labels'</span>:validation_labels_enhancers})</span>
<span id="cb32-1359"><a href="#cb32-1359" aria-hidden="true" tabindex="-1"></a>ds_test_enhancers <span class="op">=</span> Dataset.from_dict({<span class="st">"data"</span>: test_sequences_enhancers,<span class="st">'labels'</span>:test_labels_enhancers})</span>
<span id="cb32-1360"><a href="#cb32-1360" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-1361"><a href="#cb32-1361" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1364"><a href="#cb32-1364" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-1365"><a href="#cb32-1365" aria-hidden="true" tabindex="-1"></a><span class="co">#| colab: {base_uri: https://localhost:8080/, height: 113, referenced_widgets: [ba7e00d625434246b0d9350c43c2b685, 26153eb79538457aa9b1c42d0cc2b1c4, 9fa042736574449aa0d7480db94948f9, 064305970f744149a30fe4806bd769b6, 6659a84f236345c7aa7c42fc59e9d562, ca0322f52d0b48a8a487c36f0bf980e8, 706281fc44d747d6afd0df8c92fce1ea, 0c3a06f8b6b14f2db7cb6ace3776d6b0, 6f28360f35a0406ba6bee5b6d13a06e4, 7db13094f0564c528b4699e2ee294c44, 16d1f0703ee34a4a98fcebe46f50ddce, 4fab4aad92e046c9876a8690b2f398d9, f9e505f993374b07b29c5452271e552c, 040e1b122e434ad38fc49117aa3ef901, 1f00650d54884828af3769a2c0b73eac, d9782bda299e4eb38be264626e171b39, e37a0625f77844c1b14f4d007249009c, 33e3372d213249ae8465b61a1d402c35, e0d4ab1da2a3416fa87ebc1cf33a24ff, 876f665776b84cb6a42a46e2cf7e7ca9, 49aa2d61956344238ddb701b201e51fc, 042d641a8937436580a1f12ceb000253, 41c39de1a11d410eaad74c197be09db1, 6020be2a91f94c1a9652d776609aa6d5, 712129fac1c9437f9cefc3375c2edb93, 191c4ac205884d3a9748e80ff95ab1f7, 2afe23f4ce2947edb63e11ceb415f990, ff2d244001b24897af9c2c0956dbb873, 9b2819a5df8d41b59bc13701bde9d98f, dbf334fe7dce46639273e45c814fe6ee, 5575b6539450472c97f8e6f67b32698e, b84620c051d34654a0bd628f1388c6f5, f9f09835e67b4cd1bc4459de6cd6ab18]}</span></span>
<span id="cb32-1366"><a href="#cb32-1366" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> tokenize_function(examples):</span>
<span id="cb32-1367"><a href="#cb32-1367" aria-hidden="true" tabindex="-1"></a>  min_length <span class="op">=</span> <span class="bu">min</span>(<span class="bu">len</span>(i) <span class="cf">for</span> i <span class="kw">in</span> examples[<span class="st">'data'</span>])</span>
<span id="cb32-1368"><a href="#cb32-1368" aria-hidden="true" tabindex="-1"></a>  outputs <span class="op">=</span> np.empty((<span class="dv">0</span>, min_length), dtype<span class="op">=</span><span class="st">'int16'</span>)</span>
<span id="cb32-1369"><a href="#cb32-1369" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> example <span class="kw">in</span> examples[<span class="st">"data"</span>]:</span>
<span id="cb32-1370"><a href="#cb32-1370" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> np.vstack([outputs, encode(example[:min_length])])</span>
<span id="cb32-1371"><a href="#cb32-1371" aria-hidden="true" tabindex="-1"></a>  <span class="cf">return</span> {</span>
<span id="cb32-1372"><a href="#cb32-1372" aria-hidden="true" tabindex="-1"></a>      <span class="st">'input_ids'</span>: outputs,</span>
<span id="cb32-1373"><a href="#cb32-1373" aria-hidden="true" tabindex="-1"></a>      <span class="st">'attention_mask'</span>: [<span class="dv">1</span>] <span class="op">*</span> <span class="bu">len</span>(outputs),</span>
<span id="cb32-1374"><a href="#cb32-1374" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb32-1375"><a href="#cb32-1375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1376"><a href="#cb32-1376" aria-hidden="true" tabindex="-1"></a>tokenized_datasets_train_enhancer <span class="op">=</span> ds_train_enhancers.<span class="bu">map</span>(</span>
<span id="cb32-1377"><a href="#cb32-1377" aria-hidden="true" tabindex="-1"></a>    tokenize_function,</span>
<span id="cb32-1378"><a href="#cb32-1378" aria-hidden="true" tabindex="-1"></a>    batched<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb32-1379"><a href="#cb32-1379" aria-hidden="true" tabindex="-1"></a>    remove_columns<span class="op">=</span>[<span class="st">'data'</span>],</span>
<span id="cb32-1380"><a href="#cb32-1380" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb32-1381"><a href="#cb32-1381" aria-hidden="true" tabindex="-1"></a>tokenized_datasets_validation_enhancer <span class="op">=</span> ds_validation_enhancers.<span class="bu">map</span>(</span>
<span id="cb32-1382"><a href="#cb32-1382" aria-hidden="true" tabindex="-1"></a>    tokenize_function,</span>
<span id="cb32-1383"><a href="#cb32-1383" aria-hidden="true" tabindex="-1"></a>    batched<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb32-1384"><a href="#cb32-1384" aria-hidden="true" tabindex="-1"></a>    remove_columns<span class="op">=</span>[<span class="st">'data'</span>],</span>
<span id="cb32-1385"><a href="#cb32-1385" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb32-1386"><a href="#cb32-1386" aria-hidden="true" tabindex="-1"></a>tokenized_datasets_test_enhancer <span class="op">=</span> ds_test_enhancers.<span class="bu">map</span>(</span>
<span id="cb32-1387"><a href="#cb32-1387" aria-hidden="true" tabindex="-1"></a>    tokenize_function,</span>
<span id="cb32-1388"><a href="#cb32-1388" aria-hidden="true" tabindex="-1"></a>    batched<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb32-1389"><a href="#cb32-1389" aria-hidden="true" tabindex="-1"></a>    remove_columns<span class="op">=</span>[<span class="st">'data'</span>],</span>
<span id="cb32-1390"><a href="#cb32-1390" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb32-1391"><a href="#cb32-1391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1392"><a href="#cb32-1392" aria-hidden="true" tabindex="-1"></a>train_dataset <span class="op">=</span> TensorDataset(torch.tensor(tokenized_datasets_train_enhancer[<span class="st">'input_ids'</span>], device<span class="op">=</span>device),</span>
<span id="cb32-1393"><a href="#cb32-1393" aria-hidden="true" tabindex="-1"></a>                              torch.tensor(tokenized_datasets_train_enhancer[<span class="st">'labels'</span>], device<span class="op">=</span>device))</span>
<span id="cb32-1394"><a href="#cb32-1394" aria-hidden="true" tabindex="-1"></a>train_dataloader <span class="op">=</span> DataLoader(train_dataset, shuffle<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=</span><span class="dv">64</span>)</span>
<span id="cb32-1395"><a href="#cb32-1395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1396"><a href="#cb32-1396" aria-hidden="true" tabindex="-1"></a>validation_dataset <span class="op">=</span> TensorDataset(torch.tensor(tokenized_datasets_validation_enhancer[<span class="st">'input_ids'</span>], device<span class="op">=</span>device),</span>
<span id="cb32-1397"><a href="#cb32-1397" aria-hidden="true" tabindex="-1"></a>                              torch.tensor(tokenized_datasets_validation_enhancer[<span class="st">'labels'</span>], device<span class="op">=</span>device))</span>
<span id="cb32-1398"><a href="#cb32-1398" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1399"><a href="#cb32-1399" aria-hidden="true" tabindex="-1"></a>validation_dataloader <span class="op">=</span> DataLoader(validation_dataset, shuffle<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb32-1400"><a href="#cb32-1400" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1401"><a href="#cb32-1401" aria-hidden="true" tabindex="-1"></a>test_dataset <span class="op">=</span> TensorDataset(torch.tensor(tokenized_datasets_test_enhancer[<span class="st">'input_ids'</span>], device<span class="op">=</span>device),</span>
<span id="cb32-1402"><a href="#cb32-1402" aria-hidden="true" tabindex="-1"></a>                              torch.tensor(tokenized_datasets_test_enhancer[<span class="st">'labels'</span>], device<span class="op">=</span>device))</span>
<span id="cb32-1403"><a href="#cb32-1403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1404"><a href="#cb32-1404" aria-hidden="true" tabindex="-1"></a>test_dataloader <span class="op">=</span> DataLoader(test_dataset, shuffle<span class="op">=</span><span class="va">True</span>, batch_size<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb32-1405"><a href="#cb32-1405" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-1406"><a href="#cb32-1406" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1407"><a href="#cb32-1407" aria-hidden="true" tabindex="-1"></a>Now, let's load back in our **original** gLM, i.e. not the one we fine-tuned to classify promoters. This time, we need it to classify on 3 labels rather than 2, so it is simpler to just recreate a new version and fine-tune it from scratch.</span>
<span id="cb32-1408"><a href="#cb32-1408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1411"><a href="#cb32-1411" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-1412"><a href="#cb32-1412" aria-hidden="true" tabindex="-1"></a><span class="co">#| colab: {base_uri: https://localhost:8080/}</span></span>
<span id="cb32-1413"><a href="#cb32-1413" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------------------------</span></span>
<span id="cb32-1414"><a href="#cb32-1414" aria-hidden="true" tabindex="-1"></a>init_from <span class="op">=</span> <span class="st">'resume'</span> <span class="co"># either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')</span></span>
<span id="cb32-1415"><a href="#cb32-1415" aria-hidden="true" tabindex="-1"></a>out_dir <span class="op">=</span> <span class="st">'/root/data/out'</span> <span class="co"># ignored if init_from is not 'resume'</span></span>
<span id="cb32-1416"><a href="#cb32-1416" aria-hidden="true" tabindex="-1"></a>seed <span class="op">=</span> <span class="dv">1337</span></span>
<span id="cb32-1417"><a href="#cb32-1417" aria-hidden="true" tabindex="-1"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="co"># examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.</span></span>
<span id="cb32-1418"><a href="#cb32-1418" aria-hidden="true" tabindex="-1"></a>dtype <span class="op">=</span> <span class="st">'bfloat16'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="kw">and</span> torch.cuda.is_bf16_supported() <span class="cf">else</span> <span class="st">'float16'</span> <span class="co"># 'float32' or 'bfloat16' or 'float16'</span></span>
<span id="cb32-1419"><a href="#cb32-1419" aria-hidden="true" tabindex="-1"></a><span class="bu">compile</span> <span class="op">=</span> <span class="va">True</span> <span class="co"># use PyTorch 2.0 to compile the model to be faster</span></span>
<span id="cb32-1420"><a href="#cb32-1420" aria-hidden="true" tabindex="-1"></a><span class="co"># -----------------------------------------------------------------------------</span></span>
<span id="cb32-1421"><a href="#cb32-1421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1422"><a href="#cb32-1422" aria-hidden="true" tabindex="-1"></a>torch.cuda.init()</span>
<span id="cb32-1423"><a href="#cb32-1423" aria-hidden="true" tabindex="-1"></a>torch.manual_seed(seed)</span>
<span id="cb32-1424"><a href="#cb32-1424" aria-hidden="true" tabindex="-1"></a>torch.cuda.manual_seed(seed)</span>
<span id="cb32-1425"><a href="#cb32-1425" aria-hidden="true" tabindex="-1"></a>torch.backends.cuda.matmul.allow_tf32 <span class="op">=</span> <span class="va">True</span> <span class="co"># allow tf32 on matmul</span></span>
<span id="cb32-1426"><a href="#cb32-1426" aria-hidden="true" tabindex="-1"></a>torch.backends.cudnn.allow_tf32 <span class="op">=</span> <span class="va">True</span> <span class="co"># allow tf32 on cudnn</span></span>
<span id="cb32-1427"><a href="#cb32-1427" aria-hidden="true" tabindex="-1"></a>device_type <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> <span class="st">'cuda'</span> <span class="kw">in</span> device <span class="cf">else</span> <span class="st">'cpu'</span> <span class="co"># for later use in torch.autocast</span></span>
<span id="cb32-1428"><a href="#cb32-1428" aria-hidden="true" tabindex="-1"></a>ptdtype <span class="op">=</span> {<span class="st">'float32'</span>: torch.float32, <span class="st">'bfloat16'</span>: torch.bfloat16, <span class="st">'float16'</span>: torch.float16}[<span class="st">'float16'</span>]</span>
<span id="cb32-1429"><a href="#cb32-1429" aria-hidden="true" tabindex="-1"></a>ctx <span class="op">=</span> nullcontext() <span class="cf">if</span> device_type <span class="op">==</span> <span class="st">'cpu'</span> <span class="cf">else</span> torch.amp.autocast(device_type<span class="op">=</span>device_type, dtype<span class="op">=</span>ptdtype)</span>
<span id="cb32-1430"><a href="#cb32-1430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1431"><a href="#cb32-1431" aria-hidden="true" tabindex="-1"></a><span class="co"># model</span></span>
<span id="cb32-1432"><a href="#cb32-1432" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> init_from <span class="op">==</span> <span class="st">'resume'</span>:</span>
<span id="cb32-1433"><a href="#cb32-1433" aria-hidden="true" tabindex="-1"></a>    <span class="co"># init from a model saved in a specific directory</span></span>
<span id="cb32-1434"><a href="#cb32-1434" aria-hidden="true" tabindex="-1"></a>    ckpt_path <span class="op">=</span> os.path.join(out_dir, <span class="st">'ckpt.pt'</span>)</span>
<span id="cb32-1435"><a href="#cb32-1435" aria-hidden="true" tabindex="-1"></a>    checkpoint <span class="op">=</span> torch.load(ckpt_path, map_location<span class="op">=</span>device)</span>
<span id="cb32-1436"><a href="#cb32-1436" aria-hidden="true" tabindex="-1"></a>    gptconf <span class="op">=</span> GPTConfig(<span class="op">**</span>checkpoint[<span class="st">'model_args'</span>])</span>
<span id="cb32-1437"><a href="#cb32-1437" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> GPT(gptconf)</span>
<span id="cb32-1438"><a href="#cb32-1438" aria-hidden="true" tabindex="-1"></a>    state_dict <span class="op">=</span> checkpoint[<span class="st">'model'</span>]</span>
<span id="cb32-1439"><a href="#cb32-1439" aria-hidden="true" tabindex="-1"></a>    unwanted_prefix <span class="op">=</span> <span class="st">'_orig_mod.'</span></span>
<span id="cb32-1440"><a href="#cb32-1440" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k,v <span class="kw">in</span> <span class="bu">list</span>(state_dict.items()):</span>
<span id="cb32-1441"><a href="#cb32-1441" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> k.startswith(unwanted_prefix):</span>
<span id="cb32-1442"><a href="#cb32-1442" aria-hidden="true" tabindex="-1"></a>            state_dict[k[<span class="bu">len</span>(unwanted_prefix):]] <span class="op">=</span> state_dict.pop(k)</span>
<span id="cb32-1443"><a href="#cb32-1443" aria-hidden="true" tabindex="-1"></a>    model.load_state_dict(state_dict)</span>
<span id="cb32-1444"><a href="#cb32-1444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1445"><a href="#cb32-1445" aria-hidden="true" tabindex="-1"></a>new_model <span class="op">=</span> ClassificationModel(base_model<span class="op">=</span>model, num_labels<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb32-1446"><a href="#cb32-1446" aria-hidden="true" tabindex="-1"></a>new_model.<span class="bu">eval</span>()</span>
<span id="cb32-1447"><a href="#cb32-1447" aria-hidden="true" tabindex="-1"></a>new_model.to(device)</span>
<span id="cb32-1448"><a href="#cb32-1448" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="bu">compile</span>:</span>
<span id="cb32-1449"><a href="#cb32-1449" aria-hidden="true" tabindex="-1"></a>    new_model <span class="op">=</span> torch.<span class="bu">compile</span>(new_model) <span class="co"># requires PyTorch 2.0 (optional)</span></span>
<span id="cb32-1450"><a href="#cb32-1450" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-1451"><a href="#cb32-1451" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1452"><a href="#cb32-1452" aria-hidden="true" tabindex="-1"></a>Now, let's run through the same training loop. You may notice that we have greatly increased the number of epochs we are training for. This is partially due to the fact that the enhancer dataset is much smaller than the promoter dataset (so we need more epochs to get the equivalent number of training batches), but the task is also more complex, so it benefits our model to have more training iterations in general.</span>
<span id="cb32-1453"><a href="#cb32-1453" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1456"><a href="#cb32-1456" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-1457"><a href="#cb32-1457" aria-hidden="true" tabindex="-1"></a><span class="co">#| colab: {base_uri: https://localhost:8080/}</span></span>
<span id="cb32-1458"><a href="#cb32-1458" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(new_model.parameters(), lr<span class="op">=</span><span class="fl">2e-5</span>)</span>
<span id="cb32-1459"><a href="#cb32-1459" aria-hidden="true" tabindex="-1"></a>num_epochs<span class="op">=</span><span class="dv">10</span></span>
<span id="cb32-1460"><a href="#cb32-1460" aria-hidden="true" tabindex="-1"></a>total_steps <span class="op">=</span> <span class="bu">len</span>(train_dataloader) <span class="op">*</span> num_epochs</span>
<span id="cb32-1461"><a href="#cb32-1461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1462"><a href="#cb32-1462" aria-hidden="true" tabindex="-1"></a>val_predictions <span class="op">=</span> []</span>
<span id="cb32-1463"><a href="#cb32-1463" aria-hidden="true" tabindex="-1"></a>val_true_labels <span class="op">=</span> []</span>
<span id="cb32-1464"><a href="#cb32-1464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1465"><a href="#cb32-1465" aria-hidden="true" tabindex="-1"></a><span class="co">#Training Loop</span></span>
<span id="cb32-1466"><a href="#cb32-1466" aria-hidden="true" tabindex="-1"></a>loss_lst <span class="op">=</span> []</span>
<span id="cb32-1467"><a href="#cb32-1467" aria-hidden="true" tabindex="-1"></a>val_loss_lst <span class="op">=</span> []</span>
<span id="cb32-1468"><a href="#cb32-1468" aria-hidden="true" tabindex="-1"></a>mcc_scores <span class="op">=</span> []</span>
<span id="cb32-1469"><a href="#cb32-1469" aria-hidden="true" tabindex="-1"></a>new_model.train()</span>
<span id="cb32-1470"><a href="#cb32-1470" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(num_epochs):</span>
<span id="cb32-1471"><a href="#cb32-1471" aria-hidden="true" tabindex="-1"></a>    i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-1472"><a href="#cb32-1472" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> train_dataloader:</span>
<span id="cb32-1473"><a href="#cb32-1473" aria-hidden="true" tabindex="-1"></a>        input_ids, labels <span class="op">=</span> batch</span>
<span id="cb32-1474"><a href="#cb32-1474" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb32-1475"><a href="#cb32-1475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1476"><a href="#cb32-1476" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Forward pass</span></span>
<span id="cb32-1477"><a href="#cb32-1477" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> new_model(input_ids, labels<span class="op">=</span>labels)</span>
<span id="cb32-1478"><a href="#cb32-1478" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> outputs[<span class="dv">1</span>]</span>
<span id="cb32-1479"><a href="#cb32-1479" aria-hidden="true" tabindex="-1"></a>        loss_lst.append(loss.item())</span>
<span id="cb32-1480"><a href="#cb32-1480" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1481"><a href="#cb32-1481" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Backward pass and optimization</span></span>
<span id="cb32-1482"><a href="#cb32-1482" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb32-1483"><a href="#cb32-1483" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb32-1484"><a href="#cb32-1484" aria-hidden="true" tabindex="-1"></a>        i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-1485"><a href="#cb32-1485" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">%</span> <span class="dv">100</span> <span class="op">==</span> <span class="dv">99</span>:</span>
<span id="cb32-1486"><a href="#cb32-1486" aria-hidden="true" tabindex="-1"></a>            new_model.<span class="bu">eval</span>()</span>
<span id="cb32-1487"><a href="#cb32-1487" aria-hidden="true" tabindex="-1"></a>            <span class="cf">with</span> torch.no_grad():</span>
<span id="cb32-1488"><a href="#cb32-1488" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> val_batch <span class="kw">in</span> validation_dataloader:</span>
<span id="cb32-1489"><a href="#cb32-1489" aria-hidden="true" tabindex="-1"></a>                    val_input_ids, val_labels <span class="op">=</span> val_batch</span>
<span id="cb32-1490"><a href="#cb32-1490" aria-hidden="true" tabindex="-1"></a>                    optimizer.zero_grad()</span>
<span id="cb32-1491"><a href="#cb32-1491" aria-hidden="true" tabindex="-1"></a>                    val_outputs <span class="op">=</span> new_model(val_input_ids, labels<span class="op">=</span>val_labels)</span>
<span id="cb32-1492"><a href="#cb32-1492" aria-hidden="true" tabindex="-1"></a>                    val_loss <span class="op">=</span> val_outputs[<span class="dv">1</span>]</span>
<span id="cb32-1493"><a href="#cb32-1493" aria-hidden="true" tabindex="-1"></a>                    val_loss_lst.append(val_loss.item())</span>
<span id="cb32-1494"><a href="#cb32-1494" aria-hidden="true" tabindex="-1"></a>                    val_logits <span class="op">=</span> val_outputs[<span class="dv">0</span>]  <span class="co"># Get the logits</span></span>
<span id="cb32-1495"><a href="#cb32-1495" aria-hidden="true" tabindex="-1"></a>                    val_predictions.append(torch.argmax(val_logits).item())</span>
<span id="cb32-1496"><a href="#cb32-1496" aria-hidden="true" tabindex="-1"></a>                    val_true_labels.append(val_labels.item())</span>
<span id="cb32-1497"><a href="#cb32-1497" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f"Epoch </span><span class="sc">{</span>epoch<span class="sc">}</span><span class="ss">, Batch </span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">, Train Loss: </span><span class="sc">{</span>np<span class="sc">.</span>mean(loss_lst)<span class="sc">}</span><span class="ss">, Val Loss: </span><span class="sc">{</span>np<span class="sc">.</span>mean(val_loss_lst)<span class="sc">}</span><span class="ss">, Val MCC: </span><span class="sc">{</span>matthews_corrcoef(val_true_labels, val_predictions)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb32-1498"><a href="#cb32-1498" aria-hidden="true" tabindex="-1"></a>            mcc_scores.append(matthews_corrcoef(val_true_labels, val_predictions))</span>
<span id="cb32-1499"><a href="#cb32-1499" aria-hidden="true" tabindex="-1"></a>            loss_lst <span class="op">=</span> []</span>
<span id="cb32-1500"><a href="#cb32-1500" aria-hidden="true" tabindex="-1"></a>            val_loss_lst <span class="op">=</span> []</span>
<span id="cb32-1501"><a href="#cb32-1501" aria-hidden="true" tabindex="-1"></a>            new_model.train()</span>
<span id="cb32-1502"><a href="#cb32-1502" aria-hidden="true" tabindex="-1"></a>            val_predictions <span class="op">=</span> []</span>
<span id="cb32-1503"><a href="#cb32-1503" aria-hidden="true" tabindex="-1"></a>            val_true_labels <span class="op">=</span> []</span>
<span id="cb32-1504"><a href="#cb32-1504" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-1505"><a href="#cb32-1505" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1506"><a href="#cb32-1506" aria-hidden="true" tabindex="-1"></a>Now that we've finished training, we can see that our training loss remains significantly higher for this task than our promoter prediction task, as expected. Note that we use a slightly different performance metric here as well (MCC), which gives us a single scalar value for classification performance (whereas using F1 again would not be so simple). In any case, let's see how this model performs on the test data.</span>
<span id="cb32-1507"><a href="#cb32-1507" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1510"><a href="#cb32-1510" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-1511"><a href="#cb32-1511" aria-hidden="true" tabindex="-1"></a><span class="co">#| colab: {base_uri: https://localhost:8080/, height: 1000}</span></span>
<span id="cb32-1512"><a href="#cb32-1512" aria-hidden="true" tabindex="-1"></a>new_model.<span class="bu">eval</span>()  <span class="co"># Set the model to evaluation mode</span></span>
<span id="cb32-1513"><a href="#cb32-1513" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1514"><a href="#cb32-1514" aria-hidden="true" tabindex="-1"></a>predictions <span class="op">=</span> []</span>
<span id="cb32-1515"><a href="#cb32-1515" aria-hidden="true" tabindex="-1"></a>true_labels <span class="op">=</span> []</span>
<span id="cb32-1516"><a href="#cb32-1516" aria-hidden="true" tabindex="-1"></a>i <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-1517"><a href="#cb32-1517" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1518"><a href="#cb32-1518" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():  <span class="co"># Disable gradient calculation for inference</span></span>
<span id="cb32-1519"><a href="#cb32-1519" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> batch <span class="kw">in</span> test_dataloader:</span>
<span id="cb32-1520"><a href="#cb32-1520" aria-hidden="true" tabindex="-1"></a>        input_ids, labels <span class="op">=</span> batch</span>
<span id="cb32-1521"><a href="#cb32-1521" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> new_model(input_ids)</span>
<span id="cb32-1522"><a href="#cb32-1522" aria-hidden="true" tabindex="-1"></a>        logits <span class="op">=</span> outputs[<span class="dv">0</span>]  <span class="co"># Get the logits</span></span>
<span id="cb32-1523"><a href="#cb32-1523" aria-hidden="true" tabindex="-1"></a>        predictions.append(torch.argmax(logits).item())</span>
<span id="cb32-1524"><a href="#cb32-1524" aria-hidden="true" tabindex="-1"></a>        true_labels.append(labels.item())</span>
<span id="cb32-1525"><a href="#cb32-1525" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">==</span> <span class="dv">200</span>:</span>
<span id="cb32-1526"><a href="#cb32-1526" aria-hidden="true" tabindex="-1"></a>            l <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-1527"><a href="#cb32-1527" aria-hidden="true" tabindex="-1"></a>            fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb32-1528"><a href="#cb32-1528" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> np.arange(<span class="dv">2</span>)</span>
<span id="cb32-1529"><a href="#cb32-1529" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> np.arange(<span class="dv">2</span>)</span>
<span id="cb32-1530"><a href="#cb32-1530" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> x:</span>
<span id="cb32-1531"><a href="#cb32-1531" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> k <span class="kw">in</span> y:</span>
<span id="cb32-1532"><a href="#cb32-1532" aria-hidden="true" tabindex="-1"></a>                    axs[j,k].imshow(outputs[<span class="dv">1</span>][<span class="dv">0</span>][l].cpu().detach().numpy(), cmap<span class="op">=</span><span class="st">'gist_stern'</span>, interpolation<span class="op">=</span><span class="st">'nearest'</span>)</span>
<span id="cb32-1533"><a href="#cb32-1533" aria-hidden="true" tabindex="-1"></a>                    l <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-1534"><a href="#cb32-1534" aria-hidden="true" tabindex="-1"></a>            fig.set_figwidth(<span class="dv">10</span>)</span>
<span id="cb32-1535"><a href="#cb32-1535" aria-hidden="true" tabindex="-1"></a>            fig.set_figheight(<span class="dv">6</span>)</span>
<span id="cb32-1536"><a href="#cb32-1536" aria-hidden="true" tabindex="-1"></a>            fig.suptitle(<span class="st">"Attention weights of Each Head in Layer 0"</span>)</span>
<span id="cb32-1537"><a href="#cb32-1537" aria-hidden="true" tabindex="-1"></a>            plt.show()</span>
<span id="cb32-1538"><a href="#cb32-1538" aria-hidden="true" tabindex="-1"></a>            l <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-1539"><a href="#cb32-1539" aria-hidden="true" tabindex="-1"></a>            fig, axs <span class="op">=</span> plt.subplots(<span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb32-1540"><a href="#cb32-1540" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> np.arange(<span class="dv">2</span>)</span>
<span id="cb32-1541"><a href="#cb32-1541" aria-hidden="true" tabindex="-1"></a>            y <span class="op">=</span> np.arange(<span class="dv">2</span>)</span>
<span id="cb32-1542"><a href="#cb32-1542" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> j <span class="kw">in</span> x:</span>
<span id="cb32-1543"><a href="#cb32-1543" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> k <span class="kw">in</span> y:</span>
<span id="cb32-1544"><a href="#cb32-1544" aria-hidden="true" tabindex="-1"></a>                    axs[j,k].imshow(torch.mean(outputs[<span class="dv">1</span>][l], dim<span class="op">=</span><span class="dv">0</span>).cpu().detach().numpy(), cmap<span class="op">=</span><span class="st">'gist_stern'</span>, interpolation<span class="op">=</span><span class="st">'nearest'</span>)</span>
<span id="cb32-1545"><a href="#cb32-1545" aria-hidden="true" tabindex="-1"></a>                    l <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-1546"><a href="#cb32-1546" aria-hidden="true" tabindex="-1"></a>            fig.set_figwidth(<span class="dv">10</span>)</span>
<span id="cb32-1547"><a href="#cb32-1547" aria-hidden="true" tabindex="-1"></a>            fig.set_figheight(<span class="dv">6</span>)</span>
<span id="cb32-1548"><a href="#cb32-1548" aria-hidden="true" tabindex="-1"></a>            fig.suptitle(<span class="st">"Attention Weights of Each Layer Averaged Across Heads"</span>)</span>
<span id="cb32-1549"><a href="#cb32-1549" aria-hidden="true" tabindex="-1"></a>            plt.show()</span>
<span id="cb32-1550"><a href="#cb32-1550" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(labels.item())</span>
<span id="cb32-1551"><a href="#cb32-1551" aria-hidden="true" tabindex="-1"></a>        i <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-1552"><a href="#cb32-1552" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-1553"><a href="#cb32-1553" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1556"><a href="#cb32-1556" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb32-1557"><a href="#cb32-1557" aria-hidden="true" tabindex="-1"></a><span class="co">#| colab: {base_uri: https://localhost:8080/}</span></span>
<span id="cb32-1558"><a href="#cb32-1558" aria-hidden="true" tabindex="-1"></a>correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb32-1559"><a href="#cb32-1559" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="bu">len</span>(predictions))</span>
<span id="cb32-1560"><a href="#cb32-1560" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> np.arange(<span class="bu">len</span>(predictions)) :</span>
<span id="cb32-1561"><a href="#cb32-1561" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> predictions[i] <span class="op">==</span> true_labels[i]:</span>
<span id="cb32-1562"><a href="#cb32-1562" aria-hidden="true" tabindex="-1"></a>        correct <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb32-1563"><a href="#cb32-1563" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Accuracy: </span><span class="sc">{</span><span class="bu">int</span>(correct <span class="op">/</span> <span class="bu">len</span>(predictions) <span class="op">*</span> <span class="dv">100</span>)<span class="sc">:,}</span><span class="ss">%"</span>)</span>
<span id="cb32-1564"><a href="#cb32-1564" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1565"><a href="#cb32-1565" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MCC: </span><span class="sc">{</span><span class="bu">round</span>(matthews_corrcoef(true_labels, predictions), <span class="dv">2</span>)<span class="sc">:,}</span><span class="ss">"</span>)</span>
<span id="cb32-1566"><a href="#cb32-1566" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb32-1567"><a href="#cb32-1567" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1568"><a href="#cb32-1568" aria-hidden="true" tabindex="-1"></a>Oof, those numbers are a little rough, especially compared to our previous model's performance on promoter classification. If you look at the attention weight heatmaps, you can also see that the weights are significantly more spread out compared to the promoter model. This could mean many things, and it would take more digging to understand exactly what is happening, but one hypothesis is that our model couldn't quite understand what exactly to focus on for classification, so we get more "cloudy" attention signal.</span>
<span id="cb32-1569"><a href="#cb32-1569" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1570"><a href="#cb32-1570" aria-hidden="true" tabindex="-1"></a>In any case, this goes to show that sometimes there are limitations to the complexity that a single model can accurately represent. If we increased the size of our initial gLM and let it run for more training iterations, our performance on this task would likely improve dramatically. Unfortunately, we don't have the time or computing resources for that in this notebook, but I would encourage all of you to continue to investigate if you are interested!</span>
<span id="cb32-1571"><a href="#cb32-1571" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1572"><a href="#cb32-1572" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Questions</span></span>
<span id="cb32-1573"><a href="#cb32-1573" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1574"><a href="#cb32-1574" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1575"><a href="#cb32-1575" aria-hidden="true" tabindex="-1"></a><span class="ss">1.   </span>Why is this enhancer classification problem harder than the promoter classification problem we looked at previously?</span>
<span id="cb32-1576"><a href="#cb32-1576" aria-hidden="true" tabindex="-1"></a><span class="ss">2.   </span>Similarly to the question above, provide your interpretation of the graphs of Attention weights.</span>
<span id="cb32-1577"><a href="#cb32-1577" aria-hidden="true" tabindex="-1"></a><span class="ss">3.   </span>One characteristic of enhancers is that they tend to have some distance from the gene they influence (much greater than 300 bp). How could we potentially change our original gLM in order to potentially improve our performance in this problem?</span>
<span id="cb32-1578"><a href="#cb32-1578" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1579"><a href="#cb32-1579" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1580"><a href="#cb32-1580" aria-hidden="true" tabindex="-1"></a><span class="fu">## Conclusion</span></span>
<span id="cb32-1581"><a href="#cb32-1581" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1582"><a href="#cb32-1582" aria-hidden="true" tabindex="-1"></a>To sum up everything we have gone through in this notebook, here are a few major points:</span>
<span id="cb32-1583"><a href="#cb32-1583" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1584"><a href="#cb32-1584" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1585"><a href="#cb32-1585" aria-hidden="true" tabindex="-1"></a><span class="ss">1.   </span>Large language models (LLMs) have become well-known for their popular use (i.e. ChatGPT), but they have strong potential for use as research tools in various fields as well.</span>
<span id="cb32-1586"><a href="#cb32-1586" aria-hidden="true" tabindex="-1"></a><span class="ss">2.   </span>Genomic langauge models (gLMs) are a type of LLM that attempt to learn relationships between different of regions of DNA directly from DNA sequence.</span>
<span id="cb32-1587"><a href="#cb32-1587" aria-hidden="true" tabindex="-1"></a><span class="ss">3.   </span>In general, gLMs can be created similarly to any other type of language model (usually using the Attention mechanism), however strategies for tokenization can differ due to the simplified vocabulary of DNA sequence.</span>
<span id="cb32-1588"><a href="#cb32-1588" aria-hidden="true" tabindex="-1"></a><span class="ss">4.   </span>Once tokenization and embeddings are defined, a gLM can be trained just like any other language model.</span>
<span id="cb32-1589"><a href="#cb32-1589" aria-hidden="true" tabindex="-1"></a><span class="ss">5.   </span>gLMs can be fine-tuned to perform specific tasks.</span>
<span id="cb32-1590"><a href="#cb32-1590" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-1591"><a href="#cb32-1591" aria-hidden="true" tabindex="-1"></a>The applications of language modeling to functional genomic research is a very new field, and publications of high-performing models are just beginning to emerge. If you are interested in further investigation, we would highly recommend the <span class="co">[</span><span class="ot">DNABERT-2</span><span class="co">](https://arxiv.org/abs/2306.15006)</span> and <span class="co">[</span><span class="ot">Nucleotide Transformer</span><span class="co">](https://www.biorxiv.org/content/10.1101/2023.01.11.523679v3.full)</span> papers. These represent two distinctly different approaches to DNA language modeling, each with their own benefits and downsides.</span>
<span id="cb32-1592"><a href="#cb32-1592" aria-hidden="true" tabindex="-1"></a></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>