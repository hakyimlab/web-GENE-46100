<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Andrey Karpathy">
<meta name="dcterms.date" content="2025-04-11">

<title>Building a GPT - companion notebook qmd – GENE 46100</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-dcc7d9e706e4adfbab8e3d6c7c60bab2.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">GENE 46100</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#building-a-gpt" id="toc-building-a-gpt" class="nav-link active" data-scroll-target="#building-a-gpt">Building a GPT</a>
  <ul class="collapse">
  <li><a href="#download-the-tiny-shakespeare-dataset" id="toc-download-the-tiny-shakespeare-dataset" class="nav-link" data-scroll-target="#download-the-tiny-shakespeare-dataset">download the tiny shakespeare dataset</a></li>
  <li><a href="#define-the-context-and-target-8-examples-in-one-batch" id="toc-define-the-context-and-target-8-examples-in-one-batch" class="nav-link" data-scroll-target="#define-the-context-and-target-8-examples-in-one-batch">define the context and target: 8 examples in one batch</a></li>
  <li><a href="#define-the-batch-size-and-get-the-batch" id="toc-define-the-batch-size-and-get-the-batch" class="nav-link" data-scroll-target="#define-the-batch-size-and-get-the-batch">define the batch size and get the batch</a></li>
  <li><a href="#start-with-a-simple-model-the-bigram-language-model" id="toc-start-with-a-simple-model-the-bigram-language-model" class="nav-link" data-scroll-target="#start-with-a-simple-model-the-bigram-language-model">start with a simple model: the bigram language model</a></li>
  <li><a href="#cross-entropy-loss" id="toc-cross-entropy-loss" class="nav-link" data-scroll-target="#cross-entropy-loss">cross entropy loss</a></li>
  <li><a href="#initialize-the-model-and-compute-the-loss" id="toc-initialize-the-model-and-compute-the-loss" class="nav-link" data-scroll-target="#initialize-the-model-and-compute-the-loss">initialize the model and compute the loss</a></li>
  <li><a href="#generate-text" id="toc-generate-text" class="nav-link" data-scroll-target="#generate-text">generate text</a></li>
  <li><a href="#choose-adamw-as-the-optimizer" id="toc-choose-adamw-as-the-optimizer" class="nav-link" data-scroll-target="#choose-adamw-as-the-optimizer">choose AdamW as the optimizer</a></li>
  <li><a href="#train-the-model" id="toc-train-the-model" class="nav-link" data-scroll-target="#train-the-model">train the model</a></li>
  <li><a href="#generate-text-starting-with-as-initial-context" id="toc-generate-text-starting-with-as-initial-context" class="nav-link" data-scroll-target="#generate-text-starting-with-as-initial-context">generate text starting with as initial context</a></li>
  </ul></li>
  <li><a href="#the-mathematical-trick-in-self-attention" id="toc-the-mathematical-trick-in-self-attention" class="nav-link" data-scroll-target="#the-mathematical-trick-in-self-attention">The mathematical trick in self-attention</a>
  <ul class="collapse">
  <li><a href="#toy-example-illustrating-how-matrix-multiplication-can-be-used-for-a-weighted-aggregation" id="toc-toy-example-illustrating-how-matrix-multiplication-can-be-used-for-a-weighted-aggregation" class="nav-link" data-scroll-target="#toy-example-illustrating-how-matrix-multiplication-can-be-used-for-a-weighted-aggregation">toy example illustrating how matrix multiplication can be used for a “weighted aggregation”</a></li>
  <li><a href="#version-1-using-a-for-loop-to-compute-the-weighted-aggregation" id="toc-version-1-using-a-for-loop-to-compute-the-weighted-aggregation" class="nav-link" data-scroll-target="#version-1-using-a-for-loop-to-compute-the-weighted-aggregation">version 1: using a for loop to compute the weighted aggregation</a></li>
  <li><a href="#version-2-using-matrix-multiply-for-a-weighted-aggregation" id="toc-version-2-using-matrix-multiply-for-a-weighted-aggregation" class="nav-link" data-scroll-target="#version-2-using-matrix-multiply-for-a-weighted-aggregation">version 2: using matrix multiply for a weighted aggregation</a></li>
  <li><a href="#version-3-use-softmax" id="toc-version-3-use-softmax" class="nav-link" data-scroll-target="#version-3-use-softmax">version 3: use Softmax</a></li>
  </ul></li>
  <li><a href="#version-4-self-attention" id="toc-version-4-self-attention" class="nav-link" data-scroll-target="#version-4-self-attention">version 4: self-attention</a>
  <ul class="collapse">
  <li><a href="#why-scaled-attention" id="toc-why-scaled-attention" class="nav-link" data-scroll-target="#why-scaled-attention">why scaled attention?</a></li>
  <li><a href="#layernorm1d" id="toc-layernorm1d" class="nav-link" data-scroll-target="#layernorm1d">LayerNorm1d</a></li>
  <li><a href="#full-finished-code-for-reference" id="toc-full-finished-code-for-reference" class="nav-link" data-scroll-target="#full-finished-code-for-reference">Full finished code, for reference</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Building a GPT - companion notebook qmd</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Andrey Karpathy </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 11, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="building-a-gpt" class="level2">
<h2 class="anchored" data-anchor-id="building-a-gpt">Building a GPT</h2>
<p>Companion notebook to the <a href="https://karpathy.ai/zero-to-hero.html">Zero To Hero</a> video on GPT. Downloaded from <a href="https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing">here</a></p>
<p>(https://github.com/karpathy/nanoGPT)</p>
<section id="download-the-tiny-shakespeare-dataset" class="level3">
<h3 class="anchored" data-anchor-id="download-the-tiny-shakespeare-dataset">download the tiny shakespeare dataset</h3>
<div id="d62311ec" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># We always start with a dataset to train on. Let's download the tiny shakespeare dataset</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="co">#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="48398be4" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># read it in to inspect it</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'input.txt'</span>, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> f:</span>
<span id="cb2-3"><a href="#cb2-3"></a>    text <span class="op">=</span> f.read()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="85c2d53a" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># print the length of the dataset</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="bu">print</span>(<span class="st">"length of dataset in characters: "</span>, <span class="bu">len</span>(text))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>length of dataset in characters:  1115394</code></pre>
</div>
</div>
<div id="6d25d44b" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="co"># let's look at the first 1000 characters</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="bu">print</span>(text[:<span class="dv">1000</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>First Citizen:
Before we proceed any further, hear me speak.

All:
Speak, speak.

First Citizen:
You are all resolved rather to die than to famish?

All:
Resolved. resolved.

First Citizen:
First, you know Caius Marcius is chief enemy to the people.

All:
We know't, we know't.

First Citizen:
Let us kill him, and we'll have corn at our own price.
Is't a verdict?

All:
No more talking on't; let it be done: away, away!

Second Citizen:
One word, good citizens.

First Citizen:
We are accounted poor citizens, the patricians good.
What authority surfeits on would relieve us: if they
would yield us but the superfluity, while it were
wholesome, we might guess they relieved us humanely;
but they think we are too dear: the leanness that
afflicts us, the object of our misery, is as an
inventory to particularise their abundance; our
sufferance is a gain to them Let us revenge this with
our pikes, ere we become rakes: for the gods know I
speak this in hunger for bread, not in thirst for revenge.

</code></pre>
</div>
</div>
<div id="6484eb10" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="co"># here are all the unique characters that occur in this text</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(text)))</span>
<span id="cb7-3"><a href="#cb7-3"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(chars)</span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="bu">print</span>(<span class="st">''</span>.join(chars))</span>
<span id="cb7-5"><a href="#cb7-5"></a><span class="bu">print</span>(vocab_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
 !$&amp;',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz
65</code></pre>
</div>
</div>
<div id="bcccb3c2" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="co"># create a mapping from characters to integers</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>stoi <span class="op">=</span> { ch:i <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }</span>
<span id="cb9-3"><a href="#cb9-3"></a>itos <span class="op">=</span> { i:ch <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }</span>
<span id="cb9-4"><a href="#cb9-4"></a>encode <span class="op">=</span> <span class="kw">lambda</span> s: [stoi[c] <span class="cf">for</span> c <span class="kw">in</span> s] <span class="co"># encoder: take a string, output a list of integers</span></span>
<span id="cb9-5"><a href="#cb9-5"></a>decode <span class="op">=</span> <span class="kw">lambda</span> l: <span class="st">''</span>.join([itos[i] <span class="cf">for</span> i <span class="kw">in</span> l]) <span class="co"># decoder: take a list of integers, output a string</span></span>
<span id="cb9-6"><a href="#cb9-6"></a></span>
<span id="cb9-7"><a href="#cb9-7"></a><span class="bu">print</span>(encode(<span class="st">"hii there"</span>))</span>
<span id="cb9-8"><a href="#cb9-8"></a><span class="bu">print</span>(decode(encode(<span class="st">"hii there"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[46, 47, 47, 1, 58, 46, 43, 56, 43]
hii there</code></pre>
</div>
</div>
<div id="6af4d1a5" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="co"># %pip install torch torchvision torchaudio scikit-learn</span></span>
<span id="cb11-2"><a href="#cb11-2"></a></span>
<span id="cb11-3"><a href="#cb11-3"></a><span class="co"># import os</span></span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="co"># import time</span></span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="co"># import math</span></span>
<span id="cb11-6"><a href="#cb11-6"></a><span class="co"># import pickle</span></span>
<span id="cb11-7"><a href="#cb11-7"></a><span class="co"># from contextlib import nullcontext</span></span>
<span id="cb11-8"><a href="#cb11-8"></a></span>
<span id="cb11-9"><a href="#cb11-9"></a><span class="co"># import numpy as np</span></span>
<span id="cb11-10"><a href="#cb11-10"></a><span class="co"># import torch</span></span>
<span id="cb11-11"><a href="#cb11-11"></a><span class="co"># from torch.nn.parallel import DistributedDataParallel as DDP</span></span>
<span id="cb11-12"><a href="#cb11-12"></a><span class="co"># from torch.distributed import init_process_group, destroy_process_group</span></span>
<span id="cb11-13"><a href="#cb11-13"></a></span>
<span id="cb11-14"><a href="#cb11-14"></a><span class="co"># from model import GPTConfig, GPT</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="94f37b12" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a><span class="co"># let's now encode the entire text dataset and store it into a torch.Tensor</span></span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="im">import</span> torch <span class="co"># we use PyTorch: https://pytorch.org</span></span>
<span id="cb12-3"><a href="#cb12-3"></a>data <span class="op">=</span> torch.tensor(encode(text), dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb12-4"><a href="#cb12-4"></a><span class="bu">print</span>(data.shape, data.dtype)</span>
<span id="cb12-5"><a href="#cb12-5"></a><span class="bu">print</span>(data[:<span class="dv">1000</span>]) <span class="co"># the 1000 characters we looked at earier will to the GPT look like this</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([1115394]) torch.int64
tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,
        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,
         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,
        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,
         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,
        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,
         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,
        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,
        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,
         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,
         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,
        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,
        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,
         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,
        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,
        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,
        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,
        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,
        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,
        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,
        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,
         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,
         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,
         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,
        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,
        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,
        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,
        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,
        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,
        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,
        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,
         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,
         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,
        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,
        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,
        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,
         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,
        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,
        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,
         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,
        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,
        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,
        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,
        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,
        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,
        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,
        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,
        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,
        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,
         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,
        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,
        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,
        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,
        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,
        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,
        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])</code></pre>
</div>
</div>
<div id="688c877a" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="co"># split up the data into train and validation sets</span></span>
<span id="cb14-2"><a href="#cb14-2"></a>n <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span><span class="op">*</span><span class="bu">len</span>(data)) <span class="co"># first 90% will be train, rest val</span></span>
<span id="cb14-3"><a href="#cb14-3"></a>train_data <span class="op">=</span> data[:n]</span>
<span id="cb14-4"><a href="#cb14-4"></a>val_data <span class="op">=</span> data[n:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="c51b4a66" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a><span class="co"># define the block size</span></span>
<span id="cb15-2"><a href="#cb15-2"></a>block_size <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb15-3"><a href="#cb15-3"></a>train_data[:block_size<span class="op">+</span><span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])</code></pre>
</div>
</div>
</section>
<section id="define-the-context-and-target-8-examples-in-one-batch" class="level3">
<h3 class="anchored" data-anchor-id="define-the-context-and-target-8-examples-in-one-batch">define the context and target: 8 examples in one batch</h3>
<div id="0a2c998e" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a>x <span class="op">=</span> train_data[:block_size]</span>
<span id="cb17-2"><a href="#cb17-2"></a>y <span class="op">=</span> train_data[<span class="dv">1</span>:block_size<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb17-3"><a href="#cb17-3"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(block_size):</span>
<span id="cb17-4"><a href="#cb17-4"></a>    context <span class="op">=</span> x[:t<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb17-5"><a href="#cb17-5"></a>    target <span class="op">=</span> y[t]</span>
<span id="cb17-6"><a href="#cb17-6"></a>    <span class="bu">print</span>(<span class="ss">f"when input is </span><span class="sc">{</span>context<span class="sc">}</span><span class="ss"> the target: </span><span class="sc">{</span>target<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>when input is tensor([18]) the target: 47
when input is tensor([18, 47]) the target: 56
when input is tensor([18, 47, 56]) the target: 57
when input is tensor([18, 47, 56, 57]) the target: 58
when input is tensor([18, 47, 56, 57, 58]) the target: 1
when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15
when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47
when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58</code></pre>
</div>
</div>
</section>
<section id="define-the-batch-size-and-get-the-batch" class="level3">
<h3 class="anchored" data-anchor-id="define-the-batch-size-and-get-the-batch">define the batch size and get the batch</h3>
<div id="32ba9b02" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb19-2"><a href="#cb19-2"></a>batch_size <span class="op">=</span> <span class="dv">4</span> <span class="co"># how many independent sequences will we process in parallel?</span></span>
<span id="cb19-3"><a href="#cb19-3"></a>block_size <span class="op">=</span> <span class="dv">8</span> <span class="co"># what is the maximum context length for predictions?</span></span>
<span id="cb19-4"><a href="#cb19-4"></a></span>
<span id="cb19-5"><a href="#cb19-5"></a><span class="kw">def</span> get_batch(split):</span>
<span id="cb19-6"><a href="#cb19-6"></a>    <span class="co"># generate a small batch of data of inputs x and targets y</span></span>
<span id="cb19-7"><a href="#cb19-7"></a>    data <span class="op">=</span> train_data <span class="cf">if</span> split <span class="op">==</span> <span class="st">'train'</span> <span class="cf">else</span> val_data</span>
<span id="cb19-8"><a href="#cb19-8"></a>    ix <span class="op">=</span> torch.randint(<span class="bu">len</span>(data) <span class="op">-</span> block_size, (batch_size,))</span>
<span id="cb19-9"><a href="#cb19-9"></a>    x <span class="op">=</span> torch.stack([data[i:i<span class="op">+</span>block_size] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb19-10"><a href="#cb19-10"></a>    y <span class="op">=</span> torch.stack([data[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span>block_size<span class="op">+</span><span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb19-11"><a href="#cb19-11"></a>    <span class="cf">return</span> x, y</span>
<span id="cb19-12"><a href="#cb19-12"></a></span>
<span id="cb19-13"><a href="#cb19-13"></a>xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb19-14"><a href="#cb19-14"></a><span class="bu">print</span>(<span class="st">'inputs:'</span>)</span>
<span id="cb19-15"><a href="#cb19-15"></a><span class="bu">print</span>(xb.shape)</span>
<span id="cb19-16"><a href="#cb19-16"></a><span class="bu">print</span>(xb)</span>
<span id="cb19-17"><a href="#cb19-17"></a><span class="bu">print</span>(<span class="st">'targets:'</span>)</span>
<span id="cb19-18"><a href="#cb19-18"></a><span class="bu">print</span>(yb.shape)</span>
<span id="cb19-19"><a href="#cb19-19"></a><span class="bu">print</span>(yb)</span>
<span id="cb19-20"><a href="#cb19-20"></a></span>
<span id="cb19-21"><a href="#cb19-21"></a><span class="bu">print</span>(<span class="st">'----'</span>)</span>
<span id="cb19-22"><a href="#cb19-22"></a></span>
<span id="cb19-23"><a href="#cb19-23"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(batch_size): <span class="co"># batch dimension</span></span>
<span id="cb19-24"><a href="#cb19-24"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(block_size): <span class="co"># time dimension</span></span>
<span id="cb19-25"><a href="#cb19-25"></a>        context <span class="op">=</span> xb[b, :t<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb19-26"><a href="#cb19-26"></a>        target <span class="op">=</span> yb[b,t]</span>
<span id="cb19-27"><a href="#cb19-27"></a>        <span class="bu">print</span>(<span class="ss">f"when input is </span><span class="sc">{</span>context<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss"> the target: </span><span class="sc">{</span>target<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>inputs:
torch.Size([4, 8])
tensor([[24, 43, 58,  5, 57,  1, 46, 43],
        [44, 53, 56,  1, 58, 46, 39, 58],
        [52, 58,  1, 58, 46, 39, 58,  1],
        [25, 17, 27, 10,  0, 21,  1, 54]])
targets:
torch.Size([4, 8])
tensor([[43, 58,  5, 57,  1, 46, 43, 39],
        [53, 56,  1, 58, 46, 39, 58,  1],
        [58,  1, 58, 46, 39, 58,  1, 46],
        [17, 27, 10,  0, 21,  1, 54, 39]])
----
when input is [24] the target: 43
when input is [24, 43] the target: 58
when input is [24, 43, 58] the target: 5
when input is [24, 43, 58, 5] the target: 57
when input is [24, 43, 58, 5, 57] the target: 1
when input is [24, 43, 58, 5, 57, 1] the target: 46
when input is [24, 43, 58, 5, 57, 1, 46] the target: 43
when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39
when input is [44] the target: 53
when input is [44, 53] the target: 56
when input is [44, 53, 56] the target: 1
when input is [44, 53, 56, 1] the target: 58
when input is [44, 53, 56, 1, 58] the target: 46
when input is [44, 53, 56, 1, 58, 46] the target: 39
when input is [44, 53, 56, 1, 58, 46, 39] the target: 58
when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1
when input is [52] the target: 58
when input is [52, 58] the target: 1
when input is [52, 58, 1] the target: 58
when input is [52, 58, 1, 58] the target: 46
when input is [52, 58, 1, 58, 46] the target: 39
when input is [52, 58, 1, 58, 46, 39] the target: 58
when input is [52, 58, 1, 58, 46, 39, 58] the target: 1
when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46
when input is [25] the target: 17
when input is [25, 17] the target: 27
when input is [25, 17, 27] the target: 10
when input is [25, 17, 27, 10] the target: 0
when input is [25, 17, 27, 10, 0] the target: 21
when input is [25, 17, 27, 10, 0, 21] the target: 1
when input is [25, 17, 27, 10, 0, 21, 1] the target: 54
when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39</code></pre>
</div>
</div>
</section>
<section id="start-with-a-simple-model-the-bigram-language-model" class="level3">
<h3 class="anchored" data-anchor-id="start-with-a-simple-model-the-bigram-language-model">start with a simple model: the bigram language model</h3>
<p>This model predicts the next character based on the current character only. It uses the logits for the next character in a lookup table.</p>
<div id="9b87d4ce" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a><span class="co"># define the bigram language model</span></span>
<span id="cb21-2"><a href="#cb21-2"></a><span class="im">import</span> torch</span>
<span id="cb21-3"><a href="#cb21-3"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb21-4"><a href="#cb21-4"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb21-5"><a href="#cb21-5"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb21-6"><a href="#cb21-6"></a></span>
<span id="cb21-7"><a href="#cb21-7"></a><span class="kw">class</span> BigramLanguageModel(nn.Module):</span>
<span id="cb21-8"><a href="#cb21-8"></a></span>
<span id="cb21-9"><a href="#cb21-9"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size):</span>
<span id="cb21-10"><a href="#cb21-10"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb21-11"><a href="#cb21-11"></a>        <span class="co"># each token directly reads off the logits for the next token from a lookup table</span></span>
<span id="cb21-12"><a href="#cb21-12"></a>        <span class="va">self</span>.token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, vocab_size)</span>
<span id="cb21-13"><a href="#cb21-13"></a></span>
<span id="cb21-14"><a href="#cb21-14"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb21-15"><a href="#cb21-15"></a></span>
<span id="cb21-16"><a href="#cb21-16"></a>        <span class="co"># idx and targets are both (B,T) tensor of integers</span></span>
<span id="cb21-17"><a href="#cb21-17"></a>        logits <span class="op">=</span> <span class="va">self</span>.token_embedding_table(idx) <span class="co"># (B,T,C)</span></span>
<span id="cb21-18"><a href="#cb21-18"></a></span>
<span id="cb21-19"><a href="#cb21-19"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb21-20"><a href="#cb21-20"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb21-21"><a href="#cb21-21"></a>        <span class="cf">else</span>:</span>
<span id="cb21-22"><a href="#cb21-22"></a>            B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb21-23"><a href="#cb21-23"></a>            logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C)</span>
<span id="cb21-24"><a href="#cb21-24"></a>            targets <span class="op">=</span> targets.view(B<span class="op">*</span>T)</span>
<span id="cb21-25"><a href="#cb21-25"></a>            loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb21-26"><a href="#cb21-26"></a></span>
<span id="cb21-27"><a href="#cb21-27"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb21-28"><a href="#cb21-28"></a></span>
<span id="cb21-29"><a href="#cb21-29"></a>    <span class="co"># Text Generation Method</span></span>
<span id="cb21-30"><a href="#cb21-30"></a>    <span class="co"># This method implements autoregressive text generation by:</span></span>
<span id="cb21-31"><a href="#cb21-31"></a>    <span class="co"># 1. Taking the current context (idx)</span></span>
<span id="cb21-32"><a href="#cb21-32"></a>    <span class="co"># 2. Predicting the next token probabilities</span></span>
<span id="cb21-33"><a href="#cb21-33"></a>    <span class="co"># 3. Sampling from these probabilities</span></span>
<span id="cb21-34"><a href="#cb21-34"></a>    <span class="co"># 4. Appending the sampled token to the context</span></span>
<span id="cb21-35"><a href="#cb21-35"></a>    <span class="co"># 5. Repeating for max_new_tokens iterations</span></span>
<span id="cb21-36"><a href="#cb21-36"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens):</span>
<span id="cb21-37"><a href="#cb21-37"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb21-38"><a href="#cb21-38"></a>            <span class="co"># get the predictions</span></span>
<span id="cb21-39"><a href="#cb21-39"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>(idx)</span>
<span id="cb21-40"><a href="#cb21-40"></a>            <span class="co"># focus only on the last time step</span></span>
<span id="cb21-41"><a href="#cb21-41"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="co"># becomes (B, C)</span></span>
<span id="cb21-42"><a href="#cb21-42"></a>            <span class="co"># apply softmax to get probabilities</span></span>
<span id="cb21-43"><a href="#cb21-43"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># (B, C)</span></span>
<span id="cb21-44"><a href="#cb21-44"></a>            <span class="co"># sample from the distribution</span></span>
<span id="cb21-45"><a href="#cb21-45"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, 1)</span></span>
<span id="cb21-46"><a href="#cb21-46"></a>            <span class="co"># append sampled index to the running sequence</span></span>
<span id="cb21-47"><a href="#cb21-47"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, T+1)</span></span>
<span id="cb21-48"><a href="#cb21-48"></a>        <span class="cf">return</span> idx</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="cross-entropy-loss" class="level3">
<h3 class="anchored" data-anchor-id="cross-entropy-loss">cross entropy loss</h3>
<p>Loss = -Σ(y * log(p)) where: - y = actual probability (0 or 1) - p = predicted probability - Σ = sum over all classes</p>
<p>This is the loss for a single token. The total loss is the average across all B*T tokens in the batch, where: - B = batch size (number of sequences processed in parallel) - T = sequence length (number of tokens in each sequence)</p>
<p>Before training, we would expect the model to predict the next character from a uniform distribution.</p>
<p>-Σ(y * log(p)) = = - ( 1 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) ) = - log(1/65) = 4.1744 per token.</p>
<p>i.e.&nbsp;at the beginning we expect loss of -log(1/65) = 4.1744 per token.</p>
</section>
<section id="initialize-the-model-and-compute-the-loss" class="level3">
<h3 class="anchored" data-anchor-id="initialize-the-model-and-compute-the-loss">initialize the model and compute the loss</h3>
<div id="b4b0ab2d" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a>m <span class="op">=</span> BigramLanguageModel(vocab_size)</span>
<span id="cb22-2"><a href="#cb22-2"></a>logits, loss <span class="op">=</span> m(xb, yb)</span>
<span id="cb22-3"><a href="#cb22-3"></a><span class="bu">print</span>(logits.shape)</span>
<span id="cb22-4"><a href="#cb22-4"></a><span class="bu">print</span>(loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([32, 65])
tensor(4.8786, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
</section>
<section id="generate-text" class="level3">
<h3 class="anchored" data-anchor-id="generate-text">generate text</h3>
<div id="c31763a1" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="bu">print</span>(decode(m.generate(idx <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>), max_new_tokens<span class="op">=</span><span class="dv">100</span>)[<span class="dv">0</span>].tolist()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp
wnYWmnxKWWev-tDqXErVKLgJ</code></pre>
</div>
</div>
</section>
<section id="choose-adamw-as-the-optimizer" class="level3">
<h3 class="anchored" data-anchor-id="choose-adamw-as-the-optimizer">choose AdamW as the optimizer</h3>
<div id="49aa952c" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(m.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="train-the-model" class="level3">
<h3 class="anchored" data-anchor-id="train-the-model">train the model</h3>
<div id="d31903b4" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb27-2"><a href="#cb27-2"></a><span class="cf">for</span> steps <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>): <span class="co"># increase number of steps for good results...</span></span>
<span id="cb27-3"><a href="#cb27-3"></a></span>
<span id="cb27-4"><a href="#cb27-4"></a>    <span class="co"># sample a batch of data</span></span>
<span id="cb27-5"><a href="#cb27-5"></a>    xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb27-6"><a href="#cb27-6"></a></span>
<span id="cb27-7"><a href="#cb27-7"></a>    <span class="co"># evaluate the loss</span></span>
<span id="cb27-8"><a href="#cb27-8"></a>    logits, loss <span class="op">=</span> m(xb, yb)</span>
<span id="cb27-9"><a href="#cb27-9"></a>    optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb27-10"><a href="#cb27-10"></a>    loss.backward()</span>
<span id="cb27-11"><a href="#cb27-11"></a>    optimizer.step()</span>
<span id="cb27-12"><a href="#cb27-12"></a></span>
<span id="cb27-13"><a href="#cb27-13"></a><span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>4.65630578994751</code></pre>
</div>
</div>
</section>
<section id="generate-text-starting-with-as-initial-context" class="level3">
<h3 class="anchored" data-anchor-id="generate-text-starting-with-as-initial-context">generate text starting with as initial context</h3>
<div id="52fa67dd" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1"></a><span class="bu">print</span>(decode(m.generate(idx <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>), max_new_tokens<span class="op">=</span><span class="dv">500</span>)[<span class="dv">0</span>].tolist()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
oTo.JUZ!!zqe!
xBP qbs$Gy'AcOmrLwwt
p$x;Seh-onQbfM?OjKbn'NwUAW -Np3fkz$FVwAUEa-wzWC -wQo-R!v -Mj?,SPiTyZ;o-opr$mOiPJEYD-CfigkzD3p3?zvS;ADz;.y?o,ivCuC'zqHxcVT cHA
rT'Fd,SBMZyOslg!NXeF$sBe,juUzLq?w-wzP-h
ERjjxlgJzPbHxf$ q,q,KCDCU fqBOQT
SV&amp;CW:xSVwZv'DG'NSPypDhKStKzC -$hslxIVzoivnp ,ethA:NCCGoi
tN!ljjP3fwJMwNelgUzzPGJlgihJ!d?q.d
pSPYgCuCJrIFtb
jQXg
pA.P LP,SPJi
DBcuBM:CixjJ$Jzkq,OLf3KLQLMGph$O 3DfiPHnXKuHMlyjxEiyZib3FaHV-oJa!zoc'XSP :CKGUhd?lgCOF$;;DTHZMlvvcmZAm;:iv'MMgO&amp;Ywbc;BLCUd&amp;vZINLIzkuTGZa
D.?</code></pre>
</div>
</div>
</section>
</section>
<section id="the-mathematical-trick-in-self-attention" class="level2">
<h2 class="anchored" data-anchor-id="the-mathematical-trick-in-self-attention">The mathematical trick in self-attention</h2>
<section id="toy-example-illustrating-how-matrix-multiplication-can-be-used-for-a-weighted-aggregation" class="level3">
<h3 class="anchored" data-anchor-id="toy-example-illustrating-how-matrix-multiplication-can-be-used-for-a-weighted-aggregation">toy example illustrating how matrix multiplication can be used for a “weighted aggregation”</h3>
<div id="f83f7b52" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb31-2"><a href="#cb31-2"></a>a <span class="op">=</span> torch.tril(torch.ones(<span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb31-3"><a href="#cb31-3"></a>a <span class="op">=</span> a <span class="op">/</span> torch.<span class="bu">sum</span>(a, <span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb31-4"><a href="#cb31-4"></a>b <span class="op">=</span> torch.randint(<span class="dv">0</span>,<span class="dv">10</span>,(<span class="dv">3</span>,<span class="dv">2</span>)).<span class="bu">float</span>()</span>
<span id="cb31-5"><a href="#cb31-5"></a>c <span class="op">=</span> a <span class="op">@</span> b</span>
<span id="cb31-6"><a href="#cb31-6"></a><span class="bu">print</span>(<span class="st">'a='</span>)</span>
<span id="cb31-7"><a href="#cb31-7"></a><span class="bu">print</span>(a)</span>
<span id="cb31-8"><a href="#cb31-8"></a><span class="bu">print</span>(<span class="st">'--'</span>)</span>
<span id="cb31-9"><a href="#cb31-9"></a><span class="bu">print</span>(<span class="st">'b='</span>)</span>
<span id="cb31-10"><a href="#cb31-10"></a><span class="bu">print</span>(b)</span>
<span id="cb31-11"><a href="#cb31-11"></a><span class="bu">print</span>(<span class="st">'--'</span>)</span>
<span id="cb31-12"><a href="#cb31-12"></a><span class="bu">print</span>(<span class="st">'c='</span>)</span>
<span id="cb31-13"><a href="#cb31-13"></a><span class="bu">print</span>(c)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>a=
tensor([[1.0000, 0.0000, 0.0000],
        [0.5000, 0.5000, 0.0000],
        [0.3333, 0.3333, 0.3333]])
--
b=
tensor([[2., 7.],
        [6., 4.],
        [6., 5.]])
--
c=
tensor([[2.0000, 7.0000],
        [4.0000, 5.5000],
        [4.6667, 5.3333]])</code></pre>
</div>
</div>
<div id="024db3e4" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a><span class="co"># consider the following toy example:</span></span>
<span id="cb33-2"><a href="#cb33-2"></a></span>
<span id="cb33-3"><a href="#cb33-3"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb33-4"><a href="#cb33-4"></a>B,T,C <span class="op">=</span> <span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">2</span> <span class="co"># batch, time, channels</span></span>
<span id="cb33-5"><a href="#cb33-5"></a>x <span class="op">=</span> torch.randn(B,T,C)</span>
<span id="cb33-6"><a href="#cb33-6"></a>x.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>torch.Size([4, 8, 2])</code></pre>
</div>
</div>
</section>
<section id="version-1-using-a-for-loop-to-compute-the-weighted-aggregation" class="level3">
<h3 class="anchored" data-anchor-id="version-1-using-a-for-loop-to-compute-the-weighted-aggregation">version 1: using a for loop to compute the weighted aggregation</h3>
<div id="2dc88b5c" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1"></a><span class="co"># We want x[b,t] = mean_{i&lt;=t} x[b,i]</span></span>
<span id="cb35-2"><a href="#cb35-2"></a>xbow <span class="op">=</span> torch.zeros((B,T,C))</span>
<span id="cb35-3"><a href="#cb35-3"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb35-4"><a href="#cb35-4"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb35-5"><a href="#cb35-5"></a>        xprev <span class="op">=</span> x[b,:t<span class="op">+</span><span class="dv">1</span>] <span class="co"># (t,C)</span></span>
<span id="cb35-6"><a href="#cb35-6"></a>        xbow[b,t] <span class="op">=</span> torch.mean(xprev, <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="version-2-using-matrix-multiply-for-a-weighted-aggregation" class="level3">
<h3 class="anchored" data-anchor-id="version-2-using-matrix-multiply-for-a-weighted-aggregation">version 2: using matrix multiply for a weighted aggregation</h3>
<div id="9ef722a4" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a>wei <span class="op">=</span> torch.tril(torch.ones(T, T))</span>
<span id="cb36-2"><a href="#cb36-2"></a>wei <span class="op">=</span> wei <span class="op">/</span> wei.<span class="bu">sum</span>(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb36-3"><a href="#cb36-3"></a>xbow2 <span class="op">=</span> wei <span class="op">@</span> x <span class="co"># (B, T, T) @ (B, T, C) ----&gt; (B, T, C)</span></span>
<span id="cb36-4"><a href="#cb36-4"></a>torch.allclose(xbow, xbow2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>True</code></pre>
</div>
</div>
</section>
<section id="version-3-use-softmax" class="level3">
<h3 class="anchored" data-anchor-id="version-3-use-softmax">version 3: use Softmax</h3>
<div id="a85f5fd4" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a>tril <span class="op">=</span> torch.tril(torch.ones(T, T))</span>
<span id="cb38-2"><a href="#cb38-2"></a>wei <span class="op">=</span> torch.zeros((T,T))</span>
<span id="cb38-3"><a href="#cb38-3"></a>wei <span class="op">=</span> wei.masked_fill(tril <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb38-4"><a href="#cb38-4"></a>wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb38-5"><a href="#cb38-5"></a>xbow3 <span class="op">=</span> wei <span class="op">@</span> x</span>
<span id="cb38-6"><a href="#cb38-6"></a>torch.allclose(xbow, xbow3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>True</code></pre>
</div>
</div>
<p><strong>softmax function</strong></p>
<p>softmax(<span class="math inline">z_i</span>) = <span class="math inline">\frac{e^{z_i}}{\sum_j e^{z_j}}</span></p>
</section>
</section>
<section id="version-4-self-attention" class="level1">
<h1>version 4: self-attention</h1>
<div id="285a1676" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb40-2"><a href="#cb40-2"></a>B,T,C <span class="op">=</span> <span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">32</span> <span class="co"># batch, time, channels</span></span>
<span id="cb40-3"><a href="#cb40-3"></a>x <span class="op">=</span> torch.randn(B,T,C)</span>
<span id="cb40-4"><a href="#cb40-4"></a></span>
<span id="cb40-5"><a href="#cb40-5"></a><span class="co"># let's see a single Head perform self-attention</span></span>
<span id="cb40-6"><a href="#cb40-6"></a>head_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb40-7"><a href="#cb40-7"></a>key <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb40-8"><a href="#cb40-8"></a>query <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb40-9"><a href="#cb40-9"></a>value <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb40-10"><a href="#cb40-10"></a>k <span class="op">=</span> key(x)   <span class="co"># (B, T, 16)</span></span>
<span id="cb40-11"><a href="#cb40-11"></a>q <span class="op">=</span> query(x) <span class="co"># (B, T, 16)</span></span>
<span id="cb40-12"><a href="#cb40-12"></a>wei <span class="op">=</span>  q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="co"># (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)</span></span>
<span id="cb40-13"><a href="#cb40-13"></a></span>
<span id="cb40-14"><a href="#cb40-14"></a>tril <span class="op">=</span> torch.tril(torch.ones(T, T))</span>
<span id="cb40-15"><a href="#cb40-15"></a><span class="co">#wei = torch.zeros((T,T))</span></span>
<span id="cb40-16"><a href="#cb40-16"></a>wei <span class="op">=</span> wei.masked_fill(tril <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb40-17"><a href="#cb40-17"></a>wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb40-18"><a href="#cb40-18"></a></span>
<span id="cb40-19"><a href="#cb40-19"></a>v <span class="op">=</span> value(x)</span>
<span id="cb40-20"><a href="#cb40-20"></a>out <span class="op">=</span> wei <span class="op">@</span> v</span>
<span id="cb40-21"><a href="#cb40-21"></a><span class="co">#out = wei @ x</span></span>
<span id="cb40-22"><a href="#cb40-22"></a></span>
<span id="cb40-23"><a href="#cb40-23"></a>out.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>torch.Size([4, 8, 16])</code></pre>
</div>
</div>
<div id="14d8afb4" class="cell" data-execution_count="25">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a>wei[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="25">
<pre><code>tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],
        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],
        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],
        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],
       grad_fn=&lt;SelectBackward0&gt;)</code></pre>
</div>
</div>
<p>Notes: - Attention is a <strong>communication mechanism</strong>. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights. - There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens. - Each example across batch dimension is of course processed completely independently and never “talk” to each other - In an “encoder” attention block just delete the single line that does masking with <code>tril</code>, allowing all tokens to communicate. This block here is called a “decoder” attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling. - “self-attention” just means that the keys and values are produced from the same source as queries. In “cross-attention”, the queries still get produced from x, but the keys and values come from some other, external source (e.g.&nbsp;an encoder module)</p>
<section id="why-scaled-attention" class="level3">
<h3 class="anchored" data-anchor-id="why-scaled-attention">why scaled attention?</h3>
<ul>
<li>“Scaled” attention additional divides <code>wei</code> by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below</li>
</ul>
<div id="d272ed2a" class="cell" data-execution_count="26">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1"></a>k <span class="op">=</span> torch.randn(B,T,head_size)</span>
<span id="cb44-2"><a href="#cb44-2"></a>q <span class="op">=</span> torch.randn(B,T,head_size)</span>
<span id="cb44-3"><a href="#cb44-3"></a>wei <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> head_size<span class="op">**-</span><span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="fbd850fd" class="cell" data-execution_count="27">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1"></a>k.var()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>tensor(1.0449)</code></pre>
</div>
</div>
<div id="e474f33f" class="cell" data-execution_count="28">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1"></a>q.var()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>tensor(1.0700)</code></pre>
</div>
</div>
<div id="49aa7d17" class="cell" data-execution_count="29">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1"></a>wei.var()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>tensor(1.0918)</code></pre>
</div>
</div>
<div id="28c82f93" class="cell" data-execution_count="30">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1"></a>torch.softmax(torch.tensor([<span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.5</span>]), dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])</code></pre>
</div>
</div>
<div id="ab5d4322" class="cell" data-execution_count="31">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1"></a>torch.softmax(torch.tensor([<span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.5</span>])<span class="op">*</span><span class="dv">8</span>, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># gets too peaky, converges to one-hot</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])</code></pre>
</div>
</div>
</section>
<section id="layernorm1d" class="level3">
<h3 class="anchored" data-anchor-id="layernorm1d">LayerNorm1d</h3>
<div id="faee1526" class="cell" data-execution_count="32">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1"></a><span class="kw">class</span> LayerNorm1d: <span class="co"># (used to be BatchNorm1d)</span></span>
<span id="cb55-2"><a href="#cb55-2"></a></span>
<span id="cb55-3"><a href="#cb55-3"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, eps<span class="op">=</span><span class="fl">1e-5</span>, momentum<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb55-4"><a href="#cb55-4"></a>    <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb55-5"><a href="#cb55-5"></a>    <span class="va">self</span>.gamma <span class="op">=</span> torch.ones(dim)</span>
<span id="cb55-6"><a href="#cb55-6"></a>    <span class="va">self</span>.beta <span class="op">=</span> torch.zeros(dim)</span>
<span id="cb55-7"><a href="#cb55-7"></a></span>
<span id="cb55-8"><a href="#cb55-8"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb55-9"><a href="#cb55-9"></a>    <span class="co"># calculate the forward pass</span></span>
<span id="cb55-10"><a href="#cb55-10"></a>    xmean <span class="op">=</span> x.mean(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch mean</span></span>
<span id="cb55-11"><a href="#cb55-11"></a>    xvar <span class="op">=</span> x.var(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch variance</span></span>
<span id="cb55-12"><a href="#cb55-12"></a>    xhat <span class="op">=</span> (x <span class="op">-</span> xmean) <span class="op">/</span> torch.sqrt(xvar <span class="op">+</span> <span class="va">self</span>.eps) <span class="co"># normalize to unit variance</span></span>
<span id="cb55-13"><a href="#cb55-13"></a>    <span class="va">self</span>.out <span class="op">=</span> <span class="va">self</span>.gamma <span class="op">*</span> xhat <span class="op">+</span> <span class="va">self</span>.beta</span>
<span id="cb55-14"><a href="#cb55-14"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb55-15"><a href="#cb55-15"></a></span>
<span id="cb55-16"><a href="#cb55-16"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb55-17"><a href="#cb55-17"></a>    <span class="cf">return</span> [<span class="va">self</span>.gamma, <span class="va">self</span>.beta]</span>
<span id="cb55-18"><a href="#cb55-18"></a></span>
<span id="cb55-19"><a href="#cb55-19"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb55-20"><a href="#cb55-20"></a>module <span class="op">=</span> LayerNorm1d(<span class="dv">100</span>)</span>
<span id="cb55-21"><a href="#cb55-21"></a>x <span class="op">=</span> torch.randn(<span class="dv">32</span>, <span class="dv">100</span>) <span class="co"># batch size 32 of 100-dimensional vectors</span></span>
<span id="cb55-22"><a href="#cb55-22"></a>x <span class="op">=</span> module(x)</span>
<span id="cb55-23"><a href="#cb55-23"></a>x.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>torch.Size([32, 100])</code></pre>
</div>
</div>
<div id="8ac37706" class="cell" data-execution_count="33">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1"></a>x[:,<span class="dv">0</span>].mean(), x[:,<span class="dv">0</span>].std() <span class="co"># mean,std of one feature across all batch inputs</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>(tensor(0.1469), tensor(0.8803))</code></pre>
</div>
</div>
<div id="733db5bd" class="cell" data-execution_count="34">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1"></a>x[<span class="dv">0</span>,:].mean(), x[<span class="dv">0</span>,:].std() <span class="co"># mean,std of a single input from the batch, of its features</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>(tensor(2.3842e-09), tensor(1.0000))</code></pre>
</div>
</div>
<div id="e4f32cc1" class="cell" data-execution_count="35">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1"></a><span class="co"># French to English translation example:</span></span>
<span id="cb61-2"><a href="#cb61-2"></a></span>
<span id="cb61-3"><a href="#cb61-3"></a><span class="co"># &lt;--------- ENCODE ------------------&gt;&lt;--------------- DECODE -----------------&gt;</span></span>
<span id="cb61-4"><a href="#cb61-4"></a><span class="co"># les réseaux de neurones sont géniaux! &lt;START&gt; neural networks are awesome!&lt;</span><span class="re">END</span><span class="co">&gt;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="full-finished-code-for-reference" class="level3">
<h3 class="anchored" data-anchor-id="full-finished-code-for-reference">Full finished code, for reference</h3>
<p>You may want to refer directly to the git repo instead though.</p>
<div id="7f62a059" class="cell" data-execution_count="36">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1"></a><span class="im">import</span> torch</span>
<span id="cb62-2"><a href="#cb62-2"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb62-3"><a href="#cb62-3"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb62-4"><a href="#cb62-4"></a></span>
<span id="cb62-5"><a href="#cb62-5"></a><span class="co"># hyperparameters</span></span>
<span id="cb62-6"><a href="#cb62-6"></a>batch_size <span class="op">=</span> <span class="dv">16</span> <span class="co"># how many independent sequences will we process in parallel?</span></span>
<span id="cb62-7"><a href="#cb62-7"></a>block_size <span class="op">=</span> <span class="dv">32</span> <span class="co"># what is the maximum context length for predictions?</span></span>
<span id="cb62-8"><a href="#cb62-8"></a>max_iters <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb62-9"><a href="#cb62-9"></a>eval_interval <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb62-10"><a href="#cb62-10"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb62-11"><a href="#cb62-11"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb62-12"><a href="#cb62-12"></a>eval_iters <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb62-13"><a href="#cb62-13"></a>n_embd <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb62-14"><a href="#cb62-14"></a>n_head <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb62-15"><a href="#cb62-15"></a>n_layer <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb62-16"><a href="#cb62-16"></a>dropout <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb62-17"><a href="#cb62-17"></a><span class="co"># ------------</span></span>
<span id="cb62-18"><a href="#cb62-18"></a></span>
<span id="cb62-19"><a href="#cb62-19"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb62-20"><a href="#cb62-20"></a></span>
<span id="cb62-21"><a href="#cb62-21"></a><span class="co"># wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinys Shakespeare/input.txt</span></span>
<span id="cb62-22"><a href="#cb62-22"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'input.txt'</span>, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> f:</span>
<span id="cb62-23"><a href="#cb62-23"></a>    text <span class="op">=</span> f.read()</span>
<span id="cb62-24"><a href="#cb62-24"></a></span>
<span id="cb62-25"><a href="#cb62-25"></a><span class="co"># here are all the unique characters that occur in this text</span></span>
<span id="cb62-26"><a href="#cb62-26"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(text)))</span>
<span id="cb62-27"><a href="#cb62-27"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(chars)</span>
<span id="cb62-28"><a href="#cb62-28"></a><span class="co"># create a mapping from characters to integers</span></span>
<span id="cb62-29"><a href="#cb62-29"></a>stoi <span class="op">=</span> { ch:i <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }</span>
<span id="cb62-30"><a href="#cb62-30"></a>itos <span class="op">=</span> { i:ch <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }</span>
<span id="cb62-31"><a href="#cb62-31"></a>encode <span class="op">=</span> <span class="kw">lambda</span> s: [stoi[c] <span class="cf">for</span> c <span class="kw">in</span> s] <span class="co"># encoder: take a string, output a list of integers</span></span>
<span id="cb62-32"><a href="#cb62-32"></a>decode <span class="op">=</span> <span class="kw">lambda</span> l: <span class="st">''</span>.join([itos[i] <span class="cf">for</span> i <span class="kw">in</span> l]) <span class="co"># decoder: take a list of integers, output a string</span></span>
<span id="cb62-33"><a href="#cb62-33"></a></span>
<span id="cb62-34"><a href="#cb62-34"></a><span class="co"># Train and test splits</span></span>
<span id="cb62-35"><a href="#cb62-35"></a>data <span class="op">=</span> torch.tensor(encode(text), dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb62-36"><a href="#cb62-36"></a>n <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span><span class="op">*</span><span class="bu">len</span>(data)) <span class="co"># first 90% will be train, rest val</span></span>
<span id="cb62-37"><a href="#cb62-37"></a>train_data <span class="op">=</span> data[:n]</span>
<span id="cb62-38"><a href="#cb62-38"></a>val_data <span class="op">=</span> data[n:]</span>
<span id="cb62-39"><a href="#cb62-39"></a></span>
<span id="cb62-40"><a href="#cb62-40"></a><span class="co"># data loading</span></span>
<span id="cb62-41"><a href="#cb62-41"></a><span class="kw">def</span> get_batch(split):</span>
<span id="cb62-42"><a href="#cb62-42"></a>    <span class="co"># generate a small batch of data of inputs x and targets y</span></span>
<span id="cb62-43"><a href="#cb62-43"></a>    data <span class="op">=</span> train_data <span class="cf">if</span> split <span class="op">==</span> <span class="st">'train'</span> <span class="cf">else</span> val_data</span>
<span id="cb62-44"><a href="#cb62-44"></a>    ix <span class="op">=</span> torch.randint(<span class="bu">len</span>(data) <span class="op">-</span> block_size, (batch_size,))</span>
<span id="cb62-45"><a href="#cb62-45"></a>    x <span class="op">=</span> torch.stack([data[i:i<span class="op">+</span>block_size] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb62-46"><a href="#cb62-46"></a>    y <span class="op">=</span> torch.stack([data[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span>block_size<span class="op">+</span><span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb62-47"><a href="#cb62-47"></a>    x, y <span class="op">=</span> x.to(device), y.to(device)</span>
<span id="cb62-48"><a href="#cb62-48"></a>    <span class="cf">return</span> x, y</span>
<span id="cb62-49"><a href="#cb62-49"></a></span>
<span id="cb62-50"><a href="#cb62-50"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb62-51"><a href="#cb62-51"></a><span class="kw">def</span> estimate_loss():</span>
<span id="cb62-52"><a href="#cb62-52"></a>    out <span class="op">=</span> {}</span>
<span id="cb62-53"><a href="#cb62-53"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb62-54"><a href="#cb62-54"></a>    <span class="cf">for</span> split <span class="kw">in</span> [<span class="st">'train'</span>, <span class="st">'val'</span>]:</span>
<span id="cb62-55"><a href="#cb62-55"></a>        losses <span class="op">=</span> torch.zeros(eval_iters)</span>
<span id="cb62-56"><a href="#cb62-56"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(eval_iters):</span>
<span id="cb62-57"><a href="#cb62-57"></a>            X, Y <span class="op">=</span> get_batch(split)</span>
<span id="cb62-58"><a href="#cb62-58"></a>            logits, loss <span class="op">=</span> model(X, Y)</span>
<span id="cb62-59"><a href="#cb62-59"></a>            losses[k] <span class="op">=</span> loss.item()</span>
<span id="cb62-60"><a href="#cb62-60"></a>        out[split] <span class="op">=</span> losses.mean()</span>
<span id="cb62-61"><a href="#cb62-61"></a>    model.train()</span>
<span id="cb62-62"><a href="#cb62-62"></a>    <span class="cf">return</span> out</span>
<span id="cb62-63"><a href="#cb62-63"></a></span>
<span id="cb62-64"><a href="#cb62-64"></a><span class="kw">class</span> Head(nn.Module):</span>
<span id="cb62-65"><a href="#cb62-65"></a>    <span class="co">""" one head of self-attention """</span></span>
<span id="cb62-66"><a href="#cb62-66"></a></span>
<span id="cb62-67"><a href="#cb62-67"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, head_size):</span>
<span id="cb62-68"><a href="#cb62-68"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb62-69"><a href="#cb62-69"></a>        <span class="va">self</span>.key <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb62-70"><a href="#cb62-70"></a>        <span class="va">self</span>.query <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb62-71"><a href="#cb62-71"></a>        <span class="va">self</span>.value <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb62-72"><a href="#cb62-72"></a>        <span class="va">self</span>.register_buffer(<span class="st">'tril'</span>, torch.tril(torch.ones(block_size, block_size)))</span>
<span id="cb62-73"><a href="#cb62-73"></a></span>
<span id="cb62-74"><a href="#cb62-74"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb62-75"><a href="#cb62-75"></a></span>
<span id="cb62-76"><a href="#cb62-76"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb62-77"><a href="#cb62-77"></a>        B,T,C <span class="op">=</span> x.shape</span>
<span id="cb62-78"><a href="#cb62-78"></a>        k <span class="op">=</span> <span class="va">self</span>.key(x)   <span class="co"># (B,T,C)</span></span>
<span id="cb62-79"><a href="#cb62-79"></a>        q <span class="op">=</span> <span class="va">self</span>.query(x) <span class="co"># (B,T,C)</span></span>
<span id="cb62-80"><a href="#cb62-80"></a>        <span class="co"># compute attention scores ("affinities")</span></span>
<span id="cb62-81"><a href="#cb62-81"></a>        wei <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> C<span class="op">**-</span><span class="fl">0.5</span> <span class="co"># (B, T, C) @ (B, C, T) -&gt; (B, T, T)</span></span>
<span id="cb62-82"><a href="#cb62-82"></a>        wei <span class="op">=</span> wei.masked_fill(<span class="va">self</span>.tril[:T, :T] <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>)) <span class="co"># (B, T, T)</span></span>
<span id="cb62-83"><a href="#cb62-83"></a>        wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># (B, T, T)</span></span>
<span id="cb62-84"><a href="#cb62-84"></a>        wei <span class="op">=</span> <span class="va">self</span>.dropout(wei)</span>
<span id="cb62-85"><a href="#cb62-85"></a>        <span class="co"># perform the weighted aggregation of the values</span></span>
<span id="cb62-86"><a href="#cb62-86"></a>        v <span class="op">=</span> <span class="va">self</span>.value(x) <span class="co"># (B,T,C)</span></span>
<span id="cb62-87"><a href="#cb62-87"></a>        out <span class="op">=</span> wei <span class="op">@</span> v <span class="co"># (B, T, T) @ (B, T, C) -&gt; (B, T, C)</span></span>
<span id="cb62-88"><a href="#cb62-88"></a>        <span class="cf">return</span> out</span>
<span id="cb62-89"><a href="#cb62-89"></a></span>
<span id="cb62-90"><a href="#cb62-90"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb62-91"><a href="#cb62-91"></a>    <span class="co">""" multiple heads of self-attention in parallel """</span></span>
<span id="cb62-92"><a href="#cb62-92"></a></span>
<span id="cb62-93"><a href="#cb62-93"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_heads, head_size):</span>
<span id="cb62-94"><a href="#cb62-94"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb62-95"><a href="#cb62-95"></a>        <span class="va">self</span>.heads <span class="op">=</span> nn.ModuleList([Head(head_size) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_heads)])</span>
<span id="cb62-96"><a href="#cb62-96"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Linear(n_embd, n_embd)</span>
<span id="cb62-97"><a href="#cb62-97"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb62-98"><a href="#cb62-98"></a></span>
<span id="cb62-99"><a href="#cb62-99"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb62-100"><a href="#cb62-100"></a>        out <span class="op">=</span> torch.cat([h(x) <span class="cf">for</span> h <span class="kw">in</span> <span class="va">self</span>.heads], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb62-101"><a href="#cb62-101"></a>        out <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.proj(out))</span>
<span id="cb62-102"><a href="#cb62-102"></a>        <span class="cf">return</span> out</span>
<span id="cb62-103"><a href="#cb62-103"></a></span>
<span id="cb62-104"><a href="#cb62-104"></a><span class="kw">class</span> FeedFoward(nn.Module):</span>
<span id="cb62-105"><a href="#cb62-105"></a>    <span class="co">""" a simple linear layer followed by a non-linearity """</span></span>
<span id="cb62-106"><a href="#cb62-106"></a></span>
<span id="cb62-107"><a href="#cb62-107"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd):</span>
<span id="cb62-108"><a href="#cb62-108"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb62-109"><a href="#cb62-109"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb62-110"><a href="#cb62-110"></a>            nn.Linear(n_embd, <span class="dv">4</span> <span class="op">*</span> n_embd),</span>
<span id="cb62-111"><a href="#cb62-111"></a>            nn.ReLU(),</span>
<span id="cb62-112"><a href="#cb62-112"></a>            nn.Linear(<span class="dv">4</span> <span class="op">*</span> n_embd, n_embd),</span>
<span id="cb62-113"><a href="#cb62-113"></a>            nn.Dropout(dropout),</span>
<span id="cb62-114"><a href="#cb62-114"></a>        )</span>
<span id="cb62-115"><a href="#cb62-115"></a></span>
<span id="cb62-116"><a href="#cb62-116"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb62-117"><a href="#cb62-117"></a>        <span class="cf">return</span> <span class="va">self</span>.net(x)</span>
<span id="cb62-118"><a href="#cb62-118"></a></span>
<span id="cb62-119"><a href="#cb62-119"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb62-120"><a href="#cb62-120"></a>    <span class="co">""" Transformer block: communication followed by computation """</span></span>
<span id="cb62-121"><a href="#cb62-121"></a></span>
<span id="cb62-122"><a href="#cb62-122"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd, n_head):</span>
<span id="cb62-123"><a href="#cb62-123"></a>        <span class="co"># n_embd: embedding dimension, n_head: the number of heads we'd like</span></span>
<span id="cb62-124"><a href="#cb62-124"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb62-125"><a href="#cb62-125"></a>        head_size <span class="op">=</span> n_embd <span class="op">//</span> n_head</span>
<span id="cb62-126"><a href="#cb62-126"></a>        <span class="va">self</span>.sa <span class="op">=</span> MultiHeadAttention(n_head, head_size)</span>
<span id="cb62-127"><a href="#cb62-127"></a>        <span class="va">self</span>.ffwd <span class="op">=</span> FeedFoward(n_embd)</span>
<span id="cb62-128"><a href="#cb62-128"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> nn.LayerNorm(n_embd)</span>
<span id="cb62-129"><a href="#cb62-129"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> nn.LayerNorm(n_embd)</span>
<span id="cb62-130"><a href="#cb62-130"></a></span>
<span id="cb62-131"><a href="#cb62-131"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb62-132"><a href="#cb62-132"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.sa(<span class="va">self</span>.ln1(x))</span>
<span id="cb62-133"><a href="#cb62-133"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffwd(<span class="va">self</span>.ln2(x))</span>
<span id="cb62-134"><a href="#cb62-134"></a>        <span class="cf">return</span> x</span>
<span id="cb62-135"><a href="#cb62-135"></a></span>
<span id="cb62-136"><a href="#cb62-136"></a><span class="co"># super simple bigram model</span></span>
<span id="cb62-137"><a href="#cb62-137"></a><span class="kw">class</span> BigramLanguageModel(nn.Module):</span>
<span id="cb62-138"><a href="#cb62-138"></a></span>
<span id="cb62-139"><a href="#cb62-139"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb62-140"><a href="#cb62-140"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb62-141"><a href="#cb62-141"></a>        <span class="co"># each token directly reads off the logits for the next token from a lookup table</span></span>
<span id="cb62-142"><a href="#cb62-142"></a>        <span class="va">self</span>.token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, n_embd)</span>
<span id="cb62-143"><a href="#cb62-143"></a>        <span class="va">self</span>.position_embedding_table <span class="op">=</span> nn.Embedding(block_size, n_embd)</span>
<span id="cb62-144"><a href="#cb62-144"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.Sequential(<span class="op">*</span>[Block(n_embd, n_head<span class="op">=</span>n_head) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_layer)])</span>
<span id="cb62-145"><a href="#cb62-145"></a>        <span class="va">self</span>.ln_f <span class="op">=</span> nn.LayerNorm(n_embd) <span class="co"># final layer norm</span></span>
<span id="cb62-146"><a href="#cb62-146"></a>        <span class="va">self</span>.lm_head <span class="op">=</span> nn.Linear(n_embd, vocab_size)</span>
<span id="cb62-147"><a href="#cb62-147"></a></span>
<span id="cb62-148"><a href="#cb62-148"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb62-149"><a href="#cb62-149"></a>        B, T <span class="op">=</span> idx.shape</span>
<span id="cb62-150"><a href="#cb62-150"></a></span>
<span id="cb62-151"><a href="#cb62-151"></a>        <span class="co"># idx and targets are both (B,T) tensor of integers</span></span>
<span id="cb62-152"><a href="#cb62-152"></a>        tok_emb <span class="op">=</span> <span class="va">self</span>.token_embedding_table(idx) <span class="co"># (B,T,C)</span></span>
<span id="cb62-153"><a href="#cb62-153"></a>        pos_emb <span class="op">=</span> <span class="va">self</span>.position_embedding_table(torch.arange(T, device<span class="op">=</span>device)) <span class="co"># (T,C)</span></span>
<span id="cb62-154"><a href="#cb62-154"></a>        x <span class="op">=</span> tok_emb <span class="op">+</span> pos_emb <span class="co"># (B,T,C)</span></span>
<span id="cb62-155"><a href="#cb62-155"></a>        x <span class="op">=</span> <span class="va">self</span>.blocks(x) <span class="co"># (B,T,C)</span></span>
<span id="cb62-156"><a href="#cb62-156"></a>        x <span class="op">=</span> <span class="va">self</span>.ln_f(x) <span class="co"># (B,T,C)</span></span>
<span id="cb62-157"><a href="#cb62-157"></a>        logits <span class="op">=</span> <span class="va">self</span>.lm_head(x) <span class="co"># (B,T,vocab_size)</span></span>
<span id="cb62-158"><a href="#cb62-158"></a></span>
<span id="cb62-159"><a href="#cb62-159"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb62-160"><a href="#cb62-160"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb62-161"><a href="#cb62-161"></a>        <span class="cf">else</span>:</span>
<span id="cb62-162"><a href="#cb62-162"></a>            B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb62-163"><a href="#cb62-163"></a>            logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C)</span>
<span id="cb62-164"><a href="#cb62-164"></a>            targets <span class="op">=</span> targets.view(B<span class="op">*</span>T)</span>
<span id="cb62-165"><a href="#cb62-165"></a>            loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb62-166"><a href="#cb62-166"></a></span>
<span id="cb62-167"><a href="#cb62-167"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb62-168"><a href="#cb62-168"></a></span>
<span id="cb62-169"><a href="#cb62-169"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens):</span>
<span id="cb62-170"><a href="#cb62-170"></a>        <span class="co"># idx is (B, T) array of indices in the current context</span></span>
<span id="cb62-171"><a href="#cb62-171"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb62-172"><a href="#cb62-172"></a>            <span class="co"># crop idx to the last block_size tokens</span></span>
<span id="cb62-173"><a href="#cb62-173"></a>            idx_cond <span class="op">=</span> idx[:, <span class="op">-</span>block_size:]</span>
<span id="cb62-174"><a href="#cb62-174"></a>            <span class="co"># get the predictions</span></span>
<span id="cb62-175"><a href="#cb62-175"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>(idx_cond)</span>
<span id="cb62-176"><a href="#cb62-176"></a>            <span class="co"># focus only on the last time step</span></span>
<span id="cb62-177"><a href="#cb62-177"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="co"># becomes (B, C)</span></span>
<span id="cb62-178"><a href="#cb62-178"></a>            <span class="co"># apply softmax to get probabilities</span></span>
<span id="cb62-179"><a href="#cb62-179"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># (B, C)</span></span>
<span id="cb62-180"><a href="#cb62-180"></a>            <span class="co"># sample from the distribution</span></span>
<span id="cb62-181"><a href="#cb62-181"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, 1)</span></span>
<span id="cb62-182"><a href="#cb62-182"></a>            <span class="co"># append sampled index to the running sequence</span></span>
<span id="cb62-183"><a href="#cb62-183"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, T+1)</span></span>
<span id="cb62-184"><a href="#cb62-184"></a>        <span class="cf">return</span> idx</span>
<span id="cb62-185"><a href="#cb62-185"></a></span>
<span id="cb62-186"><a href="#cb62-186"></a>model <span class="op">=</span> BigramLanguageModel()</span>
<span id="cb62-187"><a href="#cb62-187"></a>m <span class="op">=</span> model.to(device)</span>
<span id="cb62-188"><a href="#cb62-188"></a><span class="co"># print the number of parameters in the model</span></span>
<span id="cb62-189"><a href="#cb62-189"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> m.parameters())<span class="op">/</span><span class="fl">1e6</span>, <span class="st">'M parameters'</span>)</span>
<span id="cb62-190"><a href="#cb62-190"></a></span>
<span id="cb62-191"><a href="#cb62-191"></a><span class="co"># create a PyTorch optimizer</span></span>
<span id="cb62-192"><a href="#cb62-192"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb62-193"><a href="#cb62-193"></a></span>
<span id="cb62-194"><a href="#cb62-194"></a><span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(max_iters):</span>
<span id="cb62-195"><a href="#cb62-195"></a></span>
<span id="cb62-196"><a href="#cb62-196"></a>    <span class="co"># every once in a while evaluate the loss on train and val sets</span></span>
<span id="cb62-197"><a href="#cb62-197"></a>    <span class="cf">if</span> <span class="bu">iter</span> <span class="op">%</span> eval_interval <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> <span class="bu">iter</span> <span class="op">==</span> max_iters <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb62-198"><a href="#cb62-198"></a>        losses <span class="op">=</span> estimate_loss()</span>
<span id="cb62-199"><a href="#cb62-199"></a>        <span class="bu">print</span>(<span class="ss">f"step </span><span class="sc">{</span><span class="bu">iter</span><span class="sc">}</span><span class="ss">: train loss </span><span class="sc">{</span>losses[<span class="st">'train'</span>]<span class="sc">:.4f}</span><span class="ss">, val loss </span><span class="sc">{</span>losses[<span class="st">'val'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb62-200"><a href="#cb62-200"></a></span>
<span id="cb62-201"><a href="#cb62-201"></a>    <span class="co"># sample a batch of data</span></span>
<span id="cb62-202"><a href="#cb62-202"></a>    xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb62-203"><a href="#cb62-203"></a></span>
<span id="cb62-204"><a href="#cb62-204"></a>    <span class="co"># evaluate the loss</span></span>
<span id="cb62-205"><a href="#cb62-205"></a>    logits, loss <span class="op">=</span> model(xb, yb)</span>
<span id="cb62-206"><a href="#cb62-206"></a>    optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb62-207"><a href="#cb62-207"></a>    loss.backward()</span>
<span id="cb62-208"><a href="#cb62-208"></a>    optimizer.step()</span>
<span id="cb62-209"><a href="#cb62-209"></a></span>
<span id="cb62-210"><a href="#cb62-210"></a><span class="co"># generate from the model</span></span>
<span id="cb62-211"><a href="#cb62-211"></a>context <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>, device<span class="op">=</span>device)</span>
<span id="cb62-212"><a href="#cb62-212"></a><span class="bu">print</span>(decode(m.generate(context, max_new_tokens<span class="op">=</span><span class="dv">2000</span>)[<span class="dv">0</span>].tolist()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>0.209729 M parameters
step 0: train loss 4.4116, val loss 4.4022
step 100: train loss 2.6568, val loss 2.6670
step 200: train loss 2.5090, val loss 2.5059
step 300: train loss 2.4195, val loss 2.4336
step 400: train loss 2.3502, val loss 2.3566
step 500: train loss 2.2964, val loss 2.3128
step 600: train loss 2.2410, val loss 2.2499
step 700: train loss 2.2052, val loss 2.2190
step 800: train loss 2.1640, val loss 2.1872
step 900: train loss 2.1243, val loss 2.1506
step 1000: train loss 2.1025, val loss 2.1295
step 1100: train loss 2.0690, val loss 2.1180
step 1200: train loss 2.0370, val loss 2.0782
step 1300: train loss 2.0249, val loss 2.0641
step 1400: train loss 1.9917, val loss 2.0352
step 1500: train loss 1.9698, val loss 2.0305
step 1600: train loss 1.9644, val loss 2.0495
step 1700: train loss 1.9404, val loss 2.0141
step 1800: train loss 1.9093, val loss 1.9967
step 1900: train loss 1.9063, val loss 1.9853
step 2000: train loss 1.8859, val loss 1.9967
step 2100: train loss 1.8710, val loss 1.9753
step 2200: train loss 1.8600, val loss 1.9630
step 2300: train loss 1.8567, val loss 1.9561
step 2400: train loss 1.8424, val loss 1.9435
step 2500: train loss 1.8141, val loss 1.9422
step 2600: train loss 1.8240, val loss 1.9372
step 2700: train loss 1.8140, val loss 1.9382
step 2800: train loss 1.8061, val loss 1.9217
step 2900: train loss 1.8034, val loss 1.9297
step 3000: train loss 1.7949, val loss 1.9211
step 3100: train loss 1.7716, val loss 1.9195
step 3200: train loss 1.7515, val loss 1.9071
step 3300: train loss 1.7574, val loss 1.9046
step 3400: train loss 1.7588, val loss 1.8995
step 3500: train loss 1.7364, val loss 1.8957
step 3600: train loss 1.7274, val loss 1.8867
step 3700: train loss 1.7284, val loss 1.8823
step 3800: train loss 1.7184, val loss 1.8895
step 3900: train loss 1.7212, val loss 1.8728
step 4000: train loss 1.7147, val loss 1.8587
step 4100: train loss 1.7112, val loss 1.8750
step 4200: train loss 1.7051, val loss 1.8598
step 4300: train loss 1.6994, val loss 1.8420
step 4400: train loss 1.7088, val loss 1.8676
step 4500: train loss 1.6871, val loss 1.8471
step 4600: train loss 1.6870, val loss 1.8330
step 4700: train loss 1.6842, val loss 1.8427
step 4800: train loss 1.6650, val loss 1.8424
step 4900: train loss 1.6709, val loss 1.8404
step 4999: train loss 1.6634, val loss 1.8222

Flie?

WARWICK:
Our his you bewiter are a toom hemen breabet the TI see it
What satell in rowers clanme; artand, those it;
Gives time.

ARCHILIV:
Give sear that, wille the is news it cy as explace and serves no cause's
ey let herefords in a blood sean up sommity.

POLIXENE:
The son:
Thel as use to, set these I I vourt,-afor this proak and the rien allate a noncead,
Anger in may Crembreak in that's beant,
Roman,; I 'till! Poyant of ouchber
To
This nog must he anour,
As coutinstage: but but the beamied,
And for may lie them yet nob the boy not.

DUKE IV:
To state the be, but and too her promenator him,
But much some not make thee, for you
his stage was babsom! and now your and
towing-sweet you,
Go coursein to state of the stender of Tyberroke all gentled to plartes no adds in en shails.

PAULINGS:
With to merselfry manners for the ploceiness and wiltwien,
And adve the now my ageingrale mine!
Or he him my spaut, I somut.
You as where'shefore, I githed stide to seinged forbying me begom as goody.
Go and neer thy from me pronme-his here.

ROMEO:
Cain his you will! I but
but these it were is turnstry to all gok
shouls' boties. joy to you would dost
From Buckry to thy will;
But in an in himself I addstleb
That that gentrante well.
O, hither, is the dalts it soon to thee,
you knows wheep his seeps and in togs,
Here thou make of commets, neyes;
Whereigh my sincrees; and, yet, huse party of us Voly.

Setily;
I wear his crumpust set, that would Edwift a those as old
on his good from wit-be in his in
Harth wie the chanter: me the ortiss
And yearges us coman. Your were you, where 'twon enmer, 'word.

POMBY:
Thou king azmouTher:
What flear you must mine ry set gone,
Or sappure, contessinous to have their they as I
have heil and scorfole wefpareter the by it.

First I knounger-move,
Our wout our as of formselfess.

BUCHINTI:
As one thumsely, we it
Share dayneds flase thout?
At you, and weing as move, it by hurself is on the for the not,
Andmner, I Leturn, Warmine and you,
That wha</code></pre>
</div>
</div>


<!-- -->

</section>
</section>

<p>© <a href="https://hakyimlab.org">HakyImLab and Listed Authors</a> - <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 License</a></p></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb64" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb64-1"><a href="#cb64-1"></a><span class="co">---</span></span>
<span id="cb64-2"><a href="#cb64-2"></a><span class="an">title:</span><span class="co"> Building a GPT - companion notebook qmd</span></span>
<span id="cb64-3"><a href="#cb64-3"></a><span class="an">author:</span><span class="co"> Andrey Karpathy</span></span>
<span id="cb64-4"><a href="#cb64-4"></a><span class="an">date:</span><span class="co"> '2025-04-11'</span></span>
<span id="cb64-5"><a href="#cb64-5"></a><span class="an">freeze:</span><span class="co"> true</span></span>
<span id="cb64-6"><a href="#cb64-6"></a><span class="an">jupyter:</span><span class="co"> </span></span>
<span id="cb64-7"><a href="#cb64-7"></a><span class="co">  kernelspec:</span></span>
<span id="cb64-8"><a href="#cb64-8"></a><span class="co">    name: "conda-env-gene46100-py"</span></span>
<span id="cb64-9"><a href="#cb64-9"></a><span class="co">    language: "python"</span></span>
<span id="cb64-10"><a href="#cb64-10"></a><span class="co">    display_name: "gene46100"</span></span>
<span id="cb64-11"><a href="#cb64-11"></a><span class="an">format:</span></span>
<span id="cb64-12"><a href="#cb64-12"></a><span class="co">  html:</span></span>
<span id="cb64-13"><a href="#cb64-13"></a><span class="co">    code-fold: true</span></span>
<span id="cb64-14"><a href="#cb64-14"></a><span class="co">    code-line-numbers: true</span></span>
<span id="cb64-15"><a href="#cb64-15"></a><span class="co">    code-tools: true</span></span>
<span id="cb64-16"><a href="#cb64-16"></a><span class="co">    code-wrap: true</span></span>
<span id="cb64-17"><a href="#cb64-17"></a><span class="co">---</span></span>
<span id="cb64-18"><a href="#cb64-18"></a></span>
<span id="cb64-19"><a href="#cb64-19"></a><span class="fu">## Building a GPT</span></span>
<span id="cb64-20"><a href="#cb64-20"></a></span>
<span id="cb64-21"><a href="#cb64-21"></a>Companion notebook to the <span class="co">[</span><span class="ot">Zero To Hero</span><span class="co">](https://karpathy.ai/zero-to-hero.html)</span> video on GPT. Downloaded from <span class="co">[</span><span class="ot">here</span><span class="co">](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing)</span></span>
<span id="cb64-22"><a href="#cb64-22"></a></span>
<span id="cb64-23"><a href="#cb64-23"></a>(https://github.com/karpathy/nanoGPT)</span>
<span id="cb64-24"><a href="#cb64-24"></a></span>
<span id="cb64-25"><a href="#cb64-25"></a><span class="fu">### download the tiny shakespeare dataset</span></span>
<span id="cb64-28"><a href="#cb64-28"></a><span class="in">```{python}</span></span>
<span id="cb64-29"><a href="#cb64-29"></a><span class="co"># We always start with a dataset to train on. Let's download the tiny shakespeare dataset</span></span>
<span id="cb64-30"><a href="#cb64-30"></a><span class="co">#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt</span></span>
<span id="cb64-31"><a href="#cb64-31"></a><span class="in">```</span></span>
<span id="cb64-32"><a href="#cb64-32"></a></span>
<span id="cb64-35"><a href="#cb64-35"></a><span class="in">```{python}</span></span>
<span id="cb64-36"><a href="#cb64-36"></a><span class="co"># read it in to inspect it</span></span>
<span id="cb64-37"><a href="#cb64-37"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'input.txt'</span>, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> f:</span>
<span id="cb64-38"><a href="#cb64-38"></a>    text <span class="op">=</span> f.read()</span>
<span id="cb64-39"><a href="#cb64-39"></a><span class="in">```</span></span>
<span id="cb64-40"><a href="#cb64-40"></a></span>
<span id="cb64-43"><a href="#cb64-43"></a><span class="in">```{python}</span></span>
<span id="cb64-44"><a href="#cb64-44"></a><span class="co"># print the length of the dataset</span></span>
<span id="cb64-45"><a href="#cb64-45"></a><span class="bu">print</span>(<span class="st">"length of dataset in characters: "</span>, <span class="bu">len</span>(text))</span>
<span id="cb64-46"><a href="#cb64-46"></a><span class="in">```</span></span>
<span id="cb64-47"><a href="#cb64-47"></a></span>
<span id="cb64-50"><a href="#cb64-50"></a><span class="in">```{python}</span></span>
<span id="cb64-51"><a href="#cb64-51"></a><span class="co"># let's look at the first 1000 characters</span></span>
<span id="cb64-52"><a href="#cb64-52"></a><span class="bu">print</span>(text[:<span class="dv">1000</span>])</span>
<span id="cb64-53"><a href="#cb64-53"></a><span class="in">```</span></span>
<span id="cb64-54"><a href="#cb64-54"></a></span>
<span id="cb64-57"><a href="#cb64-57"></a><span class="in">```{python}</span></span>
<span id="cb64-58"><a href="#cb64-58"></a><span class="co"># here are all the unique characters that occur in this text</span></span>
<span id="cb64-59"><a href="#cb64-59"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(text)))</span>
<span id="cb64-60"><a href="#cb64-60"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(chars)</span>
<span id="cb64-61"><a href="#cb64-61"></a><span class="bu">print</span>(<span class="st">''</span>.join(chars))</span>
<span id="cb64-62"><a href="#cb64-62"></a><span class="bu">print</span>(vocab_size)</span>
<span id="cb64-63"><a href="#cb64-63"></a><span class="in">```</span></span>
<span id="cb64-64"><a href="#cb64-64"></a></span>
<span id="cb64-67"><a href="#cb64-67"></a><span class="in">```{python}</span></span>
<span id="cb64-68"><a href="#cb64-68"></a><span class="co"># create a mapping from characters to integers</span></span>
<span id="cb64-69"><a href="#cb64-69"></a>stoi <span class="op">=</span> { ch:i <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }</span>
<span id="cb64-70"><a href="#cb64-70"></a>itos <span class="op">=</span> { i:ch <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }</span>
<span id="cb64-71"><a href="#cb64-71"></a>encode <span class="op">=</span> <span class="kw">lambda</span> s: [stoi[c] <span class="cf">for</span> c <span class="kw">in</span> s] <span class="co"># encoder: take a string, output a list of integers</span></span>
<span id="cb64-72"><a href="#cb64-72"></a>decode <span class="op">=</span> <span class="kw">lambda</span> l: <span class="st">''</span>.join([itos[i] <span class="cf">for</span> i <span class="kw">in</span> l]) <span class="co"># decoder: take a list of integers, output a string</span></span>
<span id="cb64-73"><a href="#cb64-73"></a></span>
<span id="cb64-74"><a href="#cb64-74"></a><span class="bu">print</span>(encode(<span class="st">"hii there"</span>))</span>
<span id="cb64-75"><a href="#cb64-75"></a><span class="bu">print</span>(decode(encode(<span class="st">"hii there"</span>)))</span>
<span id="cb64-76"><a href="#cb64-76"></a><span class="in">```</span></span>
<span id="cb64-77"><a href="#cb64-77"></a></span>
<span id="cb64-78"><a href="#cb64-78"></a></span>
<span id="cb64-81"><a href="#cb64-81"></a><span class="in">```{python}</span></span>
<span id="cb64-82"><a href="#cb64-82"></a><span class="co"># %pip install torch torchvision torchaudio scikit-learn</span></span>
<span id="cb64-83"><a href="#cb64-83"></a></span>
<span id="cb64-84"><a href="#cb64-84"></a><span class="co"># import os</span></span>
<span id="cb64-85"><a href="#cb64-85"></a><span class="co"># import time</span></span>
<span id="cb64-86"><a href="#cb64-86"></a><span class="co"># import math</span></span>
<span id="cb64-87"><a href="#cb64-87"></a><span class="co"># import pickle</span></span>
<span id="cb64-88"><a href="#cb64-88"></a><span class="co"># from contextlib import nullcontext</span></span>
<span id="cb64-89"><a href="#cb64-89"></a></span>
<span id="cb64-90"><a href="#cb64-90"></a><span class="co"># import numpy as np</span></span>
<span id="cb64-91"><a href="#cb64-91"></a><span class="co"># import torch</span></span>
<span id="cb64-92"><a href="#cb64-92"></a><span class="co"># from torch.nn.parallel import DistributedDataParallel as DDP</span></span>
<span id="cb64-93"><a href="#cb64-93"></a><span class="co"># from torch.distributed import init_process_group, destroy_process_group</span></span>
<span id="cb64-94"><a href="#cb64-94"></a></span>
<span id="cb64-95"><a href="#cb64-95"></a><span class="co"># from model import GPTConfig, GPT</span></span>
<span id="cb64-96"><a href="#cb64-96"></a><span class="in">```</span></span>
<span id="cb64-97"><a href="#cb64-97"></a></span>
<span id="cb64-100"><a href="#cb64-100"></a><span class="in">```{python}</span></span>
<span id="cb64-101"><a href="#cb64-101"></a><span class="co"># let's now encode the entire text dataset and store it into a torch.Tensor</span></span>
<span id="cb64-102"><a href="#cb64-102"></a><span class="im">import</span> torch <span class="co"># we use PyTorch: https://pytorch.org</span></span>
<span id="cb64-103"><a href="#cb64-103"></a>data <span class="op">=</span> torch.tensor(encode(text), dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb64-104"><a href="#cb64-104"></a><span class="bu">print</span>(data.shape, data.dtype)</span>
<span id="cb64-105"><a href="#cb64-105"></a><span class="bu">print</span>(data[:<span class="dv">1000</span>]) <span class="co"># the 1000 characters we looked at earier will to the GPT look like this</span></span>
<span id="cb64-106"><a href="#cb64-106"></a><span class="in">```</span></span>
<span id="cb64-107"><a href="#cb64-107"></a></span>
<span id="cb64-110"><a href="#cb64-110"></a><span class="in">```{python}</span></span>
<span id="cb64-111"><a href="#cb64-111"></a><span class="co"># split up the data into train and validation sets</span></span>
<span id="cb64-112"><a href="#cb64-112"></a>n <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span><span class="op">*</span><span class="bu">len</span>(data)) <span class="co"># first 90% will be train, rest val</span></span>
<span id="cb64-113"><a href="#cb64-113"></a>train_data <span class="op">=</span> data[:n]</span>
<span id="cb64-114"><a href="#cb64-114"></a>val_data <span class="op">=</span> data[n:]</span>
<span id="cb64-115"><a href="#cb64-115"></a><span class="in">```</span></span>
<span id="cb64-116"><a href="#cb64-116"></a></span>
<span id="cb64-119"><a href="#cb64-119"></a><span class="in">```{python}</span></span>
<span id="cb64-120"><a href="#cb64-120"></a><span class="co"># define the block size</span></span>
<span id="cb64-121"><a href="#cb64-121"></a>block_size <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb64-122"><a href="#cb64-122"></a>train_data[:block_size<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb64-123"><a href="#cb64-123"></a><span class="in">```</span></span>
<span id="cb64-124"><a href="#cb64-124"></a></span>
<span id="cb64-125"><a href="#cb64-125"></a><span class="fu">### define the context and target: 8 examples in one batch</span></span>
<span id="cb64-128"><a href="#cb64-128"></a><span class="in">```{python}</span></span>
<span id="cb64-129"><a href="#cb64-129"></a>x <span class="op">=</span> train_data[:block_size]</span>
<span id="cb64-130"><a href="#cb64-130"></a>y <span class="op">=</span> train_data[<span class="dv">1</span>:block_size<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb64-131"><a href="#cb64-131"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(block_size):</span>
<span id="cb64-132"><a href="#cb64-132"></a>    context <span class="op">=</span> x[:t<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb64-133"><a href="#cb64-133"></a>    target <span class="op">=</span> y[t]</span>
<span id="cb64-134"><a href="#cb64-134"></a>    <span class="bu">print</span>(<span class="ss">f"when input is </span><span class="sc">{</span>context<span class="sc">}</span><span class="ss"> the target: </span><span class="sc">{</span>target<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb64-135"><a href="#cb64-135"></a><span class="in">```</span></span>
<span id="cb64-136"><a href="#cb64-136"></a></span>
<span id="cb64-137"><a href="#cb64-137"></a><span class="fu">### define the batch size and get the batch</span></span>
<span id="cb64-140"><a href="#cb64-140"></a><span class="in">```{python}</span></span>
<span id="cb64-141"><a href="#cb64-141"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb64-142"><a href="#cb64-142"></a>batch_size <span class="op">=</span> <span class="dv">4</span> <span class="co"># how many independent sequences will we process in parallel?</span></span>
<span id="cb64-143"><a href="#cb64-143"></a>block_size <span class="op">=</span> <span class="dv">8</span> <span class="co"># what is the maximum context length for predictions?</span></span>
<span id="cb64-144"><a href="#cb64-144"></a></span>
<span id="cb64-145"><a href="#cb64-145"></a><span class="kw">def</span> get_batch(split):</span>
<span id="cb64-146"><a href="#cb64-146"></a>    <span class="co"># generate a small batch of data of inputs x and targets y</span></span>
<span id="cb64-147"><a href="#cb64-147"></a>    data <span class="op">=</span> train_data <span class="cf">if</span> split <span class="op">==</span> <span class="st">'train'</span> <span class="cf">else</span> val_data</span>
<span id="cb64-148"><a href="#cb64-148"></a>    ix <span class="op">=</span> torch.randint(<span class="bu">len</span>(data) <span class="op">-</span> block_size, (batch_size,))</span>
<span id="cb64-149"><a href="#cb64-149"></a>    x <span class="op">=</span> torch.stack([data[i:i<span class="op">+</span>block_size] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb64-150"><a href="#cb64-150"></a>    y <span class="op">=</span> torch.stack([data[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span>block_size<span class="op">+</span><span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb64-151"><a href="#cb64-151"></a>    <span class="cf">return</span> x, y</span>
<span id="cb64-152"><a href="#cb64-152"></a></span>
<span id="cb64-153"><a href="#cb64-153"></a>xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb64-154"><a href="#cb64-154"></a><span class="bu">print</span>(<span class="st">'inputs:'</span>)</span>
<span id="cb64-155"><a href="#cb64-155"></a><span class="bu">print</span>(xb.shape)</span>
<span id="cb64-156"><a href="#cb64-156"></a><span class="bu">print</span>(xb)</span>
<span id="cb64-157"><a href="#cb64-157"></a><span class="bu">print</span>(<span class="st">'targets:'</span>)</span>
<span id="cb64-158"><a href="#cb64-158"></a><span class="bu">print</span>(yb.shape)</span>
<span id="cb64-159"><a href="#cb64-159"></a><span class="bu">print</span>(yb)</span>
<span id="cb64-160"><a href="#cb64-160"></a></span>
<span id="cb64-161"><a href="#cb64-161"></a><span class="bu">print</span>(<span class="st">'----'</span>)</span>
<span id="cb64-162"><a href="#cb64-162"></a></span>
<span id="cb64-163"><a href="#cb64-163"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(batch_size): <span class="co"># batch dimension</span></span>
<span id="cb64-164"><a href="#cb64-164"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(block_size): <span class="co"># time dimension</span></span>
<span id="cb64-165"><a href="#cb64-165"></a>        context <span class="op">=</span> xb[b, :t<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb64-166"><a href="#cb64-166"></a>        target <span class="op">=</span> yb[b,t]</span>
<span id="cb64-167"><a href="#cb64-167"></a>        <span class="bu">print</span>(<span class="ss">f"when input is </span><span class="sc">{</span>context<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss"> the target: </span><span class="sc">{</span>target<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb64-168"><a href="#cb64-168"></a><span class="in">```</span></span>
<span id="cb64-169"><a href="#cb64-169"></a></span>
<span id="cb64-170"><a href="#cb64-170"></a><span class="fu">### start with a simple model: the bigram language model</span></span>
<span id="cb64-171"><a href="#cb64-171"></a>This model predicts the next character based on the current character only. </span>
<span id="cb64-172"><a href="#cb64-172"></a>It uses the logits for the next character in a lookup table.</span>
<span id="cb64-175"><a href="#cb64-175"></a><span class="in">```{python}</span></span>
<span id="cb64-176"><a href="#cb64-176"></a><span class="co"># define the bigram language model</span></span>
<span id="cb64-177"><a href="#cb64-177"></a><span class="im">import</span> torch</span>
<span id="cb64-178"><a href="#cb64-178"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb64-179"><a href="#cb64-179"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb64-180"><a href="#cb64-180"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb64-181"><a href="#cb64-181"></a></span>
<span id="cb64-182"><a href="#cb64-182"></a><span class="kw">class</span> BigramLanguageModel(nn.Module):</span>
<span id="cb64-183"><a href="#cb64-183"></a></span>
<span id="cb64-184"><a href="#cb64-184"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size):</span>
<span id="cb64-185"><a href="#cb64-185"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb64-186"><a href="#cb64-186"></a>        <span class="co"># each token directly reads off the logits for the next token from a lookup table</span></span>
<span id="cb64-187"><a href="#cb64-187"></a>        <span class="va">self</span>.token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, vocab_size)</span>
<span id="cb64-188"><a href="#cb64-188"></a></span>
<span id="cb64-189"><a href="#cb64-189"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb64-190"><a href="#cb64-190"></a></span>
<span id="cb64-191"><a href="#cb64-191"></a>        <span class="co"># idx and targets are both (B,T) tensor of integers</span></span>
<span id="cb64-192"><a href="#cb64-192"></a>        logits <span class="op">=</span> <span class="va">self</span>.token_embedding_table(idx) <span class="co"># (B,T,C)</span></span>
<span id="cb64-193"><a href="#cb64-193"></a></span>
<span id="cb64-194"><a href="#cb64-194"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb64-195"><a href="#cb64-195"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb64-196"><a href="#cb64-196"></a>        <span class="cf">else</span>:</span>
<span id="cb64-197"><a href="#cb64-197"></a>            B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb64-198"><a href="#cb64-198"></a>            logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C)</span>
<span id="cb64-199"><a href="#cb64-199"></a>            targets <span class="op">=</span> targets.view(B<span class="op">*</span>T)</span>
<span id="cb64-200"><a href="#cb64-200"></a>            loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb64-201"><a href="#cb64-201"></a></span>
<span id="cb64-202"><a href="#cb64-202"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb64-203"><a href="#cb64-203"></a></span>
<span id="cb64-204"><a href="#cb64-204"></a>    <span class="co"># Text Generation Method</span></span>
<span id="cb64-205"><a href="#cb64-205"></a>    <span class="co"># This method implements autoregressive text generation by:</span></span>
<span id="cb64-206"><a href="#cb64-206"></a>    <span class="co"># 1. Taking the current context (idx)</span></span>
<span id="cb64-207"><a href="#cb64-207"></a>    <span class="co"># 2. Predicting the next token probabilities</span></span>
<span id="cb64-208"><a href="#cb64-208"></a>    <span class="co"># 3. Sampling from these probabilities</span></span>
<span id="cb64-209"><a href="#cb64-209"></a>    <span class="co"># 4. Appending the sampled token to the context</span></span>
<span id="cb64-210"><a href="#cb64-210"></a>    <span class="co"># 5. Repeating for max_new_tokens iterations</span></span>
<span id="cb64-211"><a href="#cb64-211"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens):</span>
<span id="cb64-212"><a href="#cb64-212"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb64-213"><a href="#cb64-213"></a>            <span class="co"># get the predictions</span></span>
<span id="cb64-214"><a href="#cb64-214"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>(idx)</span>
<span id="cb64-215"><a href="#cb64-215"></a>            <span class="co"># focus only on the last time step</span></span>
<span id="cb64-216"><a href="#cb64-216"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="co"># becomes (B, C)</span></span>
<span id="cb64-217"><a href="#cb64-217"></a>            <span class="co"># apply softmax to get probabilities</span></span>
<span id="cb64-218"><a href="#cb64-218"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># (B, C)</span></span>
<span id="cb64-219"><a href="#cb64-219"></a>            <span class="co"># sample from the distribution</span></span>
<span id="cb64-220"><a href="#cb64-220"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, 1)</span></span>
<span id="cb64-221"><a href="#cb64-221"></a>            <span class="co"># append sampled index to the running sequence</span></span>
<span id="cb64-222"><a href="#cb64-222"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, T+1)</span></span>
<span id="cb64-223"><a href="#cb64-223"></a>        <span class="cf">return</span> idx</span>
<span id="cb64-224"><a href="#cb64-224"></a><span class="in">```</span></span>
<span id="cb64-225"><a href="#cb64-225"></a></span>
<span id="cb64-226"><a href="#cb64-226"></a><span class="fu">### cross entropy loss</span></span>
<span id="cb64-227"><a href="#cb64-227"></a>Loss = -Σ(y * log(p))</span>
<span id="cb64-228"><a href="#cb64-228"></a>where:</span>
<span id="cb64-229"><a href="#cb64-229"></a><span class="ss">- </span>y = actual probability (0 or 1)</span>
<span id="cb64-230"><a href="#cb64-230"></a><span class="ss">- </span>p = predicted probability</span>
<span id="cb64-231"><a href="#cb64-231"></a><span class="ss">- </span>Σ = sum over all classes</span>
<span id="cb64-232"><a href="#cb64-232"></a></span>
<span id="cb64-233"><a href="#cb64-233"></a>This is the loss for a single token. The total loss is the average across all B*T tokens in the batch, where:</span>
<span id="cb64-234"><a href="#cb64-234"></a><span class="ss">- </span>B = batch size (number of sequences processed in parallel)</span>
<span id="cb64-235"><a href="#cb64-235"></a><span class="ss">- </span>T = sequence length (number of tokens in each sequence)</span>
<span id="cb64-236"><a href="#cb64-236"></a></span>
<span id="cb64-237"><a href="#cb64-237"></a>Before training, we would expect the model to predict the next character from a uniform distribution.</span>
<span id="cb64-238"><a href="#cb64-238"></a></span>
<span id="cb64-239"><a href="#cb64-239"></a>-Σ(y * log(p)) = </span>
<span id="cb64-240"><a href="#cb64-240"></a>= - ( 1 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) ) = - log(1/65) </span>
<span id="cb64-241"><a href="#cb64-241"></a>= 4.1744 per token.</span>
<span id="cb64-242"><a href="#cb64-242"></a></span>
<span id="cb64-243"><a href="#cb64-243"></a>i.e. at the beginning we expect loss of -log(1/65) = 4.1744 per token.</span>
<span id="cb64-244"><a href="#cb64-244"></a></span>
<span id="cb64-245"><a href="#cb64-245"></a></span>
<span id="cb64-246"><a href="#cb64-246"></a><span class="fu">### initialize the model and compute the loss</span></span>
<span id="cb64-249"><a href="#cb64-249"></a><span class="in">```{python}</span></span>
<span id="cb64-250"><a href="#cb64-250"></a>m <span class="op">=</span> BigramLanguageModel(vocab_size)</span>
<span id="cb64-251"><a href="#cb64-251"></a>logits, loss <span class="op">=</span> m(xb, yb)</span>
<span id="cb64-252"><a href="#cb64-252"></a><span class="bu">print</span>(logits.shape)</span>
<span id="cb64-253"><a href="#cb64-253"></a><span class="bu">print</span>(loss)</span>
<span id="cb64-254"><a href="#cb64-254"></a><span class="in">```</span></span>
<span id="cb64-255"><a href="#cb64-255"></a></span>
<span id="cb64-256"><a href="#cb64-256"></a><span class="fu">### generate text</span></span>
<span id="cb64-259"><a href="#cb64-259"></a><span class="in">```{python}</span></span>
<span id="cb64-260"><a href="#cb64-260"></a><span class="bu">print</span>(decode(m.generate(idx <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>), max_new_tokens<span class="op">=</span><span class="dv">100</span>)[<span class="dv">0</span>].tolist()))</span>
<span id="cb64-261"><a href="#cb64-261"></a><span class="in">```</span></span>
<span id="cb64-262"><a href="#cb64-262"></a></span>
<span id="cb64-263"><a href="#cb64-263"></a><span class="fu">### choose AdamW as the optimizer</span></span>
<span id="cb64-266"><a href="#cb64-266"></a><span class="in">```{python}</span></span>
<span id="cb64-267"><a href="#cb64-267"></a>optimizer <span class="op">=</span> torch.optim.AdamW(m.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb64-268"><a href="#cb64-268"></a><span class="in">```</span></span>
<span id="cb64-269"><a href="#cb64-269"></a></span>
<span id="cb64-270"><a href="#cb64-270"></a><span class="fu">### train the model</span></span>
<span id="cb64-273"><a href="#cb64-273"></a><span class="in">```{python}</span></span>
<span id="cb64-274"><a href="#cb64-274"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb64-275"><a href="#cb64-275"></a><span class="cf">for</span> steps <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>): <span class="co"># increase number of steps for good results...</span></span>
<span id="cb64-276"><a href="#cb64-276"></a></span>
<span id="cb64-277"><a href="#cb64-277"></a>    <span class="co"># sample a batch of data</span></span>
<span id="cb64-278"><a href="#cb64-278"></a>    xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb64-279"><a href="#cb64-279"></a></span>
<span id="cb64-280"><a href="#cb64-280"></a>    <span class="co"># evaluate the loss</span></span>
<span id="cb64-281"><a href="#cb64-281"></a>    logits, loss <span class="op">=</span> m(xb, yb)</span>
<span id="cb64-282"><a href="#cb64-282"></a>    optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-283"><a href="#cb64-283"></a>    loss.backward()</span>
<span id="cb64-284"><a href="#cb64-284"></a>    optimizer.step()</span>
<span id="cb64-285"><a href="#cb64-285"></a></span>
<span id="cb64-286"><a href="#cb64-286"></a><span class="bu">print</span>(loss.item())</span>
<span id="cb64-287"><a href="#cb64-287"></a><span class="in">```</span></span>
<span id="cb64-288"><a href="#cb64-288"></a></span>
<span id="cb64-289"><a href="#cb64-289"></a><span class="fu">### generate text starting with \n as initial context</span></span>
<span id="cb64-292"><a href="#cb64-292"></a><span class="in">```{python}</span></span>
<span id="cb64-293"><a href="#cb64-293"></a><span class="bu">print</span>(decode(m.generate(idx <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>), max_new_tokens<span class="op">=</span><span class="dv">500</span>)[<span class="dv">0</span>].tolist()))</span>
<span id="cb64-294"><a href="#cb64-294"></a><span class="in">```</span></span>
<span id="cb64-295"><a href="#cb64-295"></a></span>
<span id="cb64-296"><a href="#cb64-296"></a><span class="fu">## The mathematical trick in self-attention</span></span>
<span id="cb64-297"><a href="#cb64-297"></a></span>
<span id="cb64-298"><a href="#cb64-298"></a><span class="fu">### toy example illustrating how matrix multiplication can be used for a "weighted aggregation"</span></span>
<span id="cb64-301"><a href="#cb64-301"></a><span class="in">```{python}</span></span>
<span id="cb64-302"><a href="#cb64-302"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb64-303"><a href="#cb64-303"></a>a <span class="op">=</span> torch.tril(torch.ones(<span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb64-304"><a href="#cb64-304"></a>a <span class="op">=</span> a <span class="op">/</span> torch.<span class="bu">sum</span>(a, <span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-305"><a href="#cb64-305"></a>b <span class="op">=</span> torch.randint(<span class="dv">0</span>,<span class="dv">10</span>,(<span class="dv">3</span>,<span class="dv">2</span>)).<span class="bu">float</span>()</span>
<span id="cb64-306"><a href="#cb64-306"></a>c <span class="op">=</span> a <span class="op">@</span> b</span>
<span id="cb64-307"><a href="#cb64-307"></a><span class="bu">print</span>(<span class="st">'a='</span>)</span>
<span id="cb64-308"><a href="#cb64-308"></a><span class="bu">print</span>(a)</span>
<span id="cb64-309"><a href="#cb64-309"></a><span class="bu">print</span>(<span class="st">'--'</span>)</span>
<span id="cb64-310"><a href="#cb64-310"></a><span class="bu">print</span>(<span class="st">'b='</span>)</span>
<span id="cb64-311"><a href="#cb64-311"></a><span class="bu">print</span>(b)</span>
<span id="cb64-312"><a href="#cb64-312"></a><span class="bu">print</span>(<span class="st">'--'</span>)</span>
<span id="cb64-313"><a href="#cb64-313"></a><span class="bu">print</span>(<span class="st">'c='</span>)</span>
<span id="cb64-314"><a href="#cb64-314"></a><span class="bu">print</span>(c)</span>
<span id="cb64-315"><a href="#cb64-315"></a><span class="in">```</span></span>
<span id="cb64-316"><a href="#cb64-316"></a></span>
<span id="cb64-319"><a href="#cb64-319"></a><span class="in">```{python}</span></span>
<span id="cb64-320"><a href="#cb64-320"></a><span class="co"># consider the following toy example:</span></span>
<span id="cb64-321"><a href="#cb64-321"></a></span>
<span id="cb64-322"><a href="#cb64-322"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb64-323"><a href="#cb64-323"></a>B,T,C <span class="op">=</span> <span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">2</span> <span class="co"># batch, time, channels</span></span>
<span id="cb64-324"><a href="#cb64-324"></a>x <span class="op">=</span> torch.randn(B,T,C)</span>
<span id="cb64-325"><a href="#cb64-325"></a>x.shape</span>
<span id="cb64-326"><a href="#cb64-326"></a><span class="in">```</span></span>
<span id="cb64-327"><a href="#cb64-327"></a></span>
<span id="cb64-328"><a href="#cb64-328"></a><span class="fu">### version 1: using a for loop to compute the weighted aggregation</span></span>
<span id="cb64-331"><a href="#cb64-331"></a><span class="in">```{python}</span></span>
<span id="cb64-332"><a href="#cb64-332"></a><span class="co"># We want x[b,t] = mean_{i&lt;=t} x[b,i]</span></span>
<span id="cb64-333"><a href="#cb64-333"></a>xbow <span class="op">=</span> torch.zeros((B,T,C))</span>
<span id="cb64-334"><a href="#cb64-334"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb64-335"><a href="#cb64-335"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb64-336"><a href="#cb64-336"></a>        xprev <span class="op">=</span> x[b,:t<span class="op">+</span><span class="dv">1</span>] <span class="co"># (t,C)</span></span>
<span id="cb64-337"><a href="#cb64-337"></a>        xbow[b,t] <span class="op">=</span> torch.mean(xprev, <span class="dv">0</span>)</span>
<span id="cb64-338"><a href="#cb64-338"></a><span class="in">```</span></span>
<span id="cb64-339"><a href="#cb64-339"></a></span>
<span id="cb64-340"><a href="#cb64-340"></a><span class="fu">### version 2: using matrix multiply for a weighted aggregation</span></span>
<span id="cb64-343"><a href="#cb64-343"></a><span class="in">```{python}</span></span>
<span id="cb64-344"><a href="#cb64-344"></a>wei <span class="op">=</span> torch.tril(torch.ones(T, T))</span>
<span id="cb64-345"><a href="#cb64-345"></a>wei <span class="op">=</span> wei <span class="op">/</span> wei.<span class="bu">sum</span>(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-346"><a href="#cb64-346"></a>xbow2 <span class="op">=</span> wei <span class="op">@</span> x <span class="co"># (B, T, T) @ (B, T, C) ----&gt; (B, T, C)</span></span>
<span id="cb64-347"><a href="#cb64-347"></a>torch.allclose(xbow, xbow2)</span>
<span id="cb64-348"><a href="#cb64-348"></a><span class="in">```</span></span>
<span id="cb64-349"><a href="#cb64-349"></a></span>
<span id="cb64-350"><a href="#cb64-350"></a><span class="fu">### version 3: use Softmax</span></span>
<span id="cb64-353"><a href="#cb64-353"></a><span class="in">```{python}</span></span>
<span id="cb64-354"><a href="#cb64-354"></a>tril <span class="op">=</span> torch.tril(torch.ones(T, T))</span>
<span id="cb64-355"><a href="#cb64-355"></a>wei <span class="op">=</span> torch.zeros((T,T))</span>
<span id="cb64-356"><a href="#cb64-356"></a>wei <span class="op">=</span> wei.masked_fill(tril <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb64-357"><a href="#cb64-357"></a>wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb64-358"><a href="#cb64-358"></a>xbow3 <span class="op">=</span> wei <span class="op">@</span> x</span>
<span id="cb64-359"><a href="#cb64-359"></a>torch.allclose(xbow, xbow3)</span>
<span id="cb64-360"><a href="#cb64-360"></a><span class="in">```</span></span>
<span id="cb64-361"><a href="#cb64-361"></a></span>
<span id="cb64-362"><a href="#cb64-362"></a>**softmax function**</span>
<span id="cb64-363"><a href="#cb64-363"></a></span>
<span id="cb64-364"><a href="#cb64-364"></a>softmax($z_i$) = $\frac{e^{z_i}}{\sum_j e^{z_j}}$</span>
<span id="cb64-365"><a href="#cb64-365"></a></span>
<span id="cb64-366"><a href="#cb64-366"></a><span class="fu"># version 4: self-attention</span></span>
<span id="cb64-369"><a href="#cb64-369"></a><span class="in">```{python}</span></span>
<span id="cb64-370"><a href="#cb64-370"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb64-371"><a href="#cb64-371"></a>B,T,C <span class="op">=</span> <span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">32</span> <span class="co"># batch, time, channels</span></span>
<span id="cb64-372"><a href="#cb64-372"></a>x <span class="op">=</span> torch.randn(B,T,C)</span>
<span id="cb64-373"><a href="#cb64-373"></a></span>
<span id="cb64-374"><a href="#cb64-374"></a><span class="co"># let's see a single Head perform self-attention</span></span>
<span id="cb64-375"><a href="#cb64-375"></a>head_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb64-376"><a href="#cb64-376"></a>key <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-377"><a href="#cb64-377"></a>query <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-378"><a href="#cb64-378"></a>value <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-379"><a href="#cb64-379"></a>k <span class="op">=</span> key(x)   <span class="co"># (B, T, 16)</span></span>
<span id="cb64-380"><a href="#cb64-380"></a>q <span class="op">=</span> query(x) <span class="co"># (B, T, 16)</span></span>
<span id="cb64-381"><a href="#cb64-381"></a>wei <span class="op">=</span>  q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="co"># (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)</span></span>
<span id="cb64-382"><a href="#cb64-382"></a></span>
<span id="cb64-383"><a href="#cb64-383"></a>tril <span class="op">=</span> torch.tril(torch.ones(T, T))</span>
<span id="cb64-384"><a href="#cb64-384"></a><span class="co">#wei = torch.zeros((T,T))</span></span>
<span id="cb64-385"><a href="#cb64-385"></a>wei <span class="op">=</span> wei.masked_fill(tril <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb64-386"><a href="#cb64-386"></a>wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb64-387"><a href="#cb64-387"></a></span>
<span id="cb64-388"><a href="#cb64-388"></a>v <span class="op">=</span> value(x)</span>
<span id="cb64-389"><a href="#cb64-389"></a>out <span class="op">=</span> wei <span class="op">@</span> v</span>
<span id="cb64-390"><a href="#cb64-390"></a><span class="co">#out = wei @ x</span></span>
<span id="cb64-391"><a href="#cb64-391"></a></span>
<span id="cb64-392"><a href="#cb64-392"></a>out.shape</span>
<span id="cb64-393"><a href="#cb64-393"></a><span class="in">```</span></span>
<span id="cb64-394"><a href="#cb64-394"></a></span>
<span id="cb64-397"><a href="#cb64-397"></a><span class="in">```{python}</span></span>
<span id="cb64-398"><a href="#cb64-398"></a>wei[<span class="dv">0</span>]</span>
<span id="cb64-399"><a href="#cb64-399"></a><span class="in">```</span></span>
<span id="cb64-400"><a href="#cb64-400"></a></span>
<span id="cb64-401"><a href="#cb64-401"></a>Notes:</span>
<span id="cb64-402"><a href="#cb64-402"></a><span class="ss">- </span>Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.</span>
<span id="cb64-403"><a href="#cb64-403"></a><span class="ss">- </span>There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.</span>
<span id="cb64-404"><a href="#cb64-404"></a><span class="ss">- </span>Each example across batch dimension is of course processed completely independently and never "talk" to each other</span>
<span id="cb64-405"><a href="#cb64-405"></a><span class="ss">- </span>In an "encoder" attention block just delete the single line that does masking with <span class="in">`tril`</span>, allowing all tokens to communicate. This block here is called a "decoder" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.</span>
<span id="cb64-406"><a href="#cb64-406"></a><span class="ss">- </span>"self-attention" just means that the keys and values are produced from the same source as queries. In "cross-attention", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)</span>
<span id="cb64-407"><a href="#cb64-407"></a></span>
<span id="cb64-408"><a href="#cb64-408"></a></span>
<span id="cb64-409"><a href="#cb64-409"></a><span class="fu">### why scaled attention?</span></span>
<span id="cb64-410"><a href="#cb64-410"></a><span class="ss">- </span>"Scaled" attention additional divides <span class="in">`wei`</span> by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below</span>
<span id="cb64-411"><a href="#cb64-411"></a></span>
<span id="cb64-414"><a href="#cb64-414"></a><span class="in">```{python}</span></span>
<span id="cb64-415"><a href="#cb64-415"></a>k <span class="op">=</span> torch.randn(B,T,head_size)</span>
<span id="cb64-416"><a href="#cb64-416"></a>q <span class="op">=</span> torch.randn(B,T,head_size)</span>
<span id="cb64-417"><a href="#cb64-417"></a>wei <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> head_size<span class="op">**-</span><span class="fl">0.5</span></span>
<span id="cb64-418"><a href="#cb64-418"></a><span class="in">```</span></span>
<span id="cb64-419"><a href="#cb64-419"></a></span>
<span id="cb64-422"><a href="#cb64-422"></a><span class="in">```{python}</span></span>
<span id="cb64-423"><a href="#cb64-423"></a>k.var()</span>
<span id="cb64-424"><a href="#cb64-424"></a><span class="in">```</span></span>
<span id="cb64-425"><a href="#cb64-425"></a></span>
<span id="cb64-428"><a href="#cb64-428"></a><span class="in">```{python}</span></span>
<span id="cb64-429"><a href="#cb64-429"></a>q.var()</span>
<span id="cb64-430"><a href="#cb64-430"></a><span class="in">```</span></span>
<span id="cb64-431"><a href="#cb64-431"></a></span>
<span id="cb64-434"><a href="#cb64-434"></a><span class="in">```{python}</span></span>
<span id="cb64-435"><a href="#cb64-435"></a>wei.var()</span>
<span id="cb64-436"><a href="#cb64-436"></a><span class="in">```</span></span>
<span id="cb64-437"><a href="#cb64-437"></a></span>
<span id="cb64-440"><a href="#cb64-440"></a><span class="in">```{python}</span></span>
<span id="cb64-441"><a href="#cb64-441"></a>torch.softmax(torch.tensor([<span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.5</span>]), dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb64-442"><a href="#cb64-442"></a><span class="in">```</span></span>
<span id="cb64-443"><a href="#cb64-443"></a></span>
<span id="cb64-446"><a href="#cb64-446"></a><span class="in">```{python}</span></span>
<span id="cb64-447"><a href="#cb64-447"></a>torch.softmax(torch.tensor([<span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.5</span>])<span class="op">*</span><span class="dv">8</span>, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># gets too peaky, converges to one-hot</span></span>
<span id="cb64-448"><a href="#cb64-448"></a><span class="in">```</span></span>
<span id="cb64-449"><a href="#cb64-449"></a></span>
<span id="cb64-450"><a href="#cb64-450"></a><span class="fu">### LayerNorm1d</span></span>
<span id="cb64-453"><a href="#cb64-453"></a><span class="in">```{python}</span></span>
<span id="cb64-454"><a href="#cb64-454"></a><span class="kw">class</span> LayerNorm1d: <span class="co"># (used to be BatchNorm1d)</span></span>
<span id="cb64-455"><a href="#cb64-455"></a></span>
<span id="cb64-456"><a href="#cb64-456"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, eps<span class="op">=</span><span class="fl">1e-5</span>, momentum<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb64-457"><a href="#cb64-457"></a>    <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb64-458"><a href="#cb64-458"></a>    <span class="va">self</span>.gamma <span class="op">=</span> torch.ones(dim)</span>
<span id="cb64-459"><a href="#cb64-459"></a>    <span class="va">self</span>.beta <span class="op">=</span> torch.zeros(dim)</span>
<span id="cb64-460"><a href="#cb64-460"></a></span>
<span id="cb64-461"><a href="#cb64-461"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb64-462"><a href="#cb64-462"></a>    <span class="co"># calculate the forward pass</span></span>
<span id="cb64-463"><a href="#cb64-463"></a>    xmean <span class="op">=</span> x.mean(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch mean</span></span>
<span id="cb64-464"><a href="#cb64-464"></a>    xvar <span class="op">=</span> x.var(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch variance</span></span>
<span id="cb64-465"><a href="#cb64-465"></a>    xhat <span class="op">=</span> (x <span class="op">-</span> xmean) <span class="op">/</span> torch.sqrt(xvar <span class="op">+</span> <span class="va">self</span>.eps) <span class="co"># normalize to unit variance</span></span>
<span id="cb64-466"><a href="#cb64-466"></a>    <span class="va">self</span>.out <span class="op">=</span> <span class="va">self</span>.gamma <span class="op">*</span> xhat <span class="op">+</span> <span class="va">self</span>.beta</span>
<span id="cb64-467"><a href="#cb64-467"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb64-468"><a href="#cb64-468"></a></span>
<span id="cb64-469"><a href="#cb64-469"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb64-470"><a href="#cb64-470"></a>    <span class="cf">return</span> [<span class="va">self</span>.gamma, <span class="va">self</span>.beta]</span>
<span id="cb64-471"><a href="#cb64-471"></a></span>
<span id="cb64-472"><a href="#cb64-472"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb64-473"><a href="#cb64-473"></a>module <span class="op">=</span> LayerNorm1d(<span class="dv">100</span>)</span>
<span id="cb64-474"><a href="#cb64-474"></a>x <span class="op">=</span> torch.randn(<span class="dv">32</span>, <span class="dv">100</span>) <span class="co"># batch size 32 of 100-dimensional vectors</span></span>
<span id="cb64-475"><a href="#cb64-475"></a>x <span class="op">=</span> module(x)</span>
<span id="cb64-476"><a href="#cb64-476"></a>x.shape</span>
<span id="cb64-477"><a href="#cb64-477"></a><span class="in">```</span></span>
<span id="cb64-478"><a href="#cb64-478"></a></span>
<span id="cb64-481"><a href="#cb64-481"></a><span class="in">```{python}</span></span>
<span id="cb64-482"><a href="#cb64-482"></a>x[:,<span class="dv">0</span>].mean(), x[:,<span class="dv">0</span>].std() <span class="co"># mean,std of one feature across all batch inputs</span></span>
<span id="cb64-483"><a href="#cb64-483"></a><span class="in">```</span></span>
<span id="cb64-484"><a href="#cb64-484"></a></span>
<span id="cb64-487"><a href="#cb64-487"></a><span class="in">```{python}</span></span>
<span id="cb64-488"><a href="#cb64-488"></a>x[<span class="dv">0</span>,:].mean(), x[<span class="dv">0</span>,:].std() <span class="co"># mean,std of a single input from the batch, of its features</span></span>
<span id="cb64-489"><a href="#cb64-489"></a><span class="in">```</span></span>
<span id="cb64-490"><a href="#cb64-490"></a></span>
<span id="cb64-493"><a href="#cb64-493"></a><span class="in">```{python}</span></span>
<span id="cb64-494"><a href="#cb64-494"></a><span class="co"># French to English translation example:</span></span>
<span id="cb64-495"><a href="#cb64-495"></a></span>
<span id="cb64-496"><a href="#cb64-496"></a><span class="co"># &lt;--------- ENCODE ------------------&gt;&lt;--------------- DECODE -----------------&gt;</span></span>
<span id="cb64-497"><a href="#cb64-497"></a><span class="co"># les réseaux de neurones sont géniaux! &lt;START&gt; neural networks are awesome!&lt;</span><span class="re">END</span><span class="co">&gt;</span></span>
<span id="cb64-498"><a href="#cb64-498"></a></span>
<span id="cb64-499"><a href="#cb64-499"></a><span class="in">```</span></span>
<span id="cb64-500"><a href="#cb64-500"></a></span>
<span id="cb64-501"><a href="#cb64-501"></a><span class="fu">### Full finished code, for reference</span></span>
<span id="cb64-502"><a href="#cb64-502"></a></span>
<span id="cb64-503"><a href="#cb64-503"></a>You may want to refer directly to the git repo instead though.</span>
<span id="cb64-504"><a href="#cb64-504"></a></span>
<span id="cb64-507"><a href="#cb64-507"></a><span class="in">```{python}</span></span>
<span id="cb64-508"><a href="#cb64-508"></a><span class="im">import</span> torch</span>
<span id="cb64-509"><a href="#cb64-509"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb64-510"><a href="#cb64-510"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb64-511"><a href="#cb64-511"></a></span>
<span id="cb64-512"><a href="#cb64-512"></a><span class="co"># hyperparameters</span></span>
<span id="cb64-513"><a href="#cb64-513"></a>batch_size <span class="op">=</span> <span class="dv">16</span> <span class="co"># how many independent sequences will we process in parallel?</span></span>
<span id="cb64-514"><a href="#cb64-514"></a>block_size <span class="op">=</span> <span class="dv">32</span> <span class="co"># what is the maximum context length for predictions?</span></span>
<span id="cb64-515"><a href="#cb64-515"></a>max_iters <span class="op">=</span> <span class="dv">5000</span></span>
<span id="cb64-516"><a href="#cb64-516"></a>eval_interval <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb64-517"><a href="#cb64-517"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span></span>
<span id="cb64-518"><a href="#cb64-518"></a>device <span class="op">=</span> <span class="st">'cuda'</span> <span class="cf">if</span> torch.cuda.is_available() <span class="cf">else</span> <span class="st">'cpu'</span></span>
<span id="cb64-519"><a href="#cb64-519"></a>eval_iters <span class="op">=</span> <span class="dv">200</span></span>
<span id="cb64-520"><a href="#cb64-520"></a>n_embd <span class="op">=</span> <span class="dv">64</span></span>
<span id="cb64-521"><a href="#cb64-521"></a>n_head <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb64-522"><a href="#cb64-522"></a>n_layer <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb64-523"><a href="#cb64-523"></a>dropout <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb64-524"><a href="#cb64-524"></a><span class="co"># ------------</span></span>
<span id="cb64-525"><a href="#cb64-525"></a></span>
<span id="cb64-526"><a href="#cb64-526"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb64-527"><a href="#cb64-527"></a></span>
<span id="cb64-528"><a href="#cb64-528"></a><span class="co"># wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinys Shakespeare/input.txt</span></span>
<span id="cb64-529"><a href="#cb64-529"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'input.txt'</span>, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> f:</span>
<span id="cb64-530"><a href="#cb64-530"></a>    text <span class="op">=</span> f.read()</span>
<span id="cb64-531"><a href="#cb64-531"></a></span>
<span id="cb64-532"><a href="#cb64-532"></a><span class="co"># here are all the unique characters that occur in this text</span></span>
<span id="cb64-533"><a href="#cb64-533"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(text)))</span>
<span id="cb64-534"><a href="#cb64-534"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(chars)</span>
<span id="cb64-535"><a href="#cb64-535"></a><span class="co"># create a mapping from characters to integers</span></span>
<span id="cb64-536"><a href="#cb64-536"></a>stoi <span class="op">=</span> { ch:i <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }</span>
<span id="cb64-537"><a href="#cb64-537"></a>itos <span class="op">=</span> { i:ch <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }</span>
<span id="cb64-538"><a href="#cb64-538"></a>encode <span class="op">=</span> <span class="kw">lambda</span> s: [stoi[c] <span class="cf">for</span> c <span class="kw">in</span> s] <span class="co"># encoder: take a string, output a list of integers</span></span>
<span id="cb64-539"><a href="#cb64-539"></a>decode <span class="op">=</span> <span class="kw">lambda</span> l: <span class="st">''</span>.join([itos[i] <span class="cf">for</span> i <span class="kw">in</span> l]) <span class="co"># decoder: take a list of integers, output a string</span></span>
<span id="cb64-540"><a href="#cb64-540"></a></span>
<span id="cb64-541"><a href="#cb64-541"></a><span class="co"># Train and test splits</span></span>
<span id="cb64-542"><a href="#cb64-542"></a>data <span class="op">=</span> torch.tensor(encode(text), dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb64-543"><a href="#cb64-543"></a>n <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span><span class="op">*</span><span class="bu">len</span>(data)) <span class="co"># first 90% will be train, rest val</span></span>
<span id="cb64-544"><a href="#cb64-544"></a>train_data <span class="op">=</span> data[:n]</span>
<span id="cb64-545"><a href="#cb64-545"></a>val_data <span class="op">=</span> data[n:]</span>
<span id="cb64-546"><a href="#cb64-546"></a></span>
<span id="cb64-547"><a href="#cb64-547"></a><span class="co"># data loading</span></span>
<span id="cb64-548"><a href="#cb64-548"></a><span class="kw">def</span> get_batch(split):</span>
<span id="cb64-549"><a href="#cb64-549"></a>    <span class="co"># generate a small batch of data of inputs x and targets y</span></span>
<span id="cb64-550"><a href="#cb64-550"></a>    data <span class="op">=</span> train_data <span class="cf">if</span> split <span class="op">==</span> <span class="st">'train'</span> <span class="cf">else</span> val_data</span>
<span id="cb64-551"><a href="#cb64-551"></a>    ix <span class="op">=</span> torch.randint(<span class="bu">len</span>(data) <span class="op">-</span> block_size, (batch_size,))</span>
<span id="cb64-552"><a href="#cb64-552"></a>    x <span class="op">=</span> torch.stack([data[i:i<span class="op">+</span>block_size] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb64-553"><a href="#cb64-553"></a>    y <span class="op">=</span> torch.stack([data[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span>block_size<span class="op">+</span><span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb64-554"><a href="#cb64-554"></a>    x, y <span class="op">=</span> x.to(device), y.to(device)</span>
<span id="cb64-555"><a href="#cb64-555"></a>    <span class="cf">return</span> x, y</span>
<span id="cb64-556"><a href="#cb64-556"></a></span>
<span id="cb64-557"><a href="#cb64-557"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb64-558"><a href="#cb64-558"></a><span class="kw">def</span> estimate_loss():</span>
<span id="cb64-559"><a href="#cb64-559"></a>    out <span class="op">=</span> {}</span>
<span id="cb64-560"><a href="#cb64-560"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb64-561"><a href="#cb64-561"></a>    <span class="cf">for</span> split <span class="kw">in</span> [<span class="st">'train'</span>, <span class="st">'val'</span>]:</span>
<span id="cb64-562"><a href="#cb64-562"></a>        losses <span class="op">=</span> torch.zeros(eval_iters)</span>
<span id="cb64-563"><a href="#cb64-563"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(eval_iters):</span>
<span id="cb64-564"><a href="#cb64-564"></a>            X, Y <span class="op">=</span> get_batch(split)</span>
<span id="cb64-565"><a href="#cb64-565"></a>            logits, loss <span class="op">=</span> model(X, Y)</span>
<span id="cb64-566"><a href="#cb64-566"></a>            losses[k] <span class="op">=</span> loss.item()</span>
<span id="cb64-567"><a href="#cb64-567"></a>        out[split] <span class="op">=</span> losses.mean()</span>
<span id="cb64-568"><a href="#cb64-568"></a>    model.train()</span>
<span id="cb64-569"><a href="#cb64-569"></a>    <span class="cf">return</span> out</span>
<span id="cb64-570"><a href="#cb64-570"></a></span>
<span id="cb64-571"><a href="#cb64-571"></a><span class="kw">class</span> Head(nn.Module):</span>
<span id="cb64-572"><a href="#cb64-572"></a>    <span class="co">""" one head of self-attention """</span></span>
<span id="cb64-573"><a href="#cb64-573"></a></span>
<span id="cb64-574"><a href="#cb64-574"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, head_size):</span>
<span id="cb64-575"><a href="#cb64-575"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb64-576"><a href="#cb64-576"></a>        <span class="va">self</span>.key <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-577"><a href="#cb64-577"></a>        <span class="va">self</span>.query <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-578"><a href="#cb64-578"></a>        <span class="va">self</span>.value <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb64-579"><a href="#cb64-579"></a>        <span class="va">self</span>.register_buffer(<span class="st">'tril'</span>, torch.tril(torch.ones(block_size, block_size)))</span>
<span id="cb64-580"><a href="#cb64-580"></a></span>
<span id="cb64-581"><a href="#cb64-581"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb64-582"><a href="#cb64-582"></a></span>
<span id="cb64-583"><a href="#cb64-583"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb64-584"><a href="#cb64-584"></a>        B,T,C <span class="op">=</span> x.shape</span>
<span id="cb64-585"><a href="#cb64-585"></a>        k <span class="op">=</span> <span class="va">self</span>.key(x)   <span class="co"># (B,T,C)</span></span>
<span id="cb64-586"><a href="#cb64-586"></a>        q <span class="op">=</span> <span class="va">self</span>.query(x) <span class="co"># (B,T,C)</span></span>
<span id="cb64-587"><a href="#cb64-587"></a>        <span class="co"># compute attention scores ("affinities")</span></span>
<span id="cb64-588"><a href="#cb64-588"></a>        wei <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> C<span class="op">**-</span><span class="fl">0.5</span> <span class="co"># (B, T, C) @ (B, C, T) -&gt; (B, T, T)</span></span>
<span id="cb64-589"><a href="#cb64-589"></a>        wei <span class="op">=</span> wei.masked_fill(<span class="va">self</span>.tril[:T, :T] <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>)) <span class="co"># (B, T, T)</span></span>
<span id="cb64-590"><a href="#cb64-590"></a>        wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># (B, T, T)</span></span>
<span id="cb64-591"><a href="#cb64-591"></a>        wei <span class="op">=</span> <span class="va">self</span>.dropout(wei)</span>
<span id="cb64-592"><a href="#cb64-592"></a>        <span class="co"># perform the weighted aggregation of the values</span></span>
<span id="cb64-593"><a href="#cb64-593"></a>        v <span class="op">=</span> <span class="va">self</span>.value(x) <span class="co"># (B,T,C)</span></span>
<span id="cb64-594"><a href="#cb64-594"></a>        out <span class="op">=</span> wei <span class="op">@</span> v <span class="co"># (B, T, T) @ (B, T, C) -&gt; (B, T, C)</span></span>
<span id="cb64-595"><a href="#cb64-595"></a>        <span class="cf">return</span> out</span>
<span id="cb64-596"><a href="#cb64-596"></a></span>
<span id="cb64-597"><a href="#cb64-597"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb64-598"><a href="#cb64-598"></a>    <span class="co">""" multiple heads of self-attention in parallel """</span></span>
<span id="cb64-599"><a href="#cb64-599"></a></span>
<span id="cb64-600"><a href="#cb64-600"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_heads, head_size):</span>
<span id="cb64-601"><a href="#cb64-601"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb64-602"><a href="#cb64-602"></a>        <span class="va">self</span>.heads <span class="op">=</span> nn.ModuleList([Head(head_size) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_heads)])</span>
<span id="cb64-603"><a href="#cb64-603"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Linear(n_embd, n_embd)</span>
<span id="cb64-604"><a href="#cb64-604"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb64-605"><a href="#cb64-605"></a></span>
<span id="cb64-606"><a href="#cb64-606"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb64-607"><a href="#cb64-607"></a>        out <span class="op">=</span> torch.cat([h(x) <span class="cf">for</span> h <span class="kw">in</span> <span class="va">self</span>.heads], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb64-608"><a href="#cb64-608"></a>        out <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.proj(out))</span>
<span id="cb64-609"><a href="#cb64-609"></a>        <span class="cf">return</span> out</span>
<span id="cb64-610"><a href="#cb64-610"></a></span>
<span id="cb64-611"><a href="#cb64-611"></a><span class="kw">class</span> FeedFoward(nn.Module):</span>
<span id="cb64-612"><a href="#cb64-612"></a>    <span class="co">""" a simple linear layer followed by a non-linearity """</span></span>
<span id="cb64-613"><a href="#cb64-613"></a></span>
<span id="cb64-614"><a href="#cb64-614"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd):</span>
<span id="cb64-615"><a href="#cb64-615"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb64-616"><a href="#cb64-616"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb64-617"><a href="#cb64-617"></a>            nn.Linear(n_embd, <span class="dv">4</span> <span class="op">*</span> n_embd),</span>
<span id="cb64-618"><a href="#cb64-618"></a>            nn.ReLU(),</span>
<span id="cb64-619"><a href="#cb64-619"></a>            nn.Linear(<span class="dv">4</span> <span class="op">*</span> n_embd, n_embd),</span>
<span id="cb64-620"><a href="#cb64-620"></a>            nn.Dropout(dropout),</span>
<span id="cb64-621"><a href="#cb64-621"></a>        )</span>
<span id="cb64-622"><a href="#cb64-622"></a></span>
<span id="cb64-623"><a href="#cb64-623"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb64-624"><a href="#cb64-624"></a>        <span class="cf">return</span> <span class="va">self</span>.net(x)</span>
<span id="cb64-625"><a href="#cb64-625"></a></span>
<span id="cb64-626"><a href="#cb64-626"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb64-627"><a href="#cb64-627"></a>    <span class="co">""" Transformer block: communication followed by computation """</span></span>
<span id="cb64-628"><a href="#cb64-628"></a></span>
<span id="cb64-629"><a href="#cb64-629"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd, n_head):</span>
<span id="cb64-630"><a href="#cb64-630"></a>        <span class="co"># n_embd: embedding dimension, n_head: the number of heads we'd like</span></span>
<span id="cb64-631"><a href="#cb64-631"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb64-632"><a href="#cb64-632"></a>        head_size <span class="op">=</span> n_embd <span class="op">//</span> n_head</span>
<span id="cb64-633"><a href="#cb64-633"></a>        <span class="va">self</span>.sa <span class="op">=</span> MultiHeadAttention(n_head, head_size)</span>
<span id="cb64-634"><a href="#cb64-634"></a>        <span class="va">self</span>.ffwd <span class="op">=</span> FeedFoward(n_embd)</span>
<span id="cb64-635"><a href="#cb64-635"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> nn.LayerNorm(n_embd)</span>
<span id="cb64-636"><a href="#cb64-636"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> nn.LayerNorm(n_embd)</span>
<span id="cb64-637"><a href="#cb64-637"></a></span>
<span id="cb64-638"><a href="#cb64-638"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb64-639"><a href="#cb64-639"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.sa(<span class="va">self</span>.ln1(x))</span>
<span id="cb64-640"><a href="#cb64-640"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffwd(<span class="va">self</span>.ln2(x))</span>
<span id="cb64-641"><a href="#cb64-641"></a>        <span class="cf">return</span> x</span>
<span id="cb64-642"><a href="#cb64-642"></a></span>
<span id="cb64-643"><a href="#cb64-643"></a><span class="co"># super simple bigram model</span></span>
<span id="cb64-644"><a href="#cb64-644"></a><span class="kw">class</span> BigramLanguageModel(nn.Module):</span>
<span id="cb64-645"><a href="#cb64-645"></a></span>
<span id="cb64-646"><a href="#cb64-646"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb64-647"><a href="#cb64-647"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb64-648"><a href="#cb64-648"></a>        <span class="co"># each token directly reads off the logits for the next token from a lookup table</span></span>
<span id="cb64-649"><a href="#cb64-649"></a>        <span class="va">self</span>.token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, n_embd)</span>
<span id="cb64-650"><a href="#cb64-650"></a>        <span class="va">self</span>.position_embedding_table <span class="op">=</span> nn.Embedding(block_size, n_embd)</span>
<span id="cb64-651"><a href="#cb64-651"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.Sequential(<span class="op">*</span>[Block(n_embd, n_head<span class="op">=</span>n_head) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_layer)])</span>
<span id="cb64-652"><a href="#cb64-652"></a>        <span class="va">self</span>.ln_f <span class="op">=</span> nn.LayerNorm(n_embd) <span class="co"># final layer norm</span></span>
<span id="cb64-653"><a href="#cb64-653"></a>        <span class="va">self</span>.lm_head <span class="op">=</span> nn.Linear(n_embd, vocab_size)</span>
<span id="cb64-654"><a href="#cb64-654"></a></span>
<span id="cb64-655"><a href="#cb64-655"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb64-656"><a href="#cb64-656"></a>        B, T <span class="op">=</span> idx.shape</span>
<span id="cb64-657"><a href="#cb64-657"></a></span>
<span id="cb64-658"><a href="#cb64-658"></a>        <span class="co"># idx and targets are both (B,T) tensor of integers</span></span>
<span id="cb64-659"><a href="#cb64-659"></a>        tok_emb <span class="op">=</span> <span class="va">self</span>.token_embedding_table(idx) <span class="co"># (B,T,C)</span></span>
<span id="cb64-660"><a href="#cb64-660"></a>        pos_emb <span class="op">=</span> <span class="va">self</span>.position_embedding_table(torch.arange(T, device<span class="op">=</span>device)) <span class="co"># (T,C)</span></span>
<span id="cb64-661"><a href="#cb64-661"></a>        x <span class="op">=</span> tok_emb <span class="op">+</span> pos_emb <span class="co"># (B,T,C)</span></span>
<span id="cb64-662"><a href="#cb64-662"></a>        x <span class="op">=</span> <span class="va">self</span>.blocks(x) <span class="co"># (B,T,C)</span></span>
<span id="cb64-663"><a href="#cb64-663"></a>        x <span class="op">=</span> <span class="va">self</span>.ln_f(x) <span class="co"># (B,T,C)</span></span>
<span id="cb64-664"><a href="#cb64-664"></a>        logits <span class="op">=</span> <span class="va">self</span>.lm_head(x) <span class="co"># (B,T,vocab_size)</span></span>
<span id="cb64-665"><a href="#cb64-665"></a></span>
<span id="cb64-666"><a href="#cb64-666"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb64-667"><a href="#cb64-667"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb64-668"><a href="#cb64-668"></a>        <span class="cf">else</span>:</span>
<span id="cb64-669"><a href="#cb64-669"></a>            B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb64-670"><a href="#cb64-670"></a>            logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C)</span>
<span id="cb64-671"><a href="#cb64-671"></a>            targets <span class="op">=</span> targets.view(B<span class="op">*</span>T)</span>
<span id="cb64-672"><a href="#cb64-672"></a>            loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb64-673"><a href="#cb64-673"></a></span>
<span id="cb64-674"><a href="#cb64-674"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb64-675"><a href="#cb64-675"></a></span>
<span id="cb64-676"><a href="#cb64-676"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens):</span>
<span id="cb64-677"><a href="#cb64-677"></a>        <span class="co"># idx is (B, T) array of indices in the current context</span></span>
<span id="cb64-678"><a href="#cb64-678"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb64-679"><a href="#cb64-679"></a>            <span class="co"># crop idx to the last block_size tokens</span></span>
<span id="cb64-680"><a href="#cb64-680"></a>            idx_cond <span class="op">=</span> idx[:, <span class="op">-</span>block_size:]</span>
<span id="cb64-681"><a href="#cb64-681"></a>            <span class="co"># get the predictions</span></span>
<span id="cb64-682"><a href="#cb64-682"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>(idx_cond)</span>
<span id="cb64-683"><a href="#cb64-683"></a>            <span class="co"># focus only on the last time step</span></span>
<span id="cb64-684"><a href="#cb64-684"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="co"># becomes (B, C)</span></span>
<span id="cb64-685"><a href="#cb64-685"></a>            <span class="co"># apply softmax to get probabilities</span></span>
<span id="cb64-686"><a href="#cb64-686"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># (B, C)</span></span>
<span id="cb64-687"><a href="#cb64-687"></a>            <span class="co"># sample from the distribution</span></span>
<span id="cb64-688"><a href="#cb64-688"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, 1)</span></span>
<span id="cb64-689"><a href="#cb64-689"></a>            <span class="co"># append sampled index to the running sequence</span></span>
<span id="cb64-690"><a href="#cb64-690"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, T+1)</span></span>
<span id="cb64-691"><a href="#cb64-691"></a>        <span class="cf">return</span> idx</span>
<span id="cb64-692"><a href="#cb64-692"></a></span>
<span id="cb64-693"><a href="#cb64-693"></a>model <span class="op">=</span> BigramLanguageModel()</span>
<span id="cb64-694"><a href="#cb64-694"></a>m <span class="op">=</span> model.to(device)</span>
<span id="cb64-695"><a href="#cb64-695"></a><span class="co"># print the number of parameters in the model</span></span>
<span id="cb64-696"><a href="#cb64-696"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> m.parameters())<span class="op">/</span><span class="fl">1e6</span>, <span class="st">'M parameters'</span>)</span>
<span id="cb64-697"><a href="#cb64-697"></a></span>
<span id="cb64-698"><a href="#cb64-698"></a><span class="co"># create a PyTorch optimizer</span></span>
<span id="cb64-699"><a href="#cb64-699"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb64-700"><a href="#cb64-700"></a></span>
<span id="cb64-701"><a href="#cb64-701"></a><span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(max_iters):</span>
<span id="cb64-702"><a href="#cb64-702"></a></span>
<span id="cb64-703"><a href="#cb64-703"></a>    <span class="co"># every once in a while evaluate the loss on train and val sets</span></span>
<span id="cb64-704"><a href="#cb64-704"></a>    <span class="cf">if</span> <span class="bu">iter</span> <span class="op">%</span> eval_interval <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> <span class="bu">iter</span> <span class="op">==</span> max_iters <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb64-705"><a href="#cb64-705"></a>        losses <span class="op">=</span> estimate_loss()</span>
<span id="cb64-706"><a href="#cb64-706"></a>        <span class="bu">print</span>(<span class="ss">f"step </span><span class="sc">{</span><span class="bu">iter</span><span class="sc">}</span><span class="ss">: train loss </span><span class="sc">{</span>losses[<span class="st">'train'</span>]<span class="sc">:.4f}</span><span class="ss">, val loss </span><span class="sc">{</span>losses[<span class="st">'val'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb64-707"><a href="#cb64-707"></a></span>
<span id="cb64-708"><a href="#cb64-708"></a>    <span class="co"># sample a batch of data</span></span>
<span id="cb64-709"><a href="#cb64-709"></a>    xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb64-710"><a href="#cb64-710"></a></span>
<span id="cb64-711"><a href="#cb64-711"></a>    <span class="co"># evaluate the loss</span></span>
<span id="cb64-712"><a href="#cb64-712"></a>    logits, loss <span class="op">=</span> model(xb, yb)</span>
<span id="cb64-713"><a href="#cb64-713"></a>    optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb64-714"><a href="#cb64-714"></a>    loss.backward()</span>
<span id="cb64-715"><a href="#cb64-715"></a>    optimizer.step()</span>
<span id="cb64-716"><a href="#cb64-716"></a></span>
<span id="cb64-717"><a href="#cb64-717"></a><span class="co"># generate from the model</span></span>
<span id="cb64-718"><a href="#cb64-718"></a>context <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>, device<span class="op">=</span>device)</span>
<span id="cb64-719"><a href="#cb64-719"></a><span class="bu">print</span>(decode(m.generate(context, max_new_tokens<span class="op">=</span><span class="dv">2000</span>)[<span class="dv">0</span>].tolist()))</span>
<span id="cb64-720"><a href="#cb64-720"></a><span class="in">```</span></span>
<span id="cb64-721"><a href="#cb64-721"></a></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>