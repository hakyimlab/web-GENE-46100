<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.7.31">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Andrey Karpathy">
<meta name="dcterms.date" content="2025-04-15">
<meta name="description" content="Companion notebook from Karpathys video on building a minimal GPT, annotated by cursors LLM with summary from gemini.">

<title>Building a GPT - companion notebook annotated – GENE 46100</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-e1a5c8363afafaef2c763b6775fbf3ca.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-30950e57ca2d683c179b59b95dd53cbb.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">GENE 46100</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#building-a-gpt" id="toc-building-a-gpt" class="nav-link active" data-scroll-target="#building-a-gpt">Building a GPT</a>
  <ul class="collapse">
  <li><a href="#download-the-tiny-shakespeare-dataset" id="toc-download-the-tiny-shakespeare-dataset" class="nav-link" data-scroll-target="#download-the-tiny-shakespeare-dataset">download the tiny shakespeare dataset</a></li>
  <li><a href="#mapping-characters-to-integers-and-vice-versa" id="toc-mapping-characters-to-integers-and-vice-versa" class="nav-link" data-scroll-target="#mapping-characters-to-integers-and-vice-versa">mapping characters to integers and vice versa</a></li>
  <li><a href="#encode-the-data-into-torch-tensor" id="toc-encode-the-data-into-torch-tensor" class="nav-link" data-scroll-target="#encode-the-data-into-torch-tensor">encode the data into torch tensor</a></li>
  <li><a href="#split-up-the-data-into-train-and-validation-sets" id="toc-split-up-the-data-into-train-and-validation-sets" class="nav-link" data-scroll-target="#split-up-the-data-into-train-and-validation-sets">split up the data into train and validation sets</a></li>
  <li><a href="#define-the-block-size" id="toc-define-the-block-size" class="nav-link" data-scroll-target="#define-the-block-size">define the block size</a></li>
  <li><a href="#define-the-context-and-target-8-examples-in-one-batch" id="toc-define-the-context-and-target-8-examples-in-one-batch" class="nav-link" data-scroll-target="#define-the-context-and-target-8-examples-in-one-batch">define the context and target: 8 examples in one batch</a></li>
  <li><a href="#define-the-batch-size-and-get-the-batch" id="toc-define-the-batch-size-and-get-the-batch" class="nav-link" data-scroll-target="#define-the-batch-size-and-get-the-batch">define the batch size and get the batch</a></li>
  <li><a href="#start-with-a-simple-model-the-bigram-language-model" id="toc-start-with-a-simple-model-the-bigram-language-model" class="nav-link" data-scroll-target="#start-with-a-simple-model-the-bigram-language-model">start with a simple model: the bigram language model</a></li>
  <li><a href="#cross-entropy-loss" id="toc-cross-entropy-loss" class="nav-link" data-scroll-target="#cross-entropy-loss">cross entropy loss</a></li>
  <li><a href="#initialize-the-model-and-compute-the-loss" id="toc-initialize-the-model-and-compute-the-loss" class="nav-link" data-scroll-target="#initialize-the-model-and-compute-the-loss">initialize the model and compute the loss</a></li>
  <li><a href="#generate-text" id="toc-generate-text" class="nav-link" data-scroll-target="#generate-text">generate text</a></li>
  <li><a href="#choose-adamw-as-the-optimizer" id="toc-choose-adamw-as-the-optimizer" class="nav-link" data-scroll-target="#choose-adamw-as-the-optimizer">choose AdamW as the optimizer</a></li>
  <li><a href="#train-the-model" id="toc-train-the-model" class="nav-link" data-scroll-target="#train-the-model">train the model</a></li>
  <li><a href="#generate-text-starting-with-0n-as-initial-context" id="toc-generate-text-starting-with-0n-as-initial-context" class="nav-link" data-scroll-target="#generate-text-starting-with-0n-as-initial-context">generate text starting with 0=<code>\n</code> as initial context</a></li>
  <li><a href="#the-mathematical-trick-in-self-attention" id="toc-the-mathematical-trick-in-self-attention" class="nav-link" data-scroll-target="#the-mathematical-trick-in-self-attention">The mathematical trick in self-attention</a></li>
  <li><a href="#version-1-using-a-for-loop-to-compute-the-weighted-aggregation" id="toc-version-1-using-a-for-loop-to-compute-the-weighted-aggregation" class="nav-link" data-scroll-target="#version-1-using-a-for-loop-to-compute-the-weighted-aggregation">version 1: using a for loop to compute the weighted aggregation</a></li>
  <li><a href="#version-2-using-matrix-multiply-for-a-weighted-aggregation" id="toc-version-2-using-matrix-multiply-for-a-weighted-aggregation" class="nav-link" data-scroll-target="#version-2-using-matrix-multiply-for-a-weighted-aggregation">version 2: using matrix multiply for a weighted aggregation</a></li>
  <li><a href="#version-3-use-softmax" id="toc-version-3-use-softmax" class="nav-link" data-scroll-target="#version-3-use-softmax">version 3: use Softmax</a></li>
  <li><a href="#softmax-function" id="toc-softmax-function" class="nav-link" data-scroll-target="#softmax-function">softmax function</a></li>
  <li><a href="#version-4-self-attention" id="toc-version-4-self-attention" class="nav-link" data-scroll-target="#version-4-self-attention">version 4: self-attention</a></li>
  <li><a href="#check-that-x-xc-is-is-the-correlation-matrix-if-x-is-normalized" id="toc-check-that-x-xc-is-is-the-correlation-matrix-if-x-is-normalized" class="nav-link" data-scroll-target="#check-that-x-xc-is-is-the-correlation-matrix-if-x-is-normalized">Check that X X’/C is is the correlation matrix if X is normalized</a></li>
  <li><a href="#notes" id="toc-notes" class="nav-link" data-scroll-target="#notes">Notes:</a></li>
  <li><a href="#why-scaled-attention" id="toc-why-scaled-attention" class="nav-link" data-scroll-target="#why-scaled-attention">why scaled attention?</a></li>
  <li><a href="#layernorm1d" id="toc-layernorm1d" class="nav-link" data-scroll-target="#layernorm1d">LayerNorm1d</a></li>
  <li><a href="#french-to-english-translation-example" id="toc-french-to-english-translation-example" class="nav-link" data-scroll-target="#french-to-english-translation-example">French to English translation example:</a></li>
  <li><a href="#full-finished-code-for-reference" id="toc-full-finished-code-for-reference" class="nav-link" data-scroll-target="#full-finished-code-for-reference">Full finished code, for reference</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Building a GPT - companion notebook annotated</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>

<div>
  <div class="description">
    Companion notebook from Karpathys video on building a minimal GPT, annotated by cursors LLM with summary from gemini.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Andrey Karpathy </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 15, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="building-a-gpt" class="level2">
<h2 class="anchored" data-anchor-id="building-a-gpt">Building a GPT</h2>
<p>Companion notebook to the <a href="https://karpathy.ai/zero-to-hero.html">Zero To Hero</a> video on GPT. Downloaded from <a href="https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing">here</a></p>
<p>(https://github.com/karpathy/nanoGPT)</p>
<section id="download-the-tiny-shakespeare-dataset" class="level3">
<h3 class="anchored" data-anchor-id="download-the-tiny-shakespeare-dataset">download the tiny shakespeare dataset</h3>
<div id="59fc39e7" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># Download the tiny shakespeare dataset</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="co">#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="29ba3e6c" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># read it in to inspect it</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'input.txt'</span>, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> f:</span>
<span id="cb2-3"><a href="#cb2-3"></a>    text <span class="op">=</span> f.read()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cbaabcfa" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># print the length of the dataset</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="bu">print</span>(<span class="st">"length of dataset in characters: "</span>, <span class="bu">len</span>(text))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>length of dataset in characters:  1115394</code></pre>
</div>
</div>
<div id="fa026a17" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="co"># let's look at the first 1000 characters</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="bu">print</span>(text[:<span class="dv">1000</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>First Citizen:
Before we proceed any further, hear me speak.

All:
Speak, speak.

First Citizen:
You are all resolved rather to die than to famish?

All:
Resolved. resolved.

First Citizen:
First, you know Caius Marcius is chief enemy to the people.

All:
We know't, we know't.

First Citizen:
Let us kill him, and we'll have corn at our own price.
Is't a verdict?

All:
No more talking on't; let it be done: away, away!

Second Citizen:
One word, good citizens.

First Citizen:
We are accounted poor citizens, the patricians good.
What authority surfeits on would relieve us: if they
would yield us but the superfluity, while it were
wholesome, we might guess they relieved us humanely;
but they think we are too dear: the leanness that
afflicts us, the object of our misery, is as an
inventory to particularise their abundance; our
sufferance is a gain to them Let us revenge this with
our pikes, ere we become rakes: for the gods know I
speak this in hunger for bread, not in thirst for revenge.

</code></pre>
</div>
</div>
<div id="1aac16f0" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="co"># here are all the unique characters that occur in this text</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(text)))</span>
<span id="cb7-3"><a href="#cb7-3"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(chars)</span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="bu">print</span>(<span class="st">''</span>.join(chars))</span>
<span id="cb7-5"><a href="#cb7-5"></a><span class="bu">print</span>(vocab_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
 !$&amp;',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz
65</code></pre>
</div>
</div>
</section>
<section id="mapping-characters-to-integers-and-vice-versa" class="level3">
<h3 class="anchored" data-anchor-id="mapping-characters-to-integers-and-vice-versa">mapping characters to integers and vice versa</h3>
<div id="2453f382" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="co"># create a mapping from characters to integers</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>stoi <span class="op">=</span> { ch:i <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }</span>
<span id="cb9-3"><a href="#cb9-3"></a>itos <span class="op">=</span> { i:ch <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }</span>
<span id="cb9-4"><a href="#cb9-4"></a>encode <span class="op">=</span> <span class="kw">lambda</span> s: [stoi[c] <span class="cf">for</span> c <span class="kw">in</span> s] <span class="co"># encoder: take a string, output a list of integers</span></span>
<span id="cb9-5"><a href="#cb9-5"></a>decode <span class="op">=</span> <span class="kw">lambda</span> l: <span class="st">''</span>.join([itos[i] <span class="cf">for</span> i <span class="kw">in</span> l]) <span class="co"># decoder: take a list of integers, output a string</span></span>
<span id="cb9-6"><a href="#cb9-6"></a></span>
<span id="cb9-7"><a href="#cb9-7"></a><span class="bu">print</span>(encode(<span class="st">"hii there"</span>))</span>
<span id="cb9-8"><a href="#cb9-8"></a><span class="bu">print</span>(decode(encode(<span class="st">"hii there"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[46, 47, 47, 1, 58, 46, 43, 56, 43]
hii there</code></pre>
</div>
</div>
</section>
<section id="encode-the-data-into-torch-tensor" class="level3">
<h3 class="anchored" data-anchor-id="encode-the-data-into-torch-tensor">encode the data into torch tensor</h3>
<div id="742e63db" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="co"># let's now encode the entire text dataset and store it into a torch.Tensor</span></span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="im">import</span> torch <span class="co"># we use PyTorch: [https://pytorch.org](https://pytorch.org)</span></span>
<span id="cb11-3"><a href="#cb11-3"></a>data <span class="op">=</span> torch.tensor(encode(text), dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="bu">print</span>(data.shape, data.dtype)</span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="bu">print</span>(data[:<span class="dv">1000</span>]) <span class="co"># the 1000 characters we looked at earier will to the GPT look like this</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([1115394]) torch.int64
tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,
        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,
         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,
        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,
         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,
        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,
         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,
        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,
        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,
         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,
         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,
        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,
        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,
         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,
        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,
        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,
        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,
        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,
        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,
        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,
        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,
         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,
         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,
         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,
        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,
        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,
        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,
        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,
        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,
        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,
        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,
         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,
         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,
        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,
        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,
        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,
         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,
        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,
        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,
         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,
        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,
        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,
        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,
        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,
        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,
        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,
        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,
        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,
        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,
         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,
        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,
        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,
        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,
        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,
        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,
        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])</code></pre>
</div>
</div>
</section>
<section id="split-up-the-data-into-train-and-validation-sets" class="level3">
<h3 class="anchored" data-anchor-id="split-up-the-data-into-train-and-validation-sets">split up the data into train and validation sets</h3>
<div id="14c10d42" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># split up the data into train and validation sets</span></span>
<span id="cb13-2"><a href="#cb13-2"></a>n <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span><span class="op">*</span><span class="bu">len</span>(data)) <span class="co"># first 90% will be train, rest val</span></span>
<span id="cb13-3"><a href="#cb13-3"></a>train_data <span class="op">=</span> data[:n]</span>
<span id="cb13-4"><a href="#cb13-4"></a>val_data <span class="op">=</span> data[n:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="define-the-block-size" class="level3">
<h3 class="anchored" data-anchor-id="define-the-block-size">define the block size</h3>
<div id="48af431c" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>block_size <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb14-2"><a href="#cb14-2"></a>train_data[:block_size<span class="op">+</span><span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])</code></pre>
</div>
</div>
</section>
<section id="define-the-context-and-target-8-examples-in-one-batch" class="level3">
<h3 class="anchored" data-anchor-id="define-the-context-and-target-8-examples-in-one-batch">define the context and target: 8 examples in one batch</h3>
<div id="01664152" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a>x <span class="op">=</span> train_data[:block_size]</span>
<span id="cb16-2"><a href="#cb16-2"></a>y <span class="op">=</span> train_data[<span class="dv">1</span>:block_size<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb16-3"><a href="#cb16-3"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(block_size):</span>
<span id="cb16-4"><a href="#cb16-4"></a>    context <span class="op">=</span> x[:t<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb16-5"><a href="#cb16-5"></a>    target <span class="op">=</span> y[t]</span>
<span id="cb16-6"><a href="#cb16-6"></a>    <span class="bu">print</span>(<span class="ss">f"when input is </span><span class="sc">{</span>context<span class="sc">}</span><span class="ss"> the target: </span><span class="sc">{</span>target<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>when input is tensor([18]) the target: 47
when input is tensor([18, 47]) the target: 56
when input is tensor([18, 47, 56]) the target: 57
when input is tensor([18, 47, 56, 57]) the target: 58
when input is tensor([18, 47, 56, 57, 58]) the target: 1
when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15
when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47
when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58</code></pre>
</div>
</div>
</section>
<section id="define-the-batch-size-and-get-the-batch" class="level3">
<h3 class="anchored" data-anchor-id="define-the-batch-size-and-get-the-batch">define the batch size and get the batch</h3>
<div id="d1a80647" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb18-2"><a href="#cb18-2"></a>batch_size <span class="op">=</span> <span class="dv">4</span> <span class="co"># how many independent sequences will we process in parallel?</span></span>
<span id="cb18-3"><a href="#cb18-3"></a>block_size <span class="op">=</span> <span class="dv">8</span> <span class="co"># what is the maximum context length for predictions?</span></span>
<span id="cb18-4"><a href="#cb18-4"></a></span>
<span id="cb18-5"><a href="#cb18-5"></a><span class="kw">def</span> get_batch(split):</span>
<span id="cb18-6"><a href="#cb18-6"></a>    <span class="co"># generate a small batch of data of inputs x and targets y</span></span>
<span id="cb18-7"><a href="#cb18-7"></a>    data <span class="op">=</span> train_data <span class="cf">if</span> split <span class="op">==</span> <span class="st">'train'</span> <span class="cf">else</span> val_data</span>
<span id="cb18-8"><a href="#cb18-8"></a>    ix <span class="op">=</span> torch.randint(<span class="bu">len</span>(data) <span class="op">-</span> block_size, (batch_size,))</span>
<span id="cb18-9"><a href="#cb18-9"></a>    x <span class="op">=</span> torch.stack([data[i:i<span class="op">+</span>block_size] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb18-10"><a href="#cb18-10"></a>    y <span class="op">=</span> torch.stack([data[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span>block_size<span class="op">+</span><span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb18-11"><a href="#cb18-11"></a>    <span class="cf">return</span> x, y</span>
<span id="cb18-12"><a href="#cb18-12"></a></span>
<span id="cb18-13"><a href="#cb18-13"></a>xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb18-14"><a href="#cb18-14"></a><span class="bu">print</span>(<span class="st">'inputs:'</span>)</span>
<span id="cb18-15"><a href="#cb18-15"></a><span class="bu">print</span>(xb.shape)</span>
<span id="cb18-16"><a href="#cb18-16"></a><span class="bu">print</span>(xb)</span>
<span id="cb18-17"><a href="#cb18-17"></a><span class="bu">print</span>(<span class="st">'targets:'</span>)</span>
<span id="cb18-18"><a href="#cb18-18"></a><span class="bu">print</span>(yb.shape)</span>
<span id="cb18-19"><a href="#cb18-19"></a><span class="bu">print</span>(yb)</span>
<span id="cb18-20"><a href="#cb18-20"></a></span>
<span id="cb18-21"><a href="#cb18-21"></a><span class="bu">print</span>(<span class="st">'----'</span>)</span>
<span id="cb18-22"><a href="#cb18-22"></a></span>
<span id="cb18-23"><a href="#cb18-23"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(batch_size): <span class="co"># batch dimension</span></span>
<span id="cb18-24"><a href="#cb18-24"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(block_size): <span class="co"># time dimension</span></span>
<span id="cb18-25"><a href="#cb18-25"></a>        context <span class="op">=</span> xb[b, :t<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb18-26"><a href="#cb18-26"></a>        target <span class="op">=</span> yb[b,t]</span>
<span id="cb18-27"><a href="#cb18-27"></a>        <span class="bu">print</span>(<span class="ss">f"when input is </span><span class="sc">{</span>context<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss"> the target: </span><span class="sc">{</span>target<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>inputs:
torch.Size([4, 8])
tensor([[24, 43, 58,  5, 57,  1, 46, 43],
        [44, 53, 56,  1, 58, 46, 39, 58],
        [52, 58,  1, 58, 46, 39, 58,  1],
        [25, 17, 27, 10,  0, 21,  1, 54]])
targets:
torch.Size([4, 8])
tensor([[43, 58,  5, 57,  1, 46, 43, 39],
        [53, 56,  1, 58, 46, 39, 58,  1],
        [58,  1, 58, 46, 39, 58,  1, 46],
        [17, 27, 10,  0, 21,  1, 54, 39]])
----
when input is [24] the target: 43
when input is [24, 43] the target: 58
when input is [24, 43, 58] the target: 5
when input is [24, 43, 58, 5] the target: 57
when input is [24, 43, 58, 5, 57] the target: 1
when input is [24, 43, 58, 5, 57, 1] the target: 46
when input is [24, 43, 58, 5, 57, 1, 46] the target: 43
when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39
when input is [44] the target: 53
when input is [44, 53] the target: 56
when input is [44, 53, 56] the target: 1
when input is [44, 53, 56, 1] the target: 58
when input is [44, 53, 56, 1, 58] the target: 46
when input is [44, 53, 56, 1, 58, 46] the target: 39
when input is [44, 53, 56, 1, 58, 46, 39] the target: 58
when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1
when input is [52] the target: 58
when input is [52, 58] the target: 1
when input is [52, 58, 1] the target: 58
when input is [52, 58, 1, 58] the target: 46
when input is [52, 58, 1, 58, 46] the target: 39
when input is [52, 58, 1, 58, 46, 39] the target: 58
when input is [52, 58, 1, 58, 46, 39, 58] the target: 1
when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46
when input is [25] the target: 17
when input is [25, 17] the target: 27
when input is [25, 17, 27] the target: 10
when input is [25, 17, 27, 10] the target: 0
when input is [25, 17, 27, 10, 0] the target: 21
when input is [25, 17, 27, 10, 0, 21] the target: 1
when input is [25, 17, 27, 10, 0, 21, 1] the target: 54
when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39</code></pre>
</div>
</div>
</section>
<section id="start-with-a-simple-model-the-bigram-language-model" class="level3">
<h3 class="anchored" data-anchor-id="start-with-a-simple-model-the-bigram-language-model">start with a simple model: the bigram language model</h3>
<div id="a8d1ed22" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a><span class="co"># define the bigram language model</span></span>
<span id="cb20-2"><a href="#cb20-2"></a><span class="im">import</span> torch</span>
<span id="cb20-3"><a href="#cb20-3"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb20-4"><a href="#cb20-4"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb20-5"><a href="#cb20-5"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb20-6"><a href="#cb20-6"></a></span>
<span id="cb20-7"><a href="#cb20-7"></a><span class="kw">class</span> BigramLanguageModel(nn.Module):</span>
<span id="cb20-8"><a href="#cb20-8"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size):</span>
<span id="cb20-9"><a href="#cb20-9"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb20-10"><a href="#cb20-10"></a>        <span class="va">self</span>.token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, vocab_size)</span>
<span id="cb20-11"><a href="#cb20-11"></a></span>
<span id="cb20-12"><a href="#cb20-12"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb20-13"><a href="#cb20-13"></a>        <span class="co"># idx and targets are both (B,T) tensor of integers</span></span>
<span id="cb20-14"><a href="#cb20-14"></a>        logits <span class="op">=</span> <span class="va">self</span>.token_embedding_table(idx) <span class="co"># (B,T,C)</span></span>
<span id="cb20-15"><a href="#cb20-15"></a></span>
<span id="cb20-16"><a href="#cb20-16"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb20-17"><a href="#cb20-17"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb20-18"><a href="#cb20-18"></a>        <span class="cf">else</span>:</span>
<span id="cb20-19"><a href="#cb20-19"></a>            B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb20-20"><a href="#cb20-20"></a>            logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C)</span>
<span id="cb20-21"><a href="#cb20-21"></a>            targets <span class="op">=</span> targets.view(B<span class="op">*</span>T)</span>
<span id="cb20-22"><a href="#cb20-22"></a>            loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb20-23"><a href="#cb20-23"></a></span>
<span id="cb20-24"><a href="#cb20-24"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb20-25"><a href="#cb20-25"></a></span>
<span id="cb20-26"><a href="#cb20-26"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens):</span>
<span id="cb20-27"><a href="#cb20-27"></a>        <span class="co"># idx is (B, T) array of indices in the current context</span></span>
<span id="cb20-28"><a href="#cb20-28"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb20-29"><a href="#cb20-29"></a>            <span class="co"># get the predictions</span></span>
<span id="cb20-30"><a href="#cb20-30"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>(idx)</span>
<span id="cb20-31"><a href="#cb20-31"></a>            <span class="co"># focus only on the last time step</span></span>
<span id="cb20-32"><a href="#cb20-32"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="co"># becomes (B, C)</span></span>
<span id="cb20-33"><a href="#cb20-33"></a>            <span class="co"># apply softmax to get probabilities</span></span>
<span id="cb20-34"><a href="#cb20-34"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># (B, C)</span></span>
<span id="cb20-35"><a href="#cb20-35"></a>            <span class="co"># sample from the distribution</span></span>
<span id="cb20-36"><a href="#cb20-36"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, 1)</span></span>
<span id="cb20-37"><a href="#cb20-37"></a>            <span class="co"># append sampled index to the running sequence</span></span>
<span id="cb20-38"><a href="#cb20-38"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, T+1)</span></span>
<span id="cb20-39"><a href="#cb20-39"></a>        <span class="cf">return</span> idx</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="cross-entropy-loss" class="level3">
<h3 class="anchored" data-anchor-id="cross-entropy-loss">cross entropy loss</h3>
<p>Loss = <span class="math inline">-\sum_{i}(y_i * \log(p_i))</span>x</p>
<p>where:</p>
<p><span class="math inline">y_i</span> = actual probability (0 or 1 for the <span class="math inline">i</span>-th class) <span class="math inline">p_i</span> = predicted probability for the <span class="math inline">i</span>-th class <span class="math inline">\sum</span> = sum over all classes (characters)</p>
<p>This is the loss for a single token prediction. The total loss reported by F.cross_entropy is the average loss across all B*T tokens in the batch, where:</p>
<p>B = batch_size T = block_size (sequence length)</p>
<p>Before training, we would expect the model to predict the next character from a uniform distribution (random guessing). The probability for the correct character would be <span class="math inline">1 / \text{vocab_size}</span>.</p>
<p>Expected initial loss <span class="math inline">\approx - \log(1 / \text{vocab_size}) = \log(\text{vocab_size})</span> <span class="math inline">\log(65) \approx 4.1744</span></p>
</section>
<section id="initialize-the-model-and-compute-the-loss" class="level3">
<h3 class="anchored" data-anchor-id="initialize-the-model-and-compute-the-loss">initialize the model and compute the loss</h3>
<div id="173be945" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>m <span class="op">=</span> BigramLanguageModel(vocab_size)</span>
<span id="cb21-2"><a href="#cb21-2"></a>logits, loss <span class="op">=</span> m(xb, yb) <span class="co"># xb/yb are from the previous cell (B=4, T=8)</span></span>
<span id="cb21-3"><a href="#cb21-3"></a><span class="bu">print</span>(logits.shape) <span class="co"># Expected: (B, T, C) = (4, 8, 65)</span></span>
<span id="cb21-4"><a href="#cb21-4"></a><span class="bu">print</span>(loss) <span class="co"># Expected: Around 4.17</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([32, 65])
tensor(4.8786, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
</section>
<section id="generate-text" class="level3">
<h3 class="anchored" data-anchor-id="generate-text">generate text</h3>
<div id="75f78621" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a><span class="bu">print</span>(decode(m.generate(idx <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>), max_new_tokens<span class="op">=</span><span class="dv">100</span>)[<span class="dv">0</span>].tolist()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp
wnYWmnxKWWev-tDqXErVKLgJ</code></pre>
</div>
</div>
</section>
<section id="choose-adamw-as-the-optimizer" class="level3">
<h3 class="anchored" data-anchor-id="choose-adamw-as-the-optimizer">choose AdamW as the optimizer</h3>
<div id="38dc5e3d" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(m.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="train-the-model" class="level3">
<h3 class="anchored" data-anchor-id="train-the-model">train the model</h3>
<div id="f071e081" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span> <span class="co"># Redefine batch size for training</span></span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="cf">for</span> steps <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>): <span class="co"># # sample a batch of data</span></span>
<span id="cb26-3"><a href="#cb26-3"></a>    xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb26-4"><a href="#cb26-4"></a></span>
<span id="cb26-5"><a href="#cb26-5"></a>    <span class="co"># evaluate the loss</span></span>
<span id="cb26-6"><a href="#cb26-6"></a>    logits, loss <span class="op">=</span> m(xb, yb)</span>
<span id="cb26-7"><a href="#cb26-7"></a>    optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb26-8"><a href="#cb26-8"></a>    loss.backward()</span>
<span id="cb26-9"><a href="#cb26-9"></a>    optimizer.step()</span>
<span id="cb26-10"><a href="#cb26-10"></a></span>
<span id="cb26-11"><a href="#cb26-11"></a><span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>4.65630578994751</code></pre>
</div>
</div>
</section>
<section id="generate-text-starting-with-0n-as-initial-context" class="level3">
<h3 class="anchored" data-anchor-id="generate-text-starting-with-0n-as-initial-context">generate text starting with 0=<code>\n</code> as initial context</h3>
<div id="cffc0479" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a><span class="bu">print</span>(decode(m.generate(idx <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>), max_new_tokens<span class="op">=</span><span class="dv">500</span>)[<span class="dv">0</span>].tolist()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
oTo.JUZ!!zqe!
xBP qbs$Gy'AcOmrLwwt
p$x;Seh-onQbfM?OjKbn'NwUAW -Np3fkz$FVwAUEa-wzWC -wQo-R!v -Mj?,SPiTyZ;o-opr$mOiPJEYD-CfigkzD3p3?zvS;ADz;.y?o,ivCuC'zqHxcVT cHA
rT'Fd,SBMZyOslg!NXeF$sBe,juUzLq?w-wzP-h
ERjjxlgJzPbHxf$ q,q,KCDCU fqBOQT
SV&amp;CW:xSVwZv'DG'NSPypDhKStKzC -$hslxIVzoivnp ,ethA:NCCGoi
tN!ljjP3fwJMwNelgUzzPGJlgihJ!d?q.d
pSPYgCuCJrIFtb
jQXg
pA.P LP,SPJi
DBcuBM:CixjJ$Jzkq,OLf3KLQLMGph$O 3DfiPHnXKuHMlyjxEiyZib3FaHV-oJa!zoc'XSP :CKGUhd?lgCOF$;;DTHZMlvvcmZAm;:iv'MMgO&amp;Ywbc;BLCUd&amp;vZINLIzkuTGZa
D.?</code></pre>
</div>
</div>
</section>
<section id="the-mathematical-trick-in-self-attention" class="level3">
<h3 class="anchored" data-anchor-id="the-mathematical-trick-in-self-attention">The mathematical trick in self-attention</h3>
<p>toy example illustrating how matrix multiplication can be used for a “weighted aggregation”</p>
<div id="8b585ce4" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb30-2"><a href="#cb30-2"></a>a <span class="op">=</span> torch.tril(torch.ones(<span class="dv">3</span>, <span class="dv">3</span>)) <span class="co"># Lower triangular matrix of 1s</span></span>
<span id="cb30-3"><a href="#cb30-3"></a>a <span class="op">=</span> a <span class="op">/</span> torch.<span class="bu">sum</span>(a, <span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># Normalize rows to sum to 1</span></span>
<span id="cb30-4"><a href="#cb30-4"></a>b <span class="op">=</span> torch.randint(<span class="dv">0</span>,<span class="dv">10</span>,(<span class="dv">3</span>,<span class="dv">2</span>)).<span class="bu">float</span>() <span class="co"># Some data</span></span>
<span id="cb30-5"><a href="#cb30-5"></a>c <span class="op">=</span> a <span class="op">@</span> b <span class="co"># Matrix multiply</span></span>
<span id="cb30-6"><a href="#cb30-6"></a><span class="bu">print</span>(<span class="st">'a='</span>)</span>
<span id="cb30-7"><a href="#cb30-7"></a><span class="bu">print</span>(a)</span>
<span id="cb30-8"><a href="#cb30-8"></a><span class="bu">print</span>(<span class="st">'--'</span>)</span>
<span id="cb30-9"><a href="#cb30-9"></a><span class="bu">print</span>(<span class="st">'b='</span>)</span>
<span id="cb30-10"><a href="#cb30-10"></a><span class="bu">print</span>(b)</span>
<span id="cb30-11"><a href="#cb30-11"></a><span class="bu">print</span>(<span class="st">'--'</span>)</span>
<span id="cb30-12"><a href="#cb30-12"></a><span class="bu">print</span>(<span class="st">'c='</span>)</span>
<span id="cb30-13"><a href="#cb30-13"></a><span class="bu">print</span>(c)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>a=
tensor([[1.0000, 0.0000, 0.0000],
        [0.5000, 0.5000, 0.0000],
        [0.3333, 0.3333, 0.3333]])
--
b=
tensor([[2., 7.],
        [6., 4.],
        [6., 5.]])
--
c=
tensor([[2.0000, 7.0000],
        [4.0000, 5.5000],
        [4.6667, 5.3333]])</code></pre>
</div>
</div>
<div id="e5dcfd8f" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1"></a><span class="co"># consider the following toy example:</span></span>
<span id="cb32-2"><a href="#cb32-2"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb32-3"><a href="#cb32-3"></a>B,T,C <span class="op">=</span> <span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">2</span> <span class="co"># batch, time, channels</span></span>
<span id="cb32-4"><a href="#cb32-4"></a>x <span class="op">=</span> torch.randn(B,T,C)</span>
<span id="cb32-5"><a href="#cb32-5"></a>x.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>torch.Size([4, 8, 2])</code></pre>
</div>
</div>
</section>
<section id="version-1-using-a-for-loop-to-compute-the-weighted-aggregation" class="level3">
<h3 class="anchored" data-anchor-id="version-1-using-a-for-loop-to-compute-the-weighted-aggregation">version 1: using a for loop to compute the weighted aggregation</h3>
<div id="25bd55f8" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1"></a><span class="co"># We want x[b,t] = mean_{i&lt;=t} x[b,i]</span></span>
<span id="cb34-2"><a href="#cb34-2"></a>xbow <span class="op">=</span> torch.zeros((B,T,C)) <span class="co"># x bag-of-words (running average)</span></span>
<span id="cb34-3"><a href="#cb34-3"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb34-4"><a href="#cb34-4"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb34-5"><a href="#cb34-5"></a>        xprev <span class="op">=</span> x[b,:t<span class="op">+</span><span class="dv">1</span>] <span class="co"># Select vectors from start up to time t: shape (t+1, C)</span></span>
<span id="cb34-6"><a href="#cb34-6"></a>        xbow[b,t] <span class="op">=</span> torch.mean(xprev, <span class="dv">0</span>) <span class="co"># Compute mean along the time dimension (dim 0)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="version-2-using-matrix-multiply-for-a-weighted-aggregation" class="level3">
<h3 class="anchored" data-anchor-id="version-2-using-matrix-multiply-for-a-weighted-aggregation">version 2: using matrix multiply for a weighted aggregation</h3>
<div id="b640bb4b" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1"></a><span class="co"># Create the averaging weight matrix</span></span>
<span id="cb35-2"><a href="#cb35-2"></a>wei <span class="op">=</span> torch.tril(torch.ones(T, T))</span>
<span id="cb35-3"><a href="#cb35-3"></a>wei <span class="op">=</span> wei <span class="op">/</span> wei.<span class="bu">sum</span>(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># Normalize rows to sum to 1</span></span>
<span id="cb35-4"><a href="#cb35-4"></a><span class="co"># Perform batched matrix multiplication</span></span>
<span id="cb35-5"><a href="#cb35-5"></a>xbow2 <span class="op">=</span> wei <span class="op">@</span> x <span class="co"># (T, T) @ (B, T, C) -&gt; (B, T, C) via broadcasting</span></span>
<span id="cb35-6"><a href="#cb35-6"></a>torch.allclose(xbow, xbow2) <span class="co"># Check if results are identical</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>True</code></pre>
</div>
</div>
</section>
<section id="version-3-use-softmax" class="level3">
<h3 class="anchored" data-anchor-id="version-3-use-softmax">version 3: use Softmax</h3>
<div id="cd0536c4" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1"></a>tril <span class="op">=</span> torch.tril(torch.ones(T, T))</span>
<span id="cb37-2"><a href="#cb37-2"></a>wei <span class="op">=</span> torch.zeros((T,T))</span>
<span id="cb37-3"><a href="#cb37-3"></a><span class="co"># Mask out future positions by setting them to -infinity before softmax</span></span>
<span id="cb37-4"><a href="#cb37-4"></a>wei <span class="op">=</span> wei.masked_fill(tril <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb37-5"><a href="#cb37-5"></a><span class="co"># Apply softmax to get row-wise probability distributions (weights)</span></span>
<span id="cb37-6"><a href="#cb37-6"></a>wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb37-7"><a href="#cb37-7"></a><span class="co"># Perform weighted aggregation</span></span>
<span id="cb37-8"><a href="#cb37-8"></a>xbow3 <span class="op">=</span> wei <span class="op">@</span> x</span>
<span id="cb37-9"><a href="#cb37-9"></a>torch.allclose(xbow, xbow3) <span class="co"># Check if results are identical</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>True</code></pre>
</div>
</div>
</section>
<section id="softmax-function" class="level3">
<h3 class="anchored" data-anchor-id="softmax-function">softmax function</h3>
<p>softmax(<span class="math inline">z_i</span>) = <span class="math inline">\frac{e^{z_i}}{\sum_j e^{z_j}}</span></p>
</section>
<section id="version-4-self-attention" class="level3">
<h3 class="anchored" data-anchor-id="version-4-self-attention">version 4: self-attention</h3>
<div id="bc387af0" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb39-2"><a href="#cb39-2"></a>B,T,C <span class="op">=</span> <span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">32</span> <span class="co"># batch, time, channels (embedding dimension)</span></span>
<span id="cb39-3"><a href="#cb39-3"></a>x <span class="op">=</span> torch.randn(B,T,C)</span>
<span id="cb39-4"><a href="#cb39-4"></a></span>
<span id="cb39-5"><a href="#cb39-5"></a><span class="co"># let's see a single Head perform self-attention</span></span>
<span id="cb39-6"><a href="#cb39-6"></a>head_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb39-7"><a href="#cb39-7"></a>key <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb39-8"><a href="#cb39-8"></a>query <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb39-9"><a href="#cb39-9"></a>value <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb39-10"><a href="#cb39-10"></a>k <span class="op">=</span> key(x)   <span class="co"># (B, T, head_size)</span></span>
<span id="cb39-11"><a href="#cb39-11"></a>q <span class="op">=</span> query(x) <span class="co"># (B, T, head_size)</span></span>
<span id="cb39-12"><a href="#cb39-12"></a><span class="co"># Compute attention scores ("affinities")</span></span>
<span id="cb39-13"><a href="#cb39-13"></a>wei <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="co"># (B, T, hs) @ (B, hs, T) ---&gt; (B, T, T)</span></span>
<span id="cb39-14"><a href="#cb39-14"></a></span>
<span id="cb39-15"><a href="#cb39-15"></a><span class="co"># Scale the scores</span></span>
<span id="cb39-16"><a href="#cb39-16"></a><span class="co"># Note: Karpathy uses C**-0.5 here (sqrt(embedding_dim)). Standard Transformer uses sqrt(head_size).</span></span>
<span id="cb39-17"><a href="#cb39-17"></a>wei <span class="op">=</span> wei <span class="op">*</span> (C<span class="op">**-</span><span class="fl">0.5</span>)</span>
<span id="cb39-18"><a href="#cb39-18"></a></span>
<span id="cb39-19"><a href="#cb39-19"></a><span class="co"># Apply causal mask</span></span>
<span id="cb39-20"><a href="#cb39-20"></a>tril <span class="op">=</span> torch.tril(torch.ones(T, T))</span>
<span id="cb39-21"><a href="#cb39-21"></a><span class="co">#wei = torch.zeros((T,T)) # This line is commented out in original, was from softmax demo</span></span>
<span id="cb39-22"><a href="#cb39-22"></a>wei <span class="op">=</span> wei.masked_fill(tril <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>)) <span class="co"># Mask future tokens</span></span>
<span id="cb39-23"><a href="#cb39-23"></a></span>
<span id="cb39-24"><a href="#cb39-24"></a><span class="co"># Apply softmax to get attention weights</span></span>
<span id="cb39-25"><a href="#cb39-25"></a>wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># (B, T, T)</span></span>
<span id="cb39-26"><a href="#cb39-26"></a></span>
<span id="cb39-27"><a href="#cb39-27"></a><span class="co"># Perform weighted aggregation of Values</span></span>
<span id="cb39-28"><a href="#cb39-28"></a>v <span class="op">=</span> value(x) <span class="co"># (B, T, head_size)</span></span>
<span id="cb39-29"><a href="#cb39-29"></a>out <span class="op">=</span> wei <span class="op">@</span> v <span class="co"># (B, T, T) @ (B, T, hs) ---&gt; (B, T, hs)</span></span>
<span id="cb39-30"><a href="#cb39-30"></a><span class="co">#out = wei @ x # This would aggregate original x, not the projected values 'v'</span></span>
<span id="cb39-31"><a href="#cb39-31"></a></span>
<span id="cb39-32"><a href="#cb39-32"></a>out.shape <span class="co"># Expected: (B, T, head_size) = (4, 8, 16)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>torch.Size([4, 8, 16])</code></pre>
</div>
</div>
<div id="64febc53" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1"></a>wei[<span class="dv">0</span>] <span class="co"># Show attention weights for the first sequence in the batch</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.4264, 0.5736, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3151, 0.3022, 0.3827, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.3007, 0.2272, 0.2467, 0.2253, 0.0000, 0.0000, 0.0000, 0.0000],
        [0.1635, 0.2048, 0.1776, 0.1616, 0.2926, 0.0000, 0.0000, 0.0000],
        [0.1403, 0.2272, 0.1454, 0.1244, 0.2678, 0.0949, 0.0000, 0.0000],
        [0.1554, 0.1815, 0.1224, 0.1213, 0.1428, 0.1603, 0.1164, 0.0000],
        [0.0952, 0.1217, 0.1130, 0.1453, 0.1137, 0.1180, 0.1467, 0.1464]],
       grad_fn=&lt;SelectBackward0&gt;)</code></pre>
</div>
</div>
</section>
<section id="check-that-x-xc-is-is-the-correlation-matrix-if-x-is-normalized" class="level3">
<h3 class="anchored" data-anchor-id="check-that-x-xc-is-is-the-correlation-matrix-if-x-is-normalized">Check that X X’/C is is the correlation matrix if X is normalized</h3>
<div class="sourceCode" id="cb43"><pre class="sourceCode numberSource {r showing xx'/C is correlation matrix if X normalized} number-lines code-with-copy"><code class="sourceCode"><span id="cb43-1"><a href="#cb43-1"></a></span>
<span id="cb43-2"><a href="#cb43-2"></a>nC = 64</span>
<span id="cb43-3"><a href="#cb43-3"></a>X = matrix(rnorm(4*64), nrow=4, ncol=nC)</span>
<span id="cb43-4"><a href="#cb43-4"></a>## make it so that the third token is similar to the last one</span>
<span id="cb43-5"><a href="#cb43-5"></a>X[2,] = X[4,]*0.5 + X[2,]*0.5</span>
<span id="cb43-6"><a href="#cb43-6"></a>## normalize X</span>
<span id="cb43-7"><a href="#cb43-7"></a>X = t(scale(t(X)))</span>
<span id="cb43-8"><a href="#cb43-8"></a></span>
<span id="cb43-9"><a href="#cb43-9"></a>q = X</span>
<span id="cb43-10"><a href="#cb43-10"></a>k = X</span>
<span id="cb43-11"><a href="#cb43-11"></a>v = X</span>
<span id="cb43-12"><a href="#cb43-12"></a></span>
<span id="cb43-13"><a href="#cb43-13"></a>qkt = q %*% t(k)/(nC-1)</span>
<span id="cb43-14"><a href="#cb43-14"></a>xcor = cor(t(q),t(k))</span>
<span id="cb43-15"><a href="#cb43-15"></a>dim(xcor)</span>
<span id="cb43-16"><a href="#cb43-16"></a>dim(qkt)</span>
<span id="cb43-17"><a href="#cb43-17"></a>cat("xcor\n")</span>
<span id="cb43-18"><a href="#cb43-18"></a>xcor</span>
<span id="cb43-19"><a href="#cb43-19"></a>cat("---\n qkt\n")</span>
<span id="cb43-20"><a href="#cb43-20"></a>qkt</span>
<span id="cb43-21"><a href="#cb43-21"></a></span>
<span id="cb43-22"><a href="#cb43-22"></a>cat("are xcor and qkt equal?")</span>
<span id="cb43-23"><a href="#cb43-23"></a>all.equal(xcor, qkt)</span>
<span id="cb43-24"><a href="#cb43-24"></a></span>
<span id="cb43-25"><a href="#cb43-25"></a>par(mar=c(5, 6, 4, 2) + 0.1)  # increase left margin to avoid cutting of the y label</span>
<span id="cb43-26"><a href="#cb43-26"></a>par(pty="s")  # Set plot type to "square"</span>
<span id="cb43-27"><a href="#cb43-27"></a>plot(c(xcor), c(qkt),cex=3,cex.lab=3,cex.axis=2,cex.main=2,cex.sub=2); abline(0,1)</span>
<span id="cb43-28"><a href="#cb43-28"></a>par(pty="m")  # Reset to default plot type</span>
<span id="cb43-29"><a href="#cb43-29"></a>par(mar=c(5, 4, 4, 2) + 0.1)  # Reset to default margins</span>
<span id="cb43-30"><a href="#cb43-30"></a></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="notes" class="level3">
<h3 class="anchored" data-anchor-id="notes">Notes:</h3>
<p>Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.</p>
<ul>
<li>There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens. example: “the cat sat on the mat” should be different from “the mat sat on the cat”</li>
<li>Each example across batch dimension is of course processed completely independently and never “talk” to each other.</li>
<li>In an “encoder” attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a “decoder” attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.</li>
<li>“self-attention” just means that the keys and values are produced from the same source as queries (all come from x). In “cross-attention”, the queries still get produced from x, but the keys and values come from some other, external source (e.g.&nbsp;an encoder module)</li>
</ul>
</section>
<section id="why-scaled-attention" class="level3">
<h3 class="anchored" data-anchor-id="why-scaled-attention">why scaled attention?</h3>
<p>“Scaled” attention additionaly divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below</p>
<div id="751f4b08" class="cell" data-execution_count="25">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1"></a><span class="co"># Demonstrate variance without scaling</span></span>
<span id="cb44-2"><a href="#cb44-2"></a>k_unscaled <span class="op">=</span> torch.randn(B,T,head_size)</span>
<span id="cb44-3"><a href="#cb44-3"></a>q_unscaled <span class="op">=</span> torch.randn(B,T,head_size)</span>
<span id="cb44-4"><a href="#cb44-4"></a>wei_unscaled <span class="op">=</span> q_unscaled <span class="op">@</span> k_unscaled.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb44-5"><a href="#cb44-5"></a><span class="bu">print</span>(<span class="ss">f"k var: </span><span class="sc">{</span>k_unscaled<span class="sc">.</span>var()<span class="sc">:.4f}</span><span class="ss">, q var: </span><span class="sc">{</span>q_unscaled<span class="sc">.</span>var()<span class="sc">:.4f}</span><span class="ss">, wei (unscaled) var: </span><span class="sc">{</span>wei_unscaled<span class="sc">.</span>var()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb44-6"><a href="#cb44-6"></a></span>
<span id="cb44-7"><a href="#cb44-7"></a><span class="co"># Demonstrate variance *with* scaling (using head_size for illustration)</span></span>
<span id="cb44-8"><a href="#cb44-8"></a>k <span class="op">=</span> torch.randn(B,T,head_size)</span>
<span id="cb44-9"><a href="#cb44-9"></a>q <span class="op">=</span> torch.randn(B,T,head_size)</span>
<span id="cb44-10"><a href="#cb44-10"></a>wei <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> head_size<span class="op">**-</span><span class="fl">0.5</span> <span class="co"># Scale by sqrt(head_size)</span></span>
<span id="cb44-11"><a href="#cb44-11"></a><span class="bu">print</span>(<span class="ss">f"k var: </span><span class="sc">{</span>k<span class="sc">.</span>var()<span class="sc">:.4f}</span><span class="ss">, q var: </span><span class="sc">{</span>q<span class="sc">.</span>var()<span class="sc">:.4f}</span><span class="ss">, wei (scaled) var: </span><span class="sc">{</span>wei<span class="sc">.</span>var()<span class="sc">:.4f}</span><span class="ss">"</span>) <span class="co"># Variance should be closer to 1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>k var: 1.0449, q var: 1.0700, wei (unscaled) var: 17.4690
k var: 0.9006, q var: 1.0037, wei (scaled) var: 0.9957</code></pre>
</div>
</div>
<div id="7ab86db5" class="cell" data-execution_count="26">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1"></a>k.var() <span class="co"># Should be close to 1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>tensor(0.9006)</code></pre>
</div>
</div>
<div id="79937ef8" class="cell" data-execution_count="27">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1"></a>q.var() <span class="co"># Should be close to 1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="27">
<pre><code>tensor(1.0037)</code></pre>
</div>
</div>
<div id="e21a12c0" class="cell" data-execution_count="28">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1"></a>wei.var() <span class="co"># With scaling, should be closer to 1 than head_size (16)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="28">
<pre><code>tensor(0.9957)</code></pre>
</div>
</div>
<div id="c5184f63" class="cell" data-execution_count="29">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1"></a><span class="co"># Softmax with small inputs (diffuse distribution)</span></span>
<span id="cb52-2"><a href="#cb52-2"></a>torch.softmax(torch.tensor([<span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.5</span>]), dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="29">
<pre><code>tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])</code></pre>
</div>
</div>
<div id="5de133c9" class="cell" data-execution_count="30">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1"></a><span class="co"># Softmax with large inputs (simulating unscaled attention scores) -&gt; peaks</span></span>
<span id="cb54-2"><a href="#cb54-2"></a>torch.softmax(torch.tensor([<span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.5</span>])<span class="op">*</span><span class="dv">8</span>, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># gets too peaky, converges to one-hot</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])</code></pre>
</div>
</div>
</section>
<section id="layernorm1d" class="level3">
<h3 class="anchored" data-anchor-id="layernorm1d">LayerNorm1d</h3>
<div id="5b02faec" class="cell" data-execution_count="31">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1"></a><span class="kw">class</span> LayerNorm1d: <span class="co"># (used to be BatchNorm1d)</span></span>
<span id="cb56-2"><a href="#cb56-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, eps<span class="op">=</span><span class="fl">1e-5</span>, momentum<span class="op">=</span><span class="fl">0.1</span>): <span class="co"># Momentum is not used in typical LayerNorm</span></span>
<span id="cb56-3"><a href="#cb56-3"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb56-4"><a href="#cb56-4"></a>        <span class="co"># Learnable scale and shift parameters, initialized to 1 and 0</span></span>
<span id="cb56-5"><a href="#cb56-5"></a>        <span class="va">self</span>.gamma <span class="op">=</span> torch.ones(dim)</span>
<span id="cb56-6"><a href="#cb56-6"></a>        <span class="va">self</span>.beta <span class="op">=</span> torch.zeros(dim)</span>
<span id="cb56-7"><a href="#cb56-7"></a></span>
<span id="cb56-8"><a href="#cb56-8"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb56-9"><a href="#cb56-9"></a>        <span class="co"># calculate the forward pass</span></span>
<span id="cb56-10"><a href="#cb56-10"></a>        <span class="co"># Calculate mean over the *last* dimension (features/embedding)</span></span>
<span id="cb56-11"><a href="#cb56-11"></a>        xmean <span class="op">=</span> x.mean(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch mean (shape B, 1, C if input B, T, C) --&gt; Needs adjustment for (B,C) input shape here. Assumes input is (B, dim)</span></span>
<span id="cb56-12"><a href="#cb56-12"></a>        <span class="co"># Correction: x is (32, 100). dim=1 is correct for features. Shape (32, 1)</span></span>
<span id="cb56-13"><a href="#cb56-13"></a>        xvar <span class="op">=</span> x.var(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch variance (shape 32, 1)</span></span>
<span id="cb56-14"><a href="#cb56-14"></a>        <span class="co"># Normalize each feature vector independently</span></span>
<span id="cb56-15"><a href="#cb56-15"></a>        xhat <span class="op">=</span> (x <span class="op">-</span> xmean) <span class="op">/</span> torch.sqrt(xvar <span class="op">+</span> <span class="va">self</span>.eps) <span class="co"># normalize to unit variance</span></span>
<span id="cb56-16"><a href="#cb56-16"></a>        <span class="co"># Apply scale and shift</span></span>
<span id="cb56-17"><a href="#cb56-17"></a>        <span class="va">self</span>.out <span class="op">=</span> <span class="va">self</span>.gamma <span class="op">*</span> xhat <span class="op">+</span> <span class="va">self</span>.beta</span>
<span id="cb56-18"><a href="#cb56-18"></a>        <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb56-19"><a href="#cb56-19"></a></span>
<span id="cb56-20"><a href="#cb56-20"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb56-21"><a href="#cb56-21"></a>        <span class="co"># Expose gamma and beta as learnable parameters</span></span>
<span id="cb56-22"><a href="#cb56-22"></a>        <span class="cf">return</span> [<span class="va">self</span>.gamma, <span class="va">self</span>.beta]</span>
<span id="cb56-23"><a href="#cb56-23"></a></span>
<span id="cb56-24"><a href="#cb56-24"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb56-25"><a href="#cb56-25"></a>module <span class="op">=</span> LayerNorm1d(<span class="dv">100</span>) <span class="co"># Create LayerNorm for 100 features</span></span>
<span id="cb56-26"><a href="#cb56-26"></a>x <span class="op">=</span> torch.randn(<span class="dv">32</span>, <span class="dv">100</span>) <span class="co"># batch size 32 of 100-dimensional vectors</span></span>
<span id="cb56-27"><a href="#cb56-27"></a>x <span class="op">=</span> module(x)</span>
<span id="cb56-28"><a href="#cb56-28"></a>x.shape <span class="co"># Should be (32, 100)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>torch.Size([32, 100])</code></pre>
</div>
</div>
<p>Explanation of layernorm</p>
<p>Input shape: (B, T, C) where: B = batch size T = sequence length (number of tokens) C = embedding dimension (features of each token) For each token in the sequence (each position T), LayerNorm: Takes its embedding vector of size C Calculates the mean and standard deviation of just that vector Normalizes that vector by subtracting its mean and dividing by its standard deviation Applies the learnable scale (gamma) and shift (beta) parameters So if you have a sequence like “The cat sat”, and each word is represented by a 64-dimensional embedding vector, LayerNorm would: Take “The”’s 64-dimensional vector and normalize it Take “cat”’s 64-dimensional vector and normalize it Take “sat”’s 64-dimensional vector and normalize it Each token’s vector is normalized independently of the others. This is different from BatchNorm, which would normalize across the batch dimension (i.e., looking at the same position across different examples in the batch). This per-token normalization helps maintain stable gradients during training and is particularly important in Transformers where the attention mechanism needs to work with normalized vectors to compute meaningful attention scores.</p>
<div id="3d5d4112" class="cell" data-execution_count="32">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1"></a><span class="co"># Mean and std of the first feature *across the batch*. Not expected to be 0 and 1.</span></span>
<span id="cb58-2"><a href="#cb58-2"></a>x[:,<span class="dv">0</span>].mean(), x[:,<span class="dv">0</span>].std()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>(tensor(0.1469), tensor(0.8803))</code></pre>
</div>
</div>
<div id="31a3c1fe" class="cell" data-execution_count="33">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1"></a><span class="co"># Mean and std *across features* for the first item in the batch. Expected to be ~0 and ~1.</span></span>
<span id="cb60-2"><a href="#cb60-2"></a>x[<span class="dv">0</span>,:].mean(), x[<span class="dv">0</span>,:].std()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>(tensor(2.3842e-09), tensor(1.0000))</code></pre>
</div>
</div>
</section>
<section id="french-to-english-translation-example" class="level3">
<h3 class="anchored" data-anchor-id="french-to-english-translation-example">French to English translation example:</h3>
<div id="1aea58e6" class="cell" data-execution_count="34">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1"></a><span class="co"># &lt;--------- ENCODE ------------------&gt;&lt;--------------- DECODE -----------------&gt;</span></span>
<span id="cb62-2"><a href="#cb62-2"></a><span class="co"># les réseaux de neurones sont géniaux! &lt;START&gt; neural networks are awesome!&lt;</span><span class="re">END</span><span class="co">&gt;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="full-finished-code-for-reference" class="level3">
<h3 class="anchored" data-anchor-id="full-finished-code-for-reference">Full finished code, for reference</h3>
<div id="83e132d7" class="cell" data-execution_count="35">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1"></a><span class="co"># Import necessary PyTorch modules</span></span>
<span id="cb63-2"><a href="#cb63-2"></a><span class="im">import</span> torch</span>
<span id="cb63-3"><a href="#cb63-3"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb63-4"><a href="#cb63-4"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb63-5"><a href="#cb63-5"></a></span>
<span id="cb63-6"><a href="#cb63-6"></a><span class="co"># ===== HYPERPARAMETERS =====</span></span>
<span id="cb63-7"><a href="#cb63-7"></a>batch_size <span class="op">=</span> <span class="dv">16</span>       <span class="co"># Number of sequences per batch (Smaller than Bigram training)</span></span>
<span id="cb63-8"><a href="#cb63-8"></a>block_size <span class="op">=</span> <span class="dv">32</span>       <span class="co"># Context length (Larger than Bigram demo)</span></span>
<span id="cb63-9"><a href="#cb63-9"></a>max_iters <span class="op">=</span> <span class="dv">5000</span>      <span class="co"># Total training iterations (More substantial training) </span><span class="al">TODO</span><span class="co"> change to 5000 later</span></span>
<span id="cb63-10"><a href="#cb63-10"></a>eval_interval <span class="op">=</span> <span class="dv">100</span>   <span class="co"># How often to check validation loss</span></span>
<span id="cb63-11"><a href="#cb63-11"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span>  <span class="co"># Optimizer learning rate</span></span>
<span id="cb63-12"><a href="#cb63-12"></a>eval_iters <span class="op">=</span> <span class="dv">200</span>      <span class="co"># Number of batches to average for validation loss estimate</span></span>
<span id="cb63-13"><a href="#cb63-13"></a>n_embd <span class="op">=</span> <span class="dv">64</span>           <span class="co"># Embedding dimension (Size of token vectors)</span></span>
<span id="cb63-14"><a href="#cb63-14"></a>n_head <span class="op">=</span> <span class="dv">4</span>            <span class="co"># Number of attention heads</span></span>
<span id="cb63-15"><a href="#cb63-15"></a>n_layer <span class="op">=</span> <span class="dv">4</span>           <span class="co"># Number of Transformer blocks (layers)</span></span>
<span id="cb63-16"><a href="#cb63-16"></a>dropout <span class="op">=</span> <span class="fl">0.0</span>         <span class="co"># Dropout probability (0.0 means no dropout here)</span></span>
<span id="cb63-17"><a href="#cb63-17"></a><span class="co"># ==========================</span></span>
<span id="cb63-18"><a href="#cb63-18"></a></span>
<span id="cb63-19"><a href="#cb63-19"></a><span class="co"># Device selection: MPS (Apple Silicon) &gt; CUDA &gt; CPU</span></span>
<span id="cb63-20"><a href="#cb63-20"></a><span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb63-21"><a href="#cb63-21"></a>    device <span class="op">=</span> torch.device(<span class="st">"mps"</span>)   <span class="co"># Apple Silicon GPU</span></span>
<span id="cb63-22"><a href="#cb63-22"></a><span class="cf">elif</span> torch.cuda.is_available():</span>
<span id="cb63-23"><a href="#cb63-23"></a>    device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)  <span class="co"># NVIDIA GPU</span></span>
<span id="cb63-24"><a href="#cb63-24"></a><span class="cf">else</span>:</span>
<span id="cb63-25"><a href="#cb63-25"></a>    device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)   <span class="co"># CPU fallback</span></span>
<span id="cb63-26"><a href="#cb63-26"></a><span class="bu">print</span>(<span class="ss">f"Using device: </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb63-27"><a href="#cb63-27"></a></span>
<span id="cb63-28"><a href="#cb63-28"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb63-29"><a href="#cb63-29"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb63-30"><a href="#cb63-30"></a><span class="cf">if</span> device.<span class="bu">type</span> <span class="op">==</span> <span class="st">'cuda'</span>:</span>
<span id="cb63-31"><a href="#cb63-31"></a>    torch.cuda.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb63-32"><a href="#cb63-32"></a><span class="cf">elif</span> device.<span class="bu">type</span> <span class="op">==</span> <span class="st">'mps'</span>:</span>
<span id="cb63-33"><a href="#cb63-33"></a>    torch.mps.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb63-34"><a href="#cb63-34"></a></span>
<span id="cb63-35"><a href="#cb63-35"></a><span class="co"># Load and read the training text (assuming input.txt is available)</span></span>
<span id="cb63-36"><a href="#cb63-36"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'input.txt'</span>, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> f:</span>
<span id="cb63-37"><a href="#cb63-37"></a>    text <span class="op">=</span> f.read()</span>
<span id="cb63-38"><a href="#cb63-38"></a></span>
<span id="cb63-39"><a href="#cb63-39"></a><span class="co"># ===== DATA PREPROCESSING =====</span></span>
<span id="cb63-40"><a href="#cb63-40"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(text)))</span>
<span id="cb63-41"><a href="#cb63-41"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(chars)</span>
<span id="cb63-42"><a href="#cb63-42"></a>stoi <span class="op">=</span> { ch:i <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }   <span class="co"># string to index</span></span>
<span id="cb63-43"><a href="#cb63-43"></a>itos <span class="op">=</span> { i:ch <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }   <span class="co"># index to string</span></span>
<span id="cb63-44"><a href="#cb63-44"></a>encode <span class="op">=</span> <span class="kw">lambda</span> s: [stoi[c] <span class="cf">for</span> c <span class="kw">in</span> s]   <span class="co"># convert string to list of integers</span></span>
<span id="cb63-45"><a href="#cb63-45"></a>decode <span class="op">=</span> <span class="kw">lambda</span> l: <span class="st">''</span>.join([itos[i] <span class="cf">for</span> i <span class="kw">in</span> l])   <span class="co"># convert list of integers to string</span></span>
<span id="cb63-46"><a href="#cb63-46"></a></span>
<span id="cb63-47"><a href="#cb63-47"></a><span class="co"># Split data into training and validation sets</span></span>
<span id="cb63-48"><a href="#cb63-48"></a>data <span class="op">=</span> torch.tensor(encode(text), dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb63-49"><a href="#cb63-49"></a>n <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span><span class="op">*</span><span class="bu">len</span>(data))   <span class="co"># first 90% for training</span></span>
<span id="cb63-50"><a href="#cb63-50"></a>train_data <span class="op">=</span> data[:n]</span>
<span id="cb63-51"><a href="#cb63-51"></a>val_data <span class="op">=</span> data[n:]</span>
<span id="cb63-52"><a href="#cb63-52"></a><span class="co"># =============================</span></span>
<span id="cb63-53"><a href="#cb63-53"></a></span>
<span id="cb63-54"><a href="#cb63-54"></a><span class="co"># ===== DATA LOADING FUNCTION =====</span></span>
<span id="cb63-55"><a href="#cb63-55"></a><span class="kw">def</span> get_batch(split):</span>
<span id="cb63-56"><a href="#cb63-56"></a>    <span class="co">"""Generate a batch of data for training or validation."""</span></span>
<span id="cb63-57"><a href="#cb63-57"></a>    data <span class="op">=</span> train_data <span class="cf">if</span> split <span class="op">==</span> <span class="st">'train'</span> <span class="cf">else</span> val_data</span>
<span id="cb63-58"><a href="#cb63-58"></a>    ix <span class="op">=</span> torch.randint(<span class="bu">len</span>(data) <span class="op">-</span> block_size, (batch_size,))</span>
<span id="cb63-59"><a href="#cb63-59"></a>    x <span class="op">=</span> torch.stack([data[i:i<span class="op">+</span>block_size] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb63-60"><a href="#cb63-60"></a>    y <span class="op">=</span> torch.stack([data[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span>block_size<span class="op">+</span><span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb63-61"><a href="#cb63-61"></a>    x, y <span class="op">=</span> x.to(device), y.to(device) <span class="co"># Move data to the target device</span></span>
<span id="cb63-62"><a href="#cb63-62"></a>    <span class="cf">return</span> x, y</span>
<span id="cb63-63"><a href="#cb63-63"></a><span class="co"># ================================</span></span>
<span id="cb63-64"><a href="#cb63-64"></a></span>
<span id="cb63-65"><a href="#cb63-65"></a><span class="co"># ===== LOSS ESTIMATION FUNCTION =====</span></span>
<span id="cb63-66"><a href="#cb63-66"></a><span class="at">@torch.no_grad</span>()   <span class="co"># Disable gradient calculation for efficiency</span></span>
<span id="cb63-67"><a href="#cb63-67"></a><span class="kw">def</span> estimate_loss():</span>
<span id="cb63-68"><a href="#cb63-68"></a>    <span class="co">"""Estimate the loss on training and validation sets."""</span></span>
<span id="cb63-69"><a href="#cb63-69"></a>    out <span class="op">=</span> {}</span>
<span id="cb63-70"><a href="#cb63-70"></a>    model.<span class="bu">eval</span>()   <span class="co"># Set model to evaluation mode</span></span>
<span id="cb63-71"><a href="#cb63-71"></a>    <span class="cf">for</span> split <span class="kw">in</span> [<span class="st">'train'</span>, <span class="st">'val'</span>]:</span>
<span id="cb63-72"><a href="#cb63-72"></a>        losses <span class="op">=</span> torch.zeros(eval_iters)</span>
<span id="cb63-73"><a href="#cb63-73"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(eval_iters):</span>
<span id="cb63-74"><a href="#cb63-74"></a>            X, Y <span class="op">=</span> get_batch(split)</span>
<span id="cb63-75"><a href="#cb63-75"></a>            logits, loss <span class="op">=</span> model(X, Y)</span>
<span id="cb63-76"><a href="#cb63-76"></a>            losses[k] <span class="op">=</span> loss.item()</span>
<span id="cb63-77"><a href="#cb63-77"></a>        out[split] <span class="op">=</span> losses.mean()</span>
<span id="cb63-78"><a href="#cb63-78"></a>    model.train()  <span class="co"># Set model back to training mode</span></span>
<span id="cb63-79"><a href="#cb63-79"></a>    <span class="cf">return</span> out</span>
<span id="cb63-80"><a href="#cb63-80"></a><span class="co"># ===================================</span></span>
<span id="cb63-81"><a href="#cb63-81"></a></span>
<span id="cb63-82"><a href="#cb63-82"></a><span class="co"># ===== </span><span class="al">ATTENTION</span><span class="co"> HEAD IMPLEMENTATION =====</span></span>
<span id="cb63-83"><a href="#cb63-83"></a><span class="kw">class</span> Head(nn.Module):</span>
<span id="cb63-84"><a href="#cb63-84"></a>    <span class="co">"""Single head of self-attention."""</span></span>
<span id="cb63-85"><a href="#cb63-85"></a>    </span>
<span id="cb63-86"><a href="#cb63-86"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, head_size):</span>
<span id="cb63-87"><a href="#cb63-87"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb63-88"><a href="#cb63-88"></a>        <span class="co"># Linear projections for Key, Query, Value</span></span>
<span id="cb63-89"><a href="#cb63-89"></a>        <span class="va">self</span>.key <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb63-90"><a href="#cb63-90"></a>        <span class="va">self</span>.query <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb63-91"><a href="#cb63-91"></a>        <span class="va">self</span>.value <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb63-92"><a href="#cb63-92"></a>        <span class="co"># Causal mask (tril). 'register_buffer' makes it part of the model state but not a parameter to be trained.</span></span>
<span id="cb63-93"><a href="#cb63-93"></a>        <span class="va">self</span>.register_buffer(<span class="st">'tril'</span>, torch.tril(torch.ones(block_size, block_size)))</span>
<span id="cb63-94"><a href="#cb63-94"></a>        <span class="co"># Dropout layer (applied after softmax)</span></span>
<span id="cb63-95"><a href="#cb63-95"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb63-96"><a href="#cb63-96"></a></span>
<span id="cb63-97"><a href="#cb63-97"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb63-98"><a href="#cb63-98"></a>        B,T,C <span class="op">=</span> x.shape <span class="co"># C here is n_embd</span></span>
<span id="cb63-99"><a href="#cb63-99"></a>        <span class="co"># Project input to K, Q, V</span></span>
<span id="cb63-100"><a href="#cb63-100"></a>        k <span class="op">=</span> <span class="va">self</span>.key(x)   <span class="co"># (B,T,head_size)</span></span>
<span id="cb63-101"><a href="#cb63-101"></a>        q <span class="op">=</span> <span class="va">self</span>.query(x) <span class="co"># (B,T,head_size)</span></span>
<span id="cb63-102"><a href="#cb63-102"></a>        <span class="co"># Compute attention scores, scale, mask, softmax</span></span>
<span id="cb63-103"><a href="#cb63-103"></a>        <span class="co"># Note the scaling by C**-0.5 (sqrt(n_embd)) as discussed before</span></span>
<span id="cb63-104"><a href="#cb63-104"></a>        wei <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> C<span class="op">**-</span><span class="fl">0.5</span>   <span class="co"># (B, T, T)</span></span>
<span id="cb63-105"><a href="#cb63-105"></a>        wei <span class="op">=</span> wei.masked_fill(<span class="va">self</span>.tril[:T, :T] <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))   <span class="co"># Use dynamic slicing [:T, :T] for flexibility if T &lt; block_size</span></span>
<span id="cb63-106"><a href="#cb63-106"></a>        wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>)   <span class="co"># (B, T, T)</span></span>
<span id="cb63-107"><a href="#cb63-107"></a>        wei <span class="op">=</span> <span class="va">self</span>.dropout(wei) <span class="co"># Apply dropout to attention weights</span></span>
<span id="cb63-108"><a href="#cb63-108"></a>        <span class="co"># Weighted aggregation of values</span></span>
<span id="cb63-109"><a href="#cb63-109"></a>        v <span class="op">=</span> <span class="va">self</span>.value(x) <span class="co"># (B,T,head_size)</span></span>
<span id="cb63-110"><a href="#cb63-110"></a>        out <span class="op">=</span> wei <span class="op">@</span> v <span class="co"># (B, T, T) @ (B, T, head_size) -&gt; (B, T, head_size)</span></span>
<span id="cb63-111"><a href="#cb63-111"></a>        <span class="cf">return</span> out</span>
<span id="cb63-112"><a href="#cb63-112"></a><span class="co"># ========================================</span></span>
<span id="cb63-113"><a href="#cb63-113"></a></span>
<span id="cb63-114"><a href="#cb63-114"></a><span class="co"># ===== MULTI-HEAD </span><span class="al">ATTENTION</span><span class="co"> =====</span></span>
<span id="cb63-115"><a href="#cb63-115"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb63-116"><a href="#cb63-116"></a>    <span class="co">"""Multiple heads of self-attention in parallel."""</span></span>
<span id="cb63-117"><a href="#cb63-117"></a>    </span>
<span id="cb63-118"><a href="#cb63-118"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_heads, head_size):</span>
<span id="cb63-119"><a href="#cb63-119"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb63-120"><a href="#cb63-120"></a>        <span class="va">self</span>.heads <span class="op">=</span> nn.ModuleList([Head(head_size) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_heads)])</span>
<span id="cb63-121"><a href="#cb63-121"></a>        <span class="co"># Linear layer after concatenating heads</span></span>
<span id="cb63-122"><a href="#cb63-122"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Linear(n_embd, n_embd) <span class="co"># Projects back to n_embd dimension</span></span>
<span id="cb63-123"><a href="#cb63-123"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb63-124"><a href="#cb63-124"></a></span>
<span id="cb63-125"><a href="#cb63-125"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb63-126"><a href="#cb63-126"></a>        <span class="co"># Compute attention for each head and concatenate results</span></span>
<span id="cb63-127"><a href="#cb63-127"></a>        out <span class="op">=</span> torch.cat([h(x) <span class="cf">for</span> h <span class="kw">in</span> <span class="va">self</span>.heads], dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># Shape (B, T, num_heads * head_size) = (B, T, n_embd)</span></span>
<span id="cb63-128"><a href="#cb63-128"></a>        <span class="co"># Apply final projection and dropout</span></span>
<span id="cb63-129"><a href="#cb63-129"></a>        out <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.proj(out))</span>
<span id="cb63-130"><a href="#cb63-130"></a>        <span class="cf">return</span> out</span>
<span id="cb63-131"><a href="#cb63-131"></a><span class="co"># ===============================</span></span>
<span id="cb63-132"><a href="#cb63-132"></a></span>
<span id="cb63-133"><a href="#cb63-133"></a><span class="co"># ===== FEED-FORWARD NETWORK =====</span></span>
<span id="cb63-134"><a href="#cb63-134"></a><span class="kw">class</span> FeedFoward(nn.Module):</span>
<span id="cb63-135"><a href="#cb63-135"></a>    <span class="co">"""Simple position-wise feed-forward network with one hidden layer."""</span></span>
<span id="cb63-136"><a href="#cb63-136"></a>    </span>
<span id="cb63-137"><a href="#cb63-137"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd):</span>
<span id="cb63-138"><a href="#cb63-138"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb63-139"><a href="#cb63-139"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb63-140"><a href="#cb63-140"></a>            nn.Linear(n_embd, <span class="dv">4</span> <span class="op">*</span> n_embd),   <span class="co"># Expand dimension (common practice)</span></span>
<span id="cb63-141"><a href="#cb63-141"></a>            nn.ReLU(),                      <span class="co"># Non-linearity</span></span>
<span id="cb63-142"><a href="#cb63-142"></a>            nn.Linear(<span class="dv">4</span> <span class="op">*</span> n_embd, n_embd),   <span class="co"># Project back to original dimension</span></span>
<span id="cb63-143"><a href="#cb63-143"></a>            nn.Dropout(dropout),</span>
<span id="cb63-144"><a href="#cb63-144"></a>        )</span>
<span id="cb63-145"><a href="#cb63-145"></a></span>
<span id="cb63-146"><a href="#cb63-146"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb63-147"><a href="#cb63-147"></a>        <span class="cf">return</span> <span class="va">self</span>.net(x)</span>
<span id="cb63-148"><a href="#cb63-148"></a><span class="co"># ==============================</span></span>
<span id="cb63-149"><a href="#cb63-149"></a></span>
<span id="cb63-150"><a href="#cb63-150"></a><span class="co"># ===== TRANSFORMER BLOCK =====</span></span>
<span id="cb63-151"><a href="#cb63-151"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb63-152"><a href="#cb63-152"></a>    <span class="co">"""Transformer block: communication (attention) followed by computation (FFN)."""</span></span>
<span id="cb63-153"><a href="#cb63-153"></a>    </span>
<span id="cb63-154"><a href="#cb63-154"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd, n_head):</span>
<span id="cb63-155"><a href="#cb63-155"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb63-156"><a href="#cb63-156"></a>        head_size <span class="op">=</span> n_embd <span class="op">//</span> n_head   <span class="co"># Calculate size for each head</span></span>
<span id="cb63-157"><a href="#cb63-157"></a>        <span class="va">self</span>.sa <span class="op">=</span> MultiHeadAttention(n_head, head_size) <span class="co"># Self-Attention layer</span></span>
<span id="cb63-158"><a href="#cb63-158"></a>        <span class="va">self</span>.ffwd <span class="op">=</span> FeedFoward(n_embd) <span class="co"># Feed-Forward layer</span></span>
<span id="cb63-159"><a href="#cb63-159"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> nn.LayerNorm(n_embd) <span class="co"># LayerNorm for Attention input</span></span>
<span id="cb63-160"><a href="#cb63-160"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> nn.LayerNorm(n_embd) <span class="co"># LayerNorm for FFN input</span></span>
<span id="cb63-161"><a href="#cb63-161"></a></span>
<span id="cb63-162"><a href="#cb63-162"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb63-163"><a href="#cb63-163"></a>        <span class="co"># Pre-Normalization variant: Norm -&gt; Sublayer -&gt; Residual</span></span>
<span id="cb63-164"><a href="#cb63-164"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.sa(<span class="va">self</span>.ln1(x))  <span class="co"># Attention block</span></span>
<span id="cb63-165"><a href="#cb63-165"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffwd(<span class="va">self</span>.ln2(x)) <span class="co"># Feed-forward block</span></span>
<span id="cb63-166"><a href="#cb63-166"></a>        <span class="cf">return</span> x</span>
<span id="cb63-167"><a href="#cb63-167"></a><span class="co"># ============================</span></span>
<span id="cb63-168"><a href="#cb63-168"></a></span>
<span id="cb63-169"><a href="#cb63-169"></a><span class="co"># ===== LANGUAGE MODEL =====</span></span>
<span id="cb63-170"><a href="#cb63-170"></a><span class="kw">class</span> BigramLanguageModel(nn.Module):</span>
<span id="cb63-171"><a href="#cb63-171"></a>    <span class="co">"""GPT-like language model using Transformer blocks."""</span></span>
<span id="cb63-172"><a href="#cb63-172"></a>    </span>
<span id="cb63-173"><a href="#cb63-173"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb63-174"><a href="#cb63-174"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb63-175"><a href="#cb63-175"></a>        <span class="co"># Token Embedding Table: Maps character index to embedding vector. (vocab_size, n_embd)</span></span>
<span id="cb63-176"><a href="#cb63-176"></a>        <span class="va">self</span>.token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, n_embd)</span>
<span id="cb63-177"><a href="#cb63-177"></a>        <span class="co"># Position Embedding Table: Maps position index (0 to block_size-1) to embedding vector. (block_size, n_embd)</span></span>
<span id="cb63-178"><a href="#cb63-178"></a>        <span class="va">self</span>.position_embedding_table <span class="op">=</span> nn.Embedding(block_size, n_embd)</span>
<span id="cb63-179"><a href="#cb63-179"></a>        <span class="co"># Sequence of Transformer Blocks</span></span>
<span id="cb63-180"><a href="#cb63-180"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.Sequential(<span class="op">*</span>[Block(n_embd, n_head<span class="op">=</span>n_head) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_layer)])</span>
<span id="cb63-181"><a href="#cb63-181"></a>        <span class="co"># Final Layer Normalization (applied after blocks)</span></span>
<span id="cb63-182"><a href="#cb63-182"></a>        <span class="va">self</span>.ln_f <span class="op">=</span> nn.LayerNorm(n_embd)   <span class="co"># Final layer norm</span></span>
<span id="cb63-183"><a href="#cb63-183"></a>        <span class="co"># Linear Head: Maps final embedding back to vocabulary size to get logits. (n_embd, vocab_size)</span></span>
<span id="cb63-184"><a href="#cb63-184"></a>        <span class="va">self</span>.lm_head <span class="op">=</span> nn.Linear(n_embd, vocab_size)</span>
<span id="cb63-185"><a href="#cb63-185"></a></span>
<span id="cb63-186"><a href="#cb63-186"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb63-187"><a href="#cb63-187"></a>        B, T <span class="op">=</span> idx.shape</span>
<span id="cb63-188"><a href="#cb63-188"></a>        </span>
<span id="cb63-189"><a href="#cb63-189"></a>        <span class="co"># Get token embeddings from indices: (B, T) -&gt; (B, T, n_embd)</span></span>
<span id="cb63-190"><a href="#cb63-190"></a>        tok_emb <span class="op">=</span> <span class="va">self</span>.token_embedding_table(idx)</span>
<span id="cb63-191"><a href="#cb63-191"></a>        <span class="co"># Get position embeddings: Create indices 0..T-1, look up embeddings -&gt; (T, n_embd)</span></span>
<span id="cb63-192"><a href="#cb63-192"></a>        pos_emb <span class="op">=</span> <span class="va">self</span>.position_embedding_table(torch.arange(T, device<span class="op">=</span>device))</span>
<span id="cb63-193"><a href="#cb63-193"></a>        <span class="co"># Combine token and position embeddings by addition: (B, T, n_embd). Broadcasting handles the addition.</span></span>
<span id="cb63-194"><a href="#cb63-194"></a>        x <span class="op">=</span> tok_emb <span class="op">+</span> pos_emb   <span class="co"># (B,T,C)</span></span>
<span id="cb63-195"><a href="#cb63-195"></a>        <span class="co"># Pass through Transformer blocks: (B, T, n_embd) -&gt; (B, T, n_embd)</span></span>
<span id="cb63-196"><a href="#cb63-196"></a>        x <span class="op">=</span> <span class="va">self</span>.blocks(x)</span>
<span id="cb63-197"><a href="#cb63-197"></a>        <span class="co"># Apply final LayerNorm</span></span>
<span id="cb63-198"><a href="#cb63-198"></a>        x <span class="op">=</span> <span class="va">self</span>.ln_f(x)</span>
<span id="cb63-199"><a href="#cb63-199"></a>        <span class="co"># Map to vocabulary logits: (B, T, n_embd) -&gt; (B, T, vocab_size)</span></span>
<span id="cb63-200"><a href="#cb63-200"></a>        logits <span class="op">=</span> <span class="va">self</span>.lm_head(x)</span>
<span id="cb63-201"><a href="#cb63-201"></a></span>
<span id="cb63-202"><a href="#cb63-202"></a>        <span class="co"># Calculate loss if targets are provided (same as before)</span></span>
<span id="cb63-203"><a href="#cb63-203"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb63-204"><a href="#cb63-204"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb63-205"><a href="#cb63-205"></a>        <span class="cf">else</span>:</span>
<span id="cb63-206"><a href="#cb63-206"></a>            <span class="co"># Reshape for cross_entropy: (B*T, vocab_size) and (B*T)</span></span>
<span id="cb63-207"><a href="#cb63-207"></a>            B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb63-208"><a href="#cb63-208"></a>            logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C)</span>
<span id="cb63-209"><a href="#cb63-209"></a>            targets <span class="op">=</span> targets.view(B<span class="op">*</span>T)</span>
<span id="cb63-210"><a href="#cb63-210"></a>            loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb63-211"><a href="#cb63-211"></a></span>
<span id="cb63-212"><a href="#cb63-212"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb63-213"><a href="#cb63-213"></a></span>
<span id="cb63-214"><a href="#cb63-214"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens):</span>
<span id="cb63-215"><a href="#cb63-215"></a>        <span class="co">"""Generate new text given a starting sequence."""</span></span>
<span id="cb63-216"><a href="#cb63-216"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb63-217"><a href="#cb63-217"></a>            <span class="co"># Crop context `idx` to the last `block_size` tokens. Important as position embeddings only go up to block_size.</span></span>
<span id="cb63-218"><a href="#cb63-218"></a>            idx_cond <span class="op">=</span> idx[:, <span class="op">-</span>block_size:]</span>
<span id="cb63-219"><a href="#cb63-219"></a>            <span class="co"># Get predictions (logits) from the model</span></span>
<span id="cb63-220"><a href="#cb63-220"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>(idx_cond)</span>
<span id="cb63-221"><a href="#cb63-221"></a>            <span class="co"># Focus on the logits for the *last* time step: (B, C)</span></span>
<span id="cb63-222"><a href="#cb63-222"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb63-223"><a href="#cb63-223"></a>            <span class="co"># Convert logits to probabilities via softmax</span></span>
<span id="cb63-224"><a href="#cb63-224"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)   <span class="co"># (B, C)</span></span>
<span id="cb63-225"><a href="#cb63-225"></a>            <span class="co"># Sample next token index from the probability distribution</span></span>
<span id="cb63-226"><a href="#cb63-226"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)   <span class="co"># (B, 1)</span></span>
<span id="cb63-227"><a href="#cb63-227"></a>            <span class="co"># Append the sampled index to the running sequence</span></span>
<span id="cb63-228"><a href="#cb63-228"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>)   <span class="co"># (B, T+1)</span></span>
<span id="cb63-229"><a href="#cb63-229"></a>        <span class="cf">return</span> idx</span>
<span id="cb63-230"><a href="#cb63-230"></a><span class="co"># =========================</span></span>
<span id="cb63-231"><a href="#cb63-231"></a></span>
<span id="cb63-232"><a href="#cb63-232"></a><span class="co"># ===== MODEL INITIALIZATION AND TRAINING =====</span></span>
<span id="cb63-233"><a href="#cb63-233"></a><span class="co"># Create model instance and move it to the selected device</span></span>
<span id="cb63-234"><a href="#cb63-234"></a>model <span class="op">=</span> BigramLanguageModel()</span>
<span id="cb63-235"><a href="#cb63-235"></a>m <span class="op">=</span> model.to(device)</span>
<span id="cb63-236"><a href="#cb63-236"></a><span class="co"># Print number of parameters (useful for understanding model size)</span></span>
<span id="cb63-237"><a href="#cb63-237"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> m.parameters())<span class="op">/</span><span class="fl">1e6</span>, <span class="st">'M parameters'</span>) <span class="co"># Calculate and print M parameters</span></span>
<span id="cb63-238"><a href="#cb63-238"></a></span>
<span id="cb63-239"><a href="#cb63-239"></a><span class="co"># Create optimizer (AdamW again)</span></span>
<span id="cb63-240"><a href="#cb63-240"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb63-241"><a href="#cb63-241"></a></span>
<span id="cb63-242"><a href="#cb63-242"></a><span class="co"># Training loop</span></span>
<span id="cb63-243"><a href="#cb63-243"></a><span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(max_iters):</span>
<span id="cb63-244"><a href="#cb63-244"></a>    <span class="co"># Evaluate loss periodically</span></span>
<span id="cb63-245"><a href="#cb63-245"></a>    <span class="cf">if</span> <span class="bu">iter</span> <span class="op">%</span> eval_interval <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> <span class="bu">iter</span> <span class="op">==</span> max_iters <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb63-246"><a href="#cb63-246"></a>        losses <span class="op">=</span> estimate_loss() <span class="co"># Get train/val loss using the helper function</span></span>
<span id="cb63-247"><a href="#cb63-247"></a>        <span class="bu">print</span>(<span class="ss">f"step </span><span class="sc">{</span><span class="bu">iter</span><span class="sc">}</span><span class="ss">: train loss </span><span class="sc">{</span>losses[<span class="st">'train'</span>]<span class="sc">:.4f}</span><span class="ss">, val loss </span><span class="sc">{</span>losses[<span class="st">'val'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>) <span class="co"># Print losses</span></span>
<span id="cb63-248"><a href="#cb63-248"></a></span>
<span id="cb63-249"><a href="#cb63-249"></a>    <span class="co"># Sample a batch of data</span></span>
<span id="cb63-250"><a href="#cb63-250"></a>    xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb63-251"><a href="#cb63-251"></a></span>
<span id="cb63-252"><a href="#cb63-252"></a>    <span class="co"># Forward pass: Evaluate loss</span></span>
<span id="cb63-253"><a href="#cb63-253"></a>    logits, loss <span class="op">=</span> model(xb, yb)</span>
<span id="cb63-254"><a href="#cb63-254"></a>    <span class="co"># Backward pass: Calculate gradients</span></span>
<span id="cb63-255"><a href="#cb63-255"></a>    optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>) <span class="co"># Zero gradients</span></span>
<span id="cb63-256"><a href="#cb63-256"></a>    loss.backward() <span class="co"># Backpropagation</span></span>
<span id="cb63-257"><a href="#cb63-257"></a>    <span class="co"># Update parameters</span></span>
<span id="cb63-258"><a href="#cb63-258"></a>    optimizer.step() <span class="co"># Optimizer step</span></span>
<span id="cb63-259"><a href="#cb63-259"></a></span>
<span id="cb63-260"><a href="#cb63-260"></a><span class="co"># Generate text from the trained model</span></span>
<span id="cb63-261"><a href="#cb63-261"></a>context <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>, device<span class="op">=</span>device) <span class="co"># Starting context: [[0]]</span></span>
<span id="cb63-262"><a href="#cb63-262"></a><span class="bu">print</span>(decode(m.generate(context, max_new_tokens<span class="op">=</span><span class="dv">2000</span>)[<span class="dv">0</span>].tolist()))</span>
<span id="cb63-263"><a href="#cb63-263"></a><span class="co"># ============================================</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Using device: mps
0.209729 M parameters
step 0: train loss 4.4116, val loss 4.4022
step 100: train loss 2.6568, val loss 2.6670
step 200: train loss 2.5091, val loss 2.5059
step 300: train loss 2.4194, val loss 2.4336
step 400: train loss 2.3499, val loss 2.3563
step 500: train loss 2.2963, val loss 2.3126
step 600: train loss 2.2411, val loss 2.2501
step 700: train loss 2.2053, val loss 2.2188
step 800: train loss 2.1645, val loss 2.1882
step 900: train loss 2.1238, val loss 2.1498
step 1000: train loss 2.1027, val loss 2.1297
step 1100: train loss 2.0699, val loss 2.1186
step 1200: train loss 2.0394, val loss 2.0806
step 1300: train loss 2.0255, val loss 2.0644
step 1400: train loss 1.9924, val loss 2.0376
step 1500: train loss 1.9697, val loss 2.0303
step 1600: train loss 1.9644, val loss 2.0482
step 1700: train loss 1.9413, val loss 2.0122
step 1800: train loss 1.9087, val loss 1.9949
step 1900: train loss 1.9106, val loss 1.9898
step 2000: train loss 1.8858, val loss 1.9993
step 2100: train loss 1.8722, val loss 1.9762
step 2200: train loss 1.8602, val loss 1.9636
step 2300: train loss 1.8577, val loss 1.9551
step 2400: train loss 1.8442, val loss 1.9467
step 2500: train loss 1.8153, val loss 1.9439
step 2600: train loss 1.8224, val loss 1.9363
step 2700: train loss 1.8125, val loss 1.9370
step 2800: train loss 1.8054, val loss 1.9250
step 2900: train loss 1.8045, val loss 1.9336
step 3000: train loss 1.7950, val loss 1.9202
step 3100: train loss 1.7707, val loss 1.9197
step 3200: train loss 1.7545, val loss 1.9107
step 3300: train loss 1.7569, val loss 1.9075
step 3400: train loss 1.7533, val loss 1.8942
step 3500: train loss 1.7374, val loss 1.8960
step 3600: train loss 1.7268, val loss 1.8909
step 3700: train loss 1.7277, val loss 1.8814
step 3800: train loss 1.7188, val loss 1.8889
step 3900: train loss 1.7194, val loss 1.8714
step 4000: train loss 1.7127, val loss 1.8636
step 4100: train loss 1.7073, val loss 1.8710
step 4200: train loss 1.7022, val loss 1.8597
step 4300: train loss 1.6994, val loss 1.8488
step 4400: train loss 1.7048, val loss 1.8664
step 4500: train loss 1.6860, val loss 1.8461
step 4600: train loss 1.6854, val loss 1.8304
step 4700: train loss 1.6841, val loss 1.8469
step 4800: train loss 1.6655, val loss 1.8454
step 4900: train loss 1.6713, val loss 1.8387
step 4999: train loss 1.6656, val loss 1.8277

Foast.

MENENIUS:
Praviely your niews? I cank, CORiced aggele;
Or heave worth sunt bone Ammiod, Lord,
Who is make thy batted oub! servilings
Toke as lihtch you basw to see swife,
Is letsts lown'd us; to lace and though mistrair took
And the proply enstriaghte for a shien.
Why, they foul tlead,
up is later and
behoy cried men as thou beatt his you.

HERRY VI:
There, you weaks mirre and all was imper, Then death, doth those I will read;
Weas sul't is King me, I what lady so not this dire.

ROMEO:
O, upon to death! him not this bornorow-prove.

MUCIOND:
Why leave ye no you?

DUCUCHESTEH:
But one thyies, if will the save your blages wore I mong father you hast;
Alaitle not arm thither crown tow doth.

FROM WTARDit't me reven.

WARWICK:
Or, as extress womb voishmas!
Good me you; and incaes up done! make,
Or I serigh to emmequerel, to speak, herse to supomet?

LUCIO:
The like, But twast on was theirs
poor of thou do
As hath lay but so bredaint, forweet of For which his lictless me,
That while fumseriands thy unclity,
Wheree I wam my broth? am the too to virsant, whould enterfuly,
All there, ontreman one his him;
When whom to Luvinge one the rews,
Warwixt kill himfined me the bights the with and
Thost will in him,
Mor Sonme man, make to men, Must took.

Server:
Is aid the underer you: if
The I holseld at most lost! Comioli his but a bedrip thy lord,
And then you pringent, and what you kingle is a gestreface is ears.
But take me. Tis basdeh,--
Cendom to nie,
You lordone turn to mine hath dels in woo forth.
Poy devisecity, Ineed and encont
Onking, pleasiness, here's me?
What the have of the doet.

ClaytAM:
Now tweett, cour is plose,
Ostate, and you raint this made untu
With ould to Warwith that me bone;
Will him drown the have wesest: doth,
Are goody gent yours the pot opings, time same, that BI thirself have gative. I' cown love this mind,
Nether if thou her fortune they have fight my ftlair aggainst for him burry.

BRUTUS:
Whoth lost for for leth
And, being eyes
And if for</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>With 5000 iterations, the model is able to generate text that is similar to the training text.</p>
</div>
</div>


<!-- -->

</section>
</section>

<p>© <a href="https://hakyimlab.org">HakyImLab and Listed Authors</a> - <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 License</a></p></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
        const codeEl = trigger.previousElementSibling.cloneNode(true);
        for (const childEl of codeEl.children) {
          if (isCodeAnnotation(childEl)) {
            childEl.remove();
          }
        }
        return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
    const viewSource = window.document.getElementById('quarto-view-source') ||
                       window.document.getElementById('quarto-code-tools-source');
    if (viewSource) {
      const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
      viewSource.addEventListener("click", function(e) {
        if (sourceUrl) {
          // rstudio viewer pane
          if (/\bcapabilities=\b/.test(window.location)) {
            window.open(sourceUrl);
          } else {
            window.location.href = sourceUrl;
          }
        } else {
          const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
          modal.show();
        }
        return false;
      });
    }
    function toggleCodeHandler(show) {
      return function(e) {
        const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
        for (let i=0; i<detailsSrc.length; i++) {
          const details = detailsSrc[i].parentElement;
          if (show) {
            details.open = true;
          } else {
            details.removeAttribute("open");
          }
        }
        const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
        const fromCls = show ? "hidden" : "unhidden";
        const toCls = show ? "unhidden" : "hidden";
        for (let i=0; i<cellCodeDivs.length; i++) {
          const codeDiv = cellCodeDivs[i];
          if (codeDiv.classList.contains(fromCls)) {
            codeDiv.classList.remove(fromCls);
            codeDiv.classList.add(toCls);
          } 
        }
        return false;
      }
    }
    const hideAllCode = window.document.getElementById("quarto-hide-all-code");
    if (hideAllCode) {
      hideAllCode.addEventListener("click", toggleCodeHandler(false));
    }
    const showAllCode = window.document.getElementById("quarto-show-all-code");
    if (showAllCode) {
      showAllCode.addEventListener("click", toggleCodeHandler(true));
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb65" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb65-1"><a href="#cb65-1"></a><span class="co">---</span></span>
<span id="cb65-2"><a href="#cb65-2"></a><span class="an">title:</span><span class="co"> Building a GPT - companion notebook annotated</span></span>
<span id="cb65-3"><a href="#cb65-3"></a><span class="an">author:</span><span class="co"> Andrey Karpathy</span></span>
<span id="cb65-4"><a href="#cb65-4"></a><span class="an">date:</span><span class="co"> '2025-04-15'</span></span>
<span id="cb65-5"><a href="#cb65-5"></a><span class="an">freeze:</span><span class="co"> true</span></span>
<span id="cb65-6"><a href="#cb65-6"></a><span class="an">jupyter:</span><span class="co"> </span></span>
<span id="cb65-7"><a href="#cb65-7"></a><span class="co">  kernelspec:</span></span>
<span id="cb65-8"><a href="#cb65-8"></a><span class="co">    name: "conda-env-gene46100-py"</span></span>
<span id="cb65-9"><a href="#cb65-9"></a><span class="co">    language: "python"</span></span>
<span id="cb65-10"><a href="#cb65-10"></a><span class="co">    display_name: "gene46100"</span></span>
<span id="cb65-11"><a href="#cb65-11"></a><span class="an">format:</span></span>
<span id="cb65-12"><a href="#cb65-12"></a><span class="co">  html:</span></span>
<span id="cb65-13"><a href="#cb65-13"></a><span class="co">    code-fold: true</span></span>
<span id="cb65-14"><a href="#cb65-14"></a><span class="co">    code-line-numbers: true</span></span>
<span id="cb65-15"><a href="#cb65-15"></a><span class="co">    code-tools: true</span></span>
<span id="cb65-16"><a href="#cb65-16"></a><span class="co">    code-wrap: true</span></span>
<span id="cb65-17"><a href="#cb65-17"></a><span class="an">description:</span><span class="co"> Companion notebook from Karpathys video on building a minimal GPT, annotated by cursors LLM with summary from gemini.</span></span>
<span id="cb65-18"><a href="#cb65-18"></a><span class="co">---</span></span>
<span id="cb65-19"><a href="#cb65-19"></a></span>
<span id="cb65-20"><a href="#cb65-20"></a><span class="fu">## Building a GPT</span></span>
<span id="cb65-21"><a href="#cb65-21"></a></span>
<span id="cb65-22"><a href="#cb65-22"></a>Companion notebook to the <span class="co">[</span><span class="ot">Zero To Hero</span><span class="co">](https://karpathy.ai/zero-to-hero.html)</span> video on GPT. Downloaded from <span class="co">[</span><span class="ot">here</span><span class="co">](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing)</span></span>
<span id="cb65-23"><a href="#cb65-23"></a></span>
<span id="cb65-24"><a href="#cb65-24"></a>(https://github.com/karpathy/nanoGPT)</span>
<span id="cb65-25"><a href="#cb65-25"></a></span>
<span id="cb65-26"><a href="#cb65-26"></a><span class="fu">### download the tiny shakespeare dataset</span></span>
<span id="cb65-29"><a href="#cb65-29"></a><span class="in">```{python}</span></span>
<span id="cb65-30"><a href="#cb65-30"></a><span class="co"># Download the tiny shakespeare dataset</span></span>
<span id="cb65-31"><a href="#cb65-31"></a><span class="co">#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt</span></span>
<span id="cb65-32"><a href="#cb65-32"></a></span>
<span id="cb65-33"><a href="#cb65-33"></a><span class="in">```</span></span>
<span id="cb65-34"><a href="#cb65-34"></a></span>
<span id="cb65-37"><a href="#cb65-37"></a><span class="in">```{python}</span></span>
<span id="cb65-38"><a href="#cb65-38"></a><span class="co"># read it in to inspect it</span></span>
<span id="cb65-39"><a href="#cb65-39"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'input.txt'</span>, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> f:</span>
<span id="cb65-40"><a href="#cb65-40"></a>    text <span class="op">=</span> f.read()</span>
<span id="cb65-41"><a href="#cb65-41"></a><span class="in">```</span></span>
<span id="cb65-42"><a href="#cb65-42"></a></span>
<span id="cb65-45"><a href="#cb65-45"></a><span class="in">```{python}</span></span>
<span id="cb65-46"><a href="#cb65-46"></a><span class="co"># print the length of the dataset</span></span>
<span id="cb65-47"><a href="#cb65-47"></a><span class="bu">print</span>(<span class="st">"length of dataset in characters: "</span>, <span class="bu">len</span>(text))</span>
<span id="cb65-48"><a href="#cb65-48"></a><span class="in">```</span></span>
<span id="cb65-49"><a href="#cb65-49"></a></span>
<span id="cb65-52"><a href="#cb65-52"></a><span class="in">```{python}</span></span>
<span id="cb65-53"><a href="#cb65-53"></a><span class="co"># let's look at the first 1000 characters</span></span>
<span id="cb65-54"><a href="#cb65-54"></a><span class="bu">print</span>(text[:<span class="dv">1000</span>])</span>
<span id="cb65-55"><a href="#cb65-55"></a><span class="in">```</span></span>
<span id="cb65-56"><a href="#cb65-56"></a></span>
<span id="cb65-59"><a href="#cb65-59"></a><span class="in">```{python}</span></span>
<span id="cb65-60"><a href="#cb65-60"></a><span class="co"># here are all the unique characters that occur in this text</span></span>
<span id="cb65-61"><a href="#cb65-61"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(text)))</span>
<span id="cb65-62"><a href="#cb65-62"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(chars)</span>
<span id="cb65-63"><a href="#cb65-63"></a><span class="bu">print</span>(<span class="st">''</span>.join(chars))</span>
<span id="cb65-64"><a href="#cb65-64"></a><span class="bu">print</span>(vocab_size)</span>
<span id="cb65-65"><a href="#cb65-65"></a><span class="in">```</span></span>
<span id="cb65-66"><a href="#cb65-66"></a></span>
<span id="cb65-67"><a href="#cb65-67"></a><span class="fu">### mapping characters to integers and vice versa</span></span>
<span id="cb65-70"><a href="#cb65-70"></a><span class="in">```{python}</span></span>
<span id="cb65-71"><a href="#cb65-71"></a><span class="co"># create a mapping from characters to integers</span></span>
<span id="cb65-72"><a href="#cb65-72"></a>stoi <span class="op">=</span> { ch:i <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }</span>
<span id="cb65-73"><a href="#cb65-73"></a>itos <span class="op">=</span> { i:ch <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }</span>
<span id="cb65-74"><a href="#cb65-74"></a>encode <span class="op">=</span> <span class="kw">lambda</span> s: [stoi[c] <span class="cf">for</span> c <span class="kw">in</span> s] <span class="co"># encoder: take a string, output a list of integers</span></span>
<span id="cb65-75"><a href="#cb65-75"></a>decode <span class="op">=</span> <span class="kw">lambda</span> l: <span class="st">''</span>.join([itos[i] <span class="cf">for</span> i <span class="kw">in</span> l]) <span class="co"># decoder: take a list of integers, output a string</span></span>
<span id="cb65-76"><a href="#cb65-76"></a></span>
<span id="cb65-77"><a href="#cb65-77"></a><span class="bu">print</span>(encode(<span class="st">"hii there"</span>))</span>
<span id="cb65-78"><a href="#cb65-78"></a><span class="bu">print</span>(decode(encode(<span class="st">"hii there"</span>)))</span>
<span id="cb65-79"><a href="#cb65-79"></a><span class="in">```</span></span>
<span id="cb65-80"><a href="#cb65-80"></a></span>
<span id="cb65-81"><a href="#cb65-81"></a><span class="fu">### encode the data into torch tensor</span></span>
<span id="cb65-84"><a href="#cb65-84"></a><span class="in">```{python}</span></span>
<span id="cb65-85"><a href="#cb65-85"></a><span class="co"># let's now encode the entire text dataset and store it into a torch.Tensor</span></span>
<span id="cb65-86"><a href="#cb65-86"></a><span class="im">import</span> torch <span class="co"># we use PyTorch: [https://pytorch.org](https://pytorch.org)</span></span>
<span id="cb65-87"><a href="#cb65-87"></a>data <span class="op">=</span> torch.tensor(encode(text), dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb65-88"><a href="#cb65-88"></a><span class="bu">print</span>(data.shape, data.dtype)</span>
<span id="cb65-89"><a href="#cb65-89"></a><span class="bu">print</span>(data[:<span class="dv">1000</span>]) <span class="co"># the 1000 characters we looked at earier will to the GPT look like this</span></span>
<span id="cb65-90"><a href="#cb65-90"></a><span class="in">```</span></span>
<span id="cb65-91"><a href="#cb65-91"></a></span>
<span id="cb65-92"><a href="#cb65-92"></a><span class="fu">### split up the data into train and validation sets</span></span>
<span id="cb65-95"><a href="#cb65-95"></a><span class="in">```{python}</span></span>
<span id="cb65-96"><a href="#cb65-96"></a><span class="co"># split up the data into train and validation sets</span></span>
<span id="cb65-97"><a href="#cb65-97"></a>n <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span><span class="op">*</span><span class="bu">len</span>(data)) <span class="co"># first 90% will be train, rest val</span></span>
<span id="cb65-98"><a href="#cb65-98"></a>train_data <span class="op">=</span> data[:n]</span>
<span id="cb65-99"><a href="#cb65-99"></a>val_data <span class="op">=</span> data[n:]</span>
<span id="cb65-100"><a href="#cb65-100"></a><span class="in">```</span></span>
<span id="cb65-101"><a href="#cb65-101"></a></span>
<span id="cb65-102"><a href="#cb65-102"></a><span class="fu">### define the block size</span></span>
<span id="cb65-105"><a href="#cb65-105"></a><span class="in">```{python}</span></span>
<span id="cb65-106"><a href="#cb65-106"></a>block_size <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb65-107"><a href="#cb65-107"></a>train_data[:block_size<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb65-108"><a href="#cb65-108"></a><span class="in">```</span></span>
<span id="cb65-109"><a href="#cb65-109"></a></span>
<span id="cb65-110"><a href="#cb65-110"></a><span class="fu">### define the context and target: 8 examples in one batch</span></span>
<span id="cb65-113"><a href="#cb65-113"></a><span class="in">```{python}</span></span>
<span id="cb65-114"><a href="#cb65-114"></a>x <span class="op">=</span> train_data[:block_size]</span>
<span id="cb65-115"><a href="#cb65-115"></a>y <span class="op">=</span> train_data[<span class="dv">1</span>:block_size<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb65-116"><a href="#cb65-116"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(block_size):</span>
<span id="cb65-117"><a href="#cb65-117"></a>    context <span class="op">=</span> x[:t<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb65-118"><a href="#cb65-118"></a>    target <span class="op">=</span> y[t]</span>
<span id="cb65-119"><a href="#cb65-119"></a>    <span class="bu">print</span>(<span class="ss">f"when input is </span><span class="sc">{</span>context<span class="sc">}</span><span class="ss"> the target: </span><span class="sc">{</span>target<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb65-120"><a href="#cb65-120"></a><span class="in">```</span></span>
<span id="cb65-121"><a href="#cb65-121"></a></span>
<span id="cb65-122"><a href="#cb65-122"></a><span class="fu">### define the batch size and get the batch</span></span>
<span id="cb65-123"><a href="#cb65-123"></a></span>
<span id="cb65-126"><a href="#cb65-126"></a><span class="in">```{python}</span></span>
<span id="cb65-127"><a href="#cb65-127"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb65-128"><a href="#cb65-128"></a>batch_size <span class="op">=</span> <span class="dv">4</span> <span class="co"># how many independent sequences will we process in parallel?</span></span>
<span id="cb65-129"><a href="#cb65-129"></a>block_size <span class="op">=</span> <span class="dv">8</span> <span class="co"># what is the maximum context length for predictions?</span></span>
<span id="cb65-130"><a href="#cb65-130"></a></span>
<span id="cb65-131"><a href="#cb65-131"></a><span class="kw">def</span> get_batch(split):</span>
<span id="cb65-132"><a href="#cb65-132"></a>    <span class="co"># generate a small batch of data of inputs x and targets y</span></span>
<span id="cb65-133"><a href="#cb65-133"></a>    data <span class="op">=</span> train_data <span class="cf">if</span> split <span class="op">==</span> <span class="st">'train'</span> <span class="cf">else</span> val_data</span>
<span id="cb65-134"><a href="#cb65-134"></a>    ix <span class="op">=</span> torch.randint(<span class="bu">len</span>(data) <span class="op">-</span> block_size, (batch_size,))</span>
<span id="cb65-135"><a href="#cb65-135"></a>    x <span class="op">=</span> torch.stack([data[i:i<span class="op">+</span>block_size] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb65-136"><a href="#cb65-136"></a>    y <span class="op">=</span> torch.stack([data[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span>block_size<span class="op">+</span><span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb65-137"><a href="#cb65-137"></a>    <span class="cf">return</span> x, y</span>
<span id="cb65-138"><a href="#cb65-138"></a></span>
<span id="cb65-139"><a href="#cb65-139"></a>xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb65-140"><a href="#cb65-140"></a><span class="bu">print</span>(<span class="st">'inputs:'</span>)</span>
<span id="cb65-141"><a href="#cb65-141"></a><span class="bu">print</span>(xb.shape)</span>
<span id="cb65-142"><a href="#cb65-142"></a><span class="bu">print</span>(xb)</span>
<span id="cb65-143"><a href="#cb65-143"></a><span class="bu">print</span>(<span class="st">'targets:'</span>)</span>
<span id="cb65-144"><a href="#cb65-144"></a><span class="bu">print</span>(yb.shape)</span>
<span id="cb65-145"><a href="#cb65-145"></a><span class="bu">print</span>(yb)</span>
<span id="cb65-146"><a href="#cb65-146"></a></span>
<span id="cb65-147"><a href="#cb65-147"></a><span class="bu">print</span>(<span class="st">'----'</span>)</span>
<span id="cb65-148"><a href="#cb65-148"></a></span>
<span id="cb65-149"><a href="#cb65-149"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(batch_size): <span class="co"># batch dimension</span></span>
<span id="cb65-150"><a href="#cb65-150"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(block_size): <span class="co"># time dimension</span></span>
<span id="cb65-151"><a href="#cb65-151"></a>        context <span class="op">=</span> xb[b, :t<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb65-152"><a href="#cb65-152"></a>        target <span class="op">=</span> yb[b,t]</span>
<span id="cb65-153"><a href="#cb65-153"></a>        <span class="bu">print</span>(<span class="ss">f"when input is </span><span class="sc">{</span>context<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss"> the target: </span><span class="sc">{</span>target<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb65-154"><a href="#cb65-154"></a><span class="in">```</span></span>
<span id="cb65-155"><a href="#cb65-155"></a></span>
<span id="cb65-156"><a href="#cb65-156"></a><span class="fu">### start with a simple model: the bigram language model</span></span>
<span id="cb65-157"><a href="#cb65-157"></a></span>
<span id="cb65-160"><a href="#cb65-160"></a><span class="in">```{python}</span></span>
<span id="cb65-161"><a href="#cb65-161"></a><span class="co"># define the bigram language model</span></span>
<span id="cb65-162"><a href="#cb65-162"></a><span class="im">import</span> torch</span>
<span id="cb65-163"><a href="#cb65-163"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb65-164"><a href="#cb65-164"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb65-165"><a href="#cb65-165"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb65-166"><a href="#cb65-166"></a></span>
<span id="cb65-167"><a href="#cb65-167"></a><span class="kw">class</span> BigramLanguageModel(nn.Module):</span>
<span id="cb65-168"><a href="#cb65-168"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size):</span>
<span id="cb65-169"><a href="#cb65-169"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb65-170"><a href="#cb65-170"></a>        <span class="va">self</span>.token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, vocab_size)</span>
<span id="cb65-171"><a href="#cb65-171"></a></span>
<span id="cb65-172"><a href="#cb65-172"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb65-173"><a href="#cb65-173"></a>        <span class="co"># idx and targets are both (B,T) tensor of integers</span></span>
<span id="cb65-174"><a href="#cb65-174"></a>        logits <span class="op">=</span> <span class="va">self</span>.token_embedding_table(idx) <span class="co"># (B,T,C)</span></span>
<span id="cb65-175"><a href="#cb65-175"></a></span>
<span id="cb65-176"><a href="#cb65-176"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb65-177"><a href="#cb65-177"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb65-178"><a href="#cb65-178"></a>        <span class="cf">else</span>:</span>
<span id="cb65-179"><a href="#cb65-179"></a>            B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb65-180"><a href="#cb65-180"></a>            logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C)</span>
<span id="cb65-181"><a href="#cb65-181"></a>            targets <span class="op">=</span> targets.view(B<span class="op">*</span>T)</span>
<span id="cb65-182"><a href="#cb65-182"></a>            loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb65-183"><a href="#cb65-183"></a></span>
<span id="cb65-184"><a href="#cb65-184"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb65-185"><a href="#cb65-185"></a></span>
<span id="cb65-186"><a href="#cb65-186"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens):</span>
<span id="cb65-187"><a href="#cb65-187"></a>        <span class="co"># idx is (B, T) array of indices in the current context</span></span>
<span id="cb65-188"><a href="#cb65-188"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb65-189"><a href="#cb65-189"></a>            <span class="co"># get the predictions</span></span>
<span id="cb65-190"><a href="#cb65-190"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>(idx)</span>
<span id="cb65-191"><a href="#cb65-191"></a>            <span class="co"># focus only on the last time step</span></span>
<span id="cb65-192"><a href="#cb65-192"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="co"># becomes (B, C)</span></span>
<span id="cb65-193"><a href="#cb65-193"></a>            <span class="co"># apply softmax to get probabilities</span></span>
<span id="cb65-194"><a href="#cb65-194"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># (B, C)</span></span>
<span id="cb65-195"><a href="#cb65-195"></a>            <span class="co"># sample from the distribution</span></span>
<span id="cb65-196"><a href="#cb65-196"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, 1)</span></span>
<span id="cb65-197"><a href="#cb65-197"></a>            <span class="co"># append sampled index to the running sequence</span></span>
<span id="cb65-198"><a href="#cb65-198"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, T+1)</span></span>
<span id="cb65-199"><a href="#cb65-199"></a>        <span class="cf">return</span> idx</span>
<span id="cb65-200"><a href="#cb65-200"></a><span class="in">```</span></span>
<span id="cb65-201"><a href="#cb65-201"></a></span>
<span id="cb65-202"><a href="#cb65-202"></a><span class="fu">### cross entropy loss</span></span>
<span id="cb65-203"><a href="#cb65-203"></a></span>
<span id="cb65-204"><a href="#cb65-204"></a>Loss = $-\sum_{i}(y_i * \log(p_i))$x</span>
<span id="cb65-205"><a href="#cb65-205"></a></span>
<span id="cb65-206"><a href="#cb65-206"></a>where:</span>
<span id="cb65-207"><a href="#cb65-207"></a></span>
<span id="cb65-208"><a href="#cb65-208"></a>$y_i$ = actual probability (0 or 1 for the $i$-th class)</span>
<span id="cb65-209"><a href="#cb65-209"></a>$p_i$ = predicted probability for the $i$-th class</span>
<span id="cb65-210"><a href="#cb65-210"></a>$\sum$ = sum over all classes (characters)</span>
<span id="cb65-211"><a href="#cb65-211"></a></span>
<span id="cb65-212"><a href="#cb65-212"></a>This is the loss for a single token prediction. The total loss reported by F.cross_entropy is the average loss across all B*T tokens in the batch, where:</span>
<span id="cb65-213"><a href="#cb65-213"></a></span>
<span id="cb65-214"><a href="#cb65-214"></a>B = batch_size</span>
<span id="cb65-215"><a href="#cb65-215"></a>T = block_size (sequence length)</span>
<span id="cb65-216"><a href="#cb65-216"></a></span>
<span id="cb65-217"><a href="#cb65-217"></a>Before training, we would expect the model to predict the next character from a uniform distribution (random guessing). The probability for the correct character would be $1 / \text{vocab_size}$.</span>
<span id="cb65-218"><a href="#cb65-218"></a></span>
<span id="cb65-219"><a href="#cb65-219"></a>Expected initial loss $\approx - \log(1 / \text{vocab_size}) = \log(\text{vocab_size})$</span>
<span id="cb65-220"><a href="#cb65-220"></a>$\log(65) \approx 4.1744$</span>
<span id="cb65-221"><a href="#cb65-221"></a></span>
<span id="cb65-222"><a href="#cb65-222"></a><span class="fu">### initialize the model and compute the loss</span></span>
<span id="cb65-223"><a href="#cb65-223"></a></span>
<span id="cb65-226"><a href="#cb65-226"></a><span class="in">```{python}</span></span>
<span id="cb65-227"><a href="#cb65-227"></a>m <span class="op">=</span> BigramLanguageModel(vocab_size)</span>
<span id="cb65-228"><a href="#cb65-228"></a>logits, loss <span class="op">=</span> m(xb, yb) <span class="co"># xb/yb are from the previous cell (B=4, T=8)</span></span>
<span id="cb65-229"><a href="#cb65-229"></a><span class="bu">print</span>(logits.shape) <span class="co"># Expected: (B, T, C) = (4, 8, 65)</span></span>
<span id="cb65-230"><a href="#cb65-230"></a><span class="bu">print</span>(loss) <span class="co"># Expected: Around 4.17</span></span>
<span id="cb65-231"><a href="#cb65-231"></a><span class="in">```</span></span>
<span id="cb65-232"><a href="#cb65-232"></a></span>
<span id="cb65-233"><a href="#cb65-233"></a><span class="fu">### generate text</span></span>
<span id="cb65-234"><a href="#cb65-234"></a></span>
<span id="cb65-237"><a href="#cb65-237"></a><span class="in">```{python}</span></span>
<span id="cb65-238"><a href="#cb65-238"></a><span class="bu">print</span>(decode(m.generate(idx <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>), max_new_tokens<span class="op">=</span><span class="dv">100</span>)[<span class="dv">0</span>].tolist()))</span>
<span id="cb65-239"><a href="#cb65-239"></a><span class="in">```</span></span>
<span id="cb65-240"><a href="#cb65-240"></a></span>
<span id="cb65-241"><a href="#cb65-241"></a><span class="fu">### choose AdamW as the optimizer</span></span>
<span id="cb65-242"><a href="#cb65-242"></a></span>
<span id="cb65-245"><a href="#cb65-245"></a><span class="in">```{python}</span></span>
<span id="cb65-246"><a href="#cb65-246"></a>optimizer <span class="op">=</span> torch.optim.AdamW(m.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb65-247"><a href="#cb65-247"></a><span class="in">```</span></span>
<span id="cb65-248"><a href="#cb65-248"></a></span>
<span id="cb65-249"><a href="#cb65-249"></a><span class="fu">### train the model</span></span>
<span id="cb65-250"><a href="#cb65-250"></a></span>
<span id="cb65-253"><a href="#cb65-253"></a><span class="in">```{python}</span></span>
<span id="cb65-254"><a href="#cb65-254"></a>batch_size <span class="op">=</span> <span class="dv">32</span> <span class="co"># Redefine batch size for training</span></span>
<span id="cb65-255"><a href="#cb65-255"></a><span class="cf">for</span> steps <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>): <span class="co"># # sample a batch of data</span></span>
<span id="cb65-256"><a href="#cb65-256"></a>    xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb65-257"><a href="#cb65-257"></a></span>
<span id="cb65-258"><a href="#cb65-258"></a>    <span class="co"># evaluate the loss</span></span>
<span id="cb65-259"><a href="#cb65-259"></a>    logits, loss <span class="op">=</span> m(xb, yb)</span>
<span id="cb65-260"><a href="#cb65-260"></a>    optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb65-261"><a href="#cb65-261"></a>    loss.backward()</span>
<span id="cb65-262"><a href="#cb65-262"></a>    optimizer.step()</span>
<span id="cb65-263"><a href="#cb65-263"></a></span>
<span id="cb65-264"><a href="#cb65-264"></a><span class="bu">print</span>(loss.item())</span>
<span id="cb65-265"><a href="#cb65-265"></a><span class="in">```</span></span>
<span id="cb65-266"><a href="#cb65-266"></a></span>
<span id="cb65-267"><a href="#cb65-267"></a><span class="fu">### generate text starting with 0=`\n` as initial context</span></span>
<span id="cb65-268"><a href="#cb65-268"></a></span>
<span id="cb65-271"><a href="#cb65-271"></a><span class="in">```{python}</span></span>
<span id="cb65-272"><a href="#cb65-272"></a><span class="bu">print</span>(decode(m.generate(idx <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>), max_new_tokens<span class="op">=</span><span class="dv">500</span>)[<span class="dv">0</span>].tolist()))</span>
<span id="cb65-273"><a href="#cb65-273"></a><span class="in">```</span></span>
<span id="cb65-274"><a href="#cb65-274"></a></span>
<span id="cb65-275"><a href="#cb65-275"></a><span class="fu">### The mathematical trick in self-attention</span></span>
<span id="cb65-276"><a href="#cb65-276"></a>toy example illustrating how matrix multiplication can be used for a "weighted aggregation"</span>
<span id="cb65-277"><a href="#cb65-277"></a></span>
<span id="cb65-280"><a href="#cb65-280"></a><span class="in">```{python}</span></span>
<span id="cb65-281"><a href="#cb65-281"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb65-282"><a href="#cb65-282"></a>a <span class="op">=</span> torch.tril(torch.ones(<span class="dv">3</span>, <span class="dv">3</span>)) <span class="co"># Lower triangular matrix of 1s</span></span>
<span id="cb65-283"><a href="#cb65-283"></a>a <span class="op">=</span> a <span class="op">/</span> torch.<span class="bu">sum</span>(a, <span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># Normalize rows to sum to 1</span></span>
<span id="cb65-284"><a href="#cb65-284"></a>b <span class="op">=</span> torch.randint(<span class="dv">0</span>,<span class="dv">10</span>,(<span class="dv">3</span>,<span class="dv">2</span>)).<span class="bu">float</span>() <span class="co"># Some data</span></span>
<span id="cb65-285"><a href="#cb65-285"></a>c <span class="op">=</span> a <span class="op">@</span> b <span class="co"># Matrix multiply</span></span>
<span id="cb65-286"><a href="#cb65-286"></a><span class="bu">print</span>(<span class="st">'a='</span>)</span>
<span id="cb65-287"><a href="#cb65-287"></a><span class="bu">print</span>(a)</span>
<span id="cb65-288"><a href="#cb65-288"></a><span class="bu">print</span>(<span class="st">'--'</span>)</span>
<span id="cb65-289"><a href="#cb65-289"></a><span class="bu">print</span>(<span class="st">'b='</span>)</span>
<span id="cb65-290"><a href="#cb65-290"></a><span class="bu">print</span>(b)</span>
<span id="cb65-291"><a href="#cb65-291"></a><span class="bu">print</span>(<span class="st">'--'</span>)</span>
<span id="cb65-292"><a href="#cb65-292"></a><span class="bu">print</span>(<span class="st">'c='</span>)</span>
<span id="cb65-293"><a href="#cb65-293"></a><span class="bu">print</span>(c)</span>
<span id="cb65-294"><a href="#cb65-294"></a><span class="in">```</span></span>
<span id="cb65-295"><a href="#cb65-295"></a></span>
<span id="cb65-298"><a href="#cb65-298"></a><span class="in">```{python}</span></span>
<span id="cb65-299"><a href="#cb65-299"></a><span class="co"># consider the following toy example:</span></span>
<span id="cb65-300"><a href="#cb65-300"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb65-301"><a href="#cb65-301"></a>B,T,C <span class="op">=</span> <span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">2</span> <span class="co"># batch, time, channels</span></span>
<span id="cb65-302"><a href="#cb65-302"></a>x <span class="op">=</span> torch.randn(B,T,C)</span>
<span id="cb65-303"><a href="#cb65-303"></a>x.shape</span>
<span id="cb65-304"><a href="#cb65-304"></a><span class="in">```</span></span>
<span id="cb65-305"><a href="#cb65-305"></a></span>
<span id="cb65-306"><a href="#cb65-306"></a><span class="fu">### version 1: using a for loop to compute the weighted aggregation</span></span>
<span id="cb65-307"><a href="#cb65-307"></a></span>
<span id="cb65-310"><a href="#cb65-310"></a><span class="in">```{python}</span></span>
<span id="cb65-311"><a href="#cb65-311"></a><span class="co"># We want x[b,t] = mean_{i&lt;=t} x[b,i]</span></span>
<span id="cb65-312"><a href="#cb65-312"></a>xbow <span class="op">=</span> torch.zeros((B,T,C)) <span class="co"># x bag-of-words (running average)</span></span>
<span id="cb65-313"><a href="#cb65-313"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb65-314"><a href="#cb65-314"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb65-315"><a href="#cb65-315"></a>        xprev <span class="op">=</span> x[b,:t<span class="op">+</span><span class="dv">1</span>] <span class="co"># Select vectors from start up to time t: shape (t+1, C)</span></span>
<span id="cb65-316"><a href="#cb65-316"></a>        xbow[b,t] <span class="op">=</span> torch.mean(xprev, <span class="dv">0</span>) <span class="co"># Compute mean along the time dimension (dim 0)</span></span>
<span id="cb65-317"><a href="#cb65-317"></a><span class="in">```</span></span>
<span id="cb65-318"><a href="#cb65-318"></a></span>
<span id="cb65-319"><a href="#cb65-319"></a><span class="fu">### version 2: using matrix multiply for a weighted aggregation</span></span>
<span id="cb65-320"><a href="#cb65-320"></a></span>
<span id="cb65-323"><a href="#cb65-323"></a><span class="in">```{python}</span></span>
<span id="cb65-324"><a href="#cb65-324"></a><span class="co"># Create the averaging weight matrix</span></span>
<span id="cb65-325"><a href="#cb65-325"></a>wei <span class="op">=</span> torch.tril(torch.ones(T, T))</span>
<span id="cb65-326"><a href="#cb65-326"></a>wei <span class="op">=</span> wei <span class="op">/</span> wei.<span class="bu">sum</span>(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># Normalize rows to sum to 1</span></span>
<span id="cb65-327"><a href="#cb65-327"></a><span class="co"># Perform batched matrix multiplication</span></span>
<span id="cb65-328"><a href="#cb65-328"></a>xbow2 <span class="op">=</span> wei <span class="op">@</span> x <span class="co"># (T, T) @ (B, T, C) -&gt; (B, T, C) via broadcasting</span></span>
<span id="cb65-329"><a href="#cb65-329"></a>torch.allclose(xbow, xbow2) <span class="co"># Check if results are identical</span></span>
<span id="cb65-330"><a href="#cb65-330"></a><span class="in">```</span></span>
<span id="cb65-331"><a href="#cb65-331"></a></span>
<span id="cb65-332"><a href="#cb65-332"></a><span class="fu">### version 3: use Softmax</span></span>
<span id="cb65-333"><a href="#cb65-333"></a></span>
<span id="cb65-336"><a href="#cb65-336"></a><span class="in">```{python}</span></span>
<span id="cb65-337"><a href="#cb65-337"></a>tril <span class="op">=</span> torch.tril(torch.ones(T, T))</span>
<span id="cb65-338"><a href="#cb65-338"></a>wei <span class="op">=</span> torch.zeros((T,T))</span>
<span id="cb65-339"><a href="#cb65-339"></a><span class="co"># Mask out future positions by setting them to -infinity before softmax</span></span>
<span id="cb65-340"><a href="#cb65-340"></a>wei <span class="op">=</span> wei.masked_fill(tril <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb65-341"><a href="#cb65-341"></a><span class="co"># Apply softmax to get row-wise probability distributions (weights)</span></span>
<span id="cb65-342"><a href="#cb65-342"></a>wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb65-343"><a href="#cb65-343"></a><span class="co"># Perform weighted aggregation</span></span>
<span id="cb65-344"><a href="#cb65-344"></a>xbow3 <span class="op">=</span> wei <span class="op">@</span> x</span>
<span id="cb65-345"><a href="#cb65-345"></a>torch.allclose(xbow, xbow3) <span class="co"># Check if results are identical</span></span>
<span id="cb65-346"><a href="#cb65-346"></a><span class="in">```</span></span>
<span id="cb65-347"><a href="#cb65-347"></a></span>
<span id="cb65-348"><a href="#cb65-348"></a><span class="fu">### softmax function</span></span>
<span id="cb65-349"><a href="#cb65-349"></a>softmax($z_i$) = $\frac{e^{z_i}}{\sum_j e^{z_j}}$</span>
<span id="cb65-350"><a href="#cb65-350"></a></span>
<span id="cb65-351"><a href="#cb65-351"></a><span class="fu">### version 4: self-attention</span></span>
<span id="cb65-352"><a href="#cb65-352"></a></span>
<span id="cb65-355"><a href="#cb65-355"></a><span class="in">```{python}</span></span>
<span id="cb65-356"><a href="#cb65-356"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb65-357"><a href="#cb65-357"></a>B,T,C <span class="op">=</span> <span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">32</span> <span class="co"># batch, time, channels (embedding dimension)</span></span>
<span id="cb65-358"><a href="#cb65-358"></a>x <span class="op">=</span> torch.randn(B,T,C)</span>
<span id="cb65-359"><a href="#cb65-359"></a></span>
<span id="cb65-360"><a href="#cb65-360"></a><span class="co"># let's see a single Head perform self-attention</span></span>
<span id="cb65-361"><a href="#cb65-361"></a>head_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb65-362"><a href="#cb65-362"></a>key <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb65-363"><a href="#cb65-363"></a>query <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb65-364"><a href="#cb65-364"></a>value <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb65-365"><a href="#cb65-365"></a>k <span class="op">=</span> key(x)   <span class="co"># (B, T, head_size)</span></span>
<span id="cb65-366"><a href="#cb65-366"></a>q <span class="op">=</span> query(x) <span class="co"># (B, T, head_size)</span></span>
<span id="cb65-367"><a href="#cb65-367"></a><span class="co"># Compute attention scores ("affinities")</span></span>
<span id="cb65-368"><a href="#cb65-368"></a>wei <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="co"># (B, T, hs) @ (B, hs, T) ---&gt; (B, T, T)</span></span>
<span id="cb65-369"><a href="#cb65-369"></a></span>
<span id="cb65-370"><a href="#cb65-370"></a><span class="co"># Scale the scores</span></span>
<span id="cb65-371"><a href="#cb65-371"></a><span class="co"># Note: Karpathy uses C**-0.5 here (sqrt(embedding_dim)). Standard Transformer uses sqrt(head_size).</span></span>
<span id="cb65-372"><a href="#cb65-372"></a>wei <span class="op">=</span> wei <span class="op">*</span> (C<span class="op">**-</span><span class="fl">0.5</span>)</span>
<span id="cb65-373"><a href="#cb65-373"></a></span>
<span id="cb65-374"><a href="#cb65-374"></a><span class="co"># Apply causal mask</span></span>
<span id="cb65-375"><a href="#cb65-375"></a>tril <span class="op">=</span> torch.tril(torch.ones(T, T))</span>
<span id="cb65-376"><a href="#cb65-376"></a><span class="co">#wei = torch.zeros((T,T)) # This line is commented out in original, was from softmax demo</span></span>
<span id="cb65-377"><a href="#cb65-377"></a>wei <span class="op">=</span> wei.masked_fill(tril <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>)) <span class="co"># Mask future tokens</span></span>
<span id="cb65-378"><a href="#cb65-378"></a></span>
<span id="cb65-379"><a href="#cb65-379"></a><span class="co"># Apply softmax to get attention weights</span></span>
<span id="cb65-380"><a href="#cb65-380"></a>wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># (B, T, T)</span></span>
<span id="cb65-381"><a href="#cb65-381"></a></span>
<span id="cb65-382"><a href="#cb65-382"></a><span class="co"># Perform weighted aggregation of Values</span></span>
<span id="cb65-383"><a href="#cb65-383"></a>v <span class="op">=</span> value(x) <span class="co"># (B, T, head_size)</span></span>
<span id="cb65-384"><a href="#cb65-384"></a>out <span class="op">=</span> wei <span class="op">@</span> v <span class="co"># (B, T, T) @ (B, T, hs) ---&gt; (B, T, hs)</span></span>
<span id="cb65-385"><a href="#cb65-385"></a><span class="co">#out = wei @ x # This would aggregate original x, not the projected values 'v'</span></span>
<span id="cb65-386"><a href="#cb65-386"></a></span>
<span id="cb65-387"><a href="#cb65-387"></a>out.shape <span class="co"># Expected: (B, T, head_size) = (4, 8, 16)</span></span>
<span id="cb65-388"><a href="#cb65-388"></a><span class="in">```</span></span>
<span id="cb65-389"><a href="#cb65-389"></a></span>
<span id="cb65-392"><a href="#cb65-392"></a><span class="in">```{python}</span></span>
<span id="cb65-393"><a href="#cb65-393"></a>wei[<span class="dv">0</span>] <span class="co"># Show attention weights for the first sequence in the batch</span></span>
<span id="cb65-394"><a href="#cb65-394"></a><span class="in">```</span></span>
<span id="cb65-395"><a href="#cb65-395"></a></span>
<span id="cb65-396"><a href="#cb65-396"></a></span>
<span id="cb65-397"><a href="#cb65-397"></a><span class="fu">### Check that X X'/C is is the correlation matrix if X is normalized</span></span>
<span id="cb65-398"><a href="#cb65-398"></a></span>
<span id="cb65-399"><a href="#cb65-399"></a><span class="in">```{r showing xx'/C is correlation matrix if X is normalized}</span></span>
<span id="cb65-400"><a href="#cb65-400"></a></span>
<span id="cb65-401"><a href="#cb65-401"></a>nC <span class="ot">=</span> <span class="dv">64</span></span>
<span id="cb65-402"><a href="#cb65-402"></a>X <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">4</span><span class="sc">*</span><span class="dv">64</span>), <span class="at">nrow=</span><span class="dv">4</span>, <span class="at">ncol=</span>nC)</span>
<span id="cb65-403"><a href="#cb65-403"></a><span class="do">## make it so that the third token is similar to the last one</span></span>
<span id="cb65-404"><a href="#cb65-404"></a>X[<span class="dv">2</span>,] <span class="ot">=</span> X[<span class="dv">4</span>,]<span class="sc">*</span><span class="fl">0.5</span> <span class="sc">+</span> X[<span class="dv">2</span>,]<span class="sc">*</span><span class="fl">0.5</span></span>
<span id="cb65-405"><a href="#cb65-405"></a><span class="do">## normalize X</span></span>
<span id="cb65-406"><a href="#cb65-406"></a>X <span class="ot">=</span> <span class="fu">t</span>(<span class="fu">scale</span>(<span class="fu">t</span>(X)))</span>
<span id="cb65-407"><a href="#cb65-407"></a></span>
<span id="cb65-408"><a href="#cb65-408"></a>q <span class="ot">=</span> X</span>
<span id="cb65-409"><a href="#cb65-409"></a>k <span class="ot">=</span> X</span>
<span id="cb65-410"><a href="#cb65-410"></a>v <span class="ot">=</span> X</span>
<span id="cb65-411"><a href="#cb65-411"></a></span>
<span id="cb65-412"><a href="#cb65-412"></a>qkt <span class="ot">=</span> q <span class="sc">%*%</span> <span class="fu">t</span>(k)<span class="sc">/</span>(nC<span class="dv">-1</span>)</span>
<span id="cb65-413"><a href="#cb65-413"></a>xcor <span class="ot">=</span> <span class="fu">cor</span>(<span class="fu">t</span>(q),<span class="fu">t</span>(k))</span>
<span id="cb65-414"><a href="#cb65-414"></a><span class="fu">dim</span>(xcor)</span>
<span id="cb65-415"><a href="#cb65-415"></a><span class="fu">dim</span>(qkt)</span>
<span id="cb65-416"><a href="#cb65-416"></a><span class="fu">cat</span>(<span class="st">"xcor</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb65-417"><a href="#cb65-417"></a>xcor</span>
<span id="cb65-418"><a href="#cb65-418"></a><span class="fu">cat</span>(<span class="st">"---</span><span class="sc">\n</span><span class="st"> qkt</span><span class="sc">\n</span><span class="st">"</span>)</span>
<span id="cb65-419"><a href="#cb65-419"></a>qkt</span>
<span id="cb65-420"><a href="#cb65-420"></a></span>
<span id="cb65-421"><a href="#cb65-421"></a><span class="fu">cat</span>(<span class="st">"are xcor and qkt equal?"</span>)</span>
<span id="cb65-422"><a href="#cb65-422"></a><span class="fu">all.equal</span>(xcor, qkt)</span>
<span id="cb65-423"><a href="#cb65-423"></a></span>
<span id="cb65-424"><a href="#cb65-424"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">6</span>, <span class="dv">4</span>, <span class="dv">2</span>) <span class="sc">+</span> <span class="fl">0.1</span>)  <span class="co"># increase left margin to avoid cutting of the y label</span></span>
<span id="cb65-425"><a href="#cb65-425"></a><span class="fu">par</span>(<span class="at">pty=</span><span class="st">"s"</span>)  <span class="co"># Set plot type to "square"</span></span>
<span id="cb65-426"><a href="#cb65-426"></a><span class="fu">plot</span>(<span class="fu">c</span>(xcor), <span class="fu">c</span>(qkt),<span class="at">cex=</span><span class="dv">3</span>,<span class="at">cex.lab=</span><span class="dv">3</span>,<span class="at">cex.axis=</span><span class="dv">2</span>,<span class="at">cex.main=</span><span class="dv">2</span>,<span class="at">cex.sub=</span><span class="dv">2</span>); <span class="fu">abline</span>(<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb65-427"><a href="#cb65-427"></a><span class="fu">par</span>(<span class="at">pty=</span><span class="st">"m"</span>)  <span class="co"># Reset to default plot type</span></span>
<span id="cb65-428"><a href="#cb65-428"></a><span class="fu">par</span>(<span class="at">mar=</span><span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">2</span>) <span class="sc">+</span> <span class="fl">0.1</span>)  <span class="co"># Reset to default margins</span></span>
<span id="cb65-429"><a href="#cb65-429"></a></span>
<span id="cb65-430"><a href="#cb65-430"></a></span>
<span id="cb65-431"><a href="#cb65-431"></a><span class="in">```</span></span>
<span id="cb65-432"><a href="#cb65-432"></a></span>
<span id="cb65-433"><a href="#cb65-433"></a></span>
<span id="cb65-434"><a href="#cb65-434"></a><span class="fu">### Notes:</span></span>
<span id="cb65-435"><a href="#cb65-435"></a></span>
<span id="cb65-436"><a href="#cb65-436"></a>Attention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights. </span>
<span id="cb65-437"><a href="#cb65-437"></a></span>
<span id="cb65-438"><a href="#cb65-438"></a><span class="ss">- </span>There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens. </span>
<span id="cb65-439"><a href="#cb65-439"></a>    example: "the cat sat on the mat" should be different from "the mat sat on the cat"</span>
<span id="cb65-440"><a href="#cb65-440"></a><span class="ss">- </span>Each example across batch dimension is of course processed completely independently and never "talk" to each other. </span>
<span id="cb65-441"><a href="#cb65-441"></a><span class="ss">- </span>In an "encoder" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a "decoder" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling. </span>
<span id="cb65-442"><a href="#cb65-442"></a><span class="ss">- </span>"self-attention" just means that the keys and values are produced from the same source as queries (all come from x). In "cross-attention", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)</span>
<span id="cb65-443"><a href="#cb65-443"></a></span>
<span id="cb65-444"><a href="#cb65-444"></a><span class="fu">### why scaled attention?</span></span>
<span id="cb65-445"><a href="#cb65-445"></a></span>
<span id="cb65-446"><a href="#cb65-446"></a>"Scaled" attention additionaly divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below</span>
<span id="cb65-447"><a href="#cb65-447"></a></span>
<span id="cb65-450"><a href="#cb65-450"></a><span class="in">```{python}</span></span>
<span id="cb65-451"><a href="#cb65-451"></a><span class="co"># Demonstrate variance without scaling</span></span>
<span id="cb65-452"><a href="#cb65-452"></a>k_unscaled <span class="op">=</span> torch.randn(B,T,head_size)</span>
<span id="cb65-453"><a href="#cb65-453"></a>q_unscaled <span class="op">=</span> torch.randn(B,T,head_size)</span>
<span id="cb65-454"><a href="#cb65-454"></a>wei_unscaled <span class="op">=</span> q_unscaled <span class="op">@</span> k_unscaled.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb65-455"><a href="#cb65-455"></a><span class="bu">print</span>(<span class="ss">f"k var: </span><span class="sc">{</span>k_unscaled<span class="sc">.</span>var()<span class="sc">:.4f}</span><span class="ss">, q var: </span><span class="sc">{</span>q_unscaled<span class="sc">.</span>var()<span class="sc">:.4f}</span><span class="ss">, wei (unscaled) var: </span><span class="sc">{</span>wei_unscaled<span class="sc">.</span>var()<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb65-456"><a href="#cb65-456"></a></span>
<span id="cb65-457"><a href="#cb65-457"></a><span class="co"># Demonstrate variance *with* scaling (using head_size for illustration)</span></span>
<span id="cb65-458"><a href="#cb65-458"></a>k <span class="op">=</span> torch.randn(B,T,head_size)</span>
<span id="cb65-459"><a href="#cb65-459"></a>q <span class="op">=</span> torch.randn(B,T,head_size)</span>
<span id="cb65-460"><a href="#cb65-460"></a>wei <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> head_size<span class="op">**-</span><span class="fl">0.5</span> <span class="co"># Scale by sqrt(head_size)</span></span>
<span id="cb65-461"><a href="#cb65-461"></a><span class="bu">print</span>(<span class="ss">f"k var: </span><span class="sc">{</span>k<span class="sc">.</span>var()<span class="sc">:.4f}</span><span class="ss">, q var: </span><span class="sc">{</span>q<span class="sc">.</span>var()<span class="sc">:.4f}</span><span class="ss">, wei (scaled) var: </span><span class="sc">{</span>wei<span class="sc">.</span>var()<span class="sc">:.4f}</span><span class="ss">"</span>) <span class="co"># Variance should be closer to 1</span></span>
<span id="cb65-462"><a href="#cb65-462"></a><span class="in">```</span></span>
<span id="cb65-463"><a href="#cb65-463"></a></span>
<span id="cb65-466"><a href="#cb65-466"></a><span class="in">```{python}</span></span>
<span id="cb65-467"><a href="#cb65-467"></a>k.var() <span class="co"># Should be close to 1</span></span>
<span id="cb65-468"><a href="#cb65-468"></a><span class="in">```</span></span>
<span id="cb65-469"><a href="#cb65-469"></a></span>
<span id="cb65-472"><a href="#cb65-472"></a><span class="in">```{python}</span></span>
<span id="cb65-473"><a href="#cb65-473"></a>q.var() <span class="co"># Should be close to 1</span></span>
<span id="cb65-474"><a href="#cb65-474"></a><span class="in">```</span></span>
<span id="cb65-475"><a href="#cb65-475"></a></span>
<span id="cb65-478"><a href="#cb65-478"></a><span class="in">```{python}</span></span>
<span id="cb65-479"><a href="#cb65-479"></a>wei.var() <span class="co"># With scaling, should be closer to 1 than head_size (16)</span></span>
<span id="cb65-480"><a href="#cb65-480"></a><span class="in">```</span></span>
<span id="cb65-481"><a href="#cb65-481"></a></span>
<span id="cb65-484"><a href="#cb65-484"></a><span class="in">```{python}</span></span>
<span id="cb65-485"><a href="#cb65-485"></a><span class="co"># Softmax with small inputs (diffuse distribution)</span></span>
<span id="cb65-486"><a href="#cb65-486"></a>torch.softmax(torch.tensor([<span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.5</span>]), dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb65-487"><a href="#cb65-487"></a><span class="in">```</span></span>
<span id="cb65-488"><a href="#cb65-488"></a></span>
<span id="cb65-491"><a href="#cb65-491"></a><span class="in">```{python}</span></span>
<span id="cb65-492"><a href="#cb65-492"></a><span class="co"># Softmax with large inputs (simulating unscaled attention scores) -&gt; peaks</span></span>
<span id="cb65-493"><a href="#cb65-493"></a>torch.softmax(torch.tensor([<span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.5</span>])<span class="op">*</span><span class="dv">8</span>, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># gets too peaky, converges to one-hot</span></span>
<span id="cb65-494"><a href="#cb65-494"></a><span class="in">```</span></span>
<span id="cb65-495"><a href="#cb65-495"></a></span>
<span id="cb65-496"><a href="#cb65-496"></a><span class="fu">### LayerNorm1d</span></span>
<span id="cb65-497"><a href="#cb65-497"></a></span>
<span id="cb65-500"><a href="#cb65-500"></a><span class="in">```{python}</span></span>
<span id="cb65-501"><a href="#cb65-501"></a><span class="kw">class</span> LayerNorm1d: <span class="co"># (used to be BatchNorm1d)</span></span>
<span id="cb65-502"><a href="#cb65-502"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, eps<span class="op">=</span><span class="fl">1e-5</span>, momentum<span class="op">=</span><span class="fl">0.1</span>): <span class="co"># Momentum is not used in typical LayerNorm</span></span>
<span id="cb65-503"><a href="#cb65-503"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb65-504"><a href="#cb65-504"></a>        <span class="co"># Learnable scale and shift parameters, initialized to 1 and 0</span></span>
<span id="cb65-505"><a href="#cb65-505"></a>        <span class="va">self</span>.gamma <span class="op">=</span> torch.ones(dim)</span>
<span id="cb65-506"><a href="#cb65-506"></a>        <span class="va">self</span>.beta <span class="op">=</span> torch.zeros(dim)</span>
<span id="cb65-507"><a href="#cb65-507"></a></span>
<span id="cb65-508"><a href="#cb65-508"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb65-509"><a href="#cb65-509"></a>        <span class="co"># calculate the forward pass</span></span>
<span id="cb65-510"><a href="#cb65-510"></a>        <span class="co"># Calculate mean over the *last* dimension (features/embedding)</span></span>
<span id="cb65-511"><a href="#cb65-511"></a>        xmean <span class="op">=</span> x.mean(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch mean (shape B, 1, C if input B, T, C) --&gt; Needs adjustment for (B,C) input shape here. Assumes input is (B, dim)</span></span>
<span id="cb65-512"><a href="#cb65-512"></a>        <span class="co"># Correction: x is (32, 100). dim=1 is correct for features. Shape (32, 1)</span></span>
<span id="cb65-513"><a href="#cb65-513"></a>        xvar <span class="op">=</span> x.var(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch variance (shape 32, 1)</span></span>
<span id="cb65-514"><a href="#cb65-514"></a>        <span class="co"># Normalize each feature vector independently</span></span>
<span id="cb65-515"><a href="#cb65-515"></a>        xhat <span class="op">=</span> (x <span class="op">-</span> xmean) <span class="op">/</span> torch.sqrt(xvar <span class="op">+</span> <span class="va">self</span>.eps) <span class="co"># normalize to unit variance</span></span>
<span id="cb65-516"><a href="#cb65-516"></a>        <span class="co"># Apply scale and shift</span></span>
<span id="cb65-517"><a href="#cb65-517"></a>        <span class="va">self</span>.out <span class="op">=</span> <span class="va">self</span>.gamma <span class="op">*</span> xhat <span class="op">+</span> <span class="va">self</span>.beta</span>
<span id="cb65-518"><a href="#cb65-518"></a>        <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb65-519"><a href="#cb65-519"></a></span>
<span id="cb65-520"><a href="#cb65-520"></a>    <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb65-521"><a href="#cb65-521"></a>        <span class="co"># Expose gamma and beta as learnable parameters</span></span>
<span id="cb65-522"><a href="#cb65-522"></a>        <span class="cf">return</span> [<span class="va">self</span>.gamma, <span class="va">self</span>.beta]</span>
<span id="cb65-523"><a href="#cb65-523"></a></span>
<span id="cb65-524"><a href="#cb65-524"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb65-525"><a href="#cb65-525"></a>module <span class="op">=</span> LayerNorm1d(<span class="dv">100</span>) <span class="co"># Create LayerNorm for 100 features</span></span>
<span id="cb65-526"><a href="#cb65-526"></a>x <span class="op">=</span> torch.randn(<span class="dv">32</span>, <span class="dv">100</span>) <span class="co"># batch size 32 of 100-dimensional vectors</span></span>
<span id="cb65-527"><a href="#cb65-527"></a>x <span class="op">=</span> module(x)</span>
<span id="cb65-528"><a href="#cb65-528"></a>x.shape <span class="co"># Should be (32, 100)</span></span>
<span id="cb65-529"><a href="#cb65-529"></a><span class="in">```</span></span>
<span id="cb65-530"><a href="#cb65-530"></a></span>
<span id="cb65-531"><a href="#cb65-531"></a>Explanation of layernorm</span>
<span id="cb65-532"><a href="#cb65-532"></a></span>
<span id="cb65-533"><a href="#cb65-533"></a>Input shape: (B, T, C) where:</span>
<span id="cb65-534"><a href="#cb65-534"></a>B = batch size</span>
<span id="cb65-535"><a href="#cb65-535"></a>T = sequence length (number of tokens)</span>
<span id="cb65-536"><a href="#cb65-536"></a>C = embedding dimension (features of each token)</span>
<span id="cb65-537"><a href="#cb65-537"></a>For each token in the sequence (each position T), LayerNorm:</span>
<span id="cb65-538"><a href="#cb65-538"></a>Takes its embedding vector of size C</span>
<span id="cb65-539"><a href="#cb65-539"></a>Calculates the mean and standard deviation of just that vector</span>
<span id="cb65-540"><a href="#cb65-540"></a>Normalizes that vector by subtracting its mean and dividing by its standard deviation</span>
<span id="cb65-541"><a href="#cb65-541"></a>Applies the learnable scale (gamma) and shift (beta) parameters</span>
<span id="cb65-542"><a href="#cb65-542"></a>So if you have a sequence like "The cat sat", and each word is represented by a 64-dimensional embedding vector, LayerNorm would:</span>
<span id="cb65-543"><a href="#cb65-543"></a>Take "The"'s 64-dimensional vector and normalize it</span>
<span id="cb65-544"><a href="#cb65-544"></a>Take "cat"'s 64-dimensional vector and normalize it</span>
<span id="cb65-545"><a href="#cb65-545"></a>Take "sat"'s 64-dimensional vector and normalize it</span>
<span id="cb65-546"><a href="#cb65-546"></a>Each token's vector is normalized independently of the others. This is different from BatchNorm, which would normalize across the batch dimension (i.e., looking at the same position across different examples in the batch).</span>
<span id="cb65-547"><a href="#cb65-547"></a>This per-token normalization helps maintain stable gradients during training and is particularly important in Transformers where the attention mechanism needs to work with normalized vectors to compute meaningful attention scores.</span>
<span id="cb65-548"><a href="#cb65-548"></a></span>
<span id="cb65-551"><a href="#cb65-551"></a><span class="in">```{python}</span></span>
<span id="cb65-552"><a href="#cb65-552"></a><span class="co"># Mean and std of the first feature *across the batch*. Not expected to be 0 and 1.</span></span>
<span id="cb65-553"><a href="#cb65-553"></a>x[:,<span class="dv">0</span>].mean(), x[:,<span class="dv">0</span>].std()</span>
<span id="cb65-554"><a href="#cb65-554"></a><span class="in">```</span></span>
<span id="cb65-555"><a href="#cb65-555"></a></span>
<span id="cb65-558"><a href="#cb65-558"></a><span class="in">```{python}</span></span>
<span id="cb65-559"><a href="#cb65-559"></a><span class="co"># Mean and std *across features* for the first item in the batch. Expected to be ~0 and ~1.</span></span>
<span id="cb65-560"><a href="#cb65-560"></a>x[<span class="dv">0</span>,:].mean(), x[<span class="dv">0</span>,:].std()</span>
<span id="cb65-561"><a href="#cb65-561"></a><span class="in">```</span></span>
<span id="cb65-562"><a href="#cb65-562"></a></span>
<span id="cb65-563"><a href="#cb65-563"></a><span class="fu">### French to English translation example:</span></span>
<span id="cb65-566"><a href="#cb65-566"></a><span class="in">```{python}</span></span>
<span id="cb65-567"><a href="#cb65-567"></a><span class="co"># &lt;--------- ENCODE ------------------&gt;&lt;--------------- DECODE -----------------&gt;</span></span>
<span id="cb65-568"><a href="#cb65-568"></a><span class="co"># les réseaux de neurones sont géniaux! &lt;START&gt; neural networks are awesome!&lt;</span><span class="re">END</span><span class="co">&gt;</span></span>
<span id="cb65-569"><a href="#cb65-569"></a><span class="in">```</span></span>
<span id="cb65-570"><a href="#cb65-570"></a></span>
<span id="cb65-571"><a href="#cb65-571"></a><span class="fu">### Full finished code, for reference</span></span>
<span id="cb65-572"><a href="#cb65-572"></a></span>
<span id="cb65-575"><a href="#cb65-575"></a><span class="in">```{python}</span></span>
<span id="cb65-576"><a href="#cb65-576"></a><span class="co"># Import necessary PyTorch modules</span></span>
<span id="cb65-577"><a href="#cb65-577"></a><span class="im">import</span> torch</span>
<span id="cb65-578"><a href="#cb65-578"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb65-579"><a href="#cb65-579"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb65-580"><a href="#cb65-580"></a></span>
<span id="cb65-581"><a href="#cb65-581"></a><span class="co"># ===== HYPERPARAMETERS =====</span></span>
<span id="cb65-582"><a href="#cb65-582"></a>batch_size <span class="op">=</span> <span class="dv">16</span>       <span class="co"># Number of sequences per batch (Smaller than Bigram training)</span></span>
<span id="cb65-583"><a href="#cb65-583"></a>block_size <span class="op">=</span> <span class="dv">32</span>       <span class="co"># Context length (Larger than Bigram demo)</span></span>
<span id="cb65-584"><a href="#cb65-584"></a>max_iters <span class="op">=</span> <span class="dv">5000</span>      <span class="co"># Total training iterations (More substantial training) </span><span class="al">TODO</span><span class="co"> change to 5000 later</span></span>
<span id="cb65-585"><a href="#cb65-585"></a>eval_interval <span class="op">=</span> <span class="dv">100</span>   <span class="co"># How often to check validation loss</span></span>
<span id="cb65-586"><a href="#cb65-586"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span>  <span class="co"># Optimizer learning rate</span></span>
<span id="cb65-587"><a href="#cb65-587"></a>eval_iters <span class="op">=</span> <span class="dv">200</span>      <span class="co"># Number of batches to average for validation loss estimate</span></span>
<span id="cb65-588"><a href="#cb65-588"></a>n_embd <span class="op">=</span> <span class="dv">64</span>           <span class="co"># Embedding dimension (Size of token vectors)</span></span>
<span id="cb65-589"><a href="#cb65-589"></a>n_head <span class="op">=</span> <span class="dv">4</span>            <span class="co"># Number of attention heads</span></span>
<span id="cb65-590"><a href="#cb65-590"></a>n_layer <span class="op">=</span> <span class="dv">4</span>           <span class="co"># Number of Transformer blocks (layers)</span></span>
<span id="cb65-591"><a href="#cb65-591"></a>dropout <span class="op">=</span> <span class="fl">0.0</span>         <span class="co"># Dropout probability (0.0 means no dropout here)</span></span>
<span id="cb65-592"><a href="#cb65-592"></a><span class="co"># ==========================</span></span>
<span id="cb65-593"><a href="#cb65-593"></a></span>
<span id="cb65-594"><a href="#cb65-594"></a><span class="co"># Device selection: MPS (Apple Silicon) &gt; CUDA &gt; CPU</span></span>
<span id="cb65-595"><a href="#cb65-595"></a><span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb65-596"><a href="#cb65-596"></a>    device <span class="op">=</span> torch.device(<span class="st">"mps"</span>)   <span class="co"># Apple Silicon GPU</span></span>
<span id="cb65-597"><a href="#cb65-597"></a><span class="cf">elif</span> torch.cuda.is_available():</span>
<span id="cb65-598"><a href="#cb65-598"></a>    device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)  <span class="co"># NVIDIA GPU</span></span>
<span id="cb65-599"><a href="#cb65-599"></a><span class="cf">else</span>:</span>
<span id="cb65-600"><a href="#cb65-600"></a>    device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)   <span class="co"># CPU fallback</span></span>
<span id="cb65-601"><a href="#cb65-601"></a><span class="bu">print</span>(<span class="ss">f"Using device: </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb65-602"><a href="#cb65-602"></a></span>
<span id="cb65-603"><a href="#cb65-603"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb65-604"><a href="#cb65-604"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb65-605"><a href="#cb65-605"></a><span class="cf">if</span> device.<span class="bu">type</span> <span class="op">==</span> <span class="st">'cuda'</span>:</span>
<span id="cb65-606"><a href="#cb65-606"></a>    torch.cuda.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb65-607"><a href="#cb65-607"></a><span class="cf">elif</span> device.<span class="bu">type</span> <span class="op">==</span> <span class="st">'mps'</span>:</span>
<span id="cb65-608"><a href="#cb65-608"></a>    torch.mps.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb65-609"><a href="#cb65-609"></a></span>
<span id="cb65-610"><a href="#cb65-610"></a><span class="co"># Load and read the training text (assuming input.txt is available)</span></span>
<span id="cb65-611"><a href="#cb65-611"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'input.txt'</span>, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> f:</span>
<span id="cb65-612"><a href="#cb65-612"></a>    text <span class="op">=</span> f.read()</span>
<span id="cb65-613"><a href="#cb65-613"></a></span>
<span id="cb65-614"><a href="#cb65-614"></a><span class="co"># ===== DATA PREPROCESSING =====</span></span>
<span id="cb65-615"><a href="#cb65-615"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(text)))</span>
<span id="cb65-616"><a href="#cb65-616"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(chars)</span>
<span id="cb65-617"><a href="#cb65-617"></a>stoi <span class="op">=</span> { ch:i <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }   <span class="co"># string to index</span></span>
<span id="cb65-618"><a href="#cb65-618"></a>itos <span class="op">=</span> { i:ch <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }   <span class="co"># index to string</span></span>
<span id="cb65-619"><a href="#cb65-619"></a>encode <span class="op">=</span> <span class="kw">lambda</span> s: [stoi[c] <span class="cf">for</span> c <span class="kw">in</span> s]   <span class="co"># convert string to list of integers</span></span>
<span id="cb65-620"><a href="#cb65-620"></a>decode <span class="op">=</span> <span class="kw">lambda</span> l: <span class="st">''</span>.join([itos[i] <span class="cf">for</span> i <span class="kw">in</span> l])   <span class="co"># convert list of integers to string</span></span>
<span id="cb65-621"><a href="#cb65-621"></a></span>
<span id="cb65-622"><a href="#cb65-622"></a><span class="co"># Split data into training and validation sets</span></span>
<span id="cb65-623"><a href="#cb65-623"></a>data <span class="op">=</span> torch.tensor(encode(text), dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb65-624"><a href="#cb65-624"></a>n <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span><span class="op">*</span><span class="bu">len</span>(data))   <span class="co"># first 90% for training</span></span>
<span id="cb65-625"><a href="#cb65-625"></a>train_data <span class="op">=</span> data[:n]</span>
<span id="cb65-626"><a href="#cb65-626"></a>val_data <span class="op">=</span> data[n:]</span>
<span id="cb65-627"><a href="#cb65-627"></a><span class="co"># =============================</span></span>
<span id="cb65-628"><a href="#cb65-628"></a></span>
<span id="cb65-629"><a href="#cb65-629"></a><span class="co"># ===== DATA LOADING FUNCTION =====</span></span>
<span id="cb65-630"><a href="#cb65-630"></a><span class="kw">def</span> get_batch(split):</span>
<span id="cb65-631"><a href="#cb65-631"></a>    <span class="co">"""Generate a batch of data for training or validation."""</span></span>
<span id="cb65-632"><a href="#cb65-632"></a>    data <span class="op">=</span> train_data <span class="cf">if</span> split <span class="op">==</span> <span class="st">'train'</span> <span class="cf">else</span> val_data</span>
<span id="cb65-633"><a href="#cb65-633"></a>    ix <span class="op">=</span> torch.randint(<span class="bu">len</span>(data) <span class="op">-</span> block_size, (batch_size,))</span>
<span id="cb65-634"><a href="#cb65-634"></a>    x <span class="op">=</span> torch.stack([data[i:i<span class="op">+</span>block_size] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb65-635"><a href="#cb65-635"></a>    y <span class="op">=</span> torch.stack([data[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span>block_size<span class="op">+</span><span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb65-636"><a href="#cb65-636"></a>    x, y <span class="op">=</span> x.to(device), y.to(device) <span class="co"># Move data to the target device</span></span>
<span id="cb65-637"><a href="#cb65-637"></a>    <span class="cf">return</span> x, y</span>
<span id="cb65-638"><a href="#cb65-638"></a><span class="co"># ================================</span></span>
<span id="cb65-639"><a href="#cb65-639"></a></span>
<span id="cb65-640"><a href="#cb65-640"></a><span class="co"># ===== LOSS ESTIMATION FUNCTION =====</span></span>
<span id="cb65-641"><a href="#cb65-641"></a><span class="at">@torch.no_grad</span>()   <span class="co"># Disable gradient calculation for efficiency</span></span>
<span id="cb65-642"><a href="#cb65-642"></a><span class="kw">def</span> estimate_loss():</span>
<span id="cb65-643"><a href="#cb65-643"></a>    <span class="co">"""Estimate the loss on training and validation sets."""</span></span>
<span id="cb65-644"><a href="#cb65-644"></a>    out <span class="op">=</span> {}</span>
<span id="cb65-645"><a href="#cb65-645"></a>    model.<span class="bu">eval</span>()   <span class="co"># Set model to evaluation mode</span></span>
<span id="cb65-646"><a href="#cb65-646"></a>    <span class="cf">for</span> split <span class="kw">in</span> [<span class="st">'train'</span>, <span class="st">'val'</span>]:</span>
<span id="cb65-647"><a href="#cb65-647"></a>        losses <span class="op">=</span> torch.zeros(eval_iters)</span>
<span id="cb65-648"><a href="#cb65-648"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(eval_iters):</span>
<span id="cb65-649"><a href="#cb65-649"></a>            X, Y <span class="op">=</span> get_batch(split)</span>
<span id="cb65-650"><a href="#cb65-650"></a>            logits, loss <span class="op">=</span> model(X, Y)</span>
<span id="cb65-651"><a href="#cb65-651"></a>            losses[k] <span class="op">=</span> loss.item()</span>
<span id="cb65-652"><a href="#cb65-652"></a>        out[split] <span class="op">=</span> losses.mean()</span>
<span id="cb65-653"><a href="#cb65-653"></a>    model.train()  <span class="co"># Set model back to training mode</span></span>
<span id="cb65-654"><a href="#cb65-654"></a>    <span class="cf">return</span> out</span>
<span id="cb65-655"><a href="#cb65-655"></a><span class="co"># ===================================</span></span>
<span id="cb65-656"><a href="#cb65-656"></a></span>
<span id="cb65-657"><a href="#cb65-657"></a><span class="co"># ===== </span><span class="al">ATTENTION</span><span class="co"> HEAD IMPLEMENTATION =====</span></span>
<span id="cb65-658"><a href="#cb65-658"></a><span class="kw">class</span> Head(nn.Module):</span>
<span id="cb65-659"><a href="#cb65-659"></a>    <span class="co">"""Single head of self-attention."""</span></span>
<span id="cb65-660"><a href="#cb65-660"></a>    </span>
<span id="cb65-661"><a href="#cb65-661"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, head_size):</span>
<span id="cb65-662"><a href="#cb65-662"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb65-663"><a href="#cb65-663"></a>        <span class="co"># Linear projections for Key, Query, Value</span></span>
<span id="cb65-664"><a href="#cb65-664"></a>        <span class="va">self</span>.key <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb65-665"><a href="#cb65-665"></a>        <span class="va">self</span>.query <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb65-666"><a href="#cb65-666"></a>        <span class="va">self</span>.value <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb65-667"><a href="#cb65-667"></a>        <span class="co"># Causal mask (tril). 'register_buffer' makes it part of the model state but not a parameter to be trained.</span></span>
<span id="cb65-668"><a href="#cb65-668"></a>        <span class="va">self</span>.register_buffer(<span class="st">'tril'</span>, torch.tril(torch.ones(block_size, block_size)))</span>
<span id="cb65-669"><a href="#cb65-669"></a>        <span class="co"># Dropout layer (applied after softmax)</span></span>
<span id="cb65-670"><a href="#cb65-670"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb65-671"><a href="#cb65-671"></a></span>
<span id="cb65-672"><a href="#cb65-672"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb65-673"><a href="#cb65-673"></a>        B,T,C <span class="op">=</span> x.shape <span class="co"># C here is n_embd</span></span>
<span id="cb65-674"><a href="#cb65-674"></a>        <span class="co"># Project input to K, Q, V</span></span>
<span id="cb65-675"><a href="#cb65-675"></a>        k <span class="op">=</span> <span class="va">self</span>.key(x)   <span class="co"># (B,T,head_size)</span></span>
<span id="cb65-676"><a href="#cb65-676"></a>        q <span class="op">=</span> <span class="va">self</span>.query(x) <span class="co"># (B,T,head_size)</span></span>
<span id="cb65-677"><a href="#cb65-677"></a>        <span class="co"># Compute attention scores, scale, mask, softmax</span></span>
<span id="cb65-678"><a href="#cb65-678"></a>        <span class="co"># Note the scaling by C**-0.5 (sqrt(n_embd)) as discussed before</span></span>
<span id="cb65-679"><a href="#cb65-679"></a>        wei <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> C<span class="op">**-</span><span class="fl">0.5</span>   <span class="co"># (B, T, T)</span></span>
<span id="cb65-680"><a href="#cb65-680"></a>        wei <span class="op">=</span> wei.masked_fill(<span class="va">self</span>.tril[:T, :T] <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))   <span class="co"># Use dynamic slicing [:T, :T] for flexibility if T &lt; block_size</span></span>
<span id="cb65-681"><a href="#cb65-681"></a>        wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>)   <span class="co"># (B, T, T)</span></span>
<span id="cb65-682"><a href="#cb65-682"></a>        wei <span class="op">=</span> <span class="va">self</span>.dropout(wei) <span class="co"># Apply dropout to attention weights</span></span>
<span id="cb65-683"><a href="#cb65-683"></a>        <span class="co"># Weighted aggregation of values</span></span>
<span id="cb65-684"><a href="#cb65-684"></a>        v <span class="op">=</span> <span class="va">self</span>.value(x) <span class="co"># (B,T,head_size)</span></span>
<span id="cb65-685"><a href="#cb65-685"></a>        out <span class="op">=</span> wei <span class="op">@</span> v <span class="co"># (B, T, T) @ (B, T, head_size) -&gt; (B, T, head_size)</span></span>
<span id="cb65-686"><a href="#cb65-686"></a>        <span class="cf">return</span> out</span>
<span id="cb65-687"><a href="#cb65-687"></a><span class="co"># ========================================</span></span>
<span id="cb65-688"><a href="#cb65-688"></a></span>
<span id="cb65-689"><a href="#cb65-689"></a><span class="co"># ===== MULTI-HEAD </span><span class="al">ATTENTION</span><span class="co"> =====</span></span>
<span id="cb65-690"><a href="#cb65-690"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb65-691"><a href="#cb65-691"></a>    <span class="co">"""Multiple heads of self-attention in parallel."""</span></span>
<span id="cb65-692"><a href="#cb65-692"></a>    </span>
<span id="cb65-693"><a href="#cb65-693"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_heads, head_size):</span>
<span id="cb65-694"><a href="#cb65-694"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb65-695"><a href="#cb65-695"></a>        <span class="va">self</span>.heads <span class="op">=</span> nn.ModuleList([Head(head_size) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_heads)])</span>
<span id="cb65-696"><a href="#cb65-696"></a>        <span class="co"># Linear layer after concatenating heads</span></span>
<span id="cb65-697"><a href="#cb65-697"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Linear(n_embd, n_embd) <span class="co"># Projects back to n_embd dimension</span></span>
<span id="cb65-698"><a href="#cb65-698"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb65-699"><a href="#cb65-699"></a></span>
<span id="cb65-700"><a href="#cb65-700"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb65-701"><a href="#cb65-701"></a>        <span class="co"># Compute attention for each head and concatenate results</span></span>
<span id="cb65-702"><a href="#cb65-702"></a>        out <span class="op">=</span> torch.cat([h(x) <span class="cf">for</span> h <span class="kw">in</span> <span class="va">self</span>.heads], dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># Shape (B, T, num_heads * head_size) = (B, T, n_embd)</span></span>
<span id="cb65-703"><a href="#cb65-703"></a>        <span class="co"># Apply final projection and dropout</span></span>
<span id="cb65-704"><a href="#cb65-704"></a>        out <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.proj(out))</span>
<span id="cb65-705"><a href="#cb65-705"></a>        <span class="cf">return</span> out</span>
<span id="cb65-706"><a href="#cb65-706"></a><span class="co"># ===============================</span></span>
<span id="cb65-707"><a href="#cb65-707"></a></span>
<span id="cb65-708"><a href="#cb65-708"></a><span class="co"># ===== FEED-FORWARD NETWORK =====</span></span>
<span id="cb65-709"><a href="#cb65-709"></a><span class="kw">class</span> FeedFoward(nn.Module):</span>
<span id="cb65-710"><a href="#cb65-710"></a>    <span class="co">"""Simple position-wise feed-forward network with one hidden layer."""</span></span>
<span id="cb65-711"><a href="#cb65-711"></a>    </span>
<span id="cb65-712"><a href="#cb65-712"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd):</span>
<span id="cb65-713"><a href="#cb65-713"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb65-714"><a href="#cb65-714"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb65-715"><a href="#cb65-715"></a>            nn.Linear(n_embd, <span class="dv">4</span> <span class="op">*</span> n_embd),   <span class="co"># Expand dimension (common practice)</span></span>
<span id="cb65-716"><a href="#cb65-716"></a>            nn.ReLU(),                      <span class="co"># Non-linearity</span></span>
<span id="cb65-717"><a href="#cb65-717"></a>            nn.Linear(<span class="dv">4</span> <span class="op">*</span> n_embd, n_embd),   <span class="co"># Project back to original dimension</span></span>
<span id="cb65-718"><a href="#cb65-718"></a>            nn.Dropout(dropout),</span>
<span id="cb65-719"><a href="#cb65-719"></a>        )</span>
<span id="cb65-720"><a href="#cb65-720"></a></span>
<span id="cb65-721"><a href="#cb65-721"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb65-722"><a href="#cb65-722"></a>        <span class="cf">return</span> <span class="va">self</span>.net(x)</span>
<span id="cb65-723"><a href="#cb65-723"></a><span class="co"># ==============================</span></span>
<span id="cb65-724"><a href="#cb65-724"></a></span>
<span id="cb65-725"><a href="#cb65-725"></a><span class="co"># ===== TRANSFORMER BLOCK =====</span></span>
<span id="cb65-726"><a href="#cb65-726"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb65-727"><a href="#cb65-727"></a>    <span class="co">"""Transformer block: communication (attention) followed by computation (FFN)."""</span></span>
<span id="cb65-728"><a href="#cb65-728"></a>    </span>
<span id="cb65-729"><a href="#cb65-729"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd, n_head):</span>
<span id="cb65-730"><a href="#cb65-730"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb65-731"><a href="#cb65-731"></a>        head_size <span class="op">=</span> n_embd <span class="op">//</span> n_head   <span class="co"># Calculate size for each head</span></span>
<span id="cb65-732"><a href="#cb65-732"></a>        <span class="va">self</span>.sa <span class="op">=</span> MultiHeadAttention(n_head, head_size) <span class="co"># Self-Attention layer</span></span>
<span id="cb65-733"><a href="#cb65-733"></a>        <span class="va">self</span>.ffwd <span class="op">=</span> FeedFoward(n_embd) <span class="co"># Feed-Forward layer</span></span>
<span id="cb65-734"><a href="#cb65-734"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> nn.LayerNorm(n_embd) <span class="co"># LayerNorm for Attention input</span></span>
<span id="cb65-735"><a href="#cb65-735"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> nn.LayerNorm(n_embd) <span class="co"># LayerNorm for FFN input</span></span>
<span id="cb65-736"><a href="#cb65-736"></a></span>
<span id="cb65-737"><a href="#cb65-737"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb65-738"><a href="#cb65-738"></a>        <span class="co"># Pre-Normalization variant: Norm -&gt; Sublayer -&gt; Residual</span></span>
<span id="cb65-739"><a href="#cb65-739"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.sa(<span class="va">self</span>.ln1(x))  <span class="co"># Attention block</span></span>
<span id="cb65-740"><a href="#cb65-740"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffwd(<span class="va">self</span>.ln2(x)) <span class="co"># Feed-forward block</span></span>
<span id="cb65-741"><a href="#cb65-741"></a>        <span class="cf">return</span> x</span>
<span id="cb65-742"><a href="#cb65-742"></a><span class="co"># ============================</span></span>
<span id="cb65-743"><a href="#cb65-743"></a></span>
<span id="cb65-744"><a href="#cb65-744"></a><span class="co"># ===== LANGUAGE MODEL =====</span></span>
<span id="cb65-745"><a href="#cb65-745"></a><span class="kw">class</span> BigramLanguageModel(nn.Module):</span>
<span id="cb65-746"><a href="#cb65-746"></a>    <span class="co">"""GPT-like language model using Transformer blocks."""</span></span>
<span id="cb65-747"><a href="#cb65-747"></a>    </span>
<span id="cb65-748"><a href="#cb65-748"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb65-749"><a href="#cb65-749"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb65-750"><a href="#cb65-750"></a>        <span class="co"># Token Embedding Table: Maps character index to embedding vector. (vocab_size, n_embd)</span></span>
<span id="cb65-751"><a href="#cb65-751"></a>        <span class="va">self</span>.token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, n_embd)</span>
<span id="cb65-752"><a href="#cb65-752"></a>        <span class="co"># Position Embedding Table: Maps position index (0 to block_size-1) to embedding vector. (block_size, n_embd)</span></span>
<span id="cb65-753"><a href="#cb65-753"></a>        <span class="va">self</span>.position_embedding_table <span class="op">=</span> nn.Embedding(block_size, n_embd)</span>
<span id="cb65-754"><a href="#cb65-754"></a>        <span class="co"># Sequence of Transformer Blocks</span></span>
<span id="cb65-755"><a href="#cb65-755"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.Sequential(<span class="op">*</span>[Block(n_embd, n_head<span class="op">=</span>n_head) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_layer)])</span>
<span id="cb65-756"><a href="#cb65-756"></a>        <span class="co"># Final Layer Normalization (applied after blocks)</span></span>
<span id="cb65-757"><a href="#cb65-757"></a>        <span class="va">self</span>.ln_f <span class="op">=</span> nn.LayerNorm(n_embd)   <span class="co"># Final layer norm</span></span>
<span id="cb65-758"><a href="#cb65-758"></a>        <span class="co"># Linear Head: Maps final embedding back to vocabulary size to get logits. (n_embd, vocab_size)</span></span>
<span id="cb65-759"><a href="#cb65-759"></a>        <span class="va">self</span>.lm_head <span class="op">=</span> nn.Linear(n_embd, vocab_size)</span>
<span id="cb65-760"><a href="#cb65-760"></a></span>
<span id="cb65-761"><a href="#cb65-761"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb65-762"><a href="#cb65-762"></a>        B, T <span class="op">=</span> idx.shape</span>
<span id="cb65-763"><a href="#cb65-763"></a>        </span>
<span id="cb65-764"><a href="#cb65-764"></a>        <span class="co"># Get token embeddings from indices: (B, T) -&gt; (B, T, n_embd)</span></span>
<span id="cb65-765"><a href="#cb65-765"></a>        tok_emb <span class="op">=</span> <span class="va">self</span>.token_embedding_table(idx)</span>
<span id="cb65-766"><a href="#cb65-766"></a>        <span class="co"># Get position embeddings: Create indices 0..T-1, look up embeddings -&gt; (T, n_embd)</span></span>
<span id="cb65-767"><a href="#cb65-767"></a>        pos_emb <span class="op">=</span> <span class="va">self</span>.position_embedding_table(torch.arange(T, device<span class="op">=</span>device))</span>
<span id="cb65-768"><a href="#cb65-768"></a>        <span class="co"># Combine token and position embeddings by addition: (B, T, n_embd). Broadcasting handles the addition.</span></span>
<span id="cb65-769"><a href="#cb65-769"></a>        x <span class="op">=</span> tok_emb <span class="op">+</span> pos_emb   <span class="co"># (B,T,C)</span></span>
<span id="cb65-770"><a href="#cb65-770"></a>        <span class="co"># Pass through Transformer blocks: (B, T, n_embd) -&gt; (B, T, n_embd)</span></span>
<span id="cb65-771"><a href="#cb65-771"></a>        x <span class="op">=</span> <span class="va">self</span>.blocks(x)</span>
<span id="cb65-772"><a href="#cb65-772"></a>        <span class="co"># Apply final LayerNorm</span></span>
<span id="cb65-773"><a href="#cb65-773"></a>        x <span class="op">=</span> <span class="va">self</span>.ln_f(x)</span>
<span id="cb65-774"><a href="#cb65-774"></a>        <span class="co"># Map to vocabulary logits: (B, T, n_embd) -&gt; (B, T, vocab_size)</span></span>
<span id="cb65-775"><a href="#cb65-775"></a>        logits <span class="op">=</span> <span class="va">self</span>.lm_head(x)</span>
<span id="cb65-776"><a href="#cb65-776"></a></span>
<span id="cb65-777"><a href="#cb65-777"></a>        <span class="co"># Calculate loss if targets are provided (same as before)</span></span>
<span id="cb65-778"><a href="#cb65-778"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb65-779"><a href="#cb65-779"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb65-780"><a href="#cb65-780"></a>        <span class="cf">else</span>:</span>
<span id="cb65-781"><a href="#cb65-781"></a>            <span class="co"># Reshape for cross_entropy: (B*T, vocab_size) and (B*T)</span></span>
<span id="cb65-782"><a href="#cb65-782"></a>            B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb65-783"><a href="#cb65-783"></a>            logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C)</span>
<span id="cb65-784"><a href="#cb65-784"></a>            targets <span class="op">=</span> targets.view(B<span class="op">*</span>T)</span>
<span id="cb65-785"><a href="#cb65-785"></a>            loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb65-786"><a href="#cb65-786"></a></span>
<span id="cb65-787"><a href="#cb65-787"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb65-788"><a href="#cb65-788"></a></span>
<span id="cb65-789"><a href="#cb65-789"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens):</span>
<span id="cb65-790"><a href="#cb65-790"></a>        <span class="co">"""Generate new text given a starting sequence."""</span></span>
<span id="cb65-791"><a href="#cb65-791"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb65-792"><a href="#cb65-792"></a>            <span class="co"># Crop context `idx` to the last `block_size` tokens. Important as position embeddings only go up to block_size.</span></span>
<span id="cb65-793"><a href="#cb65-793"></a>            idx_cond <span class="op">=</span> idx[:, <span class="op">-</span>block_size:]</span>
<span id="cb65-794"><a href="#cb65-794"></a>            <span class="co"># Get predictions (logits) from the model</span></span>
<span id="cb65-795"><a href="#cb65-795"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>(idx_cond)</span>
<span id="cb65-796"><a href="#cb65-796"></a>            <span class="co"># Focus on the logits for the *last* time step: (B, C)</span></span>
<span id="cb65-797"><a href="#cb65-797"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :]</span>
<span id="cb65-798"><a href="#cb65-798"></a>            <span class="co"># Convert logits to probabilities via softmax</span></span>
<span id="cb65-799"><a href="#cb65-799"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)   <span class="co"># (B, C)</span></span>
<span id="cb65-800"><a href="#cb65-800"></a>            <span class="co"># Sample next token index from the probability distribution</span></span>
<span id="cb65-801"><a href="#cb65-801"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)   <span class="co"># (B, 1)</span></span>
<span id="cb65-802"><a href="#cb65-802"></a>            <span class="co"># Append the sampled index to the running sequence</span></span>
<span id="cb65-803"><a href="#cb65-803"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>)   <span class="co"># (B, T+1)</span></span>
<span id="cb65-804"><a href="#cb65-804"></a>        <span class="cf">return</span> idx</span>
<span id="cb65-805"><a href="#cb65-805"></a><span class="co"># =========================</span></span>
<span id="cb65-806"><a href="#cb65-806"></a></span>
<span id="cb65-807"><a href="#cb65-807"></a><span class="co"># ===== MODEL INITIALIZATION AND TRAINING =====</span></span>
<span id="cb65-808"><a href="#cb65-808"></a><span class="co"># Create model instance and move it to the selected device</span></span>
<span id="cb65-809"><a href="#cb65-809"></a>model <span class="op">=</span> BigramLanguageModel()</span>
<span id="cb65-810"><a href="#cb65-810"></a>m <span class="op">=</span> model.to(device)</span>
<span id="cb65-811"><a href="#cb65-811"></a><span class="co"># Print number of parameters (useful for understanding model size)</span></span>
<span id="cb65-812"><a href="#cb65-812"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> m.parameters())<span class="op">/</span><span class="fl">1e6</span>, <span class="st">'M parameters'</span>) <span class="co"># Calculate and print M parameters</span></span>
<span id="cb65-813"><a href="#cb65-813"></a></span>
<span id="cb65-814"><a href="#cb65-814"></a><span class="co"># Create optimizer (AdamW again)</span></span>
<span id="cb65-815"><a href="#cb65-815"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb65-816"><a href="#cb65-816"></a></span>
<span id="cb65-817"><a href="#cb65-817"></a><span class="co"># Training loop</span></span>
<span id="cb65-818"><a href="#cb65-818"></a><span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(max_iters):</span>
<span id="cb65-819"><a href="#cb65-819"></a>    <span class="co"># Evaluate loss periodically</span></span>
<span id="cb65-820"><a href="#cb65-820"></a>    <span class="cf">if</span> <span class="bu">iter</span> <span class="op">%</span> eval_interval <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> <span class="bu">iter</span> <span class="op">==</span> max_iters <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb65-821"><a href="#cb65-821"></a>        losses <span class="op">=</span> estimate_loss() <span class="co"># Get train/val loss using the helper function</span></span>
<span id="cb65-822"><a href="#cb65-822"></a>        <span class="bu">print</span>(<span class="ss">f"step </span><span class="sc">{</span><span class="bu">iter</span><span class="sc">}</span><span class="ss">: train loss </span><span class="sc">{</span>losses[<span class="st">'train'</span>]<span class="sc">:.4f}</span><span class="ss">, val loss </span><span class="sc">{</span>losses[<span class="st">'val'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>) <span class="co"># Print losses</span></span>
<span id="cb65-823"><a href="#cb65-823"></a></span>
<span id="cb65-824"><a href="#cb65-824"></a>    <span class="co"># Sample a batch of data</span></span>
<span id="cb65-825"><a href="#cb65-825"></a>    xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb65-826"><a href="#cb65-826"></a></span>
<span id="cb65-827"><a href="#cb65-827"></a>    <span class="co"># Forward pass: Evaluate loss</span></span>
<span id="cb65-828"><a href="#cb65-828"></a>    logits, loss <span class="op">=</span> model(xb, yb)</span>
<span id="cb65-829"><a href="#cb65-829"></a>    <span class="co"># Backward pass: Calculate gradients</span></span>
<span id="cb65-830"><a href="#cb65-830"></a>    optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>) <span class="co"># Zero gradients</span></span>
<span id="cb65-831"><a href="#cb65-831"></a>    loss.backward() <span class="co"># Backpropagation</span></span>
<span id="cb65-832"><a href="#cb65-832"></a>    <span class="co"># Update parameters</span></span>
<span id="cb65-833"><a href="#cb65-833"></a>    optimizer.step() <span class="co"># Optimizer step</span></span>
<span id="cb65-834"><a href="#cb65-834"></a></span>
<span id="cb65-835"><a href="#cb65-835"></a><span class="co"># Generate text from the trained model</span></span>
<span id="cb65-836"><a href="#cb65-836"></a>context <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>, device<span class="op">=</span>device) <span class="co"># Starting context: [[0]]</span></span>
<span id="cb65-837"><a href="#cb65-837"></a><span class="bu">print</span>(decode(m.generate(context, max_new_tokens<span class="op">=</span><span class="dv">2000</span>)[<span class="dv">0</span>].tolist()))</span>
<span id="cb65-838"><a href="#cb65-838"></a><span class="co"># ============================================</span></span>
<span id="cb65-839"><a href="#cb65-839"></a><span class="in">```</span></span>
<span id="cb65-840"><a href="#cb65-840"></a></span>
<span id="cb65-841"><a href="#cb65-841"></a></span>
<span id="cb65-842"><a href="#cb65-842"></a>::: {.callout-note}</span>
<span id="cb65-843"><a href="#cb65-843"></a>With 5000 iterations, the model is able to generate text that is similar to the training text.</span>
<span id="cb65-844"><a href="#cb65-844"></a>:::</span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>