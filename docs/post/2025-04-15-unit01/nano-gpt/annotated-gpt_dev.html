<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Andrey Karpathy">
<meta name="dcterms.date" content="2025-04-11">
<meta name="description" content="Companion notebook from Karpathys video on building a minimal GPT, annotated by cursors LLM with summary from gemini.">

<title>Building a GPT - companion notebook qmd – GENE 46100</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-dcc7d9e706e4adfbab8e3d6c7c60bab2.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">GENE 46100</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#building-a-gpt" id="toc-building-a-gpt" class="nav-link active" data-scroll-target="#building-a-gpt">Building a GPT</a></li>
  <li><a href="#data-preparation" id="toc-data-preparation" class="nav-link" data-scroll-target="#data-preparation">Data Preparation</a>
  <ul class="collapse">
  <li><a href="#step-1-loading-inspection" id="toc-step-1-loading-inspection" class="nav-link" data-scroll-target="#step-1-loading-inspection">Step 1: Loading &amp; Inspection</a></li>
  <li><a href="#step-2-tokenization" id="toc-step-2-tokenization" class="nav-link" data-scroll-target="#step-2-tokenization">Step 2: Tokenization</a></li>
  <li><a href="#step-3-creating-batches-for-training" id="toc-step-3-creating-batches-for-training" class="nav-link" data-scroll-target="#step-3-creating-batches-for-training">Step 3: Creating Batches for Training</a></li>
  <li><a href="#understanding-the-context-and-target" id="toc-understanding-the-context-and-target" class="nav-link" data-scroll-target="#understanding-the-context-and-target">Understanding the Context and Target</a></li>
  <li><a href="#creating-batches" id="toc-creating-batches" class="nav-link" data-scroll-target="#creating-batches">Creating Batches</a></li>
  </ul></li>
  <li><a href="#model-architecture" id="toc-model-architecture" class="nav-link" data-scroll-target="#model-architecture">Model Architecture</a>
  <ul class="collapse">
  <li><a href="#starting-simple-bigram-language-model" id="toc-starting-simple-bigram-language-model" class="nav-link" data-scroll-target="#starting-simple-bigram-language-model">Starting Simple: Bigram Language Model</a></li>
  <li><a href="#understanding-cross-entropy-loss" id="toc-understanding-cross-entropy-loss" class="nav-link" data-scroll-target="#understanding-cross-entropy-loss">Understanding Cross Entropy Loss</a></li>
  <li><a href="#model-initialization-and-training" id="toc-model-initialization-and-training" class="nav-link" data-scroll-target="#model-initialization-and-training">Model Initialization and Training</a></li>
  <li><a href="#text-generation" id="toc-text-generation" class="nav-link" data-scroll-target="#text-generation">Text Generation</a></li>
  <li><a href="#training-setup" id="toc-training-setup" class="nav-link" data-scroll-target="#training-setup">Training Setup</a></li>
  <li><a href="#training-loop" id="toc-training-loop" class="nav-link" data-scroll-target="#training-loop">Training Loop</a></li>
  <li><a href="#define-the-context-and-target-8-examples-in-one-batch" id="toc-define-the-context-and-target-8-examples-in-one-batch" class="nav-link" data-scroll-target="#define-the-context-and-target-8-examples-in-one-batch">define the context and target: 8 examples in one batch</a></li>
  <li><a href="#define-the-batch-size-and-get-the-batch" id="toc-define-the-batch-size-and-get-the-batch" class="nav-link" data-scroll-target="#define-the-batch-size-and-get-the-batch">define the batch size and get the batch</a></li>
  <li><a href="#start-with-a-simple-model-the-bigram-language-model" id="toc-start-with-a-simple-model-the-bigram-language-model" class="nav-link" data-scroll-target="#start-with-a-simple-model-the-bigram-language-model">start with a simple model: the bigram language model</a></li>
  <li><a href="#cross-entropy-loss" id="toc-cross-entropy-loss" class="nav-link" data-scroll-target="#cross-entropy-loss">cross entropy loss</a></li>
  <li><a href="#initialize-the-model-and-compute-the-loss" id="toc-initialize-the-model-and-compute-the-loss" class="nav-link" data-scroll-target="#initialize-the-model-and-compute-the-loss">initialize the model and compute the loss</a></li>
  <li><a href="#generate-text" id="toc-generate-text" class="nav-link" data-scroll-target="#generate-text">generate text</a></li>
  <li><a href="#choose-adamw-as-the-optimizer" id="toc-choose-adamw-as-the-optimizer" class="nav-link" data-scroll-target="#choose-adamw-as-the-optimizer">choose AdamW as the optimizer</a></li>
  <li><a href="#train-the-model" id="toc-train-the-model" class="nav-link" data-scroll-target="#train-the-model">train the model</a></li>
  <li><a href="#generate-text-starting-with-as-initial-context" id="toc-generate-text-starting-with-as-initial-context" class="nav-link" data-scroll-target="#generate-text-starting-with-as-initial-context">generate text starting with as initial context</a></li>
  </ul></li>
  <li><a href="#self-attention-the-key-innovation" id="toc-self-attention-the-key-innovation" class="nav-link" data-scroll-target="#self-attention-the-key-innovation">Self-Attention: The Key Innovation</a>
  <ul class="collapse">
  <li><a href="#understanding-self-attention" id="toc-understanding-self-attention" class="nav-link" data-scroll-target="#understanding-self-attention">Understanding Self-Attention</a></li>
  <li><a href="#the-mathematical-trick-weighted-aggregation" id="toc-the-mathematical-trick-weighted-aggregation" class="nav-link" data-scroll-target="#the-mathematical-trick-weighted-aggregation">The Mathematical Trick: Weighted Aggregation</a></li>
  <li><a href="#version-1-using-a-for-loop" id="toc-version-1-using-a-for-loop" class="nav-link" data-scroll-target="#version-1-using-a-for-loop">Version 1: Using a For Loop</a></li>
  <li><a href="#version-2-matrix-multiplication" id="toc-version-2-matrix-multiplication" class="nav-link" data-scroll-target="#version-2-matrix-multiplication">Version 2: Matrix Multiplication</a></li>
  <li><a href="#version-3-using-softmax" id="toc-version-3-using-softmax" class="nav-link" data-scroll-target="#version-3-using-softmax">Version 3: Using Softmax</a></li>
  <li><a href="#version-4-self-attention" id="toc-version-4-self-attention" class="nav-link" data-scroll-target="#version-4-self-attention">Version 4: Self-Attention</a></li>
  <li><a href="#why-scaled-attention" id="toc-why-scaled-attention" class="nav-link" data-scroll-target="#why-scaled-attention">Why Scaled Attention?</a></li>
  </ul></li>
  <li><a href="#layer-normalization" id="toc-layer-normalization" class="nav-link" data-scroll-target="#layer-normalization">Layer Normalization</a>
  <ul class="collapse">
  <li><a href="#full-finished-code-for-reference" id="toc-full-finished-code-for-reference" class="nav-link" data-scroll-target="#full-finished-code-for-reference">Full finished code, for reference</a></li>
  </ul></li>
  <li><a href="#full-gpt-model-architecture" id="toc-full-gpt-model-architecture" class="nav-link" data-scroll-target="#full-gpt-model-architecture">Full GPT Model Architecture</a>
  <ul class="collapse">
  <li><a href="#key-components" id="toc-key-components" class="nav-link" data-scroll-target="#key-components">Key Components</a></li>
  <li><a href="#hyperparameters" id="toc-hyperparameters" class="nav-link" data-scroll-target="#hyperparameters">Hyperparameters</a></li>
  <li><a href="#device-selection" id="toc-device-selection" class="nav-link" data-scroll-target="#device-selection">Device Selection</a></li>
  <li><a href="#attention-head-implementation" id="toc-attention-head-implementation" class="nav-link" data-scroll-target="#attention-head-implementation">Attention Head Implementation</a></li>
  <li><a href="#multi-head-attention" id="toc-multi-head-attention" class="nav-link" data-scroll-target="#multi-head-attention">Multi-Head Attention</a></li>
  <li><a href="#feed-forward-network" id="toc-feed-forward-network" class="nav-link" data-scroll-target="#feed-forward-network">Feed-Forward Network</a></li>
  <li><a href="#transformer-block" id="toc-transformer-block" class="nav-link" data-scroll-target="#transformer-block">Transformer Block</a></li>
  <li><a href="#full-language-model" id="toc-full-language-model" class="nav-link" data-scroll-target="#full-language-model">Full Language Model</a></li>
  <li><a href="#training-loop-1" id="toc-training-loop-1" class="nav-link" data-scroll-target="#training-loop-1">Training Loop</a></li>
  </ul></li>
  <li><a href="#key-takeaways" id="toc-key-takeaways" class="nav-link" data-scroll-target="#key-takeaways">Key Takeaways</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Building a GPT - companion notebook qmd</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
</div>

<div>
  <div class="description">
    Companion notebook from Karpathys video on building a minimal GPT, annotated by cursors LLM with summary from gemini.
  </div>
</div>


<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Andrey Karpathy </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 11, 2025</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="building-a-gpt" class="level2">
<h2 class="anchored" data-anchor-id="building-a-gpt">Building a GPT</h2>
<p>Companion notebook to the <a href="https://karpathy.ai/zero-to-hero.html">Zero To Hero</a> video on GPT. Downloaded from <a href="https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing">here</a></p>
<p>(https://github.com/karpathy/nanoGPT)</p>
</section>
<section id="data-preparation" class="level2">
<h2 class="anchored" data-anchor-id="data-preparation">Data Preparation</h2>
<section id="step-1-loading-inspection" class="level3">
<h3 class="anchored" data-anchor-id="step-1-loading-inspection">Step 1: Loading &amp; Inspection</h3>
<div id="14f9e994" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># We always start with a dataset to train on. Let's download the tiny shakespeare dataset</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="co">#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="5b4d44f4" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="co"># read it in to inspect it</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'input.txt'</span>, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> f:</span>
<span id="cb2-3"><a href="#cb2-3"></a>    text <span class="op">=</span> f.read()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="6e583f91" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="co"># print the length of the dataset</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="bu">print</span>(<span class="st">"length of dataset in characters: "</span>, <span class="bu">len</span>(text))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>length of dataset in characters:  1115394</code></pre>
</div>
</div>
<div id="ac775ecc" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="co"># let's look at the first 1000 characters</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="bu">print</span>(text[:<span class="dv">1000</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>First Citizen:
Before we proceed any further, hear me speak.

All:
Speak, speak.

First Citizen:
You are all resolved rather to die than to famish?

All:
Resolved. resolved.

First Citizen:
First, you know Caius Marcius is chief enemy to the people.

All:
We know't, we know't.

First Citizen:
Let us kill him, and we'll have corn at our own price.
Is't a verdict?

All:
No more talking on't; let it be done: away, away!

Second Citizen:
One word, good citizens.

First Citizen:
We are accounted poor citizens, the patricians good.
What authority surfeits on would relieve us: if they
would yield us but the superfluity, while it were
wholesome, we might guess they relieved us humanely;
but they think we are too dear: the leanness that
afflicts us, the object of our misery, is as an
inventory to particularise their abundance; our
sufferance is a gain to them Let us revenge this with
our pikes, ere we become rakes: for the gods know I
speak this in hunger for bread, not in thirst for revenge.

</code></pre>
</div>
</div>
</section>
<section id="step-2-tokenization" class="level3">
<h3 class="anchored" data-anchor-id="step-2-tokenization">Step 2: Tokenization</h3>
<div id="f578971b" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a><span class="co"># here are all the unique characters that occur in this text</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(text)))</span>
<span id="cb7-3"><a href="#cb7-3"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(chars)</span>
<span id="cb7-4"><a href="#cb7-4"></a><span class="bu">print</span>(<span class="st">''</span>.join(chars))</span>
<span id="cb7-5"><a href="#cb7-5"></a><span class="bu">print</span>(vocab_size)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
 !$&amp;',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz
65</code></pre>
</div>
</div>
<div id="f4428ea4" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a><span class="co"># create a mapping from characters to integers</span></span>
<span id="cb9-2"><a href="#cb9-2"></a>stoi <span class="op">=</span> { ch:i <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }</span>
<span id="cb9-3"><a href="#cb9-3"></a>itos <span class="op">=</span> { i:ch <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }</span>
<span id="cb9-4"><a href="#cb9-4"></a>encode <span class="op">=</span> <span class="kw">lambda</span> s: [stoi[c] <span class="cf">for</span> c <span class="kw">in</span> s] <span class="co"># encoder: take a string, output a list of integers</span></span>
<span id="cb9-5"><a href="#cb9-5"></a>decode <span class="op">=</span> <span class="kw">lambda</span> l: <span class="st">''</span>.join([itos[i] <span class="cf">for</span> i <span class="kw">in</span> l]) <span class="co"># decoder: take a list of integers, output a string</span></span>
<span id="cb9-6"><a href="#cb9-6"></a></span>
<span id="cb9-7"><a href="#cb9-7"></a><span class="bu">print</span>(encode(<span class="st">"hii there"</span>))</span>
<span id="cb9-8"><a href="#cb9-8"></a><span class="bu">print</span>(decode(encode(<span class="st">"hii there"</span>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[46, 47, 47, 1, 58, 46, 43, 56, 43]
hii there</code></pre>
</div>
</div>
</section>
<section id="step-3-creating-batches-for-training" class="level3">
<h3 class="anchored" data-anchor-id="step-3-creating-batches-for-training">Step 3: Creating Batches for Training</h3>
<div id="95f3a944" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a><span class="co"># let's now encode the entire text dataset and store it into a torch.Tensor</span></span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="im">import</span> torch <span class="co"># we use PyTorch: https://pytorch.org</span></span>
<span id="cb11-3"><a href="#cb11-3"></a>data <span class="op">=</span> torch.tensor(encode(text), dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="bu">print</span>(data.shape, data.dtype)</span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="bu">print</span>(data[:<span class="dv">1000</span>]) <span class="co"># the 1000 characters we looked at earier will to the GPT look like this</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([1115394]) torch.int64
tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,
        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,
         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,
        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,
         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,
        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,
         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,
        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,
        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,
         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,
         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,
        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,
        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,
         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,
        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,
        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,
        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,
        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,
        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,
        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,
        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,
         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,
         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,
         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,
        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,
        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,
        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,
        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,
        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,
        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,
        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,
         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,
         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,
        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,
        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,
        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,
         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,
        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,
        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,
         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,
        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,
        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,
        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,
        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,
        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,
        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,
        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,
        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,
        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,
         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,
        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,
        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,
        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,
        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,
        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,
        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])</code></pre>
</div>
</div>
<div id="f22cbc39" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="co"># split up the data into train and validation sets</span></span>
<span id="cb13-2"><a href="#cb13-2"></a>n <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span><span class="op">*</span><span class="bu">len</span>(data)) <span class="co"># first 90% will be train, rest val</span></span>
<span id="cb13-3"><a href="#cb13-3"></a>train_data <span class="op">=</span> data[:n]</span>
<span id="cb13-4"><a href="#cb13-4"></a>val_data <span class="op">=</span> data[n:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="d51889a3" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a><span class="co"># define the block size (context length)</span></span>
<span id="cb14-2"><a href="#cb14-2"></a>block_size <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb14-3"><a href="#cb14-3"></a>train_data[:block_size<span class="op">+</span><span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="9">
<pre><code>tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])</code></pre>
</div>
</div>
</section>
<section id="understanding-the-context-and-target" class="level3">
<h3 class="anchored" data-anchor-id="understanding-the-context-and-target">Understanding the Context and Target</h3>
<div id="d17dbd73" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1"></a>x <span class="op">=</span> train_data[:block_size]</span>
<span id="cb16-2"><a href="#cb16-2"></a>y <span class="op">=</span> train_data[<span class="dv">1</span>:block_size<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb16-3"><a href="#cb16-3"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(block_size):</span>
<span id="cb16-4"><a href="#cb16-4"></a>    context <span class="op">=</span> x[:t<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb16-5"><a href="#cb16-5"></a>    target <span class="op">=</span> y[t]</span>
<span id="cb16-6"><a href="#cb16-6"></a>    <span class="bu">print</span>(<span class="ss">f"when input is </span><span class="sc">{</span>context<span class="sc">}</span><span class="ss"> the target: </span><span class="sc">{</span>target<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>when input is tensor([18]) the target: 47
when input is tensor([18, 47]) the target: 56
when input is tensor([18, 47, 56]) the target: 57
when input is tensor([18, 47, 56, 57]) the target: 58
when input is tensor([18, 47, 56, 57, 58]) the target: 1
when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15
when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47
when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58</code></pre>
</div>
</div>
</section>
<section id="creating-batches" class="level3">
<h3 class="anchored" data-anchor-id="creating-batches">Creating Batches</h3>
<div id="67da82d6" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb18-2"><a href="#cb18-2"></a>batch_size <span class="op">=</span> <span class="dv">4</span> <span class="co"># how many independent sequences will we process in parallel?</span></span>
<span id="cb18-3"><a href="#cb18-3"></a>block_size <span class="op">=</span> <span class="dv">8</span> <span class="co"># what is the maximum context length for predictions?</span></span>
<span id="cb18-4"><a href="#cb18-4"></a></span>
<span id="cb18-5"><a href="#cb18-5"></a><span class="kw">def</span> get_batch(split):</span>
<span id="cb18-6"><a href="#cb18-6"></a>    <span class="co"># generate a small batch of data of inputs x and targets y</span></span>
<span id="cb18-7"><a href="#cb18-7"></a>    data <span class="op">=</span> train_data <span class="cf">if</span> split <span class="op">==</span> <span class="st">'train'</span> <span class="cf">else</span> val_data</span>
<span id="cb18-8"><a href="#cb18-8"></a>    ix <span class="op">=</span> torch.randint(<span class="bu">len</span>(data) <span class="op">-</span> block_size, (batch_size,))</span>
<span id="cb18-9"><a href="#cb18-9"></a>    x <span class="op">=</span> torch.stack([data[i:i<span class="op">+</span>block_size] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb18-10"><a href="#cb18-10"></a>    y <span class="op">=</span> torch.stack([data[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span>block_size<span class="op">+</span><span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb18-11"><a href="#cb18-11"></a>    <span class="cf">return</span> x, y</span>
<span id="cb18-12"><a href="#cb18-12"></a></span>
<span id="cb18-13"><a href="#cb18-13"></a>xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb18-14"><a href="#cb18-14"></a><span class="bu">print</span>(<span class="st">'inputs:'</span>)</span>
<span id="cb18-15"><a href="#cb18-15"></a><span class="bu">print</span>(xb.shape)</span>
<span id="cb18-16"><a href="#cb18-16"></a><span class="bu">print</span>(xb)</span>
<span id="cb18-17"><a href="#cb18-17"></a><span class="bu">print</span>(<span class="st">'targets:'</span>)</span>
<span id="cb18-18"><a href="#cb18-18"></a><span class="bu">print</span>(yb.shape)</span>
<span id="cb18-19"><a href="#cb18-19"></a><span class="bu">print</span>(yb)</span>
<span id="cb18-20"><a href="#cb18-20"></a></span>
<span id="cb18-21"><a href="#cb18-21"></a><span class="bu">print</span>(<span class="st">'----'</span>)</span>
<span id="cb18-22"><a href="#cb18-22"></a></span>
<span id="cb18-23"><a href="#cb18-23"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(batch_size): <span class="co"># batch dimension</span></span>
<span id="cb18-24"><a href="#cb18-24"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(block_size): <span class="co"># time dimension</span></span>
<span id="cb18-25"><a href="#cb18-25"></a>        context <span class="op">=</span> xb[b, :t<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb18-26"><a href="#cb18-26"></a>        target <span class="op">=</span> yb[b,t]</span>
<span id="cb18-27"><a href="#cb18-27"></a>        <span class="bu">print</span>(<span class="ss">f"when input is </span><span class="sc">{</span>context<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss"> the target: </span><span class="sc">{</span>target<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>inputs:
torch.Size([4, 8])
tensor([[24, 43, 58,  5, 57,  1, 46, 43],
        [44, 53, 56,  1, 58, 46, 39, 58],
        [52, 58,  1, 58, 46, 39, 58,  1],
        [25, 17, 27, 10,  0, 21,  1, 54]])
targets:
torch.Size([4, 8])
tensor([[43, 58,  5, 57,  1, 46, 43, 39],
        [53, 56,  1, 58, 46, 39, 58,  1],
        [58,  1, 58, 46, 39, 58,  1, 46],
        [17, 27, 10,  0, 21,  1, 54, 39]])
----
when input is [24] the target: 43
when input is [24, 43] the target: 58
when input is [24, 43, 58] the target: 5
when input is [24, 43, 58, 5] the target: 57
when input is [24, 43, 58, 5, 57] the target: 1
when input is [24, 43, 58, 5, 57, 1] the target: 46
when input is [24, 43, 58, 5, 57, 1, 46] the target: 43
when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39
when input is [44] the target: 53
when input is [44, 53] the target: 56
when input is [44, 53, 56] the target: 1
when input is [44, 53, 56, 1] the target: 58
when input is [44, 53, 56, 1, 58] the target: 46
when input is [44, 53, 56, 1, 58, 46] the target: 39
when input is [44, 53, 56, 1, 58, 46, 39] the target: 58
when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1
when input is [52] the target: 58
when input is [52, 58] the target: 1
when input is [52, 58, 1] the target: 58
when input is [52, 58, 1, 58] the target: 46
when input is [52, 58, 1, 58, 46] the target: 39
when input is [52, 58, 1, 58, 46, 39] the target: 58
when input is [52, 58, 1, 58, 46, 39, 58] the target: 1
when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46
when input is [25] the target: 17
when input is [25, 17] the target: 27
when input is [25, 17, 27] the target: 10
when input is [25, 17, 27, 10] the target: 0
when input is [25, 17, 27, 10, 0] the target: 21
when input is [25, 17, 27, 10, 0, 21] the target: 1
when input is [25, 17, 27, 10, 0, 21, 1] the target: 54
when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39</code></pre>
</div>
</div>
</section>
</section>
<section id="model-architecture" class="level2">
<h2 class="anchored" data-anchor-id="model-architecture">Model Architecture</h2>
<section id="starting-simple-bigram-language-model" class="level3">
<h3 class="anchored" data-anchor-id="starting-simple-bigram-language-model">Starting Simple: Bigram Language Model</h3>
<p>This model predicts the next character based on the current character only. It uses the logits for the next character in a lookup table.</p>
<p>Key points: - Simplest possible model - Uses an Embedding Table of size vocab_size x vocab_size - Each row contains predicted scores for next character - Ignores context beyond last character</p>
<div id="c7011dcf" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a><span class="co"># define the bigram language model</span></span>
<span id="cb20-2"><a href="#cb20-2"></a><span class="im">import</span> torch</span>
<span id="cb20-3"><a href="#cb20-3"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb20-4"><a href="#cb20-4"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb20-5"><a href="#cb20-5"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb20-6"><a href="#cb20-6"></a></span>
<span id="cb20-7"><a href="#cb20-7"></a><span class="kw">class</span> BigramLanguageModel(nn.Module):</span>
<span id="cb20-8"><a href="#cb20-8"></a></span>
<span id="cb20-9"><a href="#cb20-9"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size):</span>
<span id="cb20-10"><a href="#cb20-10"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb20-11"><a href="#cb20-11"></a>        <span class="co"># each token directly reads off the logits for the next token from a lookup table</span></span>
<span id="cb20-12"><a href="#cb20-12"></a>        <span class="va">self</span>.token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, vocab_size)</span>
<span id="cb20-13"><a href="#cb20-13"></a></span>
<span id="cb20-14"><a href="#cb20-14"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb20-15"><a href="#cb20-15"></a></span>
<span id="cb20-16"><a href="#cb20-16"></a>        <span class="co"># idx and targets are both (B,T) tensor of integers</span></span>
<span id="cb20-17"><a href="#cb20-17"></a>        logits <span class="op">=</span> <span class="va">self</span>.token_embedding_table(idx) <span class="co"># (B,T,C)</span></span>
<span id="cb20-18"><a href="#cb20-18"></a></span>
<span id="cb20-19"><a href="#cb20-19"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb20-20"><a href="#cb20-20"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb20-21"><a href="#cb20-21"></a>        <span class="cf">else</span>:</span>
<span id="cb20-22"><a href="#cb20-22"></a>            B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb20-23"><a href="#cb20-23"></a>            logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C)</span>
<span id="cb20-24"><a href="#cb20-24"></a>            targets <span class="op">=</span> targets.view(B<span class="op">*</span>T)</span>
<span id="cb20-25"><a href="#cb20-25"></a>            loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb20-26"><a href="#cb20-26"></a></span>
<span id="cb20-27"><a href="#cb20-27"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb20-28"><a href="#cb20-28"></a></span>
<span id="cb20-29"><a href="#cb20-29"></a>    <span class="co"># Text Generation Method</span></span>
<span id="cb20-30"><a href="#cb20-30"></a>    <span class="co"># This method implements autoregressive text generation by:</span></span>
<span id="cb20-31"><a href="#cb20-31"></a>    <span class="co"># 1. Taking the current context (idx)</span></span>
<span id="cb20-32"><a href="#cb20-32"></a>    <span class="co"># 2. Predicting the next token probabilities</span></span>
<span id="cb20-33"><a href="#cb20-33"></a>    <span class="co"># 3. Sampling from these probabilities</span></span>
<span id="cb20-34"><a href="#cb20-34"></a>    <span class="co"># 4. Appending the sampled token to the context</span></span>
<span id="cb20-35"><a href="#cb20-35"></a>    <span class="co"># 5. Repeating for max_new_tokens iterations</span></span>
<span id="cb20-36"><a href="#cb20-36"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens):</span>
<span id="cb20-37"><a href="#cb20-37"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb20-38"><a href="#cb20-38"></a>            <span class="co"># get the predictions</span></span>
<span id="cb20-39"><a href="#cb20-39"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>(idx)</span>
<span id="cb20-40"><a href="#cb20-40"></a>            <span class="co"># focus only on the last time step</span></span>
<span id="cb20-41"><a href="#cb20-41"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="co"># becomes (B, C)</span></span>
<span id="cb20-42"><a href="#cb20-42"></a>            <span class="co"># apply softmax to get probabilities</span></span>
<span id="cb20-43"><a href="#cb20-43"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># (B, C)</span></span>
<span id="cb20-44"><a href="#cb20-44"></a>            <span class="co"># sample from the distribution</span></span>
<span id="cb20-45"><a href="#cb20-45"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, 1)</span></span>
<span id="cb20-46"><a href="#cb20-46"></a>            <span class="co"># append sampled index to the running sequence</span></span>
<span id="cb20-47"><a href="#cb20-47"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, T+1)</span></span>
<span id="cb20-48"><a href="#cb20-48"></a>        <span class="cf">return</span> idx</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="understanding-cross-entropy-loss" class="level3">
<h3 class="anchored" data-anchor-id="understanding-cross-entropy-loss">Understanding Cross Entropy Loss</h3>
<p>Loss = -Σ(y * log(p)) where: - y = actual probability (0 or 1) - p = predicted probability - Σ = sum over all classes</p>
<p>This is the loss for a single token. The total loss is the average across all B*T tokens in the batch, where: - B = batch size (number of sequences processed in parallel) - T = sequence length (number of tokens in each sequence)</p>
<p>Before training, we would expect the model to predict the next character from a uniform distribution.</p>
<p>-Σ(y * log(p)) = = - ( 1 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) ) = - log(1/65) = 4.1744 per token.</p>
<p>i.e.&nbsp;at the beginning we expect loss of -log(1/65) = 4.1744 per token.</p>
</section>
<section id="model-initialization-and-training" class="level3">
<h3 class="anchored" data-anchor-id="model-initialization-and-training">Model Initialization and Training</h3>
<div id="5421df23" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1"></a>m <span class="op">=</span> BigramLanguageModel(vocab_size)</span>
<span id="cb21-2"><a href="#cb21-2"></a>logits, loss <span class="op">=</span> m(xb, yb)</span>
<span id="cb21-3"><a href="#cb21-3"></a><span class="bu">print</span>(logits.shape)</span>
<span id="cb21-4"><a href="#cb21-4"></a><span class="bu">print</span>(loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([32, 65])
tensor(4.8786, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
</section>
<section id="text-generation" class="level3">
<h3 class="anchored" data-anchor-id="text-generation">Text Generation</h3>
<div id="0413593b" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1"></a><span class="bu">print</span>(decode(m.generate(idx <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>), max_new_tokens<span class="op">=</span><span class="dv">100</span>)[<span class="dv">0</span>].tolist()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp
wnYWmnxKWWev-tDqXErVKLgJ</code></pre>
</div>
</div>
</section>
<section id="training-setup" class="level3">
<h3 class="anchored" data-anchor-id="training-setup">Training Setup</h3>
<div id="c302bb3a" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(m.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="training-loop" class="level3">
<h3 class="anchored" data-anchor-id="training-loop">Training Loop</h3>
<div id="51aa8246" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="cf">for</span> steps <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>): <span class="co"># increase number of steps for good results...</span></span>
<span id="cb26-3"><a href="#cb26-3"></a></span>
<span id="cb26-4"><a href="#cb26-4"></a>    <span class="co"># sample a batch of data</span></span>
<span id="cb26-5"><a href="#cb26-5"></a>    xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb26-6"><a href="#cb26-6"></a></span>
<span id="cb26-7"><a href="#cb26-7"></a>    <span class="co"># evaluate the loss</span></span>
<span id="cb26-8"><a href="#cb26-8"></a>    logits, loss <span class="op">=</span> m(xb, yb)</span>
<span id="cb26-9"><a href="#cb26-9"></a>    optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb26-10"><a href="#cb26-10"></a>    loss.backward()</span>
<span id="cb26-11"><a href="#cb26-11"></a>    optimizer.step()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="57a88613" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a><span class="co"># %pip install torch torchvision torchaudio scikit-learn</span></span>
<span id="cb27-2"><a href="#cb27-2"></a></span>
<span id="cb27-3"><a href="#cb27-3"></a><span class="co"># import os</span></span>
<span id="cb27-4"><a href="#cb27-4"></a><span class="co"># import time</span></span>
<span id="cb27-5"><a href="#cb27-5"></a><span class="co"># import math</span></span>
<span id="cb27-6"><a href="#cb27-6"></a><span class="co"># import pickle</span></span>
<span id="cb27-7"><a href="#cb27-7"></a><span class="co"># from contextlib import nullcontext</span></span>
<span id="cb27-8"><a href="#cb27-8"></a></span>
<span id="cb27-9"><a href="#cb27-9"></a><span class="co"># import numpy as np</span></span>
<span id="cb27-10"><a href="#cb27-10"></a><span class="co"># import torch</span></span>
<span id="cb27-11"><a href="#cb27-11"></a><span class="co"># from torch.nn.parallel import DistributedDataParallel as DDP</span></span>
<span id="cb27-12"><a href="#cb27-12"></a><span class="co"># from torch.distributed import init_process_group, destroy_process_group</span></span>
<span id="cb27-13"><a href="#cb27-13"></a></span>
<span id="cb27-14"><a href="#cb27-14"></a><span class="co"># from model import GPTConfig, GPT</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="82cb391a" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a><span class="co"># let's now encode the entire text dataset and store it into a torch.Tensor</span></span>
<span id="cb28-2"><a href="#cb28-2"></a><span class="im">import</span> torch <span class="co"># we use PyTorch: https://pytorch.org</span></span>
<span id="cb28-3"><a href="#cb28-3"></a>data <span class="op">=</span> torch.tensor(encode(text), dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb28-4"><a href="#cb28-4"></a><span class="bu">print</span>(data.shape, data.dtype)</span>
<span id="cb28-5"><a href="#cb28-5"></a><span class="bu">print</span>(data[:<span class="dv">1000</span>]) <span class="co"># the 1000 characters we looked at earier will to the GPT look like this</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([1115394]) torch.int64
tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,
        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,
         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,
        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,
         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,
        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,
         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,
        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,
        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,
         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,
         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,
        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,
        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,
         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,
        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,
        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,
        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,
        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,
        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,
        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,
        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,
         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,
         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,
         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,
        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,
        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,
        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,
        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,
        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,
        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,
        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,
         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,
         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,
        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,
        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,
        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,
         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,
        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,
        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,
         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,
        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,
        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,
        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,
        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,
        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,
        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,
        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,
        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,
        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,
         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,
        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,
        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,
        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,
        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,
        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,
        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])</code></pre>
</div>
</div>
<div id="7d92f529" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="co"># split up the data into train and validation sets</span></span>
<span id="cb30-2"><a href="#cb30-2"></a>n <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span><span class="op">*</span><span class="bu">len</span>(data)) <span class="co"># first 90% will be train, rest val</span></span>
<span id="cb30-3"><a href="#cb30-3"></a>train_data <span class="op">=</span> data[:n]</span>
<span id="cb30-4"><a href="#cb30-4"></a>val_data <span class="op">=</span> data[n:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="5ae3ae41" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a><span class="co"># define the block size</span></span>
<span id="cb31-2"><a href="#cb31-2"></a>block_size <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb31-3"><a href="#cb31-3"></a>train_data[:block_size<span class="op">+</span><span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])</code></pre>
</div>
</div>
</section>
<section id="define-the-context-and-target-8-examples-in-one-batch" class="level3">
<h3 class="anchored" data-anchor-id="define-the-context-and-target-8-examples-in-one-batch">define the context and target: 8 examples in one batch</h3>
<div id="2aa2c93a" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a>x <span class="op">=</span> train_data[:block_size]</span>
<span id="cb33-2"><a href="#cb33-2"></a>y <span class="op">=</span> train_data[<span class="dv">1</span>:block_size<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb33-3"><a href="#cb33-3"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(block_size):</span>
<span id="cb33-4"><a href="#cb33-4"></a>    context <span class="op">=</span> x[:t<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb33-5"><a href="#cb33-5"></a>    target <span class="op">=</span> y[t]</span>
<span id="cb33-6"><a href="#cb33-6"></a>    <span class="bu">print</span>(<span class="ss">f"when input is </span><span class="sc">{</span>context<span class="sc">}</span><span class="ss"> the target: </span><span class="sc">{</span>target<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>when input is tensor([18]) the target: 47
when input is tensor([18, 47]) the target: 56
when input is tensor([18, 47, 56]) the target: 57
when input is tensor([18, 47, 56, 57]) the target: 58
when input is tensor([18, 47, 56, 57, 58]) the target: 1
when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15
when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47
when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58</code></pre>
</div>
</div>
</section>
<section id="define-the-batch-size-and-get-the-batch" class="level3">
<h3 class="anchored" data-anchor-id="define-the-batch-size-and-get-the-batch">define the batch size and get the batch</h3>
<div id="6cd7dcd3" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb35-2"><a href="#cb35-2"></a>batch_size <span class="op">=</span> <span class="dv">4</span> <span class="co"># how many independent sequences will we process in parallel?</span></span>
<span id="cb35-3"><a href="#cb35-3"></a>block_size <span class="op">=</span> <span class="dv">8</span> <span class="co"># what is the maximum context length for predictions?</span></span>
<span id="cb35-4"><a href="#cb35-4"></a></span>
<span id="cb35-5"><a href="#cb35-5"></a><span class="kw">def</span> get_batch(split):</span>
<span id="cb35-6"><a href="#cb35-6"></a>    <span class="co"># generate a small batch of data of inputs x and targets y</span></span>
<span id="cb35-7"><a href="#cb35-7"></a>    data <span class="op">=</span> train_data <span class="cf">if</span> split <span class="op">==</span> <span class="st">'train'</span> <span class="cf">else</span> val_data</span>
<span id="cb35-8"><a href="#cb35-8"></a>    ix <span class="op">=</span> torch.randint(<span class="bu">len</span>(data) <span class="op">-</span> block_size, (batch_size,))</span>
<span id="cb35-9"><a href="#cb35-9"></a>    x <span class="op">=</span> torch.stack([data[i:i<span class="op">+</span>block_size] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb35-10"><a href="#cb35-10"></a>    y <span class="op">=</span> torch.stack([data[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span>block_size<span class="op">+</span><span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb35-11"><a href="#cb35-11"></a>    <span class="cf">return</span> x, y</span>
<span id="cb35-12"><a href="#cb35-12"></a></span>
<span id="cb35-13"><a href="#cb35-13"></a>xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb35-14"><a href="#cb35-14"></a><span class="bu">print</span>(<span class="st">'inputs:'</span>)</span>
<span id="cb35-15"><a href="#cb35-15"></a><span class="bu">print</span>(xb.shape)</span>
<span id="cb35-16"><a href="#cb35-16"></a><span class="bu">print</span>(xb)</span>
<span id="cb35-17"><a href="#cb35-17"></a><span class="bu">print</span>(<span class="st">'targets:'</span>)</span>
<span id="cb35-18"><a href="#cb35-18"></a><span class="bu">print</span>(yb.shape)</span>
<span id="cb35-19"><a href="#cb35-19"></a><span class="bu">print</span>(yb)</span>
<span id="cb35-20"><a href="#cb35-20"></a></span>
<span id="cb35-21"><a href="#cb35-21"></a><span class="bu">print</span>(<span class="st">'----'</span>)</span>
<span id="cb35-22"><a href="#cb35-22"></a></span>
<span id="cb35-23"><a href="#cb35-23"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(batch_size): <span class="co"># batch dimension</span></span>
<span id="cb35-24"><a href="#cb35-24"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(block_size): <span class="co"># time dimension</span></span>
<span id="cb35-25"><a href="#cb35-25"></a>        context <span class="op">=</span> xb[b, :t<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb35-26"><a href="#cb35-26"></a>        target <span class="op">=</span> yb[b,t]</span>
<span id="cb35-27"><a href="#cb35-27"></a>        <span class="bu">print</span>(<span class="ss">f"when input is </span><span class="sc">{</span>context<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss"> the target: </span><span class="sc">{</span>target<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>inputs:
torch.Size([4, 8])
tensor([[24, 43, 58,  5, 57,  1, 46, 43],
        [44, 53, 56,  1, 58, 46, 39, 58],
        [52, 58,  1, 58, 46, 39, 58,  1],
        [25, 17, 27, 10,  0, 21,  1, 54]])
targets:
torch.Size([4, 8])
tensor([[43, 58,  5, 57,  1, 46, 43, 39],
        [53, 56,  1, 58, 46, 39, 58,  1],
        [58,  1, 58, 46, 39, 58,  1, 46],
        [17, 27, 10,  0, 21,  1, 54, 39]])
----
when input is [24] the target: 43
when input is [24, 43] the target: 58
when input is [24, 43, 58] the target: 5
when input is [24, 43, 58, 5] the target: 57
when input is [24, 43, 58, 5, 57] the target: 1
when input is [24, 43, 58, 5, 57, 1] the target: 46
when input is [24, 43, 58, 5, 57, 1, 46] the target: 43
when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39
when input is [44] the target: 53
when input is [44, 53] the target: 56
when input is [44, 53, 56] the target: 1
when input is [44, 53, 56, 1] the target: 58
when input is [44, 53, 56, 1, 58] the target: 46
when input is [44, 53, 56, 1, 58, 46] the target: 39
when input is [44, 53, 56, 1, 58, 46, 39] the target: 58
when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1
when input is [52] the target: 58
when input is [52, 58] the target: 1
when input is [52, 58, 1] the target: 58
when input is [52, 58, 1, 58] the target: 46
when input is [52, 58, 1, 58, 46] the target: 39
when input is [52, 58, 1, 58, 46, 39] the target: 58
when input is [52, 58, 1, 58, 46, 39, 58] the target: 1
when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46
when input is [25] the target: 17
when input is [25, 17] the target: 27
when input is [25, 17, 27] the target: 10
when input is [25, 17, 27, 10] the target: 0
when input is [25, 17, 27, 10, 0] the target: 21
when input is [25, 17, 27, 10, 0, 21] the target: 1
when input is [25, 17, 27, 10, 0, 21, 1] the target: 54
when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39</code></pre>
</div>
</div>
</section>
<section id="start-with-a-simple-model-the-bigram-language-model" class="level3">
<h3 class="anchored" data-anchor-id="start-with-a-simple-model-the-bigram-language-model">start with a simple model: the bigram language model</h3>
<p>This model predicts the next character based on the current character only. It uses the logits for the next character in a lookup table.</p>
<div id="7f06e56e" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1"></a><span class="co"># define the bigram language model</span></span>
<span id="cb37-2"><a href="#cb37-2"></a><span class="im">import</span> torch</span>
<span id="cb37-3"><a href="#cb37-3"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb37-4"><a href="#cb37-4"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb37-5"><a href="#cb37-5"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb37-6"><a href="#cb37-6"></a></span>
<span id="cb37-7"><a href="#cb37-7"></a><span class="kw">class</span> BigramLanguageModel(nn.Module):</span>
<span id="cb37-8"><a href="#cb37-8"></a></span>
<span id="cb37-9"><a href="#cb37-9"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size):</span>
<span id="cb37-10"><a href="#cb37-10"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb37-11"><a href="#cb37-11"></a>        <span class="co"># each token directly reads off the logits for the next token from a lookup table</span></span>
<span id="cb37-12"><a href="#cb37-12"></a>        <span class="va">self</span>.token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, vocab_size)</span>
<span id="cb37-13"><a href="#cb37-13"></a></span>
<span id="cb37-14"><a href="#cb37-14"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb37-15"><a href="#cb37-15"></a></span>
<span id="cb37-16"><a href="#cb37-16"></a>        <span class="co"># idx and targets are both (B,T) tensor of integers</span></span>
<span id="cb37-17"><a href="#cb37-17"></a>        logits <span class="op">=</span> <span class="va">self</span>.token_embedding_table(idx) <span class="co"># (B,T,C)</span></span>
<span id="cb37-18"><a href="#cb37-18"></a></span>
<span id="cb37-19"><a href="#cb37-19"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb37-20"><a href="#cb37-20"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb37-21"><a href="#cb37-21"></a>        <span class="cf">else</span>:</span>
<span id="cb37-22"><a href="#cb37-22"></a>            B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb37-23"><a href="#cb37-23"></a>            logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C)</span>
<span id="cb37-24"><a href="#cb37-24"></a>            targets <span class="op">=</span> targets.view(B<span class="op">*</span>T)</span>
<span id="cb37-25"><a href="#cb37-25"></a>            loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb37-26"><a href="#cb37-26"></a></span>
<span id="cb37-27"><a href="#cb37-27"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb37-28"><a href="#cb37-28"></a></span>
<span id="cb37-29"><a href="#cb37-29"></a>    <span class="co"># Text Generation Method</span></span>
<span id="cb37-30"><a href="#cb37-30"></a>    <span class="co"># This method implements autoregressive text generation by:</span></span>
<span id="cb37-31"><a href="#cb37-31"></a>    <span class="co"># 1. Taking the current context (idx)</span></span>
<span id="cb37-32"><a href="#cb37-32"></a>    <span class="co"># 2. Predicting the next token probabilities</span></span>
<span id="cb37-33"><a href="#cb37-33"></a>    <span class="co"># 3. Sampling from these probabilities</span></span>
<span id="cb37-34"><a href="#cb37-34"></a>    <span class="co"># 4. Appending the sampled token to the context</span></span>
<span id="cb37-35"><a href="#cb37-35"></a>    <span class="co"># 5. Repeating for max_new_tokens iterations</span></span>
<span id="cb37-36"><a href="#cb37-36"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens):</span>
<span id="cb37-37"><a href="#cb37-37"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb37-38"><a href="#cb37-38"></a>            <span class="co"># get the predictions</span></span>
<span id="cb37-39"><a href="#cb37-39"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>(idx)</span>
<span id="cb37-40"><a href="#cb37-40"></a>            <span class="co"># focus only on the last time step</span></span>
<span id="cb37-41"><a href="#cb37-41"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="co"># becomes (B, C)</span></span>
<span id="cb37-42"><a href="#cb37-42"></a>            <span class="co"># apply softmax to get probabilities</span></span>
<span id="cb37-43"><a href="#cb37-43"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># (B, C)</span></span>
<span id="cb37-44"><a href="#cb37-44"></a>            <span class="co"># sample from the distribution</span></span>
<span id="cb37-45"><a href="#cb37-45"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, 1)</span></span>
<span id="cb37-46"><a href="#cb37-46"></a>            <span class="co"># append sampled index to the running sequence</span></span>
<span id="cb37-47"><a href="#cb37-47"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, T+1)</span></span>
<span id="cb37-48"><a href="#cb37-48"></a>        <span class="cf">return</span> idx</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="cross-entropy-loss" class="level3">
<h3 class="anchored" data-anchor-id="cross-entropy-loss">cross entropy loss</h3>
<p>Loss = -Σ(y * log(p)) where: - y = actual probability (0 or 1) - p = predicted probability - Σ = sum over all classes</p>
<p>This is the loss for a single token. The total loss is the average across all B*T tokens in the batch, where: - B = batch size (number of sequences processed in parallel) - T = sequence length (number of tokens in each sequence)</p>
<p>Before training, we would expect the model to predict the next character from a uniform distribution.</p>
<p>-Σ(y * log(p)) = = - ( 1 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) ) = - log(1/65) = 4.1744 per token.</p>
<p>i.e.&nbsp;at the beginning we expect loss of -log(1/65) = 4.1744 per token.</p>
</section>
<section id="initialize-the-model-and-compute-the-loss" class="level3">
<h3 class="anchored" data-anchor-id="initialize-the-model-and-compute-the-loss">initialize the model and compute the loss</h3>
<div id="dd8077ca" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a>m <span class="op">=</span> BigramLanguageModel(vocab_size)</span>
<span id="cb38-2"><a href="#cb38-2"></a>logits, loss <span class="op">=</span> m(xb, yb)</span>
<span id="cb38-3"><a href="#cb38-3"></a><span class="bu">print</span>(logits.shape)</span>
<span id="cb38-4"><a href="#cb38-4"></a><span class="bu">print</span>(loss)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>torch.Size([32, 65])
tensor(4.8786, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
</section>
<section id="generate-text" class="level3">
<h3 class="anchored" data-anchor-id="generate-text">generate text</h3>
<div id="e91e9c07" class="cell" data-execution_count="25">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1"></a><span class="bu">print</span>(decode(m.generate(idx <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>), max_new_tokens<span class="op">=</span><span class="dv">100</span>)[<span class="dv">0</span>].tolist()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp
wnYWmnxKWWev-tDqXErVKLgJ</code></pre>
</div>
</div>
</section>
<section id="choose-adamw-as-the-optimizer" class="level3">
<h3 class="anchored" data-anchor-id="choose-adamw-as-the-optimizer">choose AdamW as the optimizer</h3>
<div id="17bb8f8f" class="cell" data-execution_count="26">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a>optimizer <span class="op">=</span> torch.optim.AdamW(m.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="train-the-model" class="level3">
<h3 class="anchored" data-anchor-id="train-the-model">train the model</h3>
<div id="081b40f6" class="cell" data-execution_count="27">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb43-2"><a href="#cb43-2"></a><span class="cf">for</span> steps <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>): <span class="co"># increase number of steps for good results...</span></span>
<span id="cb43-3"><a href="#cb43-3"></a></span>
<span id="cb43-4"><a href="#cb43-4"></a>    <span class="co"># sample a batch of data</span></span>
<span id="cb43-5"><a href="#cb43-5"></a>    xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb43-6"><a href="#cb43-6"></a></span>
<span id="cb43-7"><a href="#cb43-7"></a>    <span class="co"># evaluate the loss</span></span>
<span id="cb43-8"><a href="#cb43-8"></a>    logits, loss <span class="op">=</span> m(xb, yb)</span>
<span id="cb43-9"><a href="#cb43-9"></a>    optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb43-10"><a href="#cb43-10"></a>    loss.backward()</span>
<span id="cb43-11"><a href="#cb43-11"></a>    optimizer.step()</span>
<span id="cb43-12"><a href="#cb43-12"></a></span>
<span id="cb43-13"><a href="#cb43-13"></a><span class="bu">print</span>(loss.item())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>4.65630578994751</code></pre>
</div>
</div>
</section>
<section id="generate-text-starting-with-as-initial-context" class="level3">
<h3 class="anchored" data-anchor-id="generate-text-starting-with-as-initial-context">generate text starting with as initial context</h3>
<div id="a6c5fcbe" class="cell" data-execution_count="28">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1"></a><span class="bu">print</span>(decode(m.generate(idx <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>), max_new_tokens<span class="op">=</span><span class="dv">500</span>)[<span class="dv">0</span>].tolist()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>
oTo.JUZ!!zqe!
xBP qbs$Gy'AcOmrLwwt
p$x;Seh-onQbfM?OjKbn'NwUAW -Np3fkz$FVwAUEa-wzWC -wQo-R!v -Mj?,SPiTyZ;o-opr$mOiPJEYD-CfigkzD3p3?zvS;ADz;.y?o,ivCuC'zqHxcVT cHA
rT'Fd,SBMZyOslg!NXeF$sBe,juUzLq?w-wzP-h
ERjjxlgJzPbHxf$ q,q,KCDCU fqBOQT
SV&amp;CW:xSVwZv'DG'NSPypDhKStKzC -$hslxIVzoivnp ,ethA:NCCGoi
tN!ljjP3fwJMwNelgUzzPGJlgihJ!d?q.d
pSPYgCuCJrIFtb
jQXg
pA.P LP,SPJi
DBcuBM:CixjJ$Jzkq,OLf3KLQLMGph$O 3DfiPHnXKuHMlyjxEiyZib3FaHV-oJa!zoc'XSP :CKGUhd?lgCOF$;;DTHZMlvvcmZAm;:iv'MMgO&amp;Ywbc;BLCUd&amp;vZINLIzkuTGZa
D.?</code></pre>
</div>
</div>
</section>
</section>
<section id="self-attention-the-key-innovation" class="level2">
<h2 class="anchored" data-anchor-id="self-attention-the-key-innovation">Self-Attention: The Key Innovation</h2>
<section id="understanding-self-attention" class="level3">
<h3 class="anchored" data-anchor-id="understanding-self-attention">Understanding Self-Attention</h3>
<p>Self-attention allows tokens to communicate across the sequence in a data-dependent way. Each token can “look” at previous tokens and decide which ones are most relevant for predicting the next token.</p>
</section>
<section id="the-mathematical-trick-weighted-aggregation" class="level3">
<h3 class="anchored" data-anchor-id="the-mathematical-trick-weighted-aggregation">The Mathematical Trick: Weighted Aggregation</h3>
<div id="edd0ddcc" class="cell" data-execution_count="29">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb47-2"><a href="#cb47-2"></a>a <span class="op">=</span> torch.tril(torch.ones(<span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb47-3"><a href="#cb47-3"></a>a <span class="op">=</span> a <span class="op">/</span> torch.<span class="bu">sum</span>(a, <span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb47-4"><a href="#cb47-4"></a>b <span class="op">=</span> torch.randint(<span class="dv">0</span>,<span class="dv">10</span>,(<span class="dv">3</span>,<span class="dv">2</span>)).<span class="bu">float</span>()</span>
<span id="cb47-5"><a href="#cb47-5"></a>c <span class="op">=</span> a <span class="op">@</span> b</span>
<span id="cb47-6"><a href="#cb47-6"></a><span class="bu">print</span>(<span class="st">'a='</span>)</span>
<span id="cb47-7"><a href="#cb47-7"></a><span class="bu">print</span>(a)</span>
<span id="cb47-8"><a href="#cb47-8"></a><span class="bu">print</span>(<span class="st">'--'</span>)</span>
<span id="cb47-9"><a href="#cb47-9"></a><span class="bu">print</span>(<span class="st">'b='</span>)</span>
<span id="cb47-10"><a href="#cb47-10"></a><span class="bu">print</span>(b)</span>
<span id="cb47-11"><a href="#cb47-11"></a><span class="bu">print</span>(<span class="st">'--'</span>)</span>
<span id="cb47-12"><a href="#cb47-12"></a><span class="bu">print</span>(<span class="st">'c='</span>)</span>
<span id="cb47-13"><a href="#cb47-13"></a><span class="bu">print</span>(c)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>a=
tensor([[1.0000, 0.0000, 0.0000],
        [0.5000, 0.5000, 0.0000],
        [0.3333, 0.3333, 0.3333]])
--
b=
tensor([[2., 7.],
        [6., 4.],
        [6., 5.]])
--
c=
tensor([[2.0000, 7.0000],
        [4.0000, 5.5000],
        [4.6667, 5.3333]])</code></pre>
</div>
</div>
</section>
<section id="version-1-using-a-for-loop" class="level3">
<h3 class="anchored" data-anchor-id="version-1-using-a-for-loop">Version 1: Using a For Loop</h3>
<div id="9bae3fa0" class="cell" data-execution_count="30">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1"></a><span class="co"># consider the following toy example:</span></span>
<span id="cb49-2"><a href="#cb49-2"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb49-3"><a href="#cb49-3"></a>B,T,C <span class="op">=</span> <span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">2</span> <span class="co"># batch, time, channels</span></span>
<span id="cb49-4"><a href="#cb49-4"></a>x <span class="op">=</span> torch.randn(B,T,C)</span>
<span id="cb49-5"><a href="#cb49-5"></a>x.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="30">
<pre><code>torch.Size([4, 8, 2])</code></pre>
</div>
</div>
<div id="3062571e" class="cell" data-execution_count="31">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1"></a><span class="co"># We want x[b,t] = mean_{i&lt;=t} x[b,i]</span></span>
<span id="cb51-2"><a href="#cb51-2"></a>xbow <span class="op">=</span> torch.zeros((B,T,C))</span>
<span id="cb51-3"><a href="#cb51-3"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb51-4"><a href="#cb51-4"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb51-5"><a href="#cb51-5"></a>        xprev <span class="op">=</span> x[b,:t<span class="op">+</span><span class="dv">1</span>] <span class="co"># (t,C)</span></span>
<span id="cb51-6"><a href="#cb51-6"></a>        xbow[b,t] <span class="op">=</span> torch.mean(xprev, <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="version-2-matrix-multiplication" class="level3">
<h3 class="anchored" data-anchor-id="version-2-matrix-multiplication">Version 2: Matrix Multiplication</h3>
<div id="7a5b1731" class="cell" data-execution_count="32">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1"></a>wei <span class="op">=</span> torch.tril(torch.ones(T, T))</span>
<span id="cb52-2"><a href="#cb52-2"></a>wei <span class="op">=</span> wei <span class="op">/</span> wei.<span class="bu">sum</span>(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb52-3"><a href="#cb52-3"></a>xbow2 <span class="op">=</span> wei <span class="op">@</span> x <span class="co"># (B, T, T) @ (B, T, C) ----&gt; (B, T, C)</span></span>
<span id="cb52-4"><a href="#cb52-4"></a>torch.allclose(xbow, xbow2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>True</code></pre>
</div>
</div>
</section>
<section id="version-3-using-softmax" class="level3">
<h3 class="anchored" data-anchor-id="version-3-using-softmax">Version 3: Using Softmax</h3>
<div id="522e7e9d" class="cell" data-execution_count="33">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1"></a>tril <span class="op">=</span> torch.tril(torch.ones(T, T))</span>
<span id="cb54-2"><a href="#cb54-2"></a>wei <span class="op">=</span> torch.zeros((T,T))</span>
<span id="cb54-3"><a href="#cb54-3"></a>wei <span class="op">=</span> wei.masked_fill(tril <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb54-4"><a href="#cb54-4"></a>wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb54-5"><a href="#cb54-5"></a>xbow3 <span class="op">=</span> wei <span class="op">@</span> x</span>
<span id="cb54-6"><a href="#cb54-6"></a>torch.allclose(xbow, xbow3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>True</code></pre>
</div>
</div>
</section>
<section id="version-4-self-attention" class="level3">
<h3 class="anchored" data-anchor-id="version-4-self-attention">Version 4: Self-Attention</h3>
<div id="3a6bef18" class="cell" data-execution_count="34">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb56-2"><a href="#cb56-2"></a>B,T,C <span class="op">=</span> <span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">32</span> <span class="co"># batch, time, channels</span></span>
<span id="cb56-3"><a href="#cb56-3"></a>x <span class="op">=</span> torch.randn(B,T,C)</span>
<span id="cb56-4"><a href="#cb56-4"></a></span>
<span id="cb56-5"><a href="#cb56-5"></a><span class="co"># let's see a single Head perform self-attention</span></span>
<span id="cb56-6"><a href="#cb56-6"></a>head_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb56-7"><a href="#cb56-7"></a>key <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb56-8"><a href="#cb56-8"></a>query <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb56-9"><a href="#cb56-9"></a>value <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb56-10"><a href="#cb56-10"></a>k <span class="op">=</span> key(x)   <span class="co"># (B, T, 16)</span></span>
<span id="cb56-11"><a href="#cb56-11"></a>q <span class="op">=</span> query(x) <span class="co"># (B, T, 16)</span></span>
<span id="cb56-12"><a href="#cb56-12"></a>wei <span class="op">=</span>  q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="co"># (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)</span></span>
<span id="cb56-13"><a href="#cb56-13"></a></span>
<span id="cb56-14"><a href="#cb56-14"></a>tril <span class="op">=</span> torch.tril(torch.ones(T, T))</span>
<span id="cb56-15"><a href="#cb56-15"></a><span class="co">#wei = torch.zeros((T,T))</span></span>
<span id="cb56-16"><a href="#cb56-16"></a>wei <span class="op">=</span> wei.masked_fill(tril <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb56-17"><a href="#cb56-17"></a>wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb56-18"><a href="#cb56-18"></a></span>
<span id="cb56-19"><a href="#cb56-19"></a>v <span class="op">=</span> value(x)</span>
<span id="cb56-20"><a href="#cb56-20"></a>out <span class="op">=</span> wei <span class="op">@</span> v</span>
<span id="cb56-21"><a href="#cb56-21"></a><span class="co">#out = wei @ x</span></span>
<span id="cb56-22"><a href="#cb56-22"></a></span>
<span id="cb56-23"><a href="#cb56-23"></a>out.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>torch.Size([4, 8, 16])</code></pre>
</div>
</div>
</section>
<section id="why-scaled-attention" class="level3">
<h3 class="anchored" data-anchor-id="why-scaled-attention">Why Scaled Attention?</h3>
<ul>
<li>Dividing by sqrt(head_size) prevents scores from becoming too large</li>
<li>Keeps softmax from producing overly sharp distributions</li>
<li>Aids training stability</li>
</ul>
<div id="519c74b4" class="cell" data-execution_count="35">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1"></a>k <span class="op">=</span> torch.randn(B,T,head_size)</span>
<span id="cb58-2"><a href="#cb58-2"></a>q <span class="op">=</span> torch.randn(B,T,head_size)</span>
<span id="cb58-3"><a href="#cb58-3"></a>wei <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> head_size<span class="op">**-</span><span class="fl">0.5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="0864adf5" class="cell" data-execution_count="36">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1"></a>k.var()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>tensor(1.0449)</code></pre>
</div>
</div>
<div id="d90b1f2a" class="cell" data-execution_count="37">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1"></a>q.var()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>tensor(1.0700)</code></pre>
</div>
</div>
<div id="64a274ed" class="cell" data-execution_count="38">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1"></a>wei.var()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="38">
<pre><code>tensor(1.0918)</code></pre>
</div>
</div>
<div id="f80440e2" class="cell" data-execution_count="39">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1"></a>torch.softmax(torch.tensor([<span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.5</span>]), dim<span class="op">=-</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])</code></pre>
</div>
</div>
<div id="15cf8d3a" class="cell" data-execution_count="40">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1"></a>torch.softmax(torch.tensor([<span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.5</span>])<span class="op">*</span><span class="dv">8</span>, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># gets too peaky, converges to one-hot</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])</code></pre>
</div>
</div>
</section>
</section>
<section id="layer-normalization" class="level2">
<h2 class="anchored" data-anchor-id="layer-normalization">Layer Normalization</h2>
<p>Layer normalization normalizes features across the embedding dimension for each token independently, which helps stabilize training.</p>
<div id="ade0c38c" class="cell" data-execution_count="41">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1"></a><span class="kw">class</span> LayerNorm1d: <span class="co"># (used to be BatchNorm1d)</span></span>
<span id="cb69-2"><a href="#cb69-2"></a></span>
<span id="cb69-3"><a href="#cb69-3"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, eps<span class="op">=</span><span class="fl">1e-5</span>, momentum<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb69-4"><a href="#cb69-4"></a>    <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb69-5"><a href="#cb69-5"></a>    <span class="va">self</span>.gamma <span class="op">=</span> torch.ones(dim)</span>
<span id="cb69-6"><a href="#cb69-6"></a>    <span class="va">self</span>.beta <span class="op">=</span> torch.zeros(dim)</span>
<span id="cb69-7"><a href="#cb69-7"></a></span>
<span id="cb69-8"><a href="#cb69-8"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb69-9"><a href="#cb69-9"></a>    <span class="co"># calculate the forward pass</span></span>
<span id="cb69-10"><a href="#cb69-10"></a>    xmean <span class="op">=</span> x.mean(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch mean</span></span>
<span id="cb69-11"><a href="#cb69-11"></a>    xvar <span class="op">=</span> x.var(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch variance</span></span>
<span id="cb69-12"><a href="#cb69-12"></a>    xhat <span class="op">=</span> (x <span class="op">-</span> xmean) <span class="op">/</span> torch.sqrt(xvar <span class="op">+</span> <span class="va">self</span>.eps) <span class="co"># normalize to unit variance</span></span>
<span id="cb69-13"><a href="#cb69-13"></a>    <span class="va">self</span>.out <span class="op">=</span> <span class="va">self</span>.gamma <span class="op">*</span> xhat <span class="op">+</span> <span class="va">self</span>.beta</span>
<span id="cb69-14"><a href="#cb69-14"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb69-15"><a href="#cb69-15"></a></span>
<span id="cb69-16"><a href="#cb69-16"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb69-17"><a href="#cb69-17"></a>    <span class="cf">return</span> [<span class="va">self</span>.gamma, <span class="va">self</span>.beta]</span>
<span id="cb69-18"><a href="#cb69-18"></a></span>
<span id="cb69-19"><a href="#cb69-19"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb69-20"><a href="#cb69-20"></a>module <span class="op">=</span> LayerNorm1d(<span class="dv">100</span>)</span>
<span id="cb69-21"><a href="#cb69-21"></a>x <span class="op">=</span> torch.randn(<span class="dv">32</span>, <span class="dv">100</span>) <span class="co"># batch size 32 of 100-dimensional vectors</span></span>
<span id="cb69-22"><a href="#cb69-22"></a>x <span class="op">=</span> module(x)</span>
<span id="cb69-23"><a href="#cb69-23"></a>x.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>torch.Size([32, 100])</code></pre>
</div>
</div>
<div id="31edb7ab" class="cell" data-execution_count="42">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1"></a>x[:,<span class="dv">0</span>].mean(), x[:,<span class="dv">0</span>].std() <span class="co"># mean,std of one feature across all batch inputs</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>(tensor(0.1469), tensor(0.8803))</code></pre>
</div>
</div>
<div id="89411af9" class="cell" data-execution_count="43">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1"></a>x[<span class="dv">0</span>,:].mean(), x[<span class="dv">0</span>,:].std() <span class="co"># mean,std of a single input from the batch, of its features</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>(tensor(2.3842e-09), tensor(1.0000))</code></pre>
</div>
</div>
<div id="80db1a8e" class="cell" data-execution_count="44">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1"></a><span class="co"># French to English translation example:</span></span>
<span id="cb75-2"><a href="#cb75-2"></a></span>
<span id="cb75-3"><a href="#cb75-3"></a><span class="co"># &lt;--------- ENCODE ------------------&gt;&lt;--------------- DECODE -----------------&gt;</span></span>
<span id="cb75-4"><a href="#cb75-4"></a><span class="co"># les réseaux de neurones sont géniaux! &lt;START&gt; neural networks are awesome!&lt;</span><span class="re">END</span><span class="co">&gt;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="full-finished-code-for-reference" class="level3">
<h3 class="anchored" data-anchor-id="full-finished-code-for-reference">Full finished code, for reference</h3>
<div id="2a049efa" class="cell" data-execution_count="45">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1"></a><span class="co"># Import necessary PyTorch modules</span></span>
<span id="cb76-2"><a href="#cb76-2"></a><span class="im">import</span> torch</span>
<span id="cb76-3"><a href="#cb76-3"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb76-4"><a href="#cb76-4"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb76-5"><a href="#cb76-5"></a></span>
<span id="cb76-6"><a href="#cb76-6"></a><span class="co"># ===== HYPERPARAMETERS =====</span></span>
<span id="cb76-7"><a href="#cb76-7"></a><span class="co"># These are the key parameters that control the model's behavior and training</span></span>
<span id="cb76-8"><a href="#cb76-8"></a>batch_size <span class="op">=</span> <span class="dv">16</span>        <span class="co"># Number of independent sequences processed in parallel</span></span>
<span id="cb76-9"><a href="#cb76-9"></a>block_size <span class="op">=</span> <span class="dv">32</span>        <span class="co"># Maximum context length for predictions (like a window size)</span></span>
<span id="cb76-10"><a href="#cb76-10"></a>max_iters <span class="op">=</span> <span class="dv">5000</span>       <span class="co"># Total number of training iterations</span></span>
<span id="cb76-11"><a href="#cb76-11"></a>eval_interval <span class="op">=</span> <span class="dv">100</span>    <span class="co"># How often to evaluate the model during training</span></span>
<span id="cb76-12"><a href="#cb76-12"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span>   <span class="co"># Step size for gradient descent</span></span>
<span id="cb76-13"><a href="#cb76-13"></a>eval_iters <span class="op">=</span> <span class="dv">200</span>       <span class="co"># Number of iterations to average over when evaluating</span></span>
<span id="cb76-14"><a href="#cb76-14"></a>n_embd <span class="op">=</span> <span class="dv">64</span>           <span class="co"># Size of the embedding dimension (d_model in original paper)</span></span>
<span id="cb76-15"><a href="#cb76-15"></a>n_head <span class="op">=</span> <span class="dv">4</span>            <span class="co"># Number of attention heads</span></span>
<span id="cb76-16"><a href="#cb76-16"></a>n_layer <span class="op">=</span> <span class="dv">4</span>           <span class="co"># Number of transformer layers</span></span>
<span id="cb76-17"><a href="#cb76-17"></a>dropout <span class="op">=</span> <span class="fl">0.0</span>         <span class="co"># Probability of dropping out neurons (0 = no dropout)</span></span>
<span id="cb76-18"><a href="#cb76-18"></a><span class="co"># ==========================</span></span>
<span id="cb76-19"><a href="#cb76-19"></a></span>
<span id="cb76-20"><a href="#cb76-20"></a><span class="co"># Device selection: MPS (Apple Silicon) &gt; CUDA &gt; CPU</span></span>
<span id="cb76-21"><a href="#cb76-21"></a><span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb76-22"><a href="#cb76-22"></a>    device <span class="op">=</span> torch.device(<span class="st">"mps"</span>)  <span class="co"># Apple Silicon GPU</span></span>
<span id="cb76-23"><a href="#cb76-23"></a><span class="cf">elif</span> torch.cuda.is_available():</span>
<span id="cb76-24"><a href="#cb76-24"></a>    device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)  <span class="co"># NVIDIA GPU</span></span>
<span id="cb76-25"><a href="#cb76-25"></a><span class="cf">else</span>:</span>
<span id="cb76-26"><a href="#cb76-26"></a>    device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)   <span class="co"># CPU fallback</span></span>
<span id="cb76-27"><a href="#cb76-27"></a><span class="bu">print</span>(<span class="ss">f"Using device: </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb76-28"><a href="#cb76-28"></a></span>
<span id="cb76-29"><a href="#cb76-29"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb76-30"><a href="#cb76-30"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb76-31"><a href="#cb76-31"></a><span class="cf">if</span> device.<span class="bu">type</span> <span class="op">==</span> <span class="st">'cuda'</span>:</span>
<span id="cb76-32"><a href="#cb76-32"></a>    torch.cuda.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb76-33"><a href="#cb76-33"></a><span class="cf">elif</span> device.<span class="bu">type</span> <span class="op">==</span> <span class="st">'mps'</span>:</span>
<span id="cb76-34"><a href="#cb76-34"></a>    torch.mps.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb76-35"><a href="#cb76-35"></a></span>
<span id="cb76-36"><a href="#cb76-36"></a><span class="co"># Load and read the training text</span></span>
<span id="cb76-37"><a href="#cb76-37"></a><span class="co"># Note: This assumes 'input.txt' exists in the current directory</span></span>
<span id="cb76-38"><a href="#cb76-38"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'input.txt'</span>, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> f:</span>
<span id="cb76-39"><a href="#cb76-39"></a>    text <span class="op">=</span> f.read()</span>
<span id="cb76-40"><a href="#cb76-40"></a></span>
<span id="cb76-41"><a href="#cb76-41"></a><span class="co"># ===== DATA PREPROCESSING =====</span></span>
<span id="cb76-42"><a href="#cb76-42"></a><span class="co"># Create vocabulary from unique characters in the text</span></span>
<span id="cb76-43"><a href="#cb76-43"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(text)))</span>
<span id="cb76-44"><a href="#cb76-44"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(chars)</span>
<span id="cb76-45"><a href="#cb76-45"></a><span class="co"># Create mapping between characters and integers</span></span>
<span id="cb76-46"><a href="#cb76-46"></a>stoi <span class="op">=</span> { ch:i <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }  <span class="co"># string to index</span></span>
<span id="cb76-47"><a href="#cb76-47"></a>itos <span class="op">=</span> { i:ch <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }  <span class="co"># index to string</span></span>
<span id="cb76-48"><a href="#cb76-48"></a><span class="co"># Define encoder and decoder functions</span></span>
<span id="cb76-49"><a href="#cb76-49"></a>encode <span class="op">=</span> <span class="kw">lambda</span> s: [stoi[c] <span class="cf">for</span> c <span class="kw">in</span> s]  <span class="co"># convert string to list of integers</span></span>
<span id="cb76-50"><a href="#cb76-50"></a>decode <span class="op">=</span> <span class="kw">lambda</span> l: <span class="st">''</span>.join([itos[i] <span class="cf">for</span> i <span class="kw">in</span> l])  <span class="co"># convert list of integers to string</span></span>
<span id="cb76-51"><a href="#cb76-51"></a></span>
<span id="cb76-52"><a href="#cb76-52"></a><span class="co"># Split data into training and validation sets</span></span>
<span id="cb76-53"><a href="#cb76-53"></a>data <span class="op">=</span> torch.tensor(encode(text), dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb76-54"><a href="#cb76-54"></a>n <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span><span class="op">*</span><span class="bu">len</span>(data))  <span class="co"># first 90% for training</span></span>
<span id="cb76-55"><a href="#cb76-55"></a>train_data <span class="op">=</span> data[:n]</span>
<span id="cb76-56"><a href="#cb76-56"></a>val_data <span class="op">=</span> data[n:]</span>
<span id="cb76-57"><a href="#cb76-57"></a><span class="co"># =============================</span></span>
<span id="cb76-58"><a href="#cb76-58"></a></span>
<span id="cb76-59"><a href="#cb76-59"></a><span class="co"># ===== DATA LOADING FUNCTION =====</span></span>
<span id="cb76-60"><a href="#cb76-60"></a><span class="kw">def</span> get_batch(split):</span>
<span id="cb76-61"><a href="#cb76-61"></a>    <span class="co">"""Generate a batch of data for training or validation.</span></span>
<span id="cb76-62"><a href="#cb76-62"></a><span class="co">    </span></span>
<span id="cb76-63"><a href="#cb76-63"></a><span class="co">    Args:</span></span>
<span id="cb76-64"><a href="#cb76-64"></a><span class="co">        split: 'train' or 'val' to specify which dataset to use</span></span>
<span id="cb76-65"><a href="#cb76-65"></a><span class="co">    </span></span>
<span id="cb76-66"><a href="#cb76-66"></a><span class="co">    Returns:</span></span>
<span id="cb76-67"><a href="#cb76-67"></a><span class="co">        x: input sequence of shape (batch_size, block_size)</span></span>
<span id="cb76-68"><a href="#cb76-68"></a><span class="co">        y: target sequence of shape (batch_size, block_size)</span></span>
<span id="cb76-69"><a href="#cb76-69"></a><span class="co">    """</span></span>
<span id="cb76-70"><a href="#cb76-70"></a>    data <span class="op">=</span> train_data <span class="cf">if</span> split <span class="op">==</span> <span class="st">'train'</span> <span class="cf">else</span> val_data</span>
<span id="cb76-71"><a href="#cb76-71"></a>    <span class="co"># Randomly select starting indices for sequences</span></span>
<span id="cb76-72"><a href="#cb76-72"></a>    ix <span class="op">=</span> torch.randint(<span class="bu">len</span>(data) <span class="op">-</span> block_size, (batch_size,))</span>
<span id="cb76-73"><a href="#cb76-73"></a>    <span class="co"># Create input and target sequences</span></span>
<span id="cb76-74"><a href="#cb76-74"></a>    x <span class="op">=</span> torch.stack([data[i:i<span class="op">+</span>block_size] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb76-75"><a href="#cb76-75"></a>    y <span class="op">=</span> torch.stack([data[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span>block_size<span class="op">+</span><span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb76-76"><a href="#cb76-76"></a>    x, y <span class="op">=</span> x.to(device), y.to(device)</span>
<span id="cb76-77"><a href="#cb76-77"></a>    <span class="cf">return</span> x, y</span>
<span id="cb76-78"><a href="#cb76-78"></a><span class="co"># ================================</span></span>
<span id="cb76-79"><a href="#cb76-79"></a></span>
<span id="cb76-80"><a href="#cb76-80"></a><span class="co"># ===== LOSS ESTIMATION FUNCTION =====</span></span>
<span id="cb76-81"><a href="#cb76-81"></a><span class="at">@torch.no_grad</span>()  <span class="co"># Disable gradient calculation for efficiency</span></span>
<span id="cb76-82"><a href="#cb76-82"></a><span class="kw">def</span> estimate_loss():</span>
<span id="cb76-83"><a href="#cb76-83"></a>    <span class="co">"""Estimate the loss on training and validation sets.</span></span>
<span id="cb76-84"><a href="#cb76-84"></a><span class="co">    </span></span>
<span id="cb76-85"><a href="#cb76-85"></a><span class="co">    Returns:</span></span>
<span id="cb76-86"><a href="#cb76-86"></a><span class="co">        Dictionary containing average loss for train and validation sets</span></span>
<span id="cb76-87"><a href="#cb76-87"></a><span class="co">    """</span></span>
<span id="cb76-88"><a href="#cb76-88"></a>    out <span class="op">=</span> {}</span>
<span id="cb76-89"><a href="#cb76-89"></a>    model.<span class="bu">eval</span>()  <span class="co"># Set model to evaluation mode</span></span>
<span id="cb76-90"><a href="#cb76-90"></a>    <span class="cf">for</span> split <span class="kw">in</span> [<span class="st">'train'</span>, <span class="st">'val'</span>]:</span>
<span id="cb76-91"><a href="#cb76-91"></a>        losses <span class="op">=</span> torch.zeros(eval_iters)</span>
<span id="cb76-92"><a href="#cb76-92"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(eval_iters):</span>
<span id="cb76-93"><a href="#cb76-93"></a>            X, Y <span class="op">=</span> get_batch(split)</span>
<span id="cb76-94"><a href="#cb76-94"></a>            logits, loss <span class="op">=</span> model(X, Y)</span>
<span id="cb76-95"><a href="#cb76-95"></a>            losses[k] <span class="op">=</span> loss.item()</span>
<span id="cb76-96"><a href="#cb76-96"></a>        out[split] <span class="op">=</span> losses.mean()</span>
<span id="cb76-97"><a href="#cb76-97"></a>    model.train()  <span class="co"># Set model back to training mode</span></span>
<span id="cb76-98"><a href="#cb76-98"></a>    <span class="cf">return</span> out</span>
<span id="cb76-99"><a href="#cb76-99"></a><span class="co"># ===================================</span></span>
<span id="cb76-100"><a href="#cb76-100"></a></span>
<span id="cb76-101"><a href="#cb76-101"></a><span class="co"># ===== </span><span class="al">ATTENTION</span><span class="co"> HEAD IMPLEMENTATION =====</span></span>
<span id="cb76-102"><a href="#cb76-102"></a><span class="kw">class</span> Head(nn.Module):</span>
<span id="cb76-103"><a href="#cb76-103"></a>    <span class="co">"""Single head of self-attention."""</span></span>
<span id="cb76-104"><a href="#cb76-104"></a>    </span>
<span id="cb76-105"><a href="#cb76-105"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, head_size):</span>
<span id="cb76-106"><a href="#cb76-106"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb76-107"><a href="#cb76-107"></a>        <span class="co"># Key, Query, Value projections</span></span>
<span id="cb76-108"><a href="#cb76-108"></a>        <span class="va">self</span>.key <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb76-109"><a href="#cb76-109"></a>        <span class="va">self</span>.query <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb76-110"><a href="#cb76-110"></a>        <span class="va">self</span>.value <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb76-111"><a href="#cb76-111"></a>        <span class="co"># Create lower triangular mask for causal attention</span></span>
<span id="cb76-112"><a href="#cb76-112"></a>        <span class="va">self</span>.register_buffer(<span class="st">'tril'</span>, torch.tril(torch.ones(block_size, block_size)))</span>
<span id="cb76-113"><a href="#cb76-113"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb76-114"><a href="#cb76-114"></a></span>
<span id="cb76-115"><a href="#cb76-115"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb76-116"><a href="#cb76-116"></a>        B,T,C <span class="op">=</span> x.shape</span>
<span id="cb76-117"><a href="#cb76-117"></a>        <span class="co"># Compute key, query, and value matrices</span></span>
<span id="cb76-118"><a href="#cb76-118"></a>        k <span class="op">=</span> <span class="va">self</span>.key(x)   <span class="co"># (B,T,C)</span></span>
<span id="cb76-119"><a href="#cb76-119"></a>        q <span class="op">=</span> <span class="va">self</span>.query(x) <span class="co"># (B,T,C)</span></span>
<span id="cb76-120"><a href="#cb76-120"></a>        <span class="co"># Compute attention scores</span></span>
<span id="cb76-121"><a href="#cb76-121"></a>        wei <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> C<span class="op">**-</span><span class="fl">0.5</span>  <span class="co"># Scale by sqrt(d_k)</span></span>
<span id="cb76-122"><a href="#cb76-122"></a>        wei <span class="op">=</span> wei.masked_fill(<span class="va">self</span>.tril[:T, :T] <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))  <span class="co"># Apply mask</span></span>
<span id="cb76-123"><a href="#cb76-123"></a>        wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># Convert to probabilities</span></span>
<span id="cb76-124"><a href="#cb76-124"></a>        wei <span class="op">=</span> <span class="va">self</span>.dropout(wei)</span>
<span id="cb76-125"><a href="#cb76-125"></a>        <span class="co"># Weighted aggregation of values</span></span>
<span id="cb76-126"><a href="#cb76-126"></a>        v <span class="op">=</span> <span class="va">self</span>.value(x)</span>
<span id="cb76-127"><a href="#cb76-127"></a>        out <span class="op">=</span> wei <span class="op">@</span> v</span>
<span id="cb76-128"><a href="#cb76-128"></a>        <span class="cf">return</span> out</span>
<span id="cb76-129"><a href="#cb76-129"></a><span class="co"># ========================================</span></span>
<span id="cb76-130"><a href="#cb76-130"></a></span>
<span id="cb76-131"><a href="#cb76-131"></a><span class="co"># ===== MULTI-HEAD </span><span class="al">ATTENTION</span><span class="co"> =====</span></span>
<span id="cb76-132"><a href="#cb76-132"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb76-133"><a href="#cb76-133"></a>    <span class="co">"""Multiple heads of self-attention in parallel."""</span></span>
<span id="cb76-134"><a href="#cb76-134"></a>    </span>
<span id="cb76-135"><a href="#cb76-135"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_heads, head_size):</span>
<span id="cb76-136"><a href="#cb76-136"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb76-137"><a href="#cb76-137"></a>        <span class="va">self</span>.heads <span class="op">=</span> nn.ModuleList([Head(head_size) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_heads)])</span>
<span id="cb76-138"><a href="#cb76-138"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Linear(n_embd, n_embd)  <span class="co"># Projection layer</span></span>
<span id="cb76-139"><a href="#cb76-139"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb76-140"><a href="#cb76-140"></a></span>
<span id="cb76-141"><a href="#cb76-141"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb76-142"><a href="#cb76-142"></a>        <span class="co"># Process through each head and concatenate results</span></span>
<span id="cb76-143"><a href="#cb76-143"></a>        out <span class="op">=</span> torch.cat([h(x) <span class="cf">for</span> h <span class="kw">in</span> <span class="va">self</span>.heads], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb76-144"><a href="#cb76-144"></a>        out <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.proj(out))</span>
<span id="cb76-145"><a href="#cb76-145"></a>        <span class="cf">return</span> out</span>
<span id="cb76-146"><a href="#cb76-146"></a><span class="co"># ===============================</span></span>
<span id="cb76-147"><a href="#cb76-147"></a></span>
<span id="cb76-148"><a href="#cb76-148"></a><span class="co"># ===== FEED-FORWARD NETWORK =====</span></span>
<span id="cb76-149"><a href="#cb76-149"></a><span class="kw">class</span> FeedFoward(nn.Module):</span>
<span id="cb76-150"><a href="#cb76-150"></a>    <span class="co">"""Simple feed-forward network with one hidden layer."""</span></span>
<span id="cb76-151"><a href="#cb76-151"></a>    </span>
<span id="cb76-152"><a href="#cb76-152"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd):</span>
<span id="cb76-153"><a href="#cb76-153"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb76-154"><a href="#cb76-154"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb76-155"><a href="#cb76-155"></a>            nn.Linear(n_embd, <span class="dv">4</span> <span class="op">*</span> n_embd),  <span class="co"># Expand dimension</span></span>
<span id="cb76-156"><a href="#cb76-156"></a>            nn.ReLU(),                      <span class="co"># Non-linearity</span></span>
<span id="cb76-157"><a href="#cb76-157"></a>            nn.Linear(<span class="dv">4</span> <span class="op">*</span> n_embd, n_embd),  <span class="co"># Project back</span></span>
<span id="cb76-158"><a href="#cb76-158"></a>            nn.Dropout(dropout),</span>
<span id="cb76-159"><a href="#cb76-159"></a>        )</span>
<span id="cb76-160"><a href="#cb76-160"></a></span>
<span id="cb76-161"><a href="#cb76-161"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb76-162"><a href="#cb76-162"></a>        <span class="cf">return</span> <span class="va">self</span>.net(x)</span>
<span id="cb76-163"><a href="#cb76-163"></a><span class="co"># ==============================</span></span>
<span id="cb76-164"><a href="#cb76-164"></a></span>
<span id="cb76-165"><a href="#cb76-165"></a><span class="co"># ===== TRANSFORMER BLOCK =====</span></span>
<span id="cb76-166"><a href="#cb76-166"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb76-167"><a href="#cb76-167"></a>    <span class="co">"""Transformer block: communication (attention) followed by computation (FFN)."""</span></span>
<span id="cb76-168"><a href="#cb76-168"></a>    </span>
<span id="cb76-169"><a href="#cb76-169"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd, n_head):</span>
<span id="cb76-170"><a href="#cb76-170"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb76-171"><a href="#cb76-171"></a>        head_size <span class="op">=</span> n_embd <span class="op">//</span> n_head  <span class="co"># floor(n_embd / n_head)</span></span>
<span id="cb76-172"><a href="#cb76-172"></a>        <span class="va">self</span>.sa <span class="op">=</span> MultiHeadAttention(n_head, head_size)</span>
<span id="cb76-173"><a href="#cb76-173"></a>        <span class="va">self</span>.ffwd <span class="op">=</span> FeedFoward(n_embd)</span>
<span id="cb76-174"><a href="#cb76-174"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> nn.LayerNorm(n_embd)  <span class="co"># Layer normalization</span></span>
<span id="cb76-175"><a href="#cb76-175"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> nn.LayerNorm(n_embd)</span>
<span id="cb76-176"><a href="#cb76-176"></a></span>
<span id="cb76-177"><a href="#cb76-177"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb76-178"><a href="#cb76-178"></a>        <span class="co"># Apply attention with residual connection</span></span>
<span id="cb76-179"><a href="#cb76-179"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.sa(<span class="va">self</span>.ln1(x))</span>
<span id="cb76-180"><a href="#cb76-180"></a>        <span class="co"># Apply feed-forward with residual connection</span></span>
<span id="cb76-181"><a href="#cb76-181"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffwd(<span class="va">self</span>.ln2(x))</span>
<span id="cb76-182"><a href="#cb76-182"></a>        <span class="cf">return</span> x</span>
<span id="cb76-183"><a href="#cb76-183"></a><span class="co"># ============================</span></span>
<span id="cb76-184"><a href="#cb76-184"></a></span>
<span id="cb76-185"><a href="#cb76-185"></a><span class="co"># ===== LANGUAGE MODEL =====</span></span>
<span id="cb76-186"><a href="#cb76-186"></a><span class="kw">class</span> BigramLanguageModel(nn.Module):</span>
<span id="cb76-187"><a href="#cb76-187"></a>    <span class="co">"""Simple bigram language model with transformer architecture."""</span></span>
<span id="cb76-188"><a href="#cb76-188"></a>    </span>
<span id="cb76-189"><a href="#cb76-189"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb76-190"><a href="#cb76-190"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb76-191"><a href="#cb76-191"></a>        <span class="co"># Token embeddings</span></span>
<span id="cb76-192"><a href="#cb76-192"></a>        <span class="va">self</span>.token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, n_embd)</span>
<span id="cb76-193"><a href="#cb76-193"></a>        <span class="co"># Position embeddings</span></span>
<span id="cb76-194"><a href="#cb76-194"></a>        <span class="va">self</span>.position_embedding_table <span class="op">=</span> nn.Embedding(block_size, n_embd)</span>
<span id="cb76-195"><a href="#cb76-195"></a>        <span class="co"># Stack of transformer blocks</span></span>
<span id="cb76-196"><a href="#cb76-196"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.Sequential(<span class="op">*</span>[Block(n_embd, n_head<span class="op">=</span>n_head) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_layer)])</span>
<span id="cb76-197"><a href="#cb76-197"></a>        <span class="va">self</span>.ln_f <span class="op">=</span> nn.LayerNorm(n_embd)  <span class="co"># Final layer norm</span></span>
<span id="cb76-198"><a href="#cb76-198"></a>        <span class="va">self</span>.lm_head <span class="op">=</span> nn.Linear(n_embd, vocab_size)  <span class="co"># Language model head</span></span>
<span id="cb76-199"><a href="#cb76-199"></a></span>
<span id="cb76-200"><a href="#cb76-200"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb76-201"><a href="#cb76-201"></a>        B, T <span class="op">=</span> idx.shape</span>
<span id="cb76-202"><a href="#cb76-202"></a>        </span>
<span id="cb76-203"><a href="#cb76-203"></a>        <span class="co"># Get token embeddings</span></span>
<span id="cb76-204"><a href="#cb76-204"></a>        tok_emb <span class="op">=</span> <span class="va">self</span>.token_embedding_table(idx)  <span class="co"># (B,T,C)</span></span>
<span id="cb76-205"><a href="#cb76-205"></a>        <span class="co"># Get position embeddings</span></span>
<span id="cb76-206"><a href="#cb76-206"></a>        pos_emb <span class="op">=</span> <span class="va">self</span>.position_embedding_table(torch.arange(T, device<span class="op">=</span>device))  <span class="co"># (T,C)</span></span>
<span id="cb76-207"><a href="#cb76-207"></a>        <span class="co"># Combine token and position embeddings</span></span>
<span id="cb76-208"><a href="#cb76-208"></a>        x <span class="op">=</span> tok_emb <span class="op">+</span> pos_emb  <span class="co"># (B,T,C)</span></span>
<span id="cb76-209"><a href="#cb76-209"></a>        <span class="co"># Process through transformer blocks</span></span>
<span id="cb76-210"><a href="#cb76-210"></a>        x <span class="op">=</span> <span class="va">self</span>.blocks(x)  <span class="co"># (B,T,C)</span></span>
<span id="cb76-211"><a href="#cb76-211"></a>        x <span class="op">=</span> <span class="va">self</span>.ln_f(x)  <span class="co"># (B,T,C)</span></span>
<span id="cb76-212"><a href="#cb76-212"></a>        logits <span class="op">=</span> <span class="va">self</span>.lm_head(x)  <span class="co"># (B,T,vocab_size)</span></span>
<span id="cb76-213"><a href="#cb76-213"></a></span>
<span id="cb76-214"><a href="#cb76-214"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb76-215"><a href="#cb76-215"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb76-216"><a href="#cb76-216"></a>        <span class="cf">else</span>:</span>
<span id="cb76-217"><a href="#cb76-217"></a>            <span class="co"># Reshape for loss calculation</span></span>
<span id="cb76-218"><a href="#cb76-218"></a>            B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb76-219"><a href="#cb76-219"></a>            logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C)</span>
<span id="cb76-220"><a href="#cb76-220"></a>            targets <span class="op">=</span> targets.view(B<span class="op">*</span>T)</span>
<span id="cb76-221"><a href="#cb76-221"></a>            loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb76-222"><a href="#cb76-222"></a></span>
<span id="cb76-223"><a href="#cb76-223"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb76-224"><a href="#cb76-224"></a></span>
<span id="cb76-225"><a href="#cb76-225"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens):</span>
<span id="cb76-226"><a href="#cb76-226"></a>        <span class="co">"""Generate new text given a starting sequence."""</span></span>
<span id="cb76-227"><a href="#cb76-227"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb76-228"><a href="#cb76-228"></a>            <span class="co"># Crop context to block_size</span></span>
<span id="cb76-229"><a href="#cb76-229"></a>            idx_cond <span class="op">=</span> idx[:, <span class="op">-</span>block_size:]</span>
<span id="cb76-230"><a href="#cb76-230"></a>            <span class="co"># Get predictions</span></span>
<span id="cb76-231"><a href="#cb76-231"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>(idx_cond)</span>
<span id="cb76-232"><a href="#cb76-232"></a>            <span class="co"># Focus on last time step</span></span>
<span id="cb76-233"><a href="#cb76-233"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :]  <span class="co"># (B, C)</span></span>
<span id="cb76-234"><a href="#cb76-234"></a>            <span class="co"># Convert to probabilities</span></span>
<span id="cb76-235"><a href="#cb76-235"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># (B, C)</span></span>
<span id="cb76-236"><a href="#cb76-236"></a>            <span class="co"># Sample from distribution</span></span>
<span id="cb76-237"><a href="#cb76-237"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)  <span class="co"># (B, 1)</span></span>
<span id="cb76-238"><a href="#cb76-238"></a>            <span class="co"># Append to sequence</span></span>
<span id="cb76-239"><a href="#cb76-239"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># (B, T+1)</span></span>
<span id="cb76-240"><a href="#cb76-240"></a>        <span class="cf">return</span> idx</span>
<span id="cb76-241"><a href="#cb76-241"></a><span class="co"># =========================</span></span>
<span id="cb76-242"><a href="#cb76-242"></a></span>
<span id="cb76-243"><a href="#cb76-243"></a><span class="co"># ===== MODEL INITIALIZATION AND TRAINING =====</span></span>
<span id="cb76-244"><a href="#cb76-244"></a><span class="co"># Create model instance</span></span>
<span id="cb76-245"><a href="#cb76-245"></a>model <span class="op">=</span> BigramLanguageModel()</span>
<span id="cb76-246"><a href="#cb76-246"></a>m <span class="op">=</span> model.to(device)</span>
<span id="cb76-247"><a href="#cb76-247"></a><span class="co"># Print number of parameters</span></span>
<span id="cb76-248"><a href="#cb76-248"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> m.parameters())<span class="op">/</span><span class="fl">1e6</span>, <span class="st">'M parameters'</span>)</span>
<span id="cb76-249"><a href="#cb76-249"></a></span>
<span id="cb76-250"><a href="#cb76-250"></a><span class="co"># Create optimizer</span></span>
<span id="cb76-251"><a href="#cb76-251"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb76-252"><a href="#cb76-252"></a></span>
<span id="cb76-253"><a href="#cb76-253"></a><span class="co"># Training loop</span></span>
<span id="cb76-254"><a href="#cb76-254"></a><span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(max_iters):</span>
<span id="cb76-255"><a href="#cb76-255"></a>    <span class="co"># Evaluate loss periodically</span></span>
<span id="cb76-256"><a href="#cb76-256"></a>    <span class="cf">if</span> <span class="bu">iter</span> <span class="op">%</span> eval_interval <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> <span class="bu">iter</span> <span class="op">==</span> max_iters <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb76-257"><a href="#cb76-257"></a>        losses <span class="op">=</span> estimate_loss()</span>
<span id="cb76-258"><a href="#cb76-258"></a>        <span class="bu">print</span>(<span class="ss">f"step </span><span class="sc">{</span><span class="bu">iter</span><span class="sc">}</span><span class="ss">: train loss </span><span class="sc">{</span>losses[<span class="st">'train'</span>]<span class="sc">:.4f}</span><span class="ss">, val loss </span><span class="sc">{</span>losses[<span class="st">'val'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb76-259"><a href="#cb76-259"></a></span>
<span id="cb76-260"><a href="#cb76-260"></a>    <span class="co"># Get batch of data</span></span>
<span id="cb76-261"><a href="#cb76-261"></a>    xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb76-262"><a href="#cb76-262"></a></span>
<span id="cb76-263"><a href="#cb76-263"></a>    <span class="co"># Forward pass</span></span>
<span id="cb76-264"><a href="#cb76-264"></a>    logits, loss <span class="op">=</span> model(xb, yb)</span>
<span id="cb76-265"><a href="#cb76-265"></a>    <span class="co"># Backward pass</span></span>
<span id="cb76-266"><a href="#cb76-266"></a>    optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb76-267"><a href="#cb76-267"></a>    loss.backward()</span>
<span id="cb76-268"><a href="#cb76-268"></a>    optimizer.step()</span>
<span id="cb76-269"><a href="#cb76-269"></a></span>
<span id="cb76-270"><a href="#cb76-270"></a><span class="co"># Generate text from the model</span></span>
<span id="cb76-271"><a href="#cb76-271"></a>context <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>, device<span class="op">=</span>device)</span>
<span id="cb76-272"><a href="#cb76-272"></a><span class="bu">print</span>(decode(m.generate(context, max_new_tokens<span class="op">=</span><span class="dv">2000</span>)[<span class="dv">0</span>].tolist()))</span>
<span id="cb76-273"><a href="#cb76-273"></a><span class="co"># ============================================</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Using device: mps
0.209729 M parameters
step 0: train loss 4.4116, val loss 4.4022
step 100: train loss 2.6568, val loss 2.6670
step 200: train loss 2.5091, val loss 2.5059
step 300: train loss 2.4201, val loss 2.4342
step 400: train loss 2.3506, val loss 2.3568
step 500: train loss 2.2966, val loss 2.3132
step 600: train loss 2.2407, val loss 2.2496
step 700: train loss 2.2049, val loss 2.2189
step 800: train loss 2.1638, val loss 2.1870
step 900: train loss 2.1237, val loss 2.1507
step 1000: train loss 2.1022, val loss 2.1299
step 1100: train loss 2.0686, val loss 2.1176
step 1200: train loss 2.0376, val loss 2.0790
step 1300: train loss 2.0236, val loss 2.0630
step 1400: train loss 1.9928, val loss 2.0362
step 1500: train loss 1.9707, val loss 2.0299
step 1600: train loss 1.9610, val loss 2.0458
step 1700: train loss 1.9418, val loss 2.0132
step 1800: train loss 1.9071, val loss 1.9940
step 1900: train loss 1.9110, val loss 1.9885
step 2000: train loss 1.8843, val loss 1.9938
step 2100: train loss 1.8713, val loss 1.9753
step 2200: train loss 1.8598, val loss 1.9625
step 2300: train loss 1.8571, val loss 1.9541
step 2400: train loss 1.8412, val loss 1.9435
step 2500: train loss 1.8171, val loss 1.9435
step 2600: train loss 1.8257, val loss 1.9373
step 2700: train loss 1.8096, val loss 1.9340
step 2800: train loss 1.8064, val loss 1.9245
step 2900: train loss 1.8060, val loss 1.9332
step 3000: train loss 1.7945, val loss 1.9203
step 3100: train loss 1.7676, val loss 1.9189
step 3200: train loss 1.7548, val loss 1.9131
step 3300: train loss 1.7564, val loss 1.9080
step 3400: train loss 1.7552, val loss 1.8949
step 3500: train loss 1.7385, val loss 1.8981
step 3600: train loss 1.7248, val loss 1.8911
step 3700: train loss 1.7266, val loss 1.8810
step 3800: train loss 1.7206, val loss 1.8961
step 3900: train loss 1.7223, val loss 1.8753
step 4000: train loss 1.7117, val loss 1.8621
step 4100: train loss 1.7129, val loss 1.8762
step 4200: train loss 1.7057, val loss 1.8645
step 4300: train loss 1.6988, val loss 1.8513
step 4400: train loss 1.7059, val loss 1.8685
step 4500: train loss 1.6897, val loss 1.8517
step 4600: train loss 1.6877, val loss 1.8358
step 4700: train loss 1.6818, val loss 1.8459
step 4800: train loss 1.6672, val loss 1.8465
step 4900: train loss 1.6663, val loss 1.8391
step 4999: train loss 1.6670, val loss 1.8286

Foast.

MENENIENIUS:
I sist upon
in him our knom I heave.

ANTANGA:
Yet, you harn be fear thousande of yourse proy way art deed sirrife.
You boke, and-here, if notw to see gainest,
Albus strawn'd us; to lace ands would most
jeak my dream sleast's of so prewit for a shien you that's fourtyle
Thau tiscont ruthforesty cried men
and un mampet hither smeried divoth.
I'll that shollirt's that stake,
Hazone appad to the preth, and this receas thous late's King me of what thou son,
Kingived Fear nother have frave death!

BOLINGBOANGE:
Yetroy-pritch.
My suntrove, we hus.
 noble will ropt: the in your et-timeman,
willst I save your bloy shalle I mone faults not fleat?
Angul stry armsedians, the artsweds, broth off,
Both gome toots. 'We'll.

DUKE VINCENTIO:
Have voichs slay of me you; and incae,
whou comman I sea I sering to emberrer!
I would thou scange of the toger.

MOPTERS:
Stunt-my was I'll fackers.

ROPUMLANF:
God my mantoreny.

NORTLA:
O distreed asomon's thought his lictly:
First I know hose, Or the
soul hur lifess herse fortune dequeen;
Claid to. And the ten weouly entruct you long.

KING HUMNRY:
Nout Vility:
Let thou be you?

ANGELO:
We'll now
In am kingly wflent my preson to enpern of an yourst
So loing long me So me man, mad the's noccurpet in now.

GLOUCESTER:
Uncunderer you: if
The I hold thee faintelle
We ither! hatte's a bed'simter acchar tagus affet pringent,
And what you four give a griern and it:
With the late man'd the were not,
And thy nie,
You lordone turn to mincture us:
se no down'd my Poy debrown'd, if their her is the king.

Secry most: I bust my crarkes and good,
Woul, the nambes; what the stwlet teraturs,
It state, and you raint this mad
with 
Without of my prist, that me bourth there
med of himsends these doth they rings wall guarives the petierce saidin as meak made I scrue.

SICINIUS:
Now first.

JOHUMNINA:
And, he pronaint; baut his house,
Havilt, firew thou laie aggainst for him barry.

BRUKE RINCE:
But sweet for althon, dobesty, you
and scauge</code></pre>
</div>
</div>
</section>
</section>
<section id="full-gpt-model-architecture" class="level2">
<h2 class="anchored" data-anchor-id="full-gpt-model-architecture">Full GPT Model Architecture</h2>
<section id="key-components" class="level3">
<h3 class="anchored" data-anchor-id="key-components">Key Components</h3>
<ol type="1">
<li><strong>Token Embeddings</strong>: Convert input tokens to vectors</li>
<li><strong>Position Embeddings</strong>: Add positional information</li>
<li><strong>Transformer Blocks</strong>: Process information through attention and feed-forward layers</li>
<li><strong>Layer Normalization</strong>: Stabilize training</li>
<li><strong>Language Model Head</strong>: Predict next token probabilities</li>
</ol>
</section>
<section id="hyperparameters" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameters">Hyperparameters</h3>
<div id="65ca7e84" class="cell" data-execution_count="46">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1"></a><span class="co"># ===== HYPERPARAMETERS =====</span></span>
<span id="cb78-2"><a href="#cb78-2"></a>batch_size <span class="op">=</span> <span class="dv">16</span>        <span class="co"># Number of independent sequences processed in parallel</span></span>
<span id="cb78-3"><a href="#cb78-3"></a>block_size <span class="op">=</span> <span class="dv">32</span>        <span class="co"># Maximum context length for predictions</span></span>
<span id="cb78-4"><a href="#cb78-4"></a>max_iters <span class="op">=</span> <span class="dv">5000</span>       <span class="co"># Total number of training iterations</span></span>
<span id="cb78-5"><a href="#cb78-5"></a>eval_interval <span class="op">=</span> <span class="dv">100</span>    <span class="co"># How often to evaluate the model</span></span>
<span id="cb78-6"><a href="#cb78-6"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span>   <span class="co"># Step size for gradient descent</span></span>
<span id="cb78-7"><a href="#cb78-7"></a>eval_iters <span class="op">=</span> <span class="dv">200</span>       <span class="co"># Number of iterations to average over when evaluating</span></span>
<span id="cb78-8"><a href="#cb78-8"></a>n_embd <span class="op">=</span> <span class="dv">64</span>           <span class="co"># Size of the embedding dimension</span></span>
<span id="cb78-9"><a href="#cb78-9"></a>n_head <span class="op">=</span> <span class="dv">4</span>            <span class="co"># Number of attention heads</span></span>
<span id="cb78-10"><a href="#cb78-10"></a>n_layer <span class="op">=</span> <span class="dv">4</span>           <span class="co"># Number of transformer layers</span></span>
<span id="cb78-11"><a href="#cb78-11"></a>dropout <span class="op">=</span> <span class="fl">0.0</span>         <span class="co"># Probability of dropping out neurons</span></span>
<span id="cb78-12"><a href="#cb78-12"></a><span class="co"># ==========================</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="device-selection" class="level3">
<h3 class="anchored" data-anchor-id="device-selection">Device Selection</h3>
<div id="8ea17665" class="cell" data-execution_count="47">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1"></a><span class="co"># Device selection: MPS (Apple Silicon) &gt; CUDA &gt; CPU</span></span>
<span id="cb79-2"><a href="#cb79-2"></a><span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb79-3"><a href="#cb79-3"></a>    device <span class="op">=</span> torch.device(<span class="st">"mps"</span>)  <span class="co"># Apple Silicon GPU</span></span>
<span id="cb79-4"><a href="#cb79-4"></a><span class="cf">elif</span> torch.cuda.is_available():</span>
<span id="cb79-5"><a href="#cb79-5"></a>    device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)  <span class="co"># NVIDIA GPU</span></span>
<span id="cb79-6"><a href="#cb79-6"></a><span class="cf">else</span>:</span>
<span id="cb79-7"><a href="#cb79-7"></a>    device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)   <span class="co"># CPU fallback</span></span>
<span id="cb79-8"><a href="#cb79-8"></a><span class="bu">print</span>(<span class="ss">f"Using device: </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Using device: mps</code></pre>
</div>
</div>
</section>
<section id="attention-head-implementation" class="level3">
<h3 class="anchored" data-anchor-id="attention-head-implementation">Attention Head Implementation</h3>
<div id="b4356b43" class="cell" data-execution_count="48">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1"></a><span class="kw">class</span> Head(nn.Module):</span>
<span id="cb81-2"><a href="#cb81-2"></a>    <span class="co">"""Single head of self-attention."""</span></span>
<span id="cb81-3"><a href="#cb81-3"></a>    </span>
<span id="cb81-4"><a href="#cb81-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, head_size):</span>
<span id="cb81-5"><a href="#cb81-5"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb81-6"><a href="#cb81-6"></a>        <span class="co"># Key, Query, Value projections</span></span>
<span id="cb81-7"><a href="#cb81-7"></a>        <span class="va">self</span>.key <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb81-8"><a href="#cb81-8"></a>        <span class="va">self</span>.query <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb81-9"><a href="#cb81-9"></a>        <span class="va">self</span>.value <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb81-10"><a href="#cb81-10"></a>        <span class="co"># Create lower triangular mask for causal attention</span></span>
<span id="cb81-11"><a href="#cb81-11"></a>        <span class="va">self</span>.register_buffer(<span class="st">'tril'</span>, torch.tril(torch.ones(block_size, block_size)))</span>
<span id="cb81-12"><a href="#cb81-12"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb81-13"><a href="#cb81-13"></a></span>
<span id="cb81-14"><a href="#cb81-14"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb81-15"><a href="#cb81-15"></a>        B,T,C <span class="op">=</span> x.shape</span>
<span id="cb81-16"><a href="#cb81-16"></a>        <span class="co"># Compute key, query, and value matrices</span></span>
<span id="cb81-17"><a href="#cb81-17"></a>        k <span class="op">=</span> <span class="va">self</span>.key(x)   <span class="co"># (B,T,C)</span></span>
<span id="cb81-18"><a href="#cb81-18"></a>        q <span class="op">=</span> <span class="va">self</span>.query(x) <span class="co"># (B,T,C)</span></span>
<span id="cb81-19"><a href="#cb81-19"></a>        <span class="co"># Compute attention scores</span></span>
<span id="cb81-20"><a href="#cb81-20"></a>        wei <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> C<span class="op">**-</span><span class="fl">0.5</span>  <span class="co"># Scale by sqrt(d_k)</span></span>
<span id="cb81-21"><a href="#cb81-21"></a>        wei <span class="op">=</span> wei.masked_fill(<span class="va">self</span>.tril[:T, :T] <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))  <span class="co"># Apply mask</span></span>
<span id="cb81-22"><a href="#cb81-22"></a>        wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># Convert to probabilities</span></span>
<span id="cb81-23"><a href="#cb81-23"></a>        wei <span class="op">=</span> <span class="va">self</span>.dropout(wei)</span>
<span id="cb81-24"><a href="#cb81-24"></a>        <span class="co"># Weighted aggregation of values</span></span>
<span id="cb81-25"><a href="#cb81-25"></a>        v <span class="op">=</span> <span class="va">self</span>.value(x)</span>
<span id="cb81-26"><a href="#cb81-26"></a>        out <span class="op">=</span> wei <span class="op">@</span> v</span>
<span id="cb81-27"><a href="#cb81-27"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="multi-head-attention" class="level3">
<h3 class="anchored" data-anchor-id="multi-head-attention">Multi-Head Attention</h3>
<div id="a3c779c3" class="cell" data-execution_count="49">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb82-2"><a href="#cb82-2"></a>    <span class="co">"""Multiple heads of self-attention in parallel."""</span></span>
<span id="cb82-3"><a href="#cb82-3"></a>    </span>
<span id="cb82-4"><a href="#cb82-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_heads, head_size):</span>
<span id="cb82-5"><a href="#cb82-5"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb82-6"><a href="#cb82-6"></a>        <span class="va">self</span>.heads <span class="op">=</span> nn.ModuleList([Head(head_size) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_heads)])</span>
<span id="cb82-7"><a href="#cb82-7"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Linear(n_embd, n_embd)  <span class="co"># Projection layer</span></span>
<span id="cb82-8"><a href="#cb82-8"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb82-9"><a href="#cb82-9"></a></span>
<span id="cb82-10"><a href="#cb82-10"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb82-11"><a href="#cb82-11"></a>        <span class="co"># Process through each head and concatenate results</span></span>
<span id="cb82-12"><a href="#cb82-12"></a>        out <span class="op">=</span> torch.cat([h(x) <span class="cf">for</span> h <span class="kw">in</span> <span class="va">self</span>.heads], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb82-13"><a href="#cb82-13"></a>        out <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.proj(out))</span>
<span id="cb82-14"><a href="#cb82-14"></a>        <span class="cf">return</span> out</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="feed-forward-network" class="level3">
<h3 class="anchored" data-anchor-id="feed-forward-network">Feed-Forward Network</h3>
<div id="75b3f78a" class="cell" data-execution_count="50">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1"></a><span class="kw">class</span> FeedFoward(nn.Module):</span>
<span id="cb83-2"><a href="#cb83-2"></a>    <span class="co">"""Simple feed-forward network with one hidden layer."""</span></span>
<span id="cb83-3"><a href="#cb83-3"></a>    </span>
<span id="cb83-4"><a href="#cb83-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd):</span>
<span id="cb83-5"><a href="#cb83-5"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb83-6"><a href="#cb83-6"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb83-7"><a href="#cb83-7"></a>            nn.Linear(n_embd, <span class="dv">4</span> <span class="op">*</span> n_embd),  <span class="co"># Expand dimension</span></span>
<span id="cb83-8"><a href="#cb83-8"></a>            nn.ReLU(),                      <span class="co"># Non-linearity</span></span>
<span id="cb83-9"><a href="#cb83-9"></a>            nn.Linear(<span class="dv">4</span> <span class="op">*</span> n_embd, n_embd),  <span class="co"># Project back</span></span>
<span id="cb83-10"><a href="#cb83-10"></a>            nn.Dropout(dropout),</span>
<span id="cb83-11"><a href="#cb83-11"></a>        )</span>
<span id="cb83-12"><a href="#cb83-12"></a></span>
<span id="cb83-13"><a href="#cb83-13"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb83-14"><a href="#cb83-14"></a>        <span class="cf">return</span> <span class="va">self</span>.net(x)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="transformer-block" class="level3">
<h3 class="anchored" data-anchor-id="transformer-block">Transformer Block</h3>
<div id="38e33bb3" class="cell" data-execution_count="51">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb84-2"><a href="#cb84-2"></a>    <span class="co">"""Transformer block: communication (attention) followed by computation (FFN)."""</span></span>
<span id="cb84-3"><a href="#cb84-3"></a>    </span>
<span id="cb84-4"><a href="#cb84-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd, n_head):</span>
<span id="cb84-5"><a href="#cb84-5"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb84-6"><a href="#cb84-6"></a>        head_size <span class="op">=</span> n_embd <span class="op">//</span> n_head  <span class="co"># floor(n_embd / n_head)</span></span>
<span id="cb84-7"><a href="#cb84-7"></a>        <span class="va">self</span>.sa <span class="op">=</span> MultiHeadAttention(n_head, head_size)</span>
<span id="cb84-8"><a href="#cb84-8"></a>        <span class="va">self</span>.ffwd <span class="op">=</span> FeedFoward(n_embd)</span>
<span id="cb84-9"><a href="#cb84-9"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> nn.LayerNorm(n_embd)  <span class="co"># Layer normalization</span></span>
<span id="cb84-10"><a href="#cb84-10"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> nn.LayerNorm(n_embd)</span>
<span id="cb84-11"><a href="#cb84-11"></a></span>
<span id="cb84-12"><a href="#cb84-12"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb84-13"><a href="#cb84-13"></a>        <span class="co"># Apply attention with residual connection</span></span>
<span id="cb84-14"><a href="#cb84-14"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.sa(<span class="va">self</span>.ln1(x))</span>
<span id="cb84-15"><a href="#cb84-15"></a>        <span class="co"># Apply feed-forward with residual connection</span></span>
<span id="cb84-16"><a href="#cb84-16"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffwd(<span class="va">self</span>.ln2(x))</span>
<span id="cb84-17"><a href="#cb84-17"></a>        <span class="cf">return</span> x</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="full-language-model" class="level3">
<h3 class="anchored" data-anchor-id="full-language-model">Full Language Model</h3>
<div id="e2000868" class="cell" data-execution_count="52">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1"></a><span class="kw">class</span> BigramLanguageModel(nn.Module):</span>
<span id="cb85-2"><a href="#cb85-2"></a>    <span class="co">"""Simple bigram language model with transformer architecture."""</span></span>
<span id="cb85-3"><a href="#cb85-3"></a>    </span>
<span id="cb85-4"><a href="#cb85-4"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb85-5"><a href="#cb85-5"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb85-6"><a href="#cb85-6"></a>        <span class="co"># Token embeddings</span></span>
<span id="cb85-7"><a href="#cb85-7"></a>        <span class="va">self</span>.token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, n_embd)</span>
<span id="cb85-8"><a href="#cb85-8"></a>        <span class="co"># Position embeddings</span></span>
<span id="cb85-9"><a href="#cb85-9"></a>        <span class="va">self</span>.position_embedding_table <span class="op">=</span> nn.Embedding(block_size, n_embd)</span>
<span id="cb85-10"><a href="#cb85-10"></a>        <span class="co"># Stack of transformer blocks</span></span>
<span id="cb85-11"><a href="#cb85-11"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.Sequential(<span class="op">*</span>[Block(n_embd, n_head<span class="op">=</span>n_head) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_layer)])</span>
<span id="cb85-12"><a href="#cb85-12"></a>        <span class="va">self</span>.ln_f <span class="op">=</span> nn.LayerNorm(n_embd)  <span class="co"># Final layer norm</span></span>
<span id="cb85-13"><a href="#cb85-13"></a>        <span class="va">self</span>.lm_head <span class="op">=</span> nn.Linear(n_embd, vocab_size)  <span class="co"># Language model head</span></span>
<span id="cb85-14"><a href="#cb85-14"></a></span>
<span id="cb85-15"><a href="#cb85-15"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb85-16"><a href="#cb85-16"></a>        B, T <span class="op">=</span> idx.shape</span>
<span id="cb85-17"><a href="#cb85-17"></a>        </span>
<span id="cb85-18"><a href="#cb85-18"></a>        <span class="co"># Get token embeddings</span></span>
<span id="cb85-19"><a href="#cb85-19"></a>        tok_emb <span class="op">=</span> <span class="va">self</span>.token_embedding_table(idx)  <span class="co"># (B,T,C)</span></span>
<span id="cb85-20"><a href="#cb85-20"></a>        <span class="co"># Get position embeddings</span></span>
<span id="cb85-21"><a href="#cb85-21"></a>        pos_emb <span class="op">=</span> <span class="va">self</span>.position_embedding_table(torch.arange(T, device<span class="op">=</span>device))  <span class="co"># (T,C)</span></span>
<span id="cb85-22"><a href="#cb85-22"></a>        <span class="co"># Combine token and position embeddings</span></span>
<span id="cb85-23"><a href="#cb85-23"></a>        x <span class="op">=</span> tok_emb <span class="op">+</span> pos_emb  <span class="co"># (B,T,C)</span></span>
<span id="cb85-24"><a href="#cb85-24"></a>        <span class="co"># Process through transformer blocks</span></span>
<span id="cb85-25"><a href="#cb85-25"></a>        x <span class="op">=</span> <span class="va">self</span>.blocks(x)  <span class="co"># (B,T,C)</span></span>
<span id="cb85-26"><a href="#cb85-26"></a>        x <span class="op">=</span> <span class="va">self</span>.ln_f(x)  <span class="co"># (B,T,C)</span></span>
<span id="cb85-27"><a href="#cb85-27"></a>        logits <span class="op">=</span> <span class="va">self</span>.lm_head(x)  <span class="co"># (B,T,vocab_size)</span></span>
<span id="cb85-28"><a href="#cb85-28"></a></span>
<span id="cb85-29"><a href="#cb85-29"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb85-30"><a href="#cb85-30"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb85-31"><a href="#cb85-31"></a>        <span class="cf">else</span>:</span>
<span id="cb85-32"><a href="#cb85-32"></a>            <span class="co"># Reshape for loss calculation</span></span>
<span id="cb85-33"><a href="#cb85-33"></a>            B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb85-34"><a href="#cb85-34"></a>            logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C)</span>
<span id="cb85-35"><a href="#cb85-35"></a>            targets <span class="op">=</span> targets.view(B<span class="op">*</span>T)</span>
<span id="cb85-36"><a href="#cb85-36"></a>            loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb85-37"><a href="#cb85-37"></a></span>
<span id="cb85-38"><a href="#cb85-38"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb85-39"><a href="#cb85-39"></a></span>
<span id="cb85-40"><a href="#cb85-40"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens):</span>
<span id="cb85-41"><a href="#cb85-41"></a>        <span class="co">"""Generate new text given a starting sequence."""</span></span>
<span id="cb85-42"><a href="#cb85-42"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb85-43"><a href="#cb85-43"></a>            <span class="co"># Crop context to block_size</span></span>
<span id="cb85-44"><a href="#cb85-44"></a>            idx_cond <span class="op">=</span> idx[:, <span class="op">-</span>block_size:]</span>
<span id="cb85-45"><a href="#cb85-45"></a>            <span class="co"># Get predictions</span></span>
<span id="cb85-46"><a href="#cb85-46"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>(idx_cond)</span>
<span id="cb85-47"><a href="#cb85-47"></a>            <span class="co"># Focus on last time step</span></span>
<span id="cb85-48"><a href="#cb85-48"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :]  <span class="co"># (B, C)</span></span>
<span id="cb85-49"><a href="#cb85-49"></a>            <span class="co"># Convert to probabilities</span></span>
<span id="cb85-50"><a href="#cb85-50"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># (B, C)</span></span>
<span id="cb85-51"><a href="#cb85-51"></a>            <span class="co"># Sample from distribution</span></span>
<span id="cb85-52"><a href="#cb85-52"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)  <span class="co"># (B, 1)</span></span>
<span id="cb85-53"><a href="#cb85-53"></a>            <span class="co"># Append to sequence</span></span>
<span id="cb85-54"><a href="#cb85-54"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># (B, T+1)</span></span>
<span id="cb85-55"><a href="#cb85-55"></a>        <span class="cf">return</span> idx</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="training-loop-1" class="level3">
<h3 class="anchored" data-anchor-id="training-loop-1">Training Loop</h3>
<div id="b76b3302" class="cell" data-execution_count="53">
<details class="code-fold">
<summary>Show the code</summary>
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1"></a><span class="co"># Create model instance</span></span>
<span id="cb86-2"><a href="#cb86-2"></a>model <span class="op">=</span> BigramLanguageModel()</span>
<span id="cb86-3"><a href="#cb86-3"></a>m <span class="op">=</span> model.to(device)</span>
<span id="cb86-4"><a href="#cb86-4"></a><span class="co"># Print number of parameters</span></span>
<span id="cb86-5"><a href="#cb86-5"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> m.parameters())<span class="op">/</span><span class="fl">1e6</span>, <span class="st">'M parameters'</span>)</span>
<span id="cb86-6"><a href="#cb86-6"></a></span>
<span id="cb86-7"><a href="#cb86-7"></a><span class="co"># Create optimizer</span></span>
<span id="cb86-8"><a href="#cb86-8"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb86-9"><a href="#cb86-9"></a></span>
<span id="cb86-10"><a href="#cb86-10"></a><span class="co"># Training loop</span></span>
<span id="cb86-11"><a href="#cb86-11"></a><span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(max_iters):</span>
<span id="cb86-12"><a href="#cb86-12"></a>    <span class="co"># Evaluate loss periodically</span></span>
<span id="cb86-13"><a href="#cb86-13"></a>    <span class="cf">if</span> <span class="bu">iter</span> <span class="op">%</span> eval_interval <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> <span class="bu">iter</span> <span class="op">==</span> max_iters <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb86-14"><a href="#cb86-14"></a>        losses <span class="op">=</span> estimate_loss()</span>
<span id="cb86-15"><a href="#cb86-15"></a>        <span class="bu">print</span>(<span class="ss">f"step </span><span class="sc">{</span><span class="bu">iter</span><span class="sc">}</span><span class="ss">: train loss </span><span class="sc">{</span>losses[<span class="st">'train'</span>]<span class="sc">:.4f}</span><span class="ss">, val loss </span><span class="sc">{</span>losses[<span class="st">'val'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb86-16"><a href="#cb86-16"></a></span>
<span id="cb86-17"><a href="#cb86-17"></a>    <span class="co"># Get batch of data</span></span>
<span id="cb86-18"><a href="#cb86-18"></a>    xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb86-19"><a href="#cb86-19"></a></span>
<span id="cb86-20"><a href="#cb86-20"></a>    <span class="co"># Forward pass</span></span>
<span id="cb86-21"><a href="#cb86-21"></a>    logits, loss <span class="op">=</span> model(xb, yb)</span>
<span id="cb86-22"><a href="#cb86-22"></a>    <span class="co"># Backward pass</span></span>
<span id="cb86-23"><a href="#cb86-23"></a>    optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb86-24"><a href="#cb86-24"></a>    loss.backward()</span>
<span id="cb86-25"><a href="#cb86-25"></a>    optimizer.step()</span>
<span id="cb86-26"><a href="#cb86-26"></a></span>
<span id="cb86-27"><a href="#cb86-27"></a><span class="co"># Generate text from the model</span></span>
<span id="cb86-28"><a href="#cb86-28"></a>context <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>, device<span class="op">=</span>device)</span>
<span id="cb86-29"><a href="#cb86-29"></a><span class="bu">print</span>(decode(m.generate(context, max_new_tokens<span class="op">=</span><span class="dv">2000</span>)[<span class="dv">0</span>].tolist()))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>0.209729 M parameters
step 0: train loss 4.3394, val loss 4.3431
step 100: train loss 2.6632, val loss 2.6791
step 200: train loss 2.5217, val loss 2.5418
step 300: train loss 2.4543, val loss 2.4651
step 400: train loss 2.3816, val loss 2.3964
step 500: train loss 2.3089, val loss 2.3467
step 600: train loss 2.2623, val loss 2.2752
step 700: train loss 2.2159, val loss 2.2506
step 800: train loss 2.1686, val loss 2.1930
step 900: train loss 2.1189, val loss 2.1462
step 1000: train loss 2.0918, val loss 2.1344
step 1100: train loss 2.0584, val loss 2.1208
step 1200: train loss 2.0410, val loss 2.0926
step 1300: train loss 2.0163, val loss 2.0798
step 1400: train loss 1.9794, val loss 2.0513
step 1500: train loss 1.9667, val loss 2.0361
step 1600: train loss 1.9335, val loss 2.0073
step 1700: train loss 1.9207, val loss 2.0012
step 1800: train loss 1.9107, val loss 1.9917
step 1900: train loss 1.8974, val loss 1.9897
step 2000: train loss 1.8725, val loss 1.9790
step 2100: train loss 1.8653, val loss 1.9743
step 2200: train loss 1.8462, val loss 1.9588
step 2300: train loss 1.8375, val loss 1.9578
step 2400: train loss 1.8287, val loss 1.9400
step 2500: train loss 1.8092, val loss 1.9371
step 2600: train loss 1.7903, val loss 1.9191
step 2700: train loss 1.7947, val loss 1.9100
step 2800: train loss 1.7814, val loss 1.9046
step 2900: train loss 1.7752, val loss 1.9102
step 3000: train loss 1.7713, val loss 1.8945
step 3100: train loss 1.7475, val loss 1.8946
step 3200: train loss 1.7430, val loss 1.8887
step 3300: train loss 1.7341, val loss 1.8849
step 3400: train loss 1.7380, val loss 1.8876
step 3500: train loss 1.7289, val loss 1.8708
step 3600: train loss 1.7257, val loss 1.8767
step 3700: train loss 1.7306, val loss 1.8691
step 3800: train loss 1.7136, val loss 1.8477
step 3900: train loss 1.7038, val loss 1.8399
step 4000: train loss 1.7005, val loss 1.8497
step 4100: train loss 1.6913, val loss 1.8499
step 4200: train loss 1.7016, val loss 1.8297
step 4300: train loss 1.6835, val loss 1.8279
step 4400: train loss 1.6818, val loss 1.8297
step 4500: train loss 1.6785, val loss 1.8418
step 4600: train loss 1.6710, val loss 1.8431
step 4700: train loss 1.6779, val loss 1.8463
step 4800: train loss 1.6799, val loss 1.8353
step 4900: train loss 1.6737, val loss 1.8174
step 4999: train loss 1.6602, val loss 1.7982


SICINIUS:
Say 't a at them, that is I to the larests.

KING RICHARD III:
Them, but asceit, cen hand swereath.

PROMENE:
Rame to is the with you strange, who themselber to
From and pure-jold e'll
So you beace him lear
dear sected unomp sole beditts SA
That will.

ETENCENTIO:
Cont hom'd a grown I hund! are his could and go greace?

ANGOLY MARD III:
Which scient.

EXCK:
Comenviwing alones.

Provord:
I have may queet whate for 'sciands ill in  shouth:
I them not to thy spirit may,
Which ons' goldarwick cour ar tears, do manter'd;
A gue--
To lack son'd the heaven.
Trace But then madle here, though me bnigwas the fear upo
fidest for be and my astemp well be you whatn of the moutlent
To right the purpore aways: if the
prrest I chall scuffer and;
Sell discoray'der, thou se
I-have you.
That I vatige the as sodvoliem--
 ign who recomes thing immerous. Look, and an it mineas, there is we caule thy latel'd I your bid onurit,
Shrue havel of yen to go; ket you our anot bod?

First Clitcem; and not what's fathen.

CORIOLANUS:
Soke cive les, thus?

ISABEXET:

ANTES MOW:
Thy fairs of my airts, vowike thee
So the savies the himnour sopdy the
lows churder wran theres in holds that and a destant;
Your and upon ene--

AUTOLT:
Will fear and be old
that should be whom would lord him dO-do proves come.

SICINIUS:
Shall hebin as behower and that mine, I
Like the childners our gire misan:
That is hands love thou, thereful your made montheres;
Come foest then a preas toons serve
His orrived riften. for 'tiol; him kingly,
If you heaven of me: for, pathe his treath?

DUKE VINCENTIO:
Not you this strots, the lars.
3 KING EDWARD IV:
Come, for me is his is the more wese,
Must ston be thou honest rest of asseen for Moursedle severt their day of to the nore,
I her hend his but bid not ang not, apot
All shing fountire Cate.

DUKE VINCENTIO:
Apomieves.

Nurse:
Marry, Kits stlard caurtice, thou hiw I am have must fartmedly them that over the pard me them;
I det ost in thou ward is says day!
Thou sake </code></pre>
</div>
</div>
</section>
</section>
<section id="key-takeaways" class="level2">
<h2 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h2>
<ol type="1">
<li><strong>Data Preparation</strong>
<ul>
<li>Tokenization: Convert text to numerical representations</li>
<li>Batching: Create efficient training batches</li>
<li>Context and Targets: Understand the prediction task</li>
</ul></li>
<li><strong>Model Architecture</strong>
<ul>
<li>Token and Position Embeddings</li>
<li>Self-Attention Mechanism</li>
<li>Transformer Blocks</li>
<li>Layer Normalization</li>
<li>Language Model Head</li>
</ul></li>
<li><strong>Training Process</strong>
<ul>
<li>Loss Function: Cross-Entropy</li>
<li>Optimization: AdamW</li>
<li>Evaluation: Periodic validation checks</li>
<li>Generation: Autoregressive text generation</li>
</ul></li>
<li><strong>Key Concepts</strong>
<ul>
<li>Self-Attention: Communication between tokens</li>
<li>Positional Encoding: Preserving sequence order</li>
<li>Residual Connections: Helping gradient flow</li>
<li>Layer Normalization: Stabilizing training</li>
<li>Multi-Head Attention: Capturing different relationships ```</li>
</ul></li>
</ol>


<!-- -->

</section>

<p>© <a href="https://hakyimlab.org">HakyImLab and Listed Authors</a> - <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 License</a></p></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb88" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb88-1"><a href="#cb88-1"></a><span class="co">---</span></span>
<span id="cb88-2"><a href="#cb88-2"></a><span class="an">title:</span><span class="co"> Building a GPT - companion notebook qmd</span></span>
<span id="cb88-3"><a href="#cb88-3"></a><span class="an">author:</span><span class="co"> Andrey Karpathy</span></span>
<span id="cb88-4"><a href="#cb88-4"></a><span class="an">date:</span><span class="co"> '2025-04-11'</span></span>
<span id="cb88-5"><a href="#cb88-5"></a><span class="an">freeze:</span><span class="co"> true</span></span>
<span id="cb88-6"><a href="#cb88-6"></a><span class="an">jupyter:</span><span class="co"> </span></span>
<span id="cb88-7"><a href="#cb88-7"></a><span class="co">  kernelspec:</span></span>
<span id="cb88-8"><a href="#cb88-8"></a><span class="co">    name: "conda-env-gene46100-py"</span></span>
<span id="cb88-9"><a href="#cb88-9"></a><span class="co">    language: "python"</span></span>
<span id="cb88-10"><a href="#cb88-10"></a><span class="co">    display_name: "gene46100"</span></span>
<span id="cb88-11"><a href="#cb88-11"></a><span class="an">format:</span></span>
<span id="cb88-12"><a href="#cb88-12"></a><span class="co">  html:</span></span>
<span id="cb88-13"><a href="#cb88-13"></a><span class="co">    code-fold: true</span></span>
<span id="cb88-14"><a href="#cb88-14"></a><span class="co">    code-line-numbers: true</span></span>
<span id="cb88-15"><a href="#cb88-15"></a><span class="co">    code-tools: true</span></span>
<span id="cb88-16"><a href="#cb88-16"></a><span class="co">    code-wrap: true</span></span>
<span id="cb88-17"><a href="#cb88-17"></a><span class="an">description:</span><span class="co"> Companion notebook from Karpathys video on building a minimal GPT, annotated by cursors LLM with summary from gemini.</span></span>
<span id="cb88-18"><a href="#cb88-18"></a><span class="co">---</span></span>
<span id="cb88-19"><a href="#cb88-19"></a></span>
<span id="cb88-20"><a href="#cb88-20"></a><span class="fu">## Building a GPT</span></span>
<span id="cb88-21"><a href="#cb88-21"></a></span>
<span id="cb88-22"><a href="#cb88-22"></a>Companion notebook to the <span class="co">[</span><span class="ot">Zero To Hero</span><span class="co">](https://karpathy.ai/zero-to-hero.html)</span> video on GPT. Downloaded from <span class="co">[</span><span class="ot">here</span><span class="co">](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing)</span></span>
<span id="cb88-23"><a href="#cb88-23"></a></span>
<span id="cb88-24"><a href="#cb88-24"></a>(https://github.com/karpathy/nanoGPT)</span>
<span id="cb88-25"><a href="#cb88-25"></a></span>
<span id="cb88-26"><a href="#cb88-26"></a><span class="fu">## Data Preparation</span></span>
<span id="cb88-27"><a href="#cb88-27"></a></span>
<span id="cb88-28"><a href="#cb88-28"></a><span class="fu">### Step 1: Loading &amp; Inspection</span></span>
<span id="cb88-31"><a href="#cb88-31"></a><span class="in">```{python}</span></span>
<span id="cb88-32"><a href="#cb88-32"></a><span class="co"># We always start with a dataset to train on. Let's download the tiny shakespeare dataset</span></span>
<span id="cb88-33"><a href="#cb88-33"></a><span class="co">#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt</span></span>
<span id="cb88-34"><a href="#cb88-34"></a><span class="in">```</span></span>
<span id="cb88-35"><a href="#cb88-35"></a></span>
<span id="cb88-38"><a href="#cb88-38"></a><span class="in">```{python}</span></span>
<span id="cb88-39"><a href="#cb88-39"></a><span class="co"># read it in to inspect it</span></span>
<span id="cb88-40"><a href="#cb88-40"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'input.txt'</span>, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> f:</span>
<span id="cb88-41"><a href="#cb88-41"></a>    text <span class="op">=</span> f.read()</span>
<span id="cb88-42"><a href="#cb88-42"></a><span class="in">```</span></span>
<span id="cb88-43"><a href="#cb88-43"></a></span>
<span id="cb88-46"><a href="#cb88-46"></a><span class="in">```{python}</span></span>
<span id="cb88-47"><a href="#cb88-47"></a><span class="co"># print the length of the dataset</span></span>
<span id="cb88-48"><a href="#cb88-48"></a><span class="bu">print</span>(<span class="st">"length of dataset in characters: "</span>, <span class="bu">len</span>(text))</span>
<span id="cb88-49"><a href="#cb88-49"></a><span class="in">```</span></span>
<span id="cb88-50"><a href="#cb88-50"></a></span>
<span id="cb88-53"><a href="#cb88-53"></a><span class="in">```{python}</span></span>
<span id="cb88-54"><a href="#cb88-54"></a><span class="co"># let's look at the first 1000 characters</span></span>
<span id="cb88-55"><a href="#cb88-55"></a><span class="bu">print</span>(text[:<span class="dv">1000</span>])</span>
<span id="cb88-56"><a href="#cb88-56"></a><span class="in">```</span></span>
<span id="cb88-57"><a href="#cb88-57"></a></span>
<span id="cb88-58"><a href="#cb88-58"></a><span class="fu">### Step 2: Tokenization</span></span>
<span id="cb88-61"><a href="#cb88-61"></a><span class="in">```{python}</span></span>
<span id="cb88-62"><a href="#cb88-62"></a><span class="co"># here are all the unique characters that occur in this text</span></span>
<span id="cb88-63"><a href="#cb88-63"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(text)))</span>
<span id="cb88-64"><a href="#cb88-64"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(chars)</span>
<span id="cb88-65"><a href="#cb88-65"></a><span class="bu">print</span>(<span class="st">''</span>.join(chars))</span>
<span id="cb88-66"><a href="#cb88-66"></a><span class="bu">print</span>(vocab_size)</span>
<span id="cb88-67"><a href="#cb88-67"></a><span class="in">```</span></span>
<span id="cb88-68"><a href="#cb88-68"></a></span>
<span id="cb88-71"><a href="#cb88-71"></a><span class="in">```{python}</span></span>
<span id="cb88-72"><a href="#cb88-72"></a><span class="co"># create a mapping from characters to integers</span></span>
<span id="cb88-73"><a href="#cb88-73"></a>stoi <span class="op">=</span> { ch:i <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }</span>
<span id="cb88-74"><a href="#cb88-74"></a>itos <span class="op">=</span> { i:ch <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }</span>
<span id="cb88-75"><a href="#cb88-75"></a>encode <span class="op">=</span> <span class="kw">lambda</span> s: [stoi[c] <span class="cf">for</span> c <span class="kw">in</span> s] <span class="co"># encoder: take a string, output a list of integers</span></span>
<span id="cb88-76"><a href="#cb88-76"></a>decode <span class="op">=</span> <span class="kw">lambda</span> l: <span class="st">''</span>.join([itos[i] <span class="cf">for</span> i <span class="kw">in</span> l]) <span class="co"># decoder: take a list of integers, output a string</span></span>
<span id="cb88-77"><a href="#cb88-77"></a></span>
<span id="cb88-78"><a href="#cb88-78"></a><span class="bu">print</span>(encode(<span class="st">"hii there"</span>))</span>
<span id="cb88-79"><a href="#cb88-79"></a><span class="bu">print</span>(decode(encode(<span class="st">"hii there"</span>)))</span>
<span id="cb88-80"><a href="#cb88-80"></a><span class="in">```</span></span>
<span id="cb88-81"><a href="#cb88-81"></a></span>
<span id="cb88-82"><a href="#cb88-82"></a><span class="fu">### Step 3: Creating Batches for Training</span></span>
<span id="cb88-85"><a href="#cb88-85"></a><span class="in">```{python}</span></span>
<span id="cb88-86"><a href="#cb88-86"></a><span class="co"># let's now encode the entire text dataset and store it into a torch.Tensor</span></span>
<span id="cb88-87"><a href="#cb88-87"></a><span class="im">import</span> torch <span class="co"># we use PyTorch: https://pytorch.org</span></span>
<span id="cb88-88"><a href="#cb88-88"></a>data <span class="op">=</span> torch.tensor(encode(text), dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb88-89"><a href="#cb88-89"></a><span class="bu">print</span>(data.shape, data.dtype)</span>
<span id="cb88-90"><a href="#cb88-90"></a><span class="bu">print</span>(data[:<span class="dv">1000</span>]) <span class="co"># the 1000 characters we looked at earier will to the GPT look like this</span></span>
<span id="cb88-91"><a href="#cb88-91"></a><span class="in">```</span></span>
<span id="cb88-92"><a href="#cb88-92"></a></span>
<span id="cb88-95"><a href="#cb88-95"></a><span class="in">```{python}</span></span>
<span id="cb88-96"><a href="#cb88-96"></a><span class="co"># split up the data into train and validation sets</span></span>
<span id="cb88-97"><a href="#cb88-97"></a>n <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span><span class="op">*</span><span class="bu">len</span>(data)) <span class="co"># first 90% will be train, rest val</span></span>
<span id="cb88-98"><a href="#cb88-98"></a>train_data <span class="op">=</span> data[:n]</span>
<span id="cb88-99"><a href="#cb88-99"></a>val_data <span class="op">=</span> data[n:]</span>
<span id="cb88-100"><a href="#cb88-100"></a><span class="in">```</span></span>
<span id="cb88-101"><a href="#cb88-101"></a></span>
<span id="cb88-104"><a href="#cb88-104"></a><span class="in">```{python}</span></span>
<span id="cb88-105"><a href="#cb88-105"></a><span class="co"># define the block size (context length)</span></span>
<span id="cb88-106"><a href="#cb88-106"></a>block_size <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb88-107"><a href="#cb88-107"></a>train_data[:block_size<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb88-108"><a href="#cb88-108"></a><span class="in">```</span></span>
<span id="cb88-109"><a href="#cb88-109"></a></span>
<span id="cb88-110"><a href="#cb88-110"></a><span class="fu">### Understanding the Context and Target</span></span>
<span id="cb88-113"><a href="#cb88-113"></a><span class="in">```{python}</span></span>
<span id="cb88-114"><a href="#cb88-114"></a>x <span class="op">=</span> train_data[:block_size]</span>
<span id="cb88-115"><a href="#cb88-115"></a>y <span class="op">=</span> train_data[<span class="dv">1</span>:block_size<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb88-116"><a href="#cb88-116"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(block_size):</span>
<span id="cb88-117"><a href="#cb88-117"></a>    context <span class="op">=</span> x[:t<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb88-118"><a href="#cb88-118"></a>    target <span class="op">=</span> y[t]</span>
<span id="cb88-119"><a href="#cb88-119"></a>    <span class="bu">print</span>(<span class="ss">f"when input is </span><span class="sc">{</span>context<span class="sc">}</span><span class="ss"> the target: </span><span class="sc">{</span>target<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb88-120"><a href="#cb88-120"></a><span class="in">```</span></span>
<span id="cb88-121"><a href="#cb88-121"></a></span>
<span id="cb88-122"><a href="#cb88-122"></a><span class="fu">### Creating Batches</span></span>
<span id="cb88-125"><a href="#cb88-125"></a><span class="in">```{python}</span></span>
<span id="cb88-126"><a href="#cb88-126"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb88-127"><a href="#cb88-127"></a>batch_size <span class="op">=</span> <span class="dv">4</span> <span class="co"># how many independent sequences will we process in parallel?</span></span>
<span id="cb88-128"><a href="#cb88-128"></a>block_size <span class="op">=</span> <span class="dv">8</span> <span class="co"># what is the maximum context length for predictions?</span></span>
<span id="cb88-129"><a href="#cb88-129"></a></span>
<span id="cb88-130"><a href="#cb88-130"></a><span class="kw">def</span> get_batch(split):</span>
<span id="cb88-131"><a href="#cb88-131"></a>    <span class="co"># generate a small batch of data of inputs x and targets y</span></span>
<span id="cb88-132"><a href="#cb88-132"></a>    data <span class="op">=</span> train_data <span class="cf">if</span> split <span class="op">==</span> <span class="st">'train'</span> <span class="cf">else</span> val_data</span>
<span id="cb88-133"><a href="#cb88-133"></a>    ix <span class="op">=</span> torch.randint(<span class="bu">len</span>(data) <span class="op">-</span> block_size, (batch_size,))</span>
<span id="cb88-134"><a href="#cb88-134"></a>    x <span class="op">=</span> torch.stack([data[i:i<span class="op">+</span>block_size] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb88-135"><a href="#cb88-135"></a>    y <span class="op">=</span> torch.stack([data[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span>block_size<span class="op">+</span><span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb88-136"><a href="#cb88-136"></a>    <span class="cf">return</span> x, y</span>
<span id="cb88-137"><a href="#cb88-137"></a></span>
<span id="cb88-138"><a href="#cb88-138"></a>xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb88-139"><a href="#cb88-139"></a><span class="bu">print</span>(<span class="st">'inputs:'</span>)</span>
<span id="cb88-140"><a href="#cb88-140"></a><span class="bu">print</span>(xb.shape)</span>
<span id="cb88-141"><a href="#cb88-141"></a><span class="bu">print</span>(xb)</span>
<span id="cb88-142"><a href="#cb88-142"></a><span class="bu">print</span>(<span class="st">'targets:'</span>)</span>
<span id="cb88-143"><a href="#cb88-143"></a><span class="bu">print</span>(yb.shape)</span>
<span id="cb88-144"><a href="#cb88-144"></a><span class="bu">print</span>(yb)</span>
<span id="cb88-145"><a href="#cb88-145"></a></span>
<span id="cb88-146"><a href="#cb88-146"></a><span class="bu">print</span>(<span class="st">'----'</span>)</span>
<span id="cb88-147"><a href="#cb88-147"></a></span>
<span id="cb88-148"><a href="#cb88-148"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(batch_size): <span class="co"># batch dimension</span></span>
<span id="cb88-149"><a href="#cb88-149"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(block_size): <span class="co"># time dimension</span></span>
<span id="cb88-150"><a href="#cb88-150"></a>        context <span class="op">=</span> xb[b, :t<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb88-151"><a href="#cb88-151"></a>        target <span class="op">=</span> yb[b,t]</span>
<span id="cb88-152"><a href="#cb88-152"></a>        <span class="bu">print</span>(<span class="ss">f"when input is </span><span class="sc">{</span>context<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss"> the target: </span><span class="sc">{</span>target<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb88-153"><a href="#cb88-153"></a><span class="in">```</span></span>
<span id="cb88-154"><a href="#cb88-154"></a></span>
<span id="cb88-155"><a href="#cb88-155"></a><span class="fu">## Model Architecture</span></span>
<span id="cb88-156"><a href="#cb88-156"></a></span>
<span id="cb88-157"><a href="#cb88-157"></a><span class="fu">### Starting Simple: Bigram Language Model</span></span>
<span id="cb88-158"><a href="#cb88-158"></a>This model predicts the next character based on the current character only. </span>
<span id="cb88-159"><a href="#cb88-159"></a>It uses the logits for the next character in a lookup table.</span>
<span id="cb88-160"><a href="#cb88-160"></a></span>
<span id="cb88-161"><a href="#cb88-161"></a>Key points:</span>
<span id="cb88-162"><a href="#cb88-162"></a><span class="ss">- </span>Simplest possible model</span>
<span id="cb88-163"><a href="#cb88-163"></a><span class="ss">- </span>Uses an Embedding Table of size vocab_size x vocab_size</span>
<span id="cb88-164"><a href="#cb88-164"></a><span class="ss">- </span>Each row contains predicted scores for next character</span>
<span id="cb88-165"><a href="#cb88-165"></a><span class="ss">- </span>Ignores context beyond last character</span>
<span id="cb88-168"><a href="#cb88-168"></a><span class="in">```{python}</span></span>
<span id="cb88-169"><a href="#cb88-169"></a><span class="co"># define the bigram language model</span></span>
<span id="cb88-170"><a href="#cb88-170"></a><span class="im">import</span> torch</span>
<span id="cb88-171"><a href="#cb88-171"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb88-172"><a href="#cb88-172"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb88-173"><a href="#cb88-173"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb88-174"><a href="#cb88-174"></a></span>
<span id="cb88-175"><a href="#cb88-175"></a><span class="kw">class</span> BigramLanguageModel(nn.Module):</span>
<span id="cb88-176"><a href="#cb88-176"></a></span>
<span id="cb88-177"><a href="#cb88-177"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size):</span>
<span id="cb88-178"><a href="#cb88-178"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb88-179"><a href="#cb88-179"></a>        <span class="co"># each token directly reads off the logits for the next token from a lookup table</span></span>
<span id="cb88-180"><a href="#cb88-180"></a>        <span class="va">self</span>.token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, vocab_size)</span>
<span id="cb88-181"><a href="#cb88-181"></a></span>
<span id="cb88-182"><a href="#cb88-182"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb88-183"><a href="#cb88-183"></a></span>
<span id="cb88-184"><a href="#cb88-184"></a>        <span class="co"># idx and targets are both (B,T) tensor of integers</span></span>
<span id="cb88-185"><a href="#cb88-185"></a>        logits <span class="op">=</span> <span class="va">self</span>.token_embedding_table(idx) <span class="co"># (B,T,C)</span></span>
<span id="cb88-186"><a href="#cb88-186"></a></span>
<span id="cb88-187"><a href="#cb88-187"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb88-188"><a href="#cb88-188"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb88-189"><a href="#cb88-189"></a>        <span class="cf">else</span>:</span>
<span id="cb88-190"><a href="#cb88-190"></a>            B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb88-191"><a href="#cb88-191"></a>            logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C)</span>
<span id="cb88-192"><a href="#cb88-192"></a>            targets <span class="op">=</span> targets.view(B<span class="op">*</span>T)</span>
<span id="cb88-193"><a href="#cb88-193"></a>            loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb88-194"><a href="#cb88-194"></a></span>
<span id="cb88-195"><a href="#cb88-195"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb88-196"><a href="#cb88-196"></a></span>
<span id="cb88-197"><a href="#cb88-197"></a>    <span class="co"># Text Generation Method</span></span>
<span id="cb88-198"><a href="#cb88-198"></a>    <span class="co"># This method implements autoregressive text generation by:</span></span>
<span id="cb88-199"><a href="#cb88-199"></a>    <span class="co"># 1. Taking the current context (idx)</span></span>
<span id="cb88-200"><a href="#cb88-200"></a>    <span class="co"># 2. Predicting the next token probabilities</span></span>
<span id="cb88-201"><a href="#cb88-201"></a>    <span class="co"># 3. Sampling from these probabilities</span></span>
<span id="cb88-202"><a href="#cb88-202"></a>    <span class="co"># 4. Appending the sampled token to the context</span></span>
<span id="cb88-203"><a href="#cb88-203"></a>    <span class="co"># 5. Repeating for max_new_tokens iterations</span></span>
<span id="cb88-204"><a href="#cb88-204"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens):</span>
<span id="cb88-205"><a href="#cb88-205"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb88-206"><a href="#cb88-206"></a>            <span class="co"># get the predictions</span></span>
<span id="cb88-207"><a href="#cb88-207"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>(idx)</span>
<span id="cb88-208"><a href="#cb88-208"></a>            <span class="co"># focus only on the last time step</span></span>
<span id="cb88-209"><a href="#cb88-209"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="co"># becomes (B, C)</span></span>
<span id="cb88-210"><a href="#cb88-210"></a>            <span class="co"># apply softmax to get probabilities</span></span>
<span id="cb88-211"><a href="#cb88-211"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># (B, C)</span></span>
<span id="cb88-212"><a href="#cb88-212"></a>            <span class="co"># sample from the distribution</span></span>
<span id="cb88-213"><a href="#cb88-213"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, 1)</span></span>
<span id="cb88-214"><a href="#cb88-214"></a>            <span class="co"># append sampled index to the running sequence</span></span>
<span id="cb88-215"><a href="#cb88-215"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, T+1)</span></span>
<span id="cb88-216"><a href="#cb88-216"></a>        <span class="cf">return</span> idx</span>
<span id="cb88-217"><a href="#cb88-217"></a><span class="in">```</span></span>
<span id="cb88-218"><a href="#cb88-218"></a></span>
<span id="cb88-219"><a href="#cb88-219"></a><span class="fu">### Understanding Cross Entropy Loss</span></span>
<span id="cb88-220"><a href="#cb88-220"></a>Loss = -Σ(y * log(p))</span>
<span id="cb88-221"><a href="#cb88-221"></a>where:</span>
<span id="cb88-222"><a href="#cb88-222"></a><span class="ss">- </span>y = actual probability (0 or 1)</span>
<span id="cb88-223"><a href="#cb88-223"></a><span class="ss">- </span>p = predicted probability</span>
<span id="cb88-224"><a href="#cb88-224"></a><span class="ss">- </span>Σ = sum over all classes</span>
<span id="cb88-225"><a href="#cb88-225"></a></span>
<span id="cb88-226"><a href="#cb88-226"></a>This is the loss for a single token. The total loss is the average across all B*T tokens in the batch, where:</span>
<span id="cb88-227"><a href="#cb88-227"></a><span class="ss">- </span>B = batch size (number of sequences processed in parallel)</span>
<span id="cb88-228"><a href="#cb88-228"></a><span class="ss">- </span>T = sequence length (number of tokens in each sequence)</span>
<span id="cb88-229"><a href="#cb88-229"></a></span>
<span id="cb88-230"><a href="#cb88-230"></a>Before training, we would expect the model to predict the next character from a uniform distribution.</span>
<span id="cb88-231"><a href="#cb88-231"></a></span>
<span id="cb88-232"><a href="#cb88-232"></a>-Σ(y * log(p)) = </span>
<span id="cb88-233"><a href="#cb88-233"></a>= - ( 1 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) ) = - log(1/65) </span>
<span id="cb88-234"><a href="#cb88-234"></a>= 4.1744 per token.</span>
<span id="cb88-235"><a href="#cb88-235"></a></span>
<span id="cb88-236"><a href="#cb88-236"></a>i.e. at the beginning we expect loss of -log(1/65) = 4.1744 per token.</span>
<span id="cb88-237"><a href="#cb88-237"></a></span>
<span id="cb88-238"><a href="#cb88-238"></a><span class="fu">### Model Initialization and Training</span></span>
<span id="cb88-241"><a href="#cb88-241"></a><span class="in">```{python}</span></span>
<span id="cb88-242"><a href="#cb88-242"></a>m <span class="op">=</span> BigramLanguageModel(vocab_size)</span>
<span id="cb88-243"><a href="#cb88-243"></a>logits, loss <span class="op">=</span> m(xb, yb)</span>
<span id="cb88-244"><a href="#cb88-244"></a><span class="bu">print</span>(logits.shape)</span>
<span id="cb88-245"><a href="#cb88-245"></a><span class="bu">print</span>(loss)</span>
<span id="cb88-246"><a href="#cb88-246"></a><span class="in">```</span></span>
<span id="cb88-247"><a href="#cb88-247"></a></span>
<span id="cb88-248"><a href="#cb88-248"></a><span class="fu">### Text Generation</span></span>
<span id="cb88-251"><a href="#cb88-251"></a><span class="in">```{python}</span></span>
<span id="cb88-252"><a href="#cb88-252"></a><span class="bu">print</span>(decode(m.generate(idx <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>), max_new_tokens<span class="op">=</span><span class="dv">100</span>)[<span class="dv">0</span>].tolist()))</span>
<span id="cb88-253"><a href="#cb88-253"></a><span class="in">```</span></span>
<span id="cb88-254"><a href="#cb88-254"></a></span>
<span id="cb88-255"><a href="#cb88-255"></a><span class="fu">### Training Setup</span></span>
<span id="cb88-258"><a href="#cb88-258"></a><span class="in">```{python}</span></span>
<span id="cb88-259"><a href="#cb88-259"></a>optimizer <span class="op">=</span> torch.optim.AdamW(m.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb88-260"><a href="#cb88-260"></a><span class="in">```</span></span>
<span id="cb88-261"><a href="#cb88-261"></a></span>
<span id="cb88-262"><a href="#cb88-262"></a><span class="fu">### Training Loop</span></span>
<span id="cb88-265"><a href="#cb88-265"></a><span class="in">```{python}</span></span>
<span id="cb88-266"><a href="#cb88-266"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb88-267"><a href="#cb88-267"></a><span class="cf">for</span> steps <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>): <span class="co"># increase number of steps for good results...</span></span>
<span id="cb88-268"><a href="#cb88-268"></a></span>
<span id="cb88-269"><a href="#cb88-269"></a>    <span class="co"># sample a batch of data</span></span>
<span id="cb88-270"><a href="#cb88-270"></a>    xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb88-271"><a href="#cb88-271"></a></span>
<span id="cb88-272"><a href="#cb88-272"></a>    <span class="co"># evaluate the loss</span></span>
<span id="cb88-273"><a href="#cb88-273"></a>    logits, loss <span class="op">=</span> m(xb, yb)</span>
<span id="cb88-274"><a href="#cb88-274"></a>    optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb88-275"><a href="#cb88-275"></a>    loss.backward()</span>
<span id="cb88-276"><a href="#cb88-276"></a>    optimizer.step()</span>
<span id="cb88-277"><a href="#cb88-277"></a><span class="in">```</span></span>
<span id="cb88-278"><a href="#cb88-278"></a></span>
<span id="cb88-281"><a href="#cb88-281"></a><span class="in">```{python}</span></span>
<span id="cb88-282"><a href="#cb88-282"></a><span class="co"># %pip install torch torchvision torchaudio scikit-learn</span></span>
<span id="cb88-283"><a href="#cb88-283"></a></span>
<span id="cb88-284"><a href="#cb88-284"></a><span class="co"># import os</span></span>
<span id="cb88-285"><a href="#cb88-285"></a><span class="co"># import time</span></span>
<span id="cb88-286"><a href="#cb88-286"></a><span class="co"># import math</span></span>
<span id="cb88-287"><a href="#cb88-287"></a><span class="co"># import pickle</span></span>
<span id="cb88-288"><a href="#cb88-288"></a><span class="co"># from contextlib import nullcontext</span></span>
<span id="cb88-289"><a href="#cb88-289"></a></span>
<span id="cb88-290"><a href="#cb88-290"></a><span class="co"># import numpy as np</span></span>
<span id="cb88-291"><a href="#cb88-291"></a><span class="co"># import torch</span></span>
<span id="cb88-292"><a href="#cb88-292"></a><span class="co"># from torch.nn.parallel import DistributedDataParallel as DDP</span></span>
<span id="cb88-293"><a href="#cb88-293"></a><span class="co"># from torch.distributed import init_process_group, destroy_process_group</span></span>
<span id="cb88-294"><a href="#cb88-294"></a></span>
<span id="cb88-295"><a href="#cb88-295"></a><span class="co"># from model import GPTConfig, GPT</span></span>
<span id="cb88-296"><a href="#cb88-296"></a><span class="in">```</span></span>
<span id="cb88-297"><a href="#cb88-297"></a></span>
<span id="cb88-300"><a href="#cb88-300"></a><span class="in">```{python}</span></span>
<span id="cb88-301"><a href="#cb88-301"></a><span class="co"># let's now encode the entire text dataset and store it into a torch.Tensor</span></span>
<span id="cb88-302"><a href="#cb88-302"></a><span class="im">import</span> torch <span class="co"># we use PyTorch: https://pytorch.org</span></span>
<span id="cb88-303"><a href="#cb88-303"></a>data <span class="op">=</span> torch.tensor(encode(text), dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb88-304"><a href="#cb88-304"></a><span class="bu">print</span>(data.shape, data.dtype)</span>
<span id="cb88-305"><a href="#cb88-305"></a><span class="bu">print</span>(data[:<span class="dv">1000</span>]) <span class="co"># the 1000 characters we looked at earier will to the GPT look like this</span></span>
<span id="cb88-306"><a href="#cb88-306"></a><span class="in">```</span></span>
<span id="cb88-307"><a href="#cb88-307"></a></span>
<span id="cb88-310"><a href="#cb88-310"></a><span class="in">```{python}</span></span>
<span id="cb88-311"><a href="#cb88-311"></a><span class="co"># split up the data into train and validation sets</span></span>
<span id="cb88-312"><a href="#cb88-312"></a>n <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span><span class="op">*</span><span class="bu">len</span>(data)) <span class="co"># first 90% will be train, rest val</span></span>
<span id="cb88-313"><a href="#cb88-313"></a>train_data <span class="op">=</span> data[:n]</span>
<span id="cb88-314"><a href="#cb88-314"></a>val_data <span class="op">=</span> data[n:]</span>
<span id="cb88-315"><a href="#cb88-315"></a><span class="in">```</span></span>
<span id="cb88-316"><a href="#cb88-316"></a></span>
<span id="cb88-319"><a href="#cb88-319"></a><span class="in">```{python}</span></span>
<span id="cb88-320"><a href="#cb88-320"></a><span class="co"># define the block size</span></span>
<span id="cb88-321"><a href="#cb88-321"></a>block_size <span class="op">=</span> <span class="dv">8</span></span>
<span id="cb88-322"><a href="#cb88-322"></a>train_data[:block_size<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb88-323"><a href="#cb88-323"></a><span class="in">```</span></span>
<span id="cb88-324"><a href="#cb88-324"></a></span>
<span id="cb88-325"><a href="#cb88-325"></a><span class="fu">### define the context and target: 8 examples in one batch</span></span>
<span id="cb88-328"><a href="#cb88-328"></a><span class="in">```{python}</span></span>
<span id="cb88-329"><a href="#cb88-329"></a>x <span class="op">=</span> train_data[:block_size]</span>
<span id="cb88-330"><a href="#cb88-330"></a>y <span class="op">=</span> train_data[<span class="dv">1</span>:block_size<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb88-331"><a href="#cb88-331"></a><span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(block_size):</span>
<span id="cb88-332"><a href="#cb88-332"></a>    context <span class="op">=</span> x[:t<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb88-333"><a href="#cb88-333"></a>    target <span class="op">=</span> y[t]</span>
<span id="cb88-334"><a href="#cb88-334"></a>    <span class="bu">print</span>(<span class="ss">f"when input is </span><span class="sc">{</span>context<span class="sc">}</span><span class="ss"> the target: </span><span class="sc">{</span>target<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb88-335"><a href="#cb88-335"></a><span class="in">```</span></span>
<span id="cb88-336"><a href="#cb88-336"></a></span>
<span id="cb88-337"><a href="#cb88-337"></a><span class="fu">### define the batch size and get the batch</span></span>
<span id="cb88-340"><a href="#cb88-340"></a><span class="in">```{python}</span></span>
<span id="cb88-341"><a href="#cb88-341"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb88-342"><a href="#cb88-342"></a>batch_size <span class="op">=</span> <span class="dv">4</span> <span class="co"># how many independent sequences will we process in parallel?</span></span>
<span id="cb88-343"><a href="#cb88-343"></a>block_size <span class="op">=</span> <span class="dv">8</span> <span class="co"># what is the maximum context length for predictions?</span></span>
<span id="cb88-344"><a href="#cb88-344"></a></span>
<span id="cb88-345"><a href="#cb88-345"></a><span class="kw">def</span> get_batch(split):</span>
<span id="cb88-346"><a href="#cb88-346"></a>    <span class="co"># generate a small batch of data of inputs x and targets y</span></span>
<span id="cb88-347"><a href="#cb88-347"></a>    data <span class="op">=</span> train_data <span class="cf">if</span> split <span class="op">==</span> <span class="st">'train'</span> <span class="cf">else</span> val_data</span>
<span id="cb88-348"><a href="#cb88-348"></a>    ix <span class="op">=</span> torch.randint(<span class="bu">len</span>(data) <span class="op">-</span> block_size, (batch_size,))</span>
<span id="cb88-349"><a href="#cb88-349"></a>    x <span class="op">=</span> torch.stack([data[i:i<span class="op">+</span>block_size] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb88-350"><a href="#cb88-350"></a>    y <span class="op">=</span> torch.stack([data[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span>block_size<span class="op">+</span><span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb88-351"><a href="#cb88-351"></a>    <span class="cf">return</span> x, y</span>
<span id="cb88-352"><a href="#cb88-352"></a></span>
<span id="cb88-353"><a href="#cb88-353"></a>xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb88-354"><a href="#cb88-354"></a><span class="bu">print</span>(<span class="st">'inputs:'</span>)</span>
<span id="cb88-355"><a href="#cb88-355"></a><span class="bu">print</span>(xb.shape)</span>
<span id="cb88-356"><a href="#cb88-356"></a><span class="bu">print</span>(xb)</span>
<span id="cb88-357"><a href="#cb88-357"></a><span class="bu">print</span>(<span class="st">'targets:'</span>)</span>
<span id="cb88-358"><a href="#cb88-358"></a><span class="bu">print</span>(yb.shape)</span>
<span id="cb88-359"><a href="#cb88-359"></a><span class="bu">print</span>(yb)</span>
<span id="cb88-360"><a href="#cb88-360"></a></span>
<span id="cb88-361"><a href="#cb88-361"></a><span class="bu">print</span>(<span class="st">'----'</span>)</span>
<span id="cb88-362"><a href="#cb88-362"></a></span>
<span id="cb88-363"><a href="#cb88-363"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(batch_size): <span class="co"># batch dimension</span></span>
<span id="cb88-364"><a href="#cb88-364"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(block_size): <span class="co"># time dimension</span></span>
<span id="cb88-365"><a href="#cb88-365"></a>        context <span class="op">=</span> xb[b, :t<span class="op">+</span><span class="dv">1</span>]</span>
<span id="cb88-366"><a href="#cb88-366"></a>        target <span class="op">=</span> yb[b,t]</span>
<span id="cb88-367"><a href="#cb88-367"></a>        <span class="bu">print</span>(<span class="ss">f"when input is </span><span class="sc">{</span>context<span class="sc">.</span>tolist()<span class="sc">}</span><span class="ss"> the target: </span><span class="sc">{</span>target<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb88-368"><a href="#cb88-368"></a><span class="in">```</span></span>
<span id="cb88-369"><a href="#cb88-369"></a></span>
<span id="cb88-370"><a href="#cb88-370"></a><span class="fu">### start with a simple model: the bigram language model</span></span>
<span id="cb88-371"><a href="#cb88-371"></a>This model predicts the next character based on the current character only. </span>
<span id="cb88-372"><a href="#cb88-372"></a>It uses the logits for the next character in a lookup table.</span>
<span id="cb88-375"><a href="#cb88-375"></a><span class="in">```{python}</span></span>
<span id="cb88-376"><a href="#cb88-376"></a><span class="co"># define the bigram language model</span></span>
<span id="cb88-377"><a href="#cb88-377"></a><span class="im">import</span> torch</span>
<span id="cb88-378"><a href="#cb88-378"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb88-379"><a href="#cb88-379"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb88-380"><a href="#cb88-380"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb88-381"><a href="#cb88-381"></a></span>
<span id="cb88-382"><a href="#cb88-382"></a><span class="kw">class</span> BigramLanguageModel(nn.Module):</span>
<span id="cb88-383"><a href="#cb88-383"></a></span>
<span id="cb88-384"><a href="#cb88-384"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size):</span>
<span id="cb88-385"><a href="#cb88-385"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb88-386"><a href="#cb88-386"></a>        <span class="co"># each token directly reads off the logits for the next token from a lookup table</span></span>
<span id="cb88-387"><a href="#cb88-387"></a>        <span class="va">self</span>.token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, vocab_size)</span>
<span id="cb88-388"><a href="#cb88-388"></a></span>
<span id="cb88-389"><a href="#cb88-389"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb88-390"><a href="#cb88-390"></a></span>
<span id="cb88-391"><a href="#cb88-391"></a>        <span class="co"># idx and targets are both (B,T) tensor of integers</span></span>
<span id="cb88-392"><a href="#cb88-392"></a>        logits <span class="op">=</span> <span class="va">self</span>.token_embedding_table(idx) <span class="co"># (B,T,C)</span></span>
<span id="cb88-393"><a href="#cb88-393"></a></span>
<span id="cb88-394"><a href="#cb88-394"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb88-395"><a href="#cb88-395"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb88-396"><a href="#cb88-396"></a>        <span class="cf">else</span>:</span>
<span id="cb88-397"><a href="#cb88-397"></a>            B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb88-398"><a href="#cb88-398"></a>            logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C)</span>
<span id="cb88-399"><a href="#cb88-399"></a>            targets <span class="op">=</span> targets.view(B<span class="op">*</span>T)</span>
<span id="cb88-400"><a href="#cb88-400"></a>            loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb88-401"><a href="#cb88-401"></a></span>
<span id="cb88-402"><a href="#cb88-402"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb88-403"><a href="#cb88-403"></a></span>
<span id="cb88-404"><a href="#cb88-404"></a>    <span class="co"># Text Generation Method</span></span>
<span id="cb88-405"><a href="#cb88-405"></a>    <span class="co"># This method implements autoregressive text generation by:</span></span>
<span id="cb88-406"><a href="#cb88-406"></a>    <span class="co"># 1. Taking the current context (idx)</span></span>
<span id="cb88-407"><a href="#cb88-407"></a>    <span class="co"># 2. Predicting the next token probabilities</span></span>
<span id="cb88-408"><a href="#cb88-408"></a>    <span class="co"># 3. Sampling from these probabilities</span></span>
<span id="cb88-409"><a href="#cb88-409"></a>    <span class="co"># 4. Appending the sampled token to the context</span></span>
<span id="cb88-410"><a href="#cb88-410"></a>    <span class="co"># 5. Repeating for max_new_tokens iterations</span></span>
<span id="cb88-411"><a href="#cb88-411"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens):</span>
<span id="cb88-412"><a href="#cb88-412"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb88-413"><a href="#cb88-413"></a>            <span class="co"># get the predictions</span></span>
<span id="cb88-414"><a href="#cb88-414"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>(idx)</span>
<span id="cb88-415"><a href="#cb88-415"></a>            <span class="co"># focus only on the last time step</span></span>
<span id="cb88-416"><a href="#cb88-416"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="co"># becomes (B, C)</span></span>
<span id="cb88-417"><a href="#cb88-417"></a>            <span class="co"># apply softmax to get probabilities</span></span>
<span id="cb88-418"><a href="#cb88-418"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># (B, C)</span></span>
<span id="cb88-419"><a href="#cb88-419"></a>            <span class="co"># sample from the distribution</span></span>
<span id="cb88-420"><a href="#cb88-420"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, 1)</span></span>
<span id="cb88-421"><a href="#cb88-421"></a>            <span class="co"># append sampled index to the running sequence</span></span>
<span id="cb88-422"><a href="#cb88-422"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>) <span class="co"># (B, T+1)</span></span>
<span id="cb88-423"><a href="#cb88-423"></a>        <span class="cf">return</span> idx</span>
<span id="cb88-424"><a href="#cb88-424"></a><span class="in">```</span></span>
<span id="cb88-425"><a href="#cb88-425"></a></span>
<span id="cb88-426"><a href="#cb88-426"></a><span class="fu">### cross entropy loss</span></span>
<span id="cb88-427"><a href="#cb88-427"></a>Loss = -Σ(y * log(p))</span>
<span id="cb88-428"><a href="#cb88-428"></a>where:</span>
<span id="cb88-429"><a href="#cb88-429"></a><span class="ss">- </span>y = actual probability (0 or 1)</span>
<span id="cb88-430"><a href="#cb88-430"></a><span class="ss">- </span>p = predicted probability</span>
<span id="cb88-431"><a href="#cb88-431"></a><span class="ss">- </span>Σ = sum over all classes</span>
<span id="cb88-432"><a href="#cb88-432"></a></span>
<span id="cb88-433"><a href="#cb88-433"></a>This is the loss for a single token. The total loss is the average across all B*T tokens in the batch, where:</span>
<span id="cb88-434"><a href="#cb88-434"></a><span class="ss">- </span>B = batch size (number of sequences processed in parallel)</span>
<span id="cb88-435"><a href="#cb88-435"></a><span class="ss">- </span>T = sequence length (number of tokens in each sequence)</span>
<span id="cb88-436"><a href="#cb88-436"></a></span>
<span id="cb88-437"><a href="#cb88-437"></a>Before training, we would expect the model to predict the next character from a uniform distribution.</span>
<span id="cb88-438"><a href="#cb88-438"></a></span>
<span id="cb88-439"><a href="#cb88-439"></a>-Σ(y * log(p)) = </span>
<span id="cb88-440"><a href="#cb88-440"></a>= - ( 1 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) ) = - log(1/65) </span>
<span id="cb88-441"><a href="#cb88-441"></a>= 4.1744 per token.</span>
<span id="cb88-442"><a href="#cb88-442"></a></span>
<span id="cb88-443"><a href="#cb88-443"></a>i.e. at the beginning we expect loss of -log(1/65) = 4.1744 per token.</span>
<span id="cb88-444"><a href="#cb88-444"></a></span>
<span id="cb88-445"><a href="#cb88-445"></a></span>
<span id="cb88-446"><a href="#cb88-446"></a><span class="fu">### initialize the model and compute the loss</span></span>
<span id="cb88-449"><a href="#cb88-449"></a><span class="in">```{python}</span></span>
<span id="cb88-450"><a href="#cb88-450"></a>m <span class="op">=</span> BigramLanguageModel(vocab_size)</span>
<span id="cb88-451"><a href="#cb88-451"></a>logits, loss <span class="op">=</span> m(xb, yb)</span>
<span id="cb88-452"><a href="#cb88-452"></a><span class="bu">print</span>(logits.shape)</span>
<span id="cb88-453"><a href="#cb88-453"></a><span class="bu">print</span>(loss)</span>
<span id="cb88-454"><a href="#cb88-454"></a><span class="in">```</span></span>
<span id="cb88-455"><a href="#cb88-455"></a></span>
<span id="cb88-456"><a href="#cb88-456"></a><span class="fu">### generate text</span></span>
<span id="cb88-459"><a href="#cb88-459"></a><span class="in">```{python}</span></span>
<span id="cb88-460"><a href="#cb88-460"></a><span class="bu">print</span>(decode(m.generate(idx <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>), max_new_tokens<span class="op">=</span><span class="dv">100</span>)[<span class="dv">0</span>].tolist()))</span>
<span id="cb88-461"><a href="#cb88-461"></a><span class="in">```</span></span>
<span id="cb88-462"><a href="#cb88-462"></a></span>
<span id="cb88-463"><a href="#cb88-463"></a><span class="fu">### choose AdamW as the optimizer</span></span>
<span id="cb88-466"><a href="#cb88-466"></a><span class="in">```{python}</span></span>
<span id="cb88-467"><a href="#cb88-467"></a>optimizer <span class="op">=</span> torch.optim.AdamW(m.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>)</span>
<span id="cb88-468"><a href="#cb88-468"></a><span class="in">```</span></span>
<span id="cb88-469"><a href="#cb88-469"></a></span>
<span id="cb88-470"><a href="#cb88-470"></a><span class="fu">### train the model</span></span>
<span id="cb88-473"><a href="#cb88-473"></a><span class="in">```{python}</span></span>
<span id="cb88-474"><a href="#cb88-474"></a>batch_size <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb88-475"><a href="#cb88-475"></a><span class="cf">for</span> steps <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>): <span class="co"># increase number of steps for good results...</span></span>
<span id="cb88-476"><a href="#cb88-476"></a></span>
<span id="cb88-477"><a href="#cb88-477"></a>    <span class="co"># sample a batch of data</span></span>
<span id="cb88-478"><a href="#cb88-478"></a>    xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb88-479"><a href="#cb88-479"></a></span>
<span id="cb88-480"><a href="#cb88-480"></a>    <span class="co"># evaluate the loss</span></span>
<span id="cb88-481"><a href="#cb88-481"></a>    logits, loss <span class="op">=</span> m(xb, yb)</span>
<span id="cb88-482"><a href="#cb88-482"></a>    optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb88-483"><a href="#cb88-483"></a>    loss.backward()</span>
<span id="cb88-484"><a href="#cb88-484"></a>    optimizer.step()</span>
<span id="cb88-485"><a href="#cb88-485"></a></span>
<span id="cb88-486"><a href="#cb88-486"></a><span class="bu">print</span>(loss.item())</span>
<span id="cb88-487"><a href="#cb88-487"></a><span class="in">```</span></span>
<span id="cb88-488"><a href="#cb88-488"></a></span>
<span id="cb88-489"><a href="#cb88-489"></a><span class="fu">### generate text starting with \n as initial context</span></span>
<span id="cb88-492"><a href="#cb88-492"></a><span class="in">```{python}</span></span>
<span id="cb88-493"><a href="#cb88-493"></a><span class="bu">print</span>(decode(m.generate(idx <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>), max_new_tokens<span class="op">=</span><span class="dv">500</span>)[<span class="dv">0</span>].tolist()))</span>
<span id="cb88-494"><a href="#cb88-494"></a><span class="in">```</span></span>
<span id="cb88-495"><a href="#cb88-495"></a></span>
<span id="cb88-496"><a href="#cb88-496"></a><span class="fu">## Self-Attention: The Key Innovation</span></span>
<span id="cb88-497"><a href="#cb88-497"></a></span>
<span id="cb88-498"><a href="#cb88-498"></a><span class="fu">### Understanding Self-Attention</span></span>
<span id="cb88-499"><a href="#cb88-499"></a>Self-attention allows tokens to communicate across the sequence in a data-dependent way. Each token can "look" at previous tokens and decide which ones are most relevant for predicting the next token.</span>
<span id="cb88-500"><a href="#cb88-500"></a></span>
<span id="cb88-501"><a href="#cb88-501"></a><span class="fu">### The Mathematical Trick: Weighted Aggregation</span></span>
<span id="cb88-504"><a href="#cb88-504"></a><span class="in">```{python}</span></span>
<span id="cb88-505"><a href="#cb88-505"></a>torch.manual_seed(<span class="dv">42</span>)</span>
<span id="cb88-506"><a href="#cb88-506"></a>a <span class="op">=</span> torch.tril(torch.ones(<span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb88-507"><a href="#cb88-507"></a>a <span class="op">=</span> a <span class="op">/</span> torch.<span class="bu">sum</span>(a, <span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb88-508"><a href="#cb88-508"></a>b <span class="op">=</span> torch.randint(<span class="dv">0</span>,<span class="dv">10</span>,(<span class="dv">3</span>,<span class="dv">2</span>)).<span class="bu">float</span>()</span>
<span id="cb88-509"><a href="#cb88-509"></a>c <span class="op">=</span> a <span class="op">@</span> b</span>
<span id="cb88-510"><a href="#cb88-510"></a><span class="bu">print</span>(<span class="st">'a='</span>)</span>
<span id="cb88-511"><a href="#cb88-511"></a><span class="bu">print</span>(a)</span>
<span id="cb88-512"><a href="#cb88-512"></a><span class="bu">print</span>(<span class="st">'--'</span>)</span>
<span id="cb88-513"><a href="#cb88-513"></a><span class="bu">print</span>(<span class="st">'b='</span>)</span>
<span id="cb88-514"><a href="#cb88-514"></a><span class="bu">print</span>(b)</span>
<span id="cb88-515"><a href="#cb88-515"></a><span class="bu">print</span>(<span class="st">'--'</span>)</span>
<span id="cb88-516"><a href="#cb88-516"></a><span class="bu">print</span>(<span class="st">'c='</span>)</span>
<span id="cb88-517"><a href="#cb88-517"></a><span class="bu">print</span>(c)</span>
<span id="cb88-518"><a href="#cb88-518"></a><span class="in">```</span></span>
<span id="cb88-519"><a href="#cb88-519"></a></span>
<span id="cb88-520"><a href="#cb88-520"></a><span class="fu">### Version 1: Using a For Loop</span></span>
<span id="cb88-523"><a href="#cb88-523"></a><span class="in">```{python}</span></span>
<span id="cb88-524"><a href="#cb88-524"></a><span class="co"># consider the following toy example:</span></span>
<span id="cb88-525"><a href="#cb88-525"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb88-526"><a href="#cb88-526"></a>B,T,C <span class="op">=</span> <span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">2</span> <span class="co"># batch, time, channels</span></span>
<span id="cb88-527"><a href="#cb88-527"></a>x <span class="op">=</span> torch.randn(B,T,C)</span>
<span id="cb88-528"><a href="#cb88-528"></a>x.shape</span>
<span id="cb88-529"><a href="#cb88-529"></a><span class="in">```</span></span>
<span id="cb88-530"><a href="#cb88-530"></a></span>
<span id="cb88-533"><a href="#cb88-533"></a><span class="in">```{python}</span></span>
<span id="cb88-534"><a href="#cb88-534"></a><span class="co"># We want x[b,t] = mean_{i&lt;=t} x[b,i]</span></span>
<span id="cb88-535"><a href="#cb88-535"></a>xbow <span class="op">=</span> torch.zeros((B,T,C))</span>
<span id="cb88-536"><a href="#cb88-536"></a><span class="cf">for</span> b <span class="kw">in</span> <span class="bu">range</span>(B):</span>
<span id="cb88-537"><a href="#cb88-537"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(T):</span>
<span id="cb88-538"><a href="#cb88-538"></a>        xprev <span class="op">=</span> x[b,:t<span class="op">+</span><span class="dv">1</span>] <span class="co"># (t,C)</span></span>
<span id="cb88-539"><a href="#cb88-539"></a>        xbow[b,t] <span class="op">=</span> torch.mean(xprev, <span class="dv">0</span>)</span>
<span id="cb88-540"><a href="#cb88-540"></a><span class="in">```</span></span>
<span id="cb88-541"><a href="#cb88-541"></a></span>
<span id="cb88-542"><a href="#cb88-542"></a><span class="fu">### Version 2: Matrix Multiplication</span></span>
<span id="cb88-545"><a href="#cb88-545"></a><span class="in">```{python}</span></span>
<span id="cb88-546"><a href="#cb88-546"></a>wei <span class="op">=</span> torch.tril(torch.ones(T, T))</span>
<span id="cb88-547"><a href="#cb88-547"></a>wei <span class="op">=</span> wei <span class="op">/</span> wei.<span class="bu">sum</span>(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb88-548"><a href="#cb88-548"></a>xbow2 <span class="op">=</span> wei <span class="op">@</span> x <span class="co"># (B, T, T) @ (B, T, C) ----&gt; (B, T, C)</span></span>
<span id="cb88-549"><a href="#cb88-549"></a>torch.allclose(xbow, xbow2)</span>
<span id="cb88-550"><a href="#cb88-550"></a><span class="in">```</span></span>
<span id="cb88-551"><a href="#cb88-551"></a></span>
<span id="cb88-552"><a href="#cb88-552"></a><span class="fu">### Version 3: Using Softmax</span></span>
<span id="cb88-555"><a href="#cb88-555"></a><span class="in">```{python}</span></span>
<span id="cb88-556"><a href="#cb88-556"></a>tril <span class="op">=</span> torch.tril(torch.ones(T, T))</span>
<span id="cb88-557"><a href="#cb88-557"></a>wei <span class="op">=</span> torch.zeros((T,T))</span>
<span id="cb88-558"><a href="#cb88-558"></a>wei <span class="op">=</span> wei.masked_fill(tril <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb88-559"><a href="#cb88-559"></a>wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb88-560"><a href="#cb88-560"></a>xbow3 <span class="op">=</span> wei <span class="op">@</span> x</span>
<span id="cb88-561"><a href="#cb88-561"></a>torch.allclose(xbow, xbow3)</span>
<span id="cb88-562"><a href="#cb88-562"></a><span class="in">```</span></span>
<span id="cb88-563"><a href="#cb88-563"></a></span>
<span id="cb88-564"><a href="#cb88-564"></a><span class="fu">### Version 4: Self-Attention</span></span>
<span id="cb88-567"><a href="#cb88-567"></a><span class="in">```{python}</span></span>
<span id="cb88-568"><a href="#cb88-568"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb88-569"><a href="#cb88-569"></a>B,T,C <span class="op">=</span> <span class="dv">4</span>,<span class="dv">8</span>,<span class="dv">32</span> <span class="co"># batch, time, channels</span></span>
<span id="cb88-570"><a href="#cb88-570"></a>x <span class="op">=</span> torch.randn(B,T,C)</span>
<span id="cb88-571"><a href="#cb88-571"></a></span>
<span id="cb88-572"><a href="#cb88-572"></a><span class="co"># let's see a single Head perform self-attention</span></span>
<span id="cb88-573"><a href="#cb88-573"></a>head_size <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb88-574"><a href="#cb88-574"></a>key <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb88-575"><a href="#cb88-575"></a>query <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb88-576"><a href="#cb88-576"></a>value <span class="op">=</span> nn.Linear(C, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb88-577"><a href="#cb88-577"></a>k <span class="op">=</span> key(x)   <span class="co"># (B, T, 16)</span></span>
<span id="cb88-578"><a href="#cb88-578"></a>q <span class="op">=</span> query(x) <span class="co"># (B, T, 16)</span></span>
<span id="cb88-579"><a href="#cb88-579"></a>wei <span class="op">=</span>  q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="co"># (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)</span></span>
<span id="cb88-580"><a href="#cb88-580"></a></span>
<span id="cb88-581"><a href="#cb88-581"></a>tril <span class="op">=</span> torch.tril(torch.ones(T, T))</span>
<span id="cb88-582"><a href="#cb88-582"></a><span class="co">#wei = torch.zeros((T,T))</span></span>
<span id="cb88-583"><a href="#cb88-583"></a>wei <span class="op">=</span> wei.masked_fill(tril <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb88-584"><a href="#cb88-584"></a>wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb88-585"><a href="#cb88-585"></a></span>
<span id="cb88-586"><a href="#cb88-586"></a>v <span class="op">=</span> value(x)</span>
<span id="cb88-587"><a href="#cb88-587"></a>out <span class="op">=</span> wei <span class="op">@</span> v</span>
<span id="cb88-588"><a href="#cb88-588"></a><span class="co">#out = wei @ x</span></span>
<span id="cb88-589"><a href="#cb88-589"></a></span>
<span id="cb88-590"><a href="#cb88-590"></a>out.shape</span>
<span id="cb88-591"><a href="#cb88-591"></a><span class="in">```</span></span>
<span id="cb88-592"><a href="#cb88-592"></a></span>
<span id="cb88-593"><a href="#cb88-593"></a><span class="fu">### Why Scaled Attention?</span></span>
<span id="cb88-594"><a href="#cb88-594"></a><span class="ss">- </span>Dividing by sqrt(head_size) prevents scores from becoming too large</span>
<span id="cb88-595"><a href="#cb88-595"></a><span class="ss">- </span>Keeps softmax from producing overly sharp distributions</span>
<span id="cb88-596"><a href="#cb88-596"></a><span class="ss">- </span>Aids training stability</span>
<span id="cb88-597"><a href="#cb88-597"></a></span>
<span id="cb88-600"><a href="#cb88-600"></a><span class="in">```{python}</span></span>
<span id="cb88-601"><a href="#cb88-601"></a>k <span class="op">=</span> torch.randn(B,T,head_size)</span>
<span id="cb88-602"><a href="#cb88-602"></a>q <span class="op">=</span> torch.randn(B,T,head_size)</span>
<span id="cb88-603"><a href="#cb88-603"></a>wei <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> head_size<span class="op">**-</span><span class="fl">0.5</span></span>
<span id="cb88-604"><a href="#cb88-604"></a><span class="in">```</span></span>
<span id="cb88-605"><a href="#cb88-605"></a></span>
<span id="cb88-608"><a href="#cb88-608"></a><span class="in">```{python}</span></span>
<span id="cb88-609"><a href="#cb88-609"></a>k.var()</span>
<span id="cb88-610"><a href="#cb88-610"></a><span class="in">```</span></span>
<span id="cb88-611"><a href="#cb88-611"></a></span>
<span id="cb88-614"><a href="#cb88-614"></a><span class="in">```{python}</span></span>
<span id="cb88-615"><a href="#cb88-615"></a>q.var()</span>
<span id="cb88-616"><a href="#cb88-616"></a><span class="in">```</span></span>
<span id="cb88-617"><a href="#cb88-617"></a></span>
<span id="cb88-620"><a href="#cb88-620"></a><span class="in">```{python}</span></span>
<span id="cb88-621"><a href="#cb88-621"></a>wei.var()</span>
<span id="cb88-622"><a href="#cb88-622"></a><span class="in">```</span></span>
<span id="cb88-623"><a href="#cb88-623"></a></span>
<span id="cb88-626"><a href="#cb88-626"></a><span class="in">```{python}</span></span>
<span id="cb88-627"><a href="#cb88-627"></a>torch.softmax(torch.tensor([<span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.5</span>]), dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb88-628"><a href="#cb88-628"></a><span class="in">```</span></span>
<span id="cb88-629"><a href="#cb88-629"></a></span>
<span id="cb88-632"><a href="#cb88-632"></a><span class="in">```{python}</span></span>
<span id="cb88-633"><a href="#cb88-633"></a>torch.softmax(torch.tensor([<span class="fl">0.1</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.3</span>, <span class="op">-</span><span class="fl">0.2</span>, <span class="fl">0.5</span>])<span class="op">*</span><span class="dv">8</span>, dim<span class="op">=-</span><span class="dv">1</span>) <span class="co"># gets too peaky, converges to one-hot</span></span>
<span id="cb88-634"><a href="#cb88-634"></a><span class="in">```</span></span>
<span id="cb88-635"><a href="#cb88-635"></a></span>
<span id="cb88-636"><a href="#cb88-636"></a><span class="fu">## Layer Normalization</span></span>
<span id="cb88-637"><a href="#cb88-637"></a>Layer normalization normalizes features across the embedding dimension for each token independently, which helps stabilize training.</span>
<span id="cb88-638"><a href="#cb88-638"></a></span>
<span id="cb88-641"><a href="#cb88-641"></a><span class="in">```{python}</span></span>
<span id="cb88-642"><a href="#cb88-642"></a><span class="kw">class</span> LayerNorm1d: <span class="co"># (used to be BatchNorm1d)</span></span>
<span id="cb88-643"><a href="#cb88-643"></a></span>
<span id="cb88-644"><a href="#cb88-644"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, dim, eps<span class="op">=</span><span class="fl">1e-5</span>, momentum<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb88-645"><a href="#cb88-645"></a>    <span class="va">self</span>.eps <span class="op">=</span> eps</span>
<span id="cb88-646"><a href="#cb88-646"></a>    <span class="va">self</span>.gamma <span class="op">=</span> torch.ones(dim)</span>
<span id="cb88-647"><a href="#cb88-647"></a>    <span class="va">self</span>.beta <span class="op">=</span> torch.zeros(dim)</span>
<span id="cb88-648"><a href="#cb88-648"></a></span>
<span id="cb88-649"><a href="#cb88-649"></a>  <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, x):</span>
<span id="cb88-650"><a href="#cb88-650"></a>    <span class="co"># calculate the forward pass</span></span>
<span id="cb88-651"><a href="#cb88-651"></a>    xmean <span class="op">=</span> x.mean(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch mean</span></span>
<span id="cb88-652"><a href="#cb88-652"></a>    xvar <span class="op">=</span> x.var(<span class="dv">1</span>, keepdim<span class="op">=</span><span class="va">True</span>) <span class="co"># batch variance</span></span>
<span id="cb88-653"><a href="#cb88-653"></a>    xhat <span class="op">=</span> (x <span class="op">-</span> xmean) <span class="op">/</span> torch.sqrt(xvar <span class="op">+</span> <span class="va">self</span>.eps) <span class="co"># normalize to unit variance</span></span>
<span id="cb88-654"><a href="#cb88-654"></a>    <span class="va">self</span>.out <span class="op">=</span> <span class="va">self</span>.gamma <span class="op">*</span> xhat <span class="op">+</span> <span class="va">self</span>.beta</span>
<span id="cb88-655"><a href="#cb88-655"></a>    <span class="cf">return</span> <span class="va">self</span>.out</span>
<span id="cb88-656"><a href="#cb88-656"></a></span>
<span id="cb88-657"><a href="#cb88-657"></a>  <span class="kw">def</span> parameters(<span class="va">self</span>):</span>
<span id="cb88-658"><a href="#cb88-658"></a>    <span class="cf">return</span> [<span class="va">self</span>.gamma, <span class="va">self</span>.beta]</span>
<span id="cb88-659"><a href="#cb88-659"></a></span>
<span id="cb88-660"><a href="#cb88-660"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb88-661"><a href="#cb88-661"></a>module <span class="op">=</span> LayerNorm1d(<span class="dv">100</span>)</span>
<span id="cb88-662"><a href="#cb88-662"></a>x <span class="op">=</span> torch.randn(<span class="dv">32</span>, <span class="dv">100</span>) <span class="co"># batch size 32 of 100-dimensional vectors</span></span>
<span id="cb88-663"><a href="#cb88-663"></a>x <span class="op">=</span> module(x)</span>
<span id="cb88-664"><a href="#cb88-664"></a>x.shape</span>
<span id="cb88-665"><a href="#cb88-665"></a><span class="in">```</span></span>
<span id="cb88-666"><a href="#cb88-666"></a></span>
<span id="cb88-669"><a href="#cb88-669"></a><span class="in">```{python}</span></span>
<span id="cb88-670"><a href="#cb88-670"></a>x[:,<span class="dv">0</span>].mean(), x[:,<span class="dv">0</span>].std() <span class="co"># mean,std of one feature across all batch inputs</span></span>
<span id="cb88-671"><a href="#cb88-671"></a><span class="in">```</span></span>
<span id="cb88-672"><a href="#cb88-672"></a></span>
<span id="cb88-675"><a href="#cb88-675"></a><span class="in">```{python}</span></span>
<span id="cb88-676"><a href="#cb88-676"></a>x[<span class="dv">0</span>,:].mean(), x[<span class="dv">0</span>,:].std() <span class="co"># mean,std of a single input from the batch, of its features</span></span>
<span id="cb88-677"><a href="#cb88-677"></a><span class="in">```</span></span>
<span id="cb88-678"><a href="#cb88-678"></a></span>
<span id="cb88-681"><a href="#cb88-681"></a><span class="in">```{python}</span></span>
<span id="cb88-682"><a href="#cb88-682"></a><span class="co"># French to English translation example:</span></span>
<span id="cb88-683"><a href="#cb88-683"></a></span>
<span id="cb88-684"><a href="#cb88-684"></a><span class="co"># &lt;--------- ENCODE ------------------&gt;&lt;--------------- DECODE -----------------&gt;</span></span>
<span id="cb88-685"><a href="#cb88-685"></a><span class="co"># les réseaux de neurones sont géniaux! &lt;START&gt; neural networks are awesome!&lt;</span><span class="re">END</span><span class="co">&gt;</span></span>
<span id="cb88-686"><a href="#cb88-686"></a></span>
<span id="cb88-687"><a href="#cb88-687"></a><span class="in">```</span></span>
<span id="cb88-688"><a href="#cb88-688"></a></span>
<span id="cb88-689"><a href="#cb88-689"></a><span class="fu">### Full finished code, for reference</span></span>
<span id="cb88-690"><a href="#cb88-690"></a></span>
<span id="cb88-693"><a href="#cb88-693"></a><span class="in">```{python}</span></span>
<span id="cb88-694"><a href="#cb88-694"></a><span class="co"># Import necessary PyTorch modules</span></span>
<span id="cb88-695"><a href="#cb88-695"></a><span class="im">import</span> torch</span>
<span id="cb88-696"><a href="#cb88-696"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb88-697"><a href="#cb88-697"></a><span class="im">from</span> torch.nn <span class="im">import</span> functional <span class="im">as</span> F</span>
<span id="cb88-698"><a href="#cb88-698"></a></span>
<span id="cb88-699"><a href="#cb88-699"></a><span class="co"># ===== HYPERPARAMETERS =====</span></span>
<span id="cb88-700"><a href="#cb88-700"></a><span class="co"># These are the key parameters that control the model's behavior and training</span></span>
<span id="cb88-701"><a href="#cb88-701"></a>batch_size <span class="op">=</span> <span class="dv">16</span>        <span class="co"># Number of independent sequences processed in parallel</span></span>
<span id="cb88-702"><a href="#cb88-702"></a>block_size <span class="op">=</span> <span class="dv">32</span>        <span class="co"># Maximum context length for predictions (like a window size)</span></span>
<span id="cb88-703"><a href="#cb88-703"></a>max_iters <span class="op">=</span> <span class="dv">5000</span>       <span class="co"># Total number of training iterations</span></span>
<span id="cb88-704"><a href="#cb88-704"></a>eval_interval <span class="op">=</span> <span class="dv">100</span>    <span class="co"># How often to evaluate the model during training</span></span>
<span id="cb88-705"><a href="#cb88-705"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span>   <span class="co"># Step size for gradient descent</span></span>
<span id="cb88-706"><a href="#cb88-706"></a>eval_iters <span class="op">=</span> <span class="dv">200</span>       <span class="co"># Number of iterations to average over when evaluating</span></span>
<span id="cb88-707"><a href="#cb88-707"></a>n_embd <span class="op">=</span> <span class="dv">64</span>           <span class="co"># Size of the embedding dimension (d_model in original paper)</span></span>
<span id="cb88-708"><a href="#cb88-708"></a>n_head <span class="op">=</span> <span class="dv">4</span>            <span class="co"># Number of attention heads</span></span>
<span id="cb88-709"><a href="#cb88-709"></a>n_layer <span class="op">=</span> <span class="dv">4</span>           <span class="co"># Number of transformer layers</span></span>
<span id="cb88-710"><a href="#cb88-710"></a>dropout <span class="op">=</span> <span class="fl">0.0</span>         <span class="co"># Probability of dropping out neurons (0 = no dropout)</span></span>
<span id="cb88-711"><a href="#cb88-711"></a><span class="co"># ==========================</span></span>
<span id="cb88-712"><a href="#cb88-712"></a></span>
<span id="cb88-713"><a href="#cb88-713"></a><span class="co"># Device selection: MPS (Apple Silicon) &gt; CUDA &gt; CPU</span></span>
<span id="cb88-714"><a href="#cb88-714"></a><span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb88-715"><a href="#cb88-715"></a>    device <span class="op">=</span> torch.device(<span class="st">"mps"</span>)  <span class="co"># Apple Silicon GPU</span></span>
<span id="cb88-716"><a href="#cb88-716"></a><span class="cf">elif</span> torch.cuda.is_available():</span>
<span id="cb88-717"><a href="#cb88-717"></a>    device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)  <span class="co"># NVIDIA GPU</span></span>
<span id="cb88-718"><a href="#cb88-718"></a><span class="cf">else</span>:</span>
<span id="cb88-719"><a href="#cb88-719"></a>    device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)   <span class="co"># CPU fallback</span></span>
<span id="cb88-720"><a href="#cb88-720"></a><span class="bu">print</span>(<span class="ss">f"Using device: </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb88-721"><a href="#cb88-721"></a></span>
<span id="cb88-722"><a href="#cb88-722"></a><span class="co"># Set random seed for reproducibility</span></span>
<span id="cb88-723"><a href="#cb88-723"></a>torch.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb88-724"><a href="#cb88-724"></a><span class="cf">if</span> device.<span class="bu">type</span> <span class="op">==</span> <span class="st">'cuda'</span>:</span>
<span id="cb88-725"><a href="#cb88-725"></a>    torch.cuda.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb88-726"><a href="#cb88-726"></a><span class="cf">elif</span> device.<span class="bu">type</span> <span class="op">==</span> <span class="st">'mps'</span>:</span>
<span id="cb88-727"><a href="#cb88-727"></a>    torch.mps.manual_seed(<span class="dv">1337</span>)</span>
<span id="cb88-728"><a href="#cb88-728"></a></span>
<span id="cb88-729"><a href="#cb88-729"></a><span class="co"># Load and read the training text</span></span>
<span id="cb88-730"><a href="#cb88-730"></a><span class="co"># Note: This assumes 'input.txt' exists in the current directory</span></span>
<span id="cb88-731"><a href="#cb88-731"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'input.txt'</span>, <span class="st">'r'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>) <span class="im">as</span> f:</span>
<span id="cb88-732"><a href="#cb88-732"></a>    text <span class="op">=</span> f.read()</span>
<span id="cb88-733"><a href="#cb88-733"></a></span>
<span id="cb88-734"><a href="#cb88-734"></a><span class="co"># ===== DATA PREPROCESSING =====</span></span>
<span id="cb88-735"><a href="#cb88-735"></a><span class="co"># Create vocabulary from unique characters in the text</span></span>
<span id="cb88-736"><a href="#cb88-736"></a>chars <span class="op">=</span> <span class="bu">sorted</span>(<span class="bu">list</span>(<span class="bu">set</span>(text)))</span>
<span id="cb88-737"><a href="#cb88-737"></a>vocab_size <span class="op">=</span> <span class="bu">len</span>(chars)</span>
<span id="cb88-738"><a href="#cb88-738"></a><span class="co"># Create mapping between characters and integers</span></span>
<span id="cb88-739"><a href="#cb88-739"></a>stoi <span class="op">=</span> { ch:i <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }  <span class="co"># string to index</span></span>
<span id="cb88-740"><a href="#cb88-740"></a>itos <span class="op">=</span> { i:ch <span class="cf">for</span> i,ch <span class="kw">in</span> <span class="bu">enumerate</span>(chars) }  <span class="co"># index to string</span></span>
<span id="cb88-741"><a href="#cb88-741"></a><span class="co"># Define encoder and decoder functions</span></span>
<span id="cb88-742"><a href="#cb88-742"></a>encode <span class="op">=</span> <span class="kw">lambda</span> s: [stoi[c] <span class="cf">for</span> c <span class="kw">in</span> s]  <span class="co"># convert string to list of integers</span></span>
<span id="cb88-743"><a href="#cb88-743"></a>decode <span class="op">=</span> <span class="kw">lambda</span> l: <span class="st">''</span>.join([itos[i] <span class="cf">for</span> i <span class="kw">in</span> l])  <span class="co"># convert list of integers to string</span></span>
<span id="cb88-744"><a href="#cb88-744"></a></span>
<span id="cb88-745"><a href="#cb88-745"></a><span class="co"># Split data into training and validation sets</span></span>
<span id="cb88-746"><a href="#cb88-746"></a>data <span class="op">=</span> torch.tensor(encode(text), dtype<span class="op">=</span>torch.<span class="bu">long</span>)</span>
<span id="cb88-747"><a href="#cb88-747"></a>n <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.9</span><span class="op">*</span><span class="bu">len</span>(data))  <span class="co"># first 90% for training</span></span>
<span id="cb88-748"><a href="#cb88-748"></a>train_data <span class="op">=</span> data[:n]</span>
<span id="cb88-749"><a href="#cb88-749"></a>val_data <span class="op">=</span> data[n:]</span>
<span id="cb88-750"><a href="#cb88-750"></a><span class="co"># =============================</span></span>
<span id="cb88-751"><a href="#cb88-751"></a></span>
<span id="cb88-752"><a href="#cb88-752"></a><span class="co"># ===== DATA LOADING FUNCTION =====</span></span>
<span id="cb88-753"><a href="#cb88-753"></a><span class="kw">def</span> get_batch(split):</span>
<span id="cb88-754"><a href="#cb88-754"></a>    <span class="co">"""Generate a batch of data for training or validation.</span></span>
<span id="cb88-755"><a href="#cb88-755"></a><span class="co">    </span></span>
<span id="cb88-756"><a href="#cb88-756"></a><span class="co">    Args:</span></span>
<span id="cb88-757"><a href="#cb88-757"></a><span class="co">        split: 'train' or 'val' to specify which dataset to use</span></span>
<span id="cb88-758"><a href="#cb88-758"></a><span class="co">    </span></span>
<span id="cb88-759"><a href="#cb88-759"></a><span class="co">    Returns:</span></span>
<span id="cb88-760"><a href="#cb88-760"></a><span class="co">        x: input sequence of shape (batch_size, block_size)</span></span>
<span id="cb88-761"><a href="#cb88-761"></a><span class="co">        y: target sequence of shape (batch_size, block_size)</span></span>
<span id="cb88-762"><a href="#cb88-762"></a><span class="co">    """</span></span>
<span id="cb88-763"><a href="#cb88-763"></a>    data <span class="op">=</span> train_data <span class="cf">if</span> split <span class="op">==</span> <span class="st">'train'</span> <span class="cf">else</span> val_data</span>
<span id="cb88-764"><a href="#cb88-764"></a>    <span class="co"># Randomly select starting indices for sequences</span></span>
<span id="cb88-765"><a href="#cb88-765"></a>    ix <span class="op">=</span> torch.randint(<span class="bu">len</span>(data) <span class="op">-</span> block_size, (batch_size,))</span>
<span id="cb88-766"><a href="#cb88-766"></a>    <span class="co"># Create input and target sequences</span></span>
<span id="cb88-767"><a href="#cb88-767"></a>    x <span class="op">=</span> torch.stack([data[i:i<span class="op">+</span>block_size] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb88-768"><a href="#cb88-768"></a>    y <span class="op">=</span> torch.stack([data[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span>block_size<span class="op">+</span><span class="dv">1</span>] <span class="cf">for</span> i <span class="kw">in</span> ix])</span>
<span id="cb88-769"><a href="#cb88-769"></a>    x, y <span class="op">=</span> x.to(device), y.to(device)</span>
<span id="cb88-770"><a href="#cb88-770"></a>    <span class="cf">return</span> x, y</span>
<span id="cb88-771"><a href="#cb88-771"></a><span class="co"># ================================</span></span>
<span id="cb88-772"><a href="#cb88-772"></a></span>
<span id="cb88-773"><a href="#cb88-773"></a><span class="co"># ===== LOSS ESTIMATION FUNCTION =====</span></span>
<span id="cb88-774"><a href="#cb88-774"></a><span class="at">@torch.no_grad</span>()  <span class="co"># Disable gradient calculation for efficiency</span></span>
<span id="cb88-775"><a href="#cb88-775"></a><span class="kw">def</span> estimate_loss():</span>
<span id="cb88-776"><a href="#cb88-776"></a>    <span class="co">"""Estimate the loss on training and validation sets.</span></span>
<span id="cb88-777"><a href="#cb88-777"></a><span class="co">    </span></span>
<span id="cb88-778"><a href="#cb88-778"></a><span class="co">    Returns:</span></span>
<span id="cb88-779"><a href="#cb88-779"></a><span class="co">        Dictionary containing average loss for train and validation sets</span></span>
<span id="cb88-780"><a href="#cb88-780"></a><span class="co">    """</span></span>
<span id="cb88-781"><a href="#cb88-781"></a>    out <span class="op">=</span> {}</span>
<span id="cb88-782"><a href="#cb88-782"></a>    model.<span class="bu">eval</span>()  <span class="co"># Set model to evaluation mode</span></span>
<span id="cb88-783"><a href="#cb88-783"></a>    <span class="cf">for</span> split <span class="kw">in</span> [<span class="st">'train'</span>, <span class="st">'val'</span>]:</span>
<span id="cb88-784"><a href="#cb88-784"></a>        losses <span class="op">=</span> torch.zeros(eval_iters)</span>
<span id="cb88-785"><a href="#cb88-785"></a>        <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(eval_iters):</span>
<span id="cb88-786"><a href="#cb88-786"></a>            X, Y <span class="op">=</span> get_batch(split)</span>
<span id="cb88-787"><a href="#cb88-787"></a>            logits, loss <span class="op">=</span> model(X, Y)</span>
<span id="cb88-788"><a href="#cb88-788"></a>            losses[k] <span class="op">=</span> loss.item()</span>
<span id="cb88-789"><a href="#cb88-789"></a>        out[split] <span class="op">=</span> losses.mean()</span>
<span id="cb88-790"><a href="#cb88-790"></a>    model.train()  <span class="co"># Set model back to training mode</span></span>
<span id="cb88-791"><a href="#cb88-791"></a>    <span class="cf">return</span> out</span>
<span id="cb88-792"><a href="#cb88-792"></a><span class="co"># ===================================</span></span>
<span id="cb88-793"><a href="#cb88-793"></a></span>
<span id="cb88-794"><a href="#cb88-794"></a><span class="co"># ===== </span><span class="al">ATTENTION</span><span class="co"> HEAD IMPLEMENTATION =====</span></span>
<span id="cb88-795"><a href="#cb88-795"></a><span class="kw">class</span> Head(nn.Module):</span>
<span id="cb88-796"><a href="#cb88-796"></a>    <span class="co">"""Single head of self-attention."""</span></span>
<span id="cb88-797"><a href="#cb88-797"></a>    </span>
<span id="cb88-798"><a href="#cb88-798"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, head_size):</span>
<span id="cb88-799"><a href="#cb88-799"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb88-800"><a href="#cb88-800"></a>        <span class="co"># Key, Query, Value projections</span></span>
<span id="cb88-801"><a href="#cb88-801"></a>        <span class="va">self</span>.key <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb88-802"><a href="#cb88-802"></a>        <span class="va">self</span>.query <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb88-803"><a href="#cb88-803"></a>        <span class="va">self</span>.value <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb88-804"><a href="#cb88-804"></a>        <span class="co"># Create lower triangular mask for causal attention</span></span>
<span id="cb88-805"><a href="#cb88-805"></a>        <span class="va">self</span>.register_buffer(<span class="st">'tril'</span>, torch.tril(torch.ones(block_size, block_size)))</span>
<span id="cb88-806"><a href="#cb88-806"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb88-807"><a href="#cb88-807"></a></span>
<span id="cb88-808"><a href="#cb88-808"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb88-809"><a href="#cb88-809"></a>        B,T,C <span class="op">=</span> x.shape</span>
<span id="cb88-810"><a href="#cb88-810"></a>        <span class="co"># Compute key, query, and value matrices</span></span>
<span id="cb88-811"><a href="#cb88-811"></a>        k <span class="op">=</span> <span class="va">self</span>.key(x)   <span class="co"># (B,T,C)</span></span>
<span id="cb88-812"><a href="#cb88-812"></a>        q <span class="op">=</span> <span class="va">self</span>.query(x) <span class="co"># (B,T,C)</span></span>
<span id="cb88-813"><a href="#cb88-813"></a>        <span class="co"># Compute attention scores</span></span>
<span id="cb88-814"><a href="#cb88-814"></a>        wei <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> C<span class="op">**-</span><span class="fl">0.5</span>  <span class="co"># Scale by sqrt(d_k)</span></span>
<span id="cb88-815"><a href="#cb88-815"></a>        wei <span class="op">=</span> wei.masked_fill(<span class="va">self</span>.tril[:T, :T] <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))  <span class="co"># Apply mask</span></span>
<span id="cb88-816"><a href="#cb88-816"></a>        wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># Convert to probabilities</span></span>
<span id="cb88-817"><a href="#cb88-817"></a>        wei <span class="op">=</span> <span class="va">self</span>.dropout(wei)</span>
<span id="cb88-818"><a href="#cb88-818"></a>        <span class="co"># Weighted aggregation of values</span></span>
<span id="cb88-819"><a href="#cb88-819"></a>        v <span class="op">=</span> <span class="va">self</span>.value(x)</span>
<span id="cb88-820"><a href="#cb88-820"></a>        out <span class="op">=</span> wei <span class="op">@</span> v</span>
<span id="cb88-821"><a href="#cb88-821"></a>        <span class="cf">return</span> out</span>
<span id="cb88-822"><a href="#cb88-822"></a><span class="co"># ========================================</span></span>
<span id="cb88-823"><a href="#cb88-823"></a></span>
<span id="cb88-824"><a href="#cb88-824"></a><span class="co"># ===== MULTI-HEAD </span><span class="al">ATTENTION</span><span class="co"> =====</span></span>
<span id="cb88-825"><a href="#cb88-825"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb88-826"><a href="#cb88-826"></a>    <span class="co">"""Multiple heads of self-attention in parallel."""</span></span>
<span id="cb88-827"><a href="#cb88-827"></a>    </span>
<span id="cb88-828"><a href="#cb88-828"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_heads, head_size):</span>
<span id="cb88-829"><a href="#cb88-829"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb88-830"><a href="#cb88-830"></a>        <span class="va">self</span>.heads <span class="op">=</span> nn.ModuleList([Head(head_size) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_heads)])</span>
<span id="cb88-831"><a href="#cb88-831"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Linear(n_embd, n_embd)  <span class="co"># Projection layer</span></span>
<span id="cb88-832"><a href="#cb88-832"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb88-833"><a href="#cb88-833"></a></span>
<span id="cb88-834"><a href="#cb88-834"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb88-835"><a href="#cb88-835"></a>        <span class="co"># Process through each head and concatenate results</span></span>
<span id="cb88-836"><a href="#cb88-836"></a>        out <span class="op">=</span> torch.cat([h(x) <span class="cf">for</span> h <span class="kw">in</span> <span class="va">self</span>.heads], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb88-837"><a href="#cb88-837"></a>        out <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.proj(out))</span>
<span id="cb88-838"><a href="#cb88-838"></a>        <span class="cf">return</span> out</span>
<span id="cb88-839"><a href="#cb88-839"></a><span class="co"># ===============================</span></span>
<span id="cb88-840"><a href="#cb88-840"></a></span>
<span id="cb88-841"><a href="#cb88-841"></a><span class="co"># ===== FEED-FORWARD NETWORK =====</span></span>
<span id="cb88-842"><a href="#cb88-842"></a><span class="kw">class</span> FeedFoward(nn.Module):</span>
<span id="cb88-843"><a href="#cb88-843"></a>    <span class="co">"""Simple feed-forward network with one hidden layer."""</span></span>
<span id="cb88-844"><a href="#cb88-844"></a>    </span>
<span id="cb88-845"><a href="#cb88-845"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd):</span>
<span id="cb88-846"><a href="#cb88-846"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb88-847"><a href="#cb88-847"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb88-848"><a href="#cb88-848"></a>            nn.Linear(n_embd, <span class="dv">4</span> <span class="op">*</span> n_embd),  <span class="co"># Expand dimension</span></span>
<span id="cb88-849"><a href="#cb88-849"></a>            nn.ReLU(),                      <span class="co"># Non-linearity</span></span>
<span id="cb88-850"><a href="#cb88-850"></a>            nn.Linear(<span class="dv">4</span> <span class="op">*</span> n_embd, n_embd),  <span class="co"># Project back</span></span>
<span id="cb88-851"><a href="#cb88-851"></a>            nn.Dropout(dropout),</span>
<span id="cb88-852"><a href="#cb88-852"></a>        )</span>
<span id="cb88-853"><a href="#cb88-853"></a></span>
<span id="cb88-854"><a href="#cb88-854"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb88-855"><a href="#cb88-855"></a>        <span class="cf">return</span> <span class="va">self</span>.net(x)</span>
<span id="cb88-856"><a href="#cb88-856"></a><span class="co"># ==============================</span></span>
<span id="cb88-857"><a href="#cb88-857"></a></span>
<span id="cb88-858"><a href="#cb88-858"></a><span class="co"># ===== TRANSFORMER BLOCK =====</span></span>
<span id="cb88-859"><a href="#cb88-859"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb88-860"><a href="#cb88-860"></a>    <span class="co">"""Transformer block: communication (attention) followed by computation (FFN)."""</span></span>
<span id="cb88-861"><a href="#cb88-861"></a>    </span>
<span id="cb88-862"><a href="#cb88-862"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd, n_head):</span>
<span id="cb88-863"><a href="#cb88-863"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb88-864"><a href="#cb88-864"></a>        head_size <span class="op">=</span> n_embd <span class="op">//</span> n_head  <span class="co"># floor(n_embd / n_head)</span></span>
<span id="cb88-865"><a href="#cb88-865"></a>        <span class="va">self</span>.sa <span class="op">=</span> MultiHeadAttention(n_head, head_size)</span>
<span id="cb88-866"><a href="#cb88-866"></a>        <span class="va">self</span>.ffwd <span class="op">=</span> FeedFoward(n_embd)</span>
<span id="cb88-867"><a href="#cb88-867"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> nn.LayerNorm(n_embd)  <span class="co"># Layer normalization</span></span>
<span id="cb88-868"><a href="#cb88-868"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> nn.LayerNorm(n_embd)</span>
<span id="cb88-869"><a href="#cb88-869"></a></span>
<span id="cb88-870"><a href="#cb88-870"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb88-871"><a href="#cb88-871"></a>        <span class="co"># Apply attention with residual connection</span></span>
<span id="cb88-872"><a href="#cb88-872"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.sa(<span class="va">self</span>.ln1(x))</span>
<span id="cb88-873"><a href="#cb88-873"></a>        <span class="co"># Apply feed-forward with residual connection</span></span>
<span id="cb88-874"><a href="#cb88-874"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffwd(<span class="va">self</span>.ln2(x))</span>
<span id="cb88-875"><a href="#cb88-875"></a>        <span class="cf">return</span> x</span>
<span id="cb88-876"><a href="#cb88-876"></a><span class="co"># ============================</span></span>
<span id="cb88-877"><a href="#cb88-877"></a></span>
<span id="cb88-878"><a href="#cb88-878"></a><span class="co"># ===== LANGUAGE MODEL =====</span></span>
<span id="cb88-879"><a href="#cb88-879"></a><span class="kw">class</span> BigramLanguageModel(nn.Module):</span>
<span id="cb88-880"><a href="#cb88-880"></a>    <span class="co">"""Simple bigram language model with transformer architecture."""</span></span>
<span id="cb88-881"><a href="#cb88-881"></a>    </span>
<span id="cb88-882"><a href="#cb88-882"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb88-883"><a href="#cb88-883"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb88-884"><a href="#cb88-884"></a>        <span class="co"># Token embeddings</span></span>
<span id="cb88-885"><a href="#cb88-885"></a>        <span class="va">self</span>.token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, n_embd)</span>
<span id="cb88-886"><a href="#cb88-886"></a>        <span class="co"># Position embeddings</span></span>
<span id="cb88-887"><a href="#cb88-887"></a>        <span class="va">self</span>.position_embedding_table <span class="op">=</span> nn.Embedding(block_size, n_embd)</span>
<span id="cb88-888"><a href="#cb88-888"></a>        <span class="co"># Stack of transformer blocks</span></span>
<span id="cb88-889"><a href="#cb88-889"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.Sequential(<span class="op">*</span>[Block(n_embd, n_head<span class="op">=</span>n_head) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_layer)])</span>
<span id="cb88-890"><a href="#cb88-890"></a>        <span class="va">self</span>.ln_f <span class="op">=</span> nn.LayerNorm(n_embd)  <span class="co"># Final layer norm</span></span>
<span id="cb88-891"><a href="#cb88-891"></a>        <span class="va">self</span>.lm_head <span class="op">=</span> nn.Linear(n_embd, vocab_size)  <span class="co"># Language model head</span></span>
<span id="cb88-892"><a href="#cb88-892"></a></span>
<span id="cb88-893"><a href="#cb88-893"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb88-894"><a href="#cb88-894"></a>        B, T <span class="op">=</span> idx.shape</span>
<span id="cb88-895"><a href="#cb88-895"></a>        </span>
<span id="cb88-896"><a href="#cb88-896"></a>        <span class="co"># Get token embeddings</span></span>
<span id="cb88-897"><a href="#cb88-897"></a>        tok_emb <span class="op">=</span> <span class="va">self</span>.token_embedding_table(idx)  <span class="co"># (B,T,C)</span></span>
<span id="cb88-898"><a href="#cb88-898"></a>        <span class="co"># Get position embeddings</span></span>
<span id="cb88-899"><a href="#cb88-899"></a>        pos_emb <span class="op">=</span> <span class="va">self</span>.position_embedding_table(torch.arange(T, device<span class="op">=</span>device))  <span class="co"># (T,C)</span></span>
<span id="cb88-900"><a href="#cb88-900"></a>        <span class="co"># Combine token and position embeddings</span></span>
<span id="cb88-901"><a href="#cb88-901"></a>        x <span class="op">=</span> tok_emb <span class="op">+</span> pos_emb  <span class="co"># (B,T,C)</span></span>
<span id="cb88-902"><a href="#cb88-902"></a>        <span class="co"># Process through transformer blocks</span></span>
<span id="cb88-903"><a href="#cb88-903"></a>        x <span class="op">=</span> <span class="va">self</span>.blocks(x)  <span class="co"># (B,T,C)</span></span>
<span id="cb88-904"><a href="#cb88-904"></a>        x <span class="op">=</span> <span class="va">self</span>.ln_f(x)  <span class="co"># (B,T,C)</span></span>
<span id="cb88-905"><a href="#cb88-905"></a>        logits <span class="op">=</span> <span class="va">self</span>.lm_head(x)  <span class="co"># (B,T,vocab_size)</span></span>
<span id="cb88-906"><a href="#cb88-906"></a></span>
<span id="cb88-907"><a href="#cb88-907"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb88-908"><a href="#cb88-908"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb88-909"><a href="#cb88-909"></a>        <span class="cf">else</span>:</span>
<span id="cb88-910"><a href="#cb88-910"></a>            <span class="co"># Reshape for loss calculation</span></span>
<span id="cb88-911"><a href="#cb88-911"></a>            B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb88-912"><a href="#cb88-912"></a>            logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C)</span>
<span id="cb88-913"><a href="#cb88-913"></a>            targets <span class="op">=</span> targets.view(B<span class="op">*</span>T)</span>
<span id="cb88-914"><a href="#cb88-914"></a>            loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb88-915"><a href="#cb88-915"></a></span>
<span id="cb88-916"><a href="#cb88-916"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb88-917"><a href="#cb88-917"></a></span>
<span id="cb88-918"><a href="#cb88-918"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens):</span>
<span id="cb88-919"><a href="#cb88-919"></a>        <span class="co">"""Generate new text given a starting sequence."""</span></span>
<span id="cb88-920"><a href="#cb88-920"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb88-921"><a href="#cb88-921"></a>            <span class="co"># Crop context to block_size</span></span>
<span id="cb88-922"><a href="#cb88-922"></a>            idx_cond <span class="op">=</span> idx[:, <span class="op">-</span>block_size:]</span>
<span id="cb88-923"><a href="#cb88-923"></a>            <span class="co"># Get predictions</span></span>
<span id="cb88-924"><a href="#cb88-924"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>(idx_cond)</span>
<span id="cb88-925"><a href="#cb88-925"></a>            <span class="co"># Focus on last time step</span></span>
<span id="cb88-926"><a href="#cb88-926"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :]  <span class="co"># (B, C)</span></span>
<span id="cb88-927"><a href="#cb88-927"></a>            <span class="co"># Convert to probabilities</span></span>
<span id="cb88-928"><a href="#cb88-928"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># (B, C)</span></span>
<span id="cb88-929"><a href="#cb88-929"></a>            <span class="co"># Sample from distribution</span></span>
<span id="cb88-930"><a href="#cb88-930"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)  <span class="co"># (B, 1)</span></span>
<span id="cb88-931"><a href="#cb88-931"></a>            <span class="co"># Append to sequence</span></span>
<span id="cb88-932"><a href="#cb88-932"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># (B, T+1)</span></span>
<span id="cb88-933"><a href="#cb88-933"></a>        <span class="cf">return</span> idx</span>
<span id="cb88-934"><a href="#cb88-934"></a><span class="co"># =========================</span></span>
<span id="cb88-935"><a href="#cb88-935"></a></span>
<span id="cb88-936"><a href="#cb88-936"></a><span class="co"># ===== MODEL INITIALIZATION AND TRAINING =====</span></span>
<span id="cb88-937"><a href="#cb88-937"></a><span class="co"># Create model instance</span></span>
<span id="cb88-938"><a href="#cb88-938"></a>model <span class="op">=</span> BigramLanguageModel()</span>
<span id="cb88-939"><a href="#cb88-939"></a>m <span class="op">=</span> model.to(device)</span>
<span id="cb88-940"><a href="#cb88-940"></a><span class="co"># Print number of parameters</span></span>
<span id="cb88-941"><a href="#cb88-941"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> m.parameters())<span class="op">/</span><span class="fl">1e6</span>, <span class="st">'M parameters'</span>)</span>
<span id="cb88-942"><a href="#cb88-942"></a></span>
<span id="cb88-943"><a href="#cb88-943"></a><span class="co"># Create optimizer</span></span>
<span id="cb88-944"><a href="#cb88-944"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb88-945"><a href="#cb88-945"></a></span>
<span id="cb88-946"><a href="#cb88-946"></a><span class="co"># Training loop</span></span>
<span id="cb88-947"><a href="#cb88-947"></a><span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(max_iters):</span>
<span id="cb88-948"><a href="#cb88-948"></a>    <span class="co"># Evaluate loss periodically</span></span>
<span id="cb88-949"><a href="#cb88-949"></a>    <span class="cf">if</span> <span class="bu">iter</span> <span class="op">%</span> eval_interval <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> <span class="bu">iter</span> <span class="op">==</span> max_iters <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb88-950"><a href="#cb88-950"></a>        losses <span class="op">=</span> estimate_loss()</span>
<span id="cb88-951"><a href="#cb88-951"></a>        <span class="bu">print</span>(<span class="ss">f"step </span><span class="sc">{</span><span class="bu">iter</span><span class="sc">}</span><span class="ss">: train loss </span><span class="sc">{</span>losses[<span class="st">'train'</span>]<span class="sc">:.4f}</span><span class="ss">, val loss </span><span class="sc">{</span>losses[<span class="st">'val'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb88-952"><a href="#cb88-952"></a></span>
<span id="cb88-953"><a href="#cb88-953"></a>    <span class="co"># Get batch of data</span></span>
<span id="cb88-954"><a href="#cb88-954"></a>    xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb88-955"><a href="#cb88-955"></a></span>
<span id="cb88-956"><a href="#cb88-956"></a>    <span class="co"># Forward pass</span></span>
<span id="cb88-957"><a href="#cb88-957"></a>    logits, loss <span class="op">=</span> model(xb, yb)</span>
<span id="cb88-958"><a href="#cb88-958"></a>    <span class="co"># Backward pass</span></span>
<span id="cb88-959"><a href="#cb88-959"></a>    optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb88-960"><a href="#cb88-960"></a>    loss.backward()</span>
<span id="cb88-961"><a href="#cb88-961"></a>    optimizer.step()</span>
<span id="cb88-962"><a href="#cb88-962"></a></span>
<span id="cb88-963"><a href="#cb88-963"></a><span class="co"># Generate text from the model</span></span>
<span id="cb88-964"><a href="#cb88-964"></a>context <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>, device<span class="op">=</span>device)</span>
<span id="cb88-965"><a href="#cb88-965"></a><span class="bu">print</span>(decode(m.generate(context, max_new_tokens<span class="op">=</span><span class="dv">2000</span>)[<span class="dv">0</span>].tolist()))</span>
<span id="cb88-966"><a href="#cb88-966"></a><span class="co"># ============================================</span></span>
<span id="cb88-967"><a href="#cb88-967"></a><span class="in">```</span></span>
<span id="cb88-968"><a href="#cb88-968"></a></span>
<span id="cb88-969"><a href="#cb88-969"></a><span class="fu">## Full GPT Model Architecture</span></span>
<span id="cb88-970"><a href="#cb88-970"></a></span>
<span id="cb88-971"><a href="#cb88-971"></a><span class="fu">### Key Components</span></span>
<span id="cb88-972"><a href="#cb88-972"></a><span class="ss">1. </span>**Token Embeddings**: Convert input tokens to vectors</span>
<span id="cb88-973"><a href="#cb88-973"></a><span class="ss">2. </span>**Position Embeddings**: Add positional information</span>
<span id="cb88-974"><a href="#cb88-974"></a><span class="ss">3. </span>**Transformer Blocks**: Process information through attention and feed-forward layers</span>
<span id="cb88-975"><a href="#cb88-975"></a><span class="ss">4. </span>**Layer Normalization**: Stabilize training</span>
<span id="cb88-976"><a href="#cb88-976"></a><span class="ss">5. </span>**Language Model Head**: Predict next token probabilities</span>
<span id="cb88-977"><a href="#cb88-977"></a></span>
<span id="cb88-978"><a href="#cb88-978"></a><span class="fu">### Hyperparameters</span></span>
<span id="cb88-981"><a href="#cb88-981"></a><span class="in">```{python}</span></span>
<span id="cb88-982"><a href="#cb88-982"></a><span class="co"># ===== HYPERPARAMETERS =====</span></span>
<span id="cb88-983"><a href="#cb88-983"></a>batch_size <span class="op">=</span> <span class="dv">16</span>        <span class="co"># Number of independent sequences processed in parallel</span></span>
<span id="cb88-984"><a href="#cb88-984"></a>block_size <span class="op">=</span> <span class="dv">32</span>        <span class="co"># Maximum context length for predictions</span></span>
<span id="cb88-985"><a href="#cb88-985"></a>max_iters <span class="op">=</span> <span class="dv">5000</span>       <span class="co"># Total number of training iterations</span></span>
<span id="cb88-986"><a href="#cb88-986"></a>eval_interval <span class="op">=</span> <span class="dv">100</span>    <span class="co"># How often to evaluate the model</span></span>
<span id="cb88-987"><a href="#cb88-987"></a>learning_rate <span class="op">=</span> <span class="fl">1e-3</span>   <span class="co"># Step size for gradient descent</span></span>
<span id="cb88-988"><a href="#cb88-988"></a>eval_iters <span class="op">=</span> <span class="dv">200</span>       <span class="co"># Number of iterations to average over when evaluating</span></span>
<span id="cb88-989"><a href="#cb88-989"></a>n_embd <span class="op">=</span> <span class="dv">64</span>           <span class="co"># Size of the embedding dimension</span></span>
<span id="cb88-990"><a href="#cb88-990"></a>n_head <span class="op">=</span> <span class="dv">4</span>            <span class="co"># Number of attention heads</span></span>
<span id="cb88-991"><a href="#cb88-991"></a>n_layer <span class="op">=</span> <span class="dv">4</span>           <span class="co"># Number of transformer layers</span></span>
<span id="cb88-992"><a href="#cb88-992"></a>dropout <span class="op">=</span> <span class="fl">0.0</span>         <span class="co"># Probability of dropping out neurons</span></span>
<span id="cb88-993"><a href="#cb88-993"></a><span class="co"># ==========================</span></span>
<span id="cb88-994"><a href="#cb88-994"></a><span class="in">```</span></span>
<span id="cb88-995"><a href="#cb88-995"></a></span>
<span id="cb88-996"><a href="#cb88-996"></a><span class="fu">### Device Selection</span></span>
<span id="cb88-999"><a href="#cb88-999"></a><span class="in">```{python}</span></span>
<span id="cb88-1000"><a href="#cb88-1000"></a><span class="co"># Device selection: MPS (Apple Silicon) &gt; CUDA &gt; CPU</span></span>
<span id="cb88-1001"><a href="#cb88-1001"></a><span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb88-1002"><a href="#cb88-1002"></a>    device <span class="op">=</span> torch.device(<span class="st">"mps"</span>)  <span class="co"># Apple Silicon GPU</span></span>
<span id="cb88-1003"><a href="#cb88-1003"></a><span class="cf">elif</span> torch.cuda.is_available():</span>
<span id="cb88-1004"><a href="#cb88-1004"></a>    device <span class="op">=</span> torch.device(<span class="st">"cuda"</span>)  <span class="co"># NVIDIA GPU</span></span>
<span id="cb88-1005"><a href="#cb88-1005"></a><span class="cf">else</span>:</span>
<span id="cb88-1006"><a href="#cb88-1006"></a>    device <span class="op">=</span> torch.device(<span class="st">"cpu"</span>)   <span class="co"># CPU fallback</span></span>
<span id="cb88-1007"><a href="#cb88-1007"></a><span class="bu">print</span>(<span class="ss">f"Using device: </span><span class="sc">{</span>device<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb88-1008"><a href="#cb88-1008"></a><span class="in">```</span></span>
<span id="cb88-1009"><a href="#cb88-1009"></a></span>
<span id="cb88-1010"><a href="#cb88-1010"></a><span class="fu">### Attention Head Implementation</span></span>
<span id="cb88-1013"><a href="#cb88-1013"></a><span class="in">```{python}</span></span>
<span id="cb88-1014"><a href="#cb88-1014"></a><span class="kw">class</span> Head(nn.Module):</span>
<span id="cb88-1015"><a href="#cb88-1015"></a>    <span class="co">"""Single head of self-attention."""</span></span>
<span id="cb88-1016"><a href="#cb88-1016"></a>    </span>
<span id="cb88-1017"><a href="#cb88-1017"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, head_size):</span>
<span id="cb88-1018"><a href="#cb88-1018"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb88-1019"><a href="#cb88-1019"></a>        <span class="co"># Key, Query, Value projections</span></span>
<span id="cb88-1020"><a href="#cb88-1020"></a>        <span class="va">self</span>.key <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb88-1021"><a href="#cb88-1021"></a>        <span class="va">self</span>.query <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb88-1022"><a href="#cb88-1022"></a>        <span class="va">self</span>.value <span class="op">=</span> nn.Linear(n_embd, head_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb88-1023"><a href="#cb88-1023"></a>        <span class="co"># Create lower triangular mask for causal attention</span></span>
<span id="cb88-1024"><a href="#cb88-1024"></a>        <span class="va">self</span>.register_buffer(<span class="st">'tril'</span>, torch.tril(torch.ones(block_size, block_size)))</span>
<span id="cb88-1025"><a href="#cb88-1025"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb88-1026"><a href="#cb88-1026"></a></span>
<span id="cb88-1027"><a href="#cb88-1027"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb88-1028"><a href="#cb88-1028"></a>        B,T,C <span class="op">=</span> x.shape</span>
<span id="cb88-1029"><a href="#cb88-1029"></a>        <span class="co"># Compute key, query, and value matrices</span></span>
<span id="cb88-1030"><a href="#cb88-1030"></a>        k <span class="op">=</span> <span class="va">self</span>.key(x)   <span class="co"># (B,T,C)</span></span>
<span id="cb88-1031"><a href="#cb88-1031"></a>        q <span class="op">=</span> <span class="va">self</span>.query(x) <span class="co"># (B,T,C)</span></span>
<span id="cb88-1032"><a href="#cb88-1032"></a>        <span class="co"># Compute attention scores</span></span>
<span id="cb88-1033"><a href="#cb88-1033"></a>        wei <span class="op">=</span> q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>) <span class="op">*</span> C<span class="op">**-</span><span class="fl">0.5</span>  <span class="co"># Scale by sqrt(d_k)</span></span>
<span id="cb88-1034"><a href="#cb88-1034"></a>        wei <span class="op">=</span> wei.masked_fill(<span class="va">self</span>.tril[:T, :T] <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))  <span class="co"># Apply mask</span></span>
<span id="cb88-1035"><a href="#cb88-1035"></a>        wei <span class="op">=</span> F.softmax(wei, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># Convert to probabilities</span></span>
<span id="cb88-1036"><a href="#cb88-1036"></a>        wei <span class="op">=</span> <span class="va">self</span>.dropout(wei)</span>
<span id="cb88-1037"><a href="#cb88-1037"></a>        <span class="co"># Weighted aggregation of values</span></span>
<span id="cb88-1038"><a href="#cb88-1038"></a>        v <span class="op">=</span> <span class="va">self</span>.value(x)</span>
<span id="cb88-1039"><a href="#cb88-1039"></a>        out <span class="op">=</span> wei <span class="op">@</span> v</span>
<span id="cb88-1040"><a href="#cb88-1040"></a>        <span class="cf">return</span> out</span>
<span id="cb88-1041"><a href="#cb88-1041"></a><span class="in">```</span></span>
<span id="cb88-1042"><a href="#cb88-1042"></a></span>
<span id="cb88-1043"><a href="#cb88-1043"></a><span class="fu">### Multi-Head Attention</span></span>
<span id="cb88-1046"><a href="#cb88-1046"></a><span class="in">```{python}</span></span>
<span id="cb88-1047"><a href="#cb88-1047"></a><span class="kw">class</span> MultiHeadAttention(nn.Module):</span>
<span id="cb88-1048"><a href="#cb88-1048"></a>    <span class="co">"""Multiple heads of self-attention in parallel."""</span></span>
<span id="cb88-1049"><a href="#cb88-1049"></a>    </span>
<span id="cb88-1050"><a href="#cb88-1050"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_heads, head_size):</span>
<span id="cb88-1051"><a href="#cb88-1051"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb88-1052"><a href="#cb88-1052"></a>        <span class="va">self</span>.heads <span class="op">=</span> nn.ModuleList([Head(head_size) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_heads)])</span>
<span id="cb88-1053"><a href="#cb88-1053"></a>        <span class="va">self</span>.proj <span class="op">=</span> nn.Linear(n_embd, n_embd)  <span class="co"># Projection layer</span></span>
<span id="cb88-1054"><a href="#cb88-1054"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(dropout)</span>
<span id="cb88-1055"><a href="#cb88-1055"></a></span>
<span id="cb88-1056"><a href="#cb88-1056"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb88-1057"><a href="#cb88-1057"></a>        <span class="co"># Process through each head and concatenate results</span></span>
<span id="cb88-1058"><a href="#cb88-1058"></a>        out <span class="op">=</span> torch.cat([h(x) <span class="cf">for</span> h <span class="kw">in</span> <span class="va">self</span>.heads], dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb88-1059"><a href="#cb88-1059"></a>        out <span class="op">=</span> <span class="va">self</span>.dropout(<span class="va">self</span>.proj(out))</span>
<span id="cb88-1060"><a href="#cb88-1060"></a>        <span class="cf">return</span> out</span>
<span id="cb88-1061"><a href="#cb88-1061"></a><span class="in">```</span></span>
<span id="cb88-1062"><a href="#cb88-1062"></a></span>
<span id="cb88-1063"><a href="#cb88-1063"></a><span class="fu">### Feed-Forward Network</span></span>
<span id="cb88-1066"><a href="#cb88-1066"></a><span class="in">```{python}</span></span>
<span id="cb88-1067"><a href="#cb88-1067"></a><span class="kw">class</span> FeedFoward(nn.Module):</span>
<span id="cb88-1068"><a href="#cb88-1068"></a>    <span class="co">"""Simple feed-forward network with one hidden layer."""</span></span>
<span id="cb88-1069"><a href="#cb88-1069"></a>    </span>
<span id="cb88-1070"><a href="#cb88-1070"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd):</span>
<span id="cb88-1071"><a href="#cb88-1071"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb88-1072"><a href="#cb88-1072"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb88-1073"><a href="#cb88-1073"></a>            nn.Linear(n_embd, <span class="dv">4</span> <span class="op">*</span> n_embd),  <span class="co"># Expand dimension</span></span>
<span id="cb88-1074"><a href="#cb88-1074"></a>            nn.ReLU(),                      <span class="co"># Non-linearity</span></span>
<span id="cb88-1075"><a href="#cb88-1075"></a>            nn.Linear(<span class="dv">4</span> <span class="op">*</span> n_embd, n_embd),  <span class="co"># Project back</span></span>
<span id="cb88-1076"><a href="#cb88-1076"></a>            nn.Dropout(dropout),</span>
<span id="cb88-1077"><a href="#cb88-1077"></a>        )</span>
<span id="cb88-1078"><a href="#cb88-1078"></a></span>
<span id="cb88-1079"><a href="#cb88-1079"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb88-1080"><a href="#cb88-1080"></a>        <span class="cf">return</span> <span class="va">self</span>.net(x)</span>
<span id="cb88-1081"><a href="#cb88-1081"></a><span class="in">```</span></span>
<span id="cb88-1082"><a href="#cb88-1082"></a></span>
<span id="cb88-1083"><a href="#cb88-1083"></a><span class="fu">### Transformer Block</span></span>
<span id="cb88-1086"><a href="#cb88-1086"></a><span class="in">```{python}</span></span>
<span id="cb88-1087"><a href="#cb88-1087"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb88-1088"><a href="#cb88-1088"></a>    <span class="co">"""Transformer block: communication (attention) followed by computation (FFN)."""</span></span>
<span id="cb88-1089"><a href="#cb88-1089"></a>    </span>
<span id="cb88-1090"><a href="#cb88-1090"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, n_embd, n_head):</span>
<span id="cb88-1091"><a href="#cb88-1091"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb88-1092"><a href="#cb88-1092"></a>        head_size <span class="op">=</span> n_embd <span class="op">//</span> n_head  <span class="co"># floor(n_embd / n_head)</span></span>
<span id="cb88-1093"><a href="#cb88-1093"></a>        <span class="va">self</span>.sa <span class="op">=</span> MultiHeadAttention(n_head, head_size)</span>
<span id="cb88-1094"><a href="#cb88-1094"></a>        <span class="va">self</span>.ffwd <span class="op">=</span> FeedFoward(n_embd)</span>
<span id="cb88-1095"><a href="#cb88-1095"></a>        <span class="va">self</span>.ln1 <span class="op">=</span> nn.LayerNorm(n_embd)  <span class="co"># Layer normalization</span></span>
<span id="cb88-1096"><a href="#cb88-1096"></a>        <span class="va">self</span>.ln2 <span class="op">=</span> nn.LayerNorm(n_embd)</span>
<span id="cb88-1097"><a href="#cb88-1097"></a></span>
<span id="cb88-1098"><a href="#cb88-1098"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb88-1099"><a href="#cb88-1099"></a>        <span class="co"># Apply attention with residual connection</span></span>
<span id="cb88-1100"><a href="#cb88-1100"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.sa(<span class="va">self</span>.ln1(x))</span>
<span id="cb88-1101"><a href="#cb88-1101"></a>        <span class="co"># Apply feed-forward with residual connection</span></span>
<span id="cb88-1102"><a href="#cb88-1102"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.ffwd(<span class="va">self</span>.ln2(x))</span>
<span id="cb88-1103"><a href="#cb88-1103"></a>        <span class="cf">return</span> x</span>
<span id="cb88-1104"><a href="#cb88-1104"></a><span class="in">```</span></span>
<span id="cb88-1105"><a href="#cb88-1105"></a></span>
<span id="cb88-1106"><a href="#cb88-1106"></a><span class="fu">### Full Language Model</span></span>
<span id="cb88-1109"><a href="#cb88-1109"></a><span class="in">```{python}</span></span>
<span id="cb88-1110"><a href="#cb88-1110"></a><span class="kw">class</span> BigramLanguageModel(nn.Module):</span>
<span id="cb88-1111"><a href="#cb88-1111"></a>    <span class="co">"""Simple bigram language model with transformer architecture."""</span></span>
<span id="cb88-1112"><a href="#cb88-1112"></a>    </span>
<span id="cb88-1113"><a href="#cb88-1113"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>):</span>
<span id="cb88-1114"><a href="#cb88-1114"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb88-1115"><a href="#cb88-1115"></a>        <span class="co"># Token embeddings</span></span>
<span id="cb88-1116"><a href="#cb88-1116"></a>        <span class="va">self</span>.token_embedding_table <span class="op">=</span> nn.Embedding(vocab_size, n_embd)</span>
<span id="cb88-1117"><a href="#cb88-1117"></a>        <span class="co"># Position embeddings</span></span>
<span id="cb88-1118"><a href="#cb88-1118"></a>        <span class="va">self</span>.position_embedding_table <span class="op">=</span> nn.Embedding(block_size, n_embd)</span>
<span id="cb88-1119"><a href="#cb88-1119"></a>        <span class="co"># Stack of transformer blocks</span></span>
<span id="cb88-1120"><a href="#cb88-1120"></a>        <span class="va">self</span>.blocks <span class="op">=</span> nn.Sequential(<span class="op">*</span>[Block(n_embd, n_head<span class="op">=</span>n_head) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(n_layer)])</span>
<span id="cb88-1121"><a href="#cb88-1121"></a>        <span class="va">self</span>.ln_f <span class="op">=</span> nn.LayerNorm(n_embd)  <span class="co"># Final layer norm</span></span>
<span id="cb88-1122"><a href="#cb88-1122"></a>        <span class="va">self</span>.lm_head <span class="op">=</span> nn.Linear(n_embd, vocab_size)  <span class="co"># Language model head</span></span>
<span id="cb88-1123"><a href="#cb88-1123"></a></span>
<span id="cb88-1124"><a href="#cb88-1124"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb88-1125"><a href="#cb88-1125"></a>        B, T <span class="op">=</span> idx.shape</span>
<span id="cb88-1126"><a href="#cb88-1126"></a>        </span>
<span id="cb88-1127"><a href="#cb88-1127"></a>        <span class="co"># Get token embeddings</span></span>
<span id="cb88-1128"><a href="#cb88-1128"></a>        tok_emb <span class="op">=</span> <span class="va">self</span>.token_embedding_table(idx)  <span class="co"># (B,T,C)</span></span>
<span id="cb88-1129"><a href="#cb88-1129"></a>        <span class="co"># Get position embeddings</span></span>
<span id="cb88-1130"><a href="#cb88-1130"></a>        pos_emb <span class="op">=</span> <span class="va">self</span>.position_embedding_table(torch.arange(T, device<span class="op">=</span>device))  <span class="co"># (T,C)</span></span>
<span id="cb88-1131"><a href="#cb88-1131"></a>        <span class="co"># Combine token and position embeddings</span></span>
<span id="cb88-1132"><a href="#cb88-1132"></a>        x <span class="op">=</span> tok_emb <span class="op">+</span> pos_emb  <span class="co"># (B,T,C)</span></span>
<span id="cb88-1133"><a href="#cb88-1133"></a>        <span class="co"># Process through transformer blocks</span></span>
<span id="cb88-1134"><a href="#cb88-1134"></a>        x <span class="op">=</span> <span class="va">self</span>.blocks(x)  <span class="co"># (B,T,C)</span></span>
<span id="cb88-1135"><a href="#cb88-1135"></a>        x <span class="op">=</span> <span class="va">self</span>.ln_f(x)  <span class="co"># (B,T,C)</span></span>
<span id="cb88-1136"><a href="#cb88-1136"></a>        logits <span class="op">=</span> <span class="va">self</span>.lm_head(x)  <span class="co"># (B,T,vocab_size)</span></span>
<span id="cb88-1137"><a href="#cb88-1137"></a></span>
<span id="cb88-1138"><a href="#cb88-1138"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb88-1139"><a href="#cb88-1139"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb88-1140"><a href="#cb88-1140"></a>        <span class="cf">else</span>:</span>
<span id="cb88-1141"><a href="#cb88-1141"></a>            <span class="co"># Reshape for loss calculation</span></span>
<span id="cb88-1142"><a href="#cb88-1142"></a>            B, T, C <span class="op">=</span> logits.shape</span>
<span id="cb88-1143"><a href="#cb88-1143"></a>            logits <span class="op">=</span> logits.view(B<span class="op">*</span>T, C)</span>
<span id="cb88-1144"><a href="#cb88-1144"></a>            targets <span class="op">=</span> targets.view(B<span class="op">*</span>T)</span>
<span id="cb88-1145"><a href="#cb88-1145"></a>            loss <span class="op">=</span> F.cross_entropy(logits, targets)</span>
<span id="cb88-1146"><a href="#cb88-1146"></a></span>
<span id="cb88-1147"><a href="#cb88-1147"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb88-1148"><a href="#cb88-1148"></a></span>
<span id="cb88-1149"><a href="#cb88-1149"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens):</span>
<span id="cb88-1150"><a href="#cb88-1150"></a>        <span class="co">"""Generate new text given a starting sequence."""</span></span>
<span id="cb88-1151"><a href="#cb88-1151"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb88-1152"><a href="#cb88-1152"></a>            <span class="co"># Crop context to block_size</span></span>
<span id="cb88-1153"><a href="#cb88-1153"></a>            idx_cond <span class="op">=</span> idx[:, <span class="op">-</span>block_size:]</span>
<span id="cb88-1154"><a href="#cb88-1154"></a>            <span class="co"># Get predictions</span></span>
<span id="cb88-1155"><a href="#cb88-1155"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>(idx_cond)</span>
<span id="cb88-1156"><a href="#cb88-1156"></a>            <span class="co"># Focus on last time step</span></span>
<span id="cb88-1157"><a href="#cb88-1157"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :]  <span class="co"># (B, C)</span></span>
<span id="cb88-1158"><a href="#cb88-1158"></a>            <span class="co"># Convert to probabilities</span></span>
<span id="cb88-1159"><a href="#cb88-1159"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)  <span class="co"># (B, C)</span></span>
<span id="cb88-1160"><a href="#cb88-1160"></a>            <span class="co"># Sample from distribution</span></span>
<span id="cb88-1161"><a href="#cb88-1161"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)  <span class="co"># (B, 1)</span></span>
<span id="cb88-1162"><a href="#cb88-1162"></a>            <span class="co"># Append to sequence</span></span>
<span id="cb88-1163"><a href="#cb88-1163"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>)  <span class="co"># (B, T+1)</span></span>
<span id="cb88-1164"><a href="#cb88-1164"></a>        <span class="cf">return</span> idx</span>
<span id="cb88-1165"><a href="#cb88-1165"></a><span class="in">```</span></span>
<span id="cb88-1166"><a href="#cb88-1166"></a></span>
<span id="cb88-1167"><a href="#cb88-1167"></a><span class="fu">### Training Loop</span></span>
<span id="cb88-1170"><a href="#cb88-1170"></a><span class="in">```{python}</span></span>
<span id="cb88-1171"><a href="#cb88-1171"></a><span class="co"># Create model instance</span></span>
<span id="cb88-1172"><a href="#cb88-1172"></a>model <span class="op">=</span> BigramLanguageModel()</span>
<span id="cb88-1173"><a href="#cb88-1173"></a>m <span class="op">=</span> model.to(device)</span>
<span id="cb88-1174"><a href="#cb88-1174"></a><span class="co"># Print number of parameters</span></span>
<span id="cb88-1175"><a href="#cb88-1175"></a><span class="bu">print</span>(<span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> m.parameters())<span class="op">/</span><span class="fl">1e6</span>, <span class="st">'M parameters'</span>)</span>
<span id="cb88-1176"><a href="#cb88-1176"></a></span>
<span id="cb88-1177"><a href="#cb88-1177"></a><span class="co"># Create optimizer</span></span>
<span id="cb88-1178"><a href="#cb88-1178"></a>optimizer <span class="op">=</span> torch.optim.AdamW(model.parameters(), lr<span class="op">=</span>learning_rate)</span>
<span id="cb88-1179"><a href="#cb88-1179"></a></span>
<span id="cb88-1180"><a href="#cb88-1180"></a><span class="co"># Training loop</span></span>
<span id="cb88-1181"><a href="#cb88-1181"></a><span class="cf">for</span> <span class="bu">iter</span> <span class="kw">in</span> <span class="bu">range</span>(max_iters):</span>
<span id="cb88-1182"><a href="#cb88-1182"></a>    <span class="co"># Evaluate loss periodically</span></span>
<span id="cb88-1183"><a href="#cb88-1183"></a>    <span class="cf">if</span> <span class="bu">iter</span> <span class="op">%</span> eval_interval <span class="op">==</span> <span class="dv">0</span> <span class="kw">or</span> <span class="bu">iter</span> <span class="op">==</span> max_iters <span class="op">-</span> <span class="dv">1</span>:</span>
<span id="cb88-1184"><a href="#cb88-1184"></a>        losses <span class="op">=</span> estimate_loss()</span>
<span id="cb88-1185"><a href="#cb88-1185"></a>        <span class="bu">print</span>(<span class="ss">f"step </span><span class="sc">{</span><span class="bu">iter</span><span class="sc">}</span><span class="ss">: train loss </span><span class="sc">{</span>losses[<span class="st">'train'</span>]<span class="sc">:.4f}</span><span class="ss">, val loss </span><span class="sc">{</span>losses[<span class="st">'val'</span>]<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb88-1186"><a href="#cb88-1186"></a></span>
<span id="cb88-1187"><a href="#cb88-1187"></a>    <span class="co"># Get batch of data</span></span>
<span id="cb88-1188"><a href="#cb88-1188"></a>    xb, yb <span class="op">=</span> get_batch(<span class="st">'train'</span>)</span>
<span id="cb88-1189"><a href="#cb88-1189"></a></span>
<span id="cb88-1190"><a href="#cb88-1190"></a>    <span class="co"># Forward pass</span></span>
<span id="cb88-1191"><a href="#cb88-1191"></a>    logits, loss <span class="op">=</span> model(xb, yb)</span>
<span id="cb88-1192"><a href="#cb88-1192"></a>    <span class="co"># Backward pass</span></span>
<span id="cb88-1193"><a href="#cb88-1193"></a>    optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb88-1194"><a href="#cb88-1194"></a>    loss.backward()</span>
<span id="cb88-1195"><a href="#cb88-1195"></a>    optimizer.step()</span>
<span id="cb88-1196"><a href="#cb88-1196"></a></span>
<span id="cb88-1197"><a href="#cb88-1197"></a><span class="co"># Generate text from the model</span></span>
<span id="cb88-1198"><a href="#cb88-1198"></a>context <span class="op">=</span> torch.zeros((<span class="dv">1</span>, <span class="dv">1</span>), dtype<span class="op">=</span>torch.<span class="bu">long</span>, device<span class="op">=</span>device)</span>
<span id="cb88-1199"><a href="#cb88-1199"></a><span class="bu">print</span>(decode(m.generate(context, max_new_tokens<span class="op">=</span><span class="dv">2000</span>)[<span class="dv">0</span>].tolist()))</span>
<span id="cb88-1200"><a href="#cb88-1200"></a><span class="in">```</span></span>
<span id="cb88-1201"><a href="#cb88-1201"></a></span>
<span id="cb88-1202"><a href="#cb88-1202"></a><span class="fu">## Key Takeaways</span></span>
<span id="cb88-1203"><a href="#cb88-1203"></a></span>
<span id="cb88-1204"><a href="#cb88-1204"></a><span class="ss">1. </span>**Data Preparation**</span>
<span id="cb88-1205"><a href="#cb88-1205"></a><span class="ss">   - </span>Tokenization: Convert text to numerical representations</span>
<span id="cb88-1206"><a href="#cb88-1206"></a><span class="ss">   - </span>Batching: Create efficient training batches</span>
<span id="cb88-1207"><a href="#cb88-1207"></a><span class="ss">   - </span>Context and Targets: Understand the prediction task</span>
<span id="cb88-1208"><a href="#cb88-1208"></a></span>
<span id="cb88-1209"><a href="#cb88-1209"></a><span class="ss">2. </span>**Model Architecture**</span>
<span id="cb88-1210"><a href="#cb88-1210"></a><span class="ss">   - </span>Token and Position Embeddings</span>
<span id="cb88-1211"><a href="#cb88-1211"></a><span class="ss">   - </span>Self-Attention Mechanism</span>
<span id="cb88-1212"><a href="#cb88-1212"></a><span class="ss">   - </span>Transformer Blocks</span>
<span id="cb88-1213"><a href="#cb88-1213"></a><span class="ss">   - </span>Layer Normalization</span>
<span id="cb88-1214"><a href="#cb88-1214"></a><span class="ss">   - </span>Language Model Head</span>
<span id="cb88-1215"><a href="#cb88-1215"></a></span>
<span id="cb88-1216"><a href="#cb88-1216"></a><span class="ss">3. </span>**Training Process**</span>
<span id="cb88-1217"><a href="#cb88-1217"></a><span class="ss">   - </span>Loss Function: Cross-Entropy</span>
<span id="cb88-1218"><a href="#cb88-1218"></a><span class="ss">   - </span>Optimization: AdamW</span>
<span id="cb88-1219"><a href="#cb88-1219"></a><span class="ss">   - </span>Evaluation: Periodic validation checks</span>
<span id="cb88-1220"><a href="#cb88-1220"></a><span class="ss">   - </span>Generation: Autoregressive text generation</span>
<span id="cb88-1221"><a href="#cb88-1221"></a></span>
<span id="cb88-1222"><a href="#cb88-1222"></a><span class="ss">4. </span>**Key Concepts**</span>
<span id="cb88-1223"><a href="#cb88-1223"></a><span class="ss">   - </span>Self-Attention: Communication between tokens</span>
<span id="cb88-1224"><a href="#cb88-1224"></a><span class="ss">   - </span>Positional Encoding: Preserving sequence order</span>
<span id="cb88-1225"><a href="#cb88-1225"></a><span class="ss">   - </span>Residual Connections: Helping gradient flow</span>
<span id="cb88-1226"><a href="#cb88-1226"></a><span class="ss">   - </span>Layer Normalization: Stabilizing training</span>
<span id="cb88-1227"><a href="#cb88-1227"></a><span class="ss">   - </span>Multi-Head Attention: Capturing different relationships</span>
<span id="cb88-1228"><a href="#cb88-1228"></a><span class="in">```</span></span>
<span id="cb88-1229"><a href="#cb88-1229"></a></span>
</code><button title="Copy to Clipboard" class="code-copy-button" data-in-quarto-modal=""><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->




</body></html>