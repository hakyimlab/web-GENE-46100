<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Haky Im">
<meta name="dcterms.date" content="2025-05-01">

<title>GENE 46100 - Enformer Architecture</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">GENE 46100</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../index.html">
 <span class="menu-text">Home</span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#enformer-architecture-overview" id="toc-enformer-architecture-overview" class="nav-link active" data-scroll-target="#enformer-architecture-overview">Enformer Architecture Overview</a>
  <ul class="collapse">
  <li><a href="#what-is-enformer" id="toc-what-is-enformer" class="nav-link" data-scroll-target="#what-is-enformer">What is Enformer?</a></li>
  <li><a href="#key-components-in-the-code" id="toc-key-components-in-the-code" class="nav-link" data-scroll-target="#key-components-in-the-code">Key Components in the Code</a>
  <ul class="collapse">
  <li><a href="#enformer-class" id="toc-enformer-class" class="nav-link" data-scroll-target="#enformer-class">1. Enformer Class</a></li>
  <li><a href="#supporting-modules" id="toc-supporting-modules" class="nav-link" data-scroll-target="#supporting-modules">2. Supporting Modules</a></li>
  <li><a href="#forward-pass" id="toc-forward-pass" class="nav-link" data-scroll-target="#forward-pass">3. Forward Pass</a></li>
  </ul></li>
  <li><a href="#why-is-enformer-special" id="toc-why-is-enformer-special" class="nav-link" data-scroll-target="#why-is-enformer-special">Why is Enformer Special?</a></li>
  <li><a href="#example-usage" id="toc-example-usage" class="nav-link" data-scroll-target="#example-usage">Example Usage</a></li>
  <li><a href="#summary-table" id="toc-summary-table" class="nav-link" data-scroll-target="#summary-table">Summary Table</a></li>
  <li><a href="#enformer-class-implementation" id="toc-enformer-class-implementation" class="nav-link" data-scroll-target="#enformer-class-implementation">Enformer Class Implementation</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Enformer Architecture</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
  <div class="quarto-categories">
    <div class="quarto-category">gene46100</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Haky Im </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">May 1, 2025</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="enformer-architecture-overview" class="level1">
<h1>Enformer Architecture Overview</h1>
<section id="what-is-enformer" class="level2">
<h2 class="anchored" data-anchor-id="what-is-enformer">What is Enformer?</h2>
<p>Enformer is a deep learning model designed for predicting gene expression and other functional genomics signals directly from DNA sequence. It was introduced by DeepMind and Calico, and is notable for its ability to model long-range interactions in DNA using transformer architectures.</p>
</section>
<section id="key-components-in-the-code" class="level2">
<h2 class="anchored" data-anchor-id="key-components-in-the-code">Key Components in the Code</h2>
<section id="enformer-class" class="level3">
<h3 class="anchored" data-anchor-id="enformer-class">1. Enformer Class</h3>
<p>The main model class (<code>Enformer</code>) inherits from <code>snt.Module</code> (Sonnet, a neural network library).</p>
<section id="inputs-and-outputs" class="level4">
<h4 class="anchored" data-anchor-id="inputs-and-outputs">Inputs and Outputs</h4>
<ul>
<li><strong>Inputs</strong>: DNA sequence (as a one-hot encoded tensor)</li>
<li><strong>Outputs</strong>: Predictions for multiple genomic tracks (e.g., gene expression, chromatin accessibility) for human and mouse</li>
</ul>
</section>
<section id="constructor-arguments" class="level4">
<h4 class="anchored" data-anchor-id="constructor-arguments">Constructor Arguments</h4>
<ul>
<li><code>channels</code>: Model width (number of convolutional filters)</li>
<li><code>num_transformer_layers</code>: Number of transformer blocks</li>
<li><code>num_heads</code>: Number of attention heads in each transformer block</li>
<li><code>pooling_type</code>: Type of pooling (attention or max)</li>
<li><code>name</code>: Module name</li>
</ul>
</section>
<section id="model-structure" class="level4">
<h4 class="anchored" data-anchor-id="model-structure">Model Structure</h4>
<ol type="1">
<li><strong>Stem</strong>: Initial convolution and pooling layers to process the input</li>
<li><strong>Convolutional Tower</strong>: Series of convolutional blocks with increasing filter sizes, interleaved with pooling</li>
<li><strong>Transformer Blocks</strong>: Stack of transformer layers to capture long-range dependencies</li>
<li><strong>Crop Layer</strong>: Crops the output to a fixed target length</li>
<li><strong>Final Pointwise Layer</strong>: Final convolution and activation</li>
<li><strong>Heads</strong>: Separate output layers for human and mouse predictions</li>
</ol>
</section>
</section>
<section id="supporting-modules" class="level3">
<h3 class="anchored" data-anchor-id="supporting-modules">2. Supporting Modules</h3>
<ul>
<li><strong>Sequential</strong>: Custom sequential container that passes is_training flag to layers that accept it</li>
<li><strong>Residual</strong>: Residual connection wrapper (adds input to output of a submodule)</li>
<li><strong>SoftmaxPooling1D</strong>: Custom pooling layer that uses softmax-weighted pooling</li>
<li><strong>TargetLengthCrop1D</strong>: Crops the sequence to a target length (centered)</li>
<li><strong>gelu</strong>: Gaussian Error Linear Unit activation function</li>
<li><strong>one_hot_encode</strong>: Utility to convert DNA sequence strings to one-hot encoded numpy arrays</li>
</ul>
</section>
<section id="forward-pass" class="level3">
<h3 class="anchored" data-anchor-id="forward-pass">3. Forward Pass</h3>
<p>The model processes the input sequence through: 1. The stem 2. Convolutional tower 3. Transformer stack 4. Cropping 5. Final pointwise layer</p>
<p>The resulting embedding is passed to each output head (for human and mouse), producing the final predictions.</p>
</section>
</section>
<section id="why-is-enformer-special" class="level2">
<h2 class="anchored" data-anchor-id="why-is-enformer-special">Why is Enformer Special?</h2>
<ul>
<li><strong>Long-range modeling</strong>: Uses transformers to capture dependencies across hundreds of thousands of base pairs</li>
<li><strong>Multi-task</strong>: Predicts thousands of genomic signals at once</li>
<li><strong>Flexible pooling</strong>: Uses attention-based pooling to better aggregate information</li>
</ul>
</section>
<section id="example-usage" class="level2">
<h2 class="anchored" data-anchor-id="example-usage">Example Usage</h2>
<ul>
<li><strong>Input</strong>: One-hot encoded DNA sequence of length 196,608 (with 4 channels for A, C, G, T)</li>
<li><strong>Output</strong>: Dictionary with predictions for human and mouse tracks</li>
</ul>
</section>
<section id="summary-table" class="level2">
<h2 class="anchored" data-anchor-id="summary-table">Summary Table</h2>
<table class="table">
<thead>
<tr class="header">
<th>Component</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Stem</td>
<td>Initial feature extraction</td>
</tr>
<tr class="even">
<td>Conv Tower</td>
<td>Hierarchical feature extraction</td>
</tr>
<tr class="odd">
<td>Transformer</td>
<td>Long-range dependency modeling</td>
</tr>
<tr class="even">
<td>Crop</td>
<td>Fix output length</td>
</tr>
<tr class="odd">
<td>Final Pointwise</td>
<td>Final feature transformation</td>
</tr>
<tr class="even">
<td>Heads</td>
<td>Task-specific outputs (human/mouse)</td>
</tr>
</tbody>
</table>
</section>
<section id="enformer-class-implementation" class="level2">
<h2 class="anchored" data-anchor-id="enformer-class-implementation">Enformer Class Implementation</h2>
<p>The following code shows the TensorFlow implementation of the Enformer model, downloaded from <a href="https://github.com/google-deepmind/deepmind-research/blob/master/enformer/enformer.py">enformer.py</a> and annotated with Gemini.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Copyright 2021 DeepMind Technologies Limited</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Licensed under the Apache License, Version 2.0</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">"""Tensorflow implementation of Enformer model.</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">"Effective gene expression prediction from sequence by integrating long-range interactions"</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">Authors: Žiga Avsec1, Vikram Agarwal2,4, Daniel Visentin1,4, Joseph R. Ledsam1,3,</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">Agnieszka Grabska-Barwinska1, Kyle R. Taylor1, Yannis Assael1, John Jumper1,</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">Pushmeet Kohli1, David R. Kelley2*</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">1 DeepMind, London, UK</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">2 Calico Life Sciences, South San Francisco, CA, USA</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">3 Google, Tokyo, Japan</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">4 These authors contributed equally.</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">* correspondence: avsec@google.com, pushmeet@google.com, drk@calicolabs.com</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> inspect</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Any, Callable, Dict, Optional, Text, Union, Iterable</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> attention_module</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sonnet <span class="im">as</span> snt</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Model configuration constants</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>SEQUENCE_LENGTH <span class="op">=</span> <span class="dv">196_608</span>  <span class="co"># Input DNA sequence length</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>BIN_SIZE <span class="op">=</span> <span class="dv">128</span>            <span class="co"># Output bin size</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>TARGET_LENGTH <span class="op">=</span> <span class="dv">896</span>       <span class="co"># Target output length</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Enformer(snt.Module):</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Main model for predicting genomic signals from DNA sequence."""</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>                 channels: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1536</span>,              <span class="co"># Model width/dimensionality</span></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>                 num_transformer_layers: <span class="bu">int</span> <span class="op">=</span> <span class="dv">11</span>,  <span class="co"># Number of transformer blocks</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>                 num_heads: <span class="bu">int</span> <span class="op">=</span> <span class="dv">8</span>,                <span class="co"># Number of attention heads</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>                 pooling_type: <span class="bu">str</span> <span class="op">=</span> <span class="st">'attention'</span>,   <span class="co"># Pooling type ('attention' or 'max')</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>                 name: <span class="bu">str</span> <span class="op">=</span> <span class="st">'enformer'</span>):           <span class="co"># Module name</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(name<span class="op">=</span>name)</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output channels for different species</span></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        heads_channels <span class="op">=</span> {<span class="st">'human'</span>: <span class="dv">5313</span>, <span class="st">'mouse'</span>: <span class="dv">1643</span>}</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        dropout_rate <span class="op">=</span> <span class="fl">0.4</span></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Validate model configuration</span></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> channels <span class="op">%</span> num_heads <span class="op">==</span> <span class="dv">0</span>, (<span class="st">'channels must be divisible by num_heads'</span>)</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Multi-head attention configuration</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        whole_attention_kwargs <span class="op">=</span> {</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>            <span class="st">'attention_dropout_rate'</span>: <span class="fl">0.05</span>,</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>            <span class="st">'initializer'</span>: <span class="va">None</span>,</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>            <span class="st">'key_size'</span>: <span class="dv">64</span>,</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>            <span class="st">'num_heads'</span>: num_heads,</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>            <span class="st">'num_relative_position_features'</span>: channels <span class="op">//</span> num_heads,</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>            <span class="st">'positional_dropout_rate'</span>: <span class="fl">0.01</span>,</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>            <span class="st">'relative_position_functions'</span>: [</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>                <span class="st">'positional_features_exponential'</span>,</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>                <span class="st">'positional_features_central_mask'</span>,</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>                <span class="st">'positional_features_gamma'</span></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>            ],</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>            <span class="st">'relative_positions'</span>: <span class="va">True</span>,</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>            <span class="st">'scaling'</span>: <span class="va">True</span>,</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a>            <span class="st">'value_size'</span>: channels <span class="op">//</span> num_heads,</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>            <span class="st">'zero_initialize'</span>: <span class="va">True</span></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Build model trunk</span></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> tf.name_scope(<span class="st">'trunk'</span>):</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Convolutional block helper</span></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>            <span class="kw">def</span> conv_block(filters, width<span class="op">=</span><span class="dv">1</span>, w_init<span class="op">=</span><span class="va">None</span>, name<span class="op">=</span><span class="st">'conv_block'</span>, <span class="op">**</span>kwargs):</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> Sequential(<span class="kw">lambda</span>: [</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>                    snt.distribute.CrossReplicaBatchNorm(</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>                        create_scale<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>                        create_offset<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>                        scale_init<span class="op">=</span>snt.initializers.Ones(),</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a>                        moving_mean<span class="op">=</span>snt.ExponentialMovingAverage(<span class="fl">0.9</span>),</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>                        moving_variance<span class="op">=</span>snt.ExponentialMovingAverage(<span class="fl">0.9</span>)),</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>                    gelu,</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>                    snt.Conv1D(filters, width, w_init<span class="op">=</span>w_init, <span class="op">**</span>kwargs)</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>                ], name<span class="op">=</span>name)</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Initial stem</span></span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>            stem <span class="op">=</span> Sequential(<span class="kw">lambda</span>: [</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>                snt.Conv1D(channels <span class="op">//</span> <span class="dv">2</span>, <span class="dv">15</span>),</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>                Residual(conv_block(channels <span class="op">//</span> <span class="dv">2</span>, <span class="dv">1</span>, name<span class="op">=</span><span class="st">'pointwise_conv_block'</span>)),</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>                pooling_module(pooling_type, pool_size<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>            ], name<span class="op">=</span><span class="st">'stem'</span>)</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Convolutional tower</span></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>            filter_list <span class="op">=</span> exponential_linspace_int(</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a>                start<span class="op">=</span>channels <span class="op">//</span> <span class="dv">2</span>, </span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>                end<span class="op">=</span>channels,</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>                num<span class="op">=</span><span class="dv">6</span>, </span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a>                divisible_by<span class="op">=</span><span class="dv">128</span></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>            conv_tower <span class="op">=</span> Sequential(<span class="kw">lambda</span>: [</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a>                Sequential(<span class="kw">lambda</span>: [</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>                    conv_block(num_filters, <span class="dv">5</span>),</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a>                    Residual(conv_block(num_filters, <span class="dv">1</span>, name<span class="op">=</span><span class="st">'pointwise_conv_block'</span>)),</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>                    pooling_module(pooling_type, pool_size<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>                ], name<span class="op">=</span><span class="ss">f'conv_tower_block_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i, num_filters <span class="kw">in</span> <span class="bu">enumerate</span>(filter_list)</span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a>            ], name<span class="op">=</span><span class="st">'conv_tower'</span>)</span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Transformer blocks</span></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>            <span class="kw">def</span> transformer_mlp():</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> Sequential(<span class="kw">lambda</span>: [</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a>                    snt.LayerNorm(axis<span class="op">=-</span><span class="dv">1</span>, create_scale<span class="op">=</span><span class="va">True</span>, create_offset<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a>                    snt.Linear(channels <span class="op">*</span> <span class="dv">2</span>),</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>                    snt.Dropout(dropout_rate),</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a>                    tf.nn.relu,</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>                    snt.Linear(channels),</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a>                    snt.Dropout(dropout_rate)</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a>                ], name<span class="op">=</span><span class="st">'mlp'</span>)</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>            transformer <span class="op">=</span> Sequential(<span class="kw">lambda</span>: [</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>                Sequential(<span class="kw">lambda</span>: [</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a>                    Residual(Sequential(<span class="kw">lambda</span>: [</span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a>                        snt.LayerNorm(axis<span class="op">=-</span><span class="dv">1</span>, create_scale<span class="op">=</span><span class="va">True</span>, create_offset<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>                                    scale_init<span class="op">=</span>snt.initializers.Ones()),</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a>                        attention_module.MultiheadAttention(<span class="op">**</span>whole_attention_kwargs,</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>                                                          name<span class="op">=</span><span class="ss">f'attention_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>),</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>                        snt.Dropout(dropout_rate)</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a>                    ], name<span class="op">=</span><span class="st">'mha'</span>)),</span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>                    Residual(transformer_mlp())</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a>                ], name<span class="op">=</span><span class="ss">f'transformer_block_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_transformer_layers)</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>            ], name<span class="op">=</span><span class="st">'transformer'</span>)</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Final layers</span></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a>            crop_final <span class="op">=</span> TargetLengthCrop1D(TARGET_LENGTH, name<span class="op">=</span><span class="st">'target_input'</span>)</span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>            final_pointwise <span class="op">=</span> Sequential(<span class="kw">lambda</span>: [</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a>                conv_block(channels <span class="op">*</span> <span class="dv">2</span>, <span class="dv">1</span>),</span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>                snt.Dropout(dropout_rate <span class="op">/</span> <span class="dv">8</span>),</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a>                gelu</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a>            ], name<span class="op">=</span><span class="st">'final_pointwise'</span>)</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Combine trunk modules</span></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._trunk <span class="op">=</span> Sequential([</span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>                stem,</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a>                conv_tower,</span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a>                transformer,</span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>                crop_final,</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>                final_pointwise</span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a>            ], name<span class="op">=</span><span class="st">'trunk'</span>)</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output heads</span></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> tf.name_scope(<span class="st">'heads'</span>):</span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._heads <span class="op">=</span> {</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>                head: Sequential(</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a>                    <span class="kw">lambda</span>: [snt.Linear(num_channels), tf.nn.softplus],</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a>                    name<span class="op">=</span><span class="ss">f'head_</span><span class="sc">{</span>head<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> head, num_channels <span class="kw">in</span> heads_channels.items()</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> trunk(<span class="va">self</span>):</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._trunk</span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> heads(<span class="va">self</span>):</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._heads</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, inputs: tf.Tensor, is_training: <span class="bu">bool</span>) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, tf.Tensor]:</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Forward pass through the model."""</span></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a>        trunk_embedding <span class="op">=</span> <span class="va">self</span>.trunk(inputs, is_training<span class="op">=</span>is_training)</span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>            head: head_module(trunk_embedding, is_training<span class="op">=</span>is_training)</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> head, head_module <span class="kw">in</span> <span class="va">self</span>.heads.items()</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>    <span class="at">@tf.function</span>(input_signature<span class="op">=</span>[</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a>        tf.TensorSpec([<span class="va">None</span>, SEQUENCE_LENGTH, <span class="dv">4</span>], tf.float32)])</span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_on_batch(<span class="va">self</span>, x):</span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Prediction method for SavedModel."""</span></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>(x, is_training<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TargetLengthCrop1D(snt.Module):</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Crops sequence to target length."""</span></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, target_length: Optional[<span class="bu">int</span>], name: <span class="bu">str</span> <span class="op">=</span> <span class="st">'target_length_crop'</span>):</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(name<span class="op">=</span>name)</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._target_length <span class="op">=</span> target_length</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, inputs):</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>._target_length <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> inputs</span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>        trim <span class="op">=</span> (inputs.shape[<span class="op">-</span><span class="dv">2</span>] <span class="op">-</span> <span class="va">self</span>._target_length) <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> trim <span class="op">&lt;</span> <span class="dv">0</span>:</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">'inputs longer than target length'</span>)</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> trim <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> inputs</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> inputs[..., trim:<span class="op">-</span>trim, :]</span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Sequential(snt.Module):</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Sequential container with is_training support."""</span></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>                 layers: Optional[Union[Callable[[], Iterable[snt.Module]],</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a>                                      Iterable[Callable[..., Any]]]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a>                 name: Optional[Text] <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(name<span class="op">=</span>name)</span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> layers <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._layers <span class="op">=</span> []</span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">hasattr</span>(layers, <span class="st">'__call__'</span>):</span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>                layers <span class="op">=</span> layers()</span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._layers <span class="op">=</span> [layer <span class="cf">for</span> layer <span class="kw">in</span> layers <span class="cf">if</span> layer <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>]</span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, inputs: tf.Tensor, is_training: <span class="bu">bool</span>, <span class="op">**</span>kwargs):</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> inputs</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> mod <span class="kw">in</span> <span class="va">self</span>._layers:</span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> accepts_is_training(mod):</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a>                outputs <span class="op">=</span> mod(outputs, is_training<span class="op">=</span>is_training, <span class="op">**</span>kwargs)</span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a>                outputs <span class="op">=</span> mod(outputs, <span class="op">**</span>kwargs)</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> outputs</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pooling_module(kind, pool_size):</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns appropriate pooling module."""</span></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> kind <span class="op">==</span> <span class="st">'attention'</span>:</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> SoftmaxPooling1D(pool_size<span class="op">=</span>pool_size, per_channel<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a>                              w_init_scale<span class="op">=</span><span class="fl">2.0</span>)</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> kind <span class="op">==</span> <span class="st">'max'</span>:</span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tf.keras.layers.MaxPool1D(pool_size<span class="op">=</span>pool_size, padding<span class="op">=</span><span class="st">'same'</span>)</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f'Invalid pooling kind: </span><span class="sc">{</span>kind<span class="sc">}</span><span class="ss">.'</span>)</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SoftmaxPooling1D(snt.Module):</span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Softmax-weighted pooling operation."""</span></span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a>                 pool_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2</span>,</span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a>                 per_channel: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a>                 w_init_scale: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.0</span>,</span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a>                 name: <span class="bu">str</span> <span class="op">=</span> <span class="st">'softmax_pooling'</span>):</span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(name<span class="op">=</span>name)</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._pool_size <span class="op">=</span> pool_size</span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._per_channel <span class="op">=</span> per_channel</span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._w_init_scale <span class="op">=</span> w_init_scale</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._logit_linear <span class="op">=</span> <span class="va">None</span></span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>    <span class="at">@snt.once</span></span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _initialize(<span class="va">self</span>, num_features):</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._logit_linear <span class="op">=</span> snt.Linear(</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a>            output_size<span class="op">=</span>num_features <span class="cf">if</span> <span class="va">self</span>._per_channel <span class="cf">else</span> <span class="dv">1</span>,</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>            with_bias<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a>            w_init<span class="op">=</span>snt.initializers.Identity(<span class="va">self</span>._w_init_scale))</span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, inputs):</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a>        _, length, num_features <span class="op">=</span> inputs.shape</span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._initialize(num_features)</span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> tf.reshape(</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a>            inputs,</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a>            (<span class="op">-</span><span class="dv">1</span>, length <span class="op">//</span> <span class="va">self</span>._pool_size, <span class="va">self</span>._pool_size, num_features))</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tf.reduce_sum(</span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a>            inputs <span class="op">*</span> tf.nn.softmax(<span class="va">self</span>._logit_linear(inputs), axis<span class="op">=-</span><span class="dv">2</span>),</span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a>            axis<span class="op">=-</span><span class="dv">2</span>)</span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Residual(snt.Module):</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Residual connection wrapper."""</span></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, module: snt.Module, name<span class="op">=</span><span class="st">'residual'</span>):</span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(name<span class="op">=</span>name)</span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._module <span class="op">=</span> module</span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, inputs: tf.Tensor, is_training: <span class="bu">bool</span>, <span class="op">*</span>args,</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a>                 <span class="op">**</span>kwargs) <span class="op">-&gt;</span> tf.Tensor:</span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> inputs <span class="op">+</span> <span class="va">self</span>._module(inputs, is_training, <span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gelu(x: tf.Tensor) <span class="op">-&gt;</span> tf.Tensor:</span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Gaussian Error Linear Unit activation function."""</span></span>
<span id="cb1-275"><a href="#cb1-275" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf.nn.sigmoid(<span class="fl">1.702</span> <span class="op">*</span> x) <span class="op">*</span> x</span>
<span id="cb1-276"><a href="#cb1-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-277"><a href="#cb1-277" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> one_hot_encode(sequence: <span class="bu">str</span>,</span>
<span id="cb1-278"><a href="#cb1-278" aria-hidden="true" tabindex="-1"></a>                   alphabet: <span class="bu">str</span> <span class="op">=</span> <span class="st">'ACGT'</span>,</span>
<span id="cb1-279"><a href="#cb1-279" aria-hidden="true" tabindex="-1"></a>                   neutral_alphabet: <span class="bu">str</span> <span class="op">=</span> <span class="st">'N'</span>,</span>
<span id="cb1-280"><a href="#cb1-280" aria-hidden="true" tabindex="-1"></a>                   neutral_value: Any <span class="op">=</span> <span class="dv">0</span>,</span>
<span id="cb1-281"><a href="#cb1-281" aria-hidden="true" tabindex="-1"></a>                   dtype<span class="op">=</span>np.float32) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb1-282"><a href="#cb1-282" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""One-hot encodes DNA sequence."""</span></span>
<span id="cb1-283"><a href="#cb1-283" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> to_uint8(string):</span>
<span id="cb1-284"><a href="#cb1-284" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.frombuffer(string.encode(<span class="st">'ascii'</span>), dtype<span class="op">=</span>np.uint8)</span>
<span id="cb1-285"><a href="#cb1-285" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-286"><a href="#cb1-286" aria-hidden="true" tabindex="-1"></a>    hash_table <span class="op">=</span> np.zeros((np.iinfo(np.uint8).<span class="bu">max</span>, <span class="bu">len</span>(alphabet)), dtype<span class="op">=</span>dtype)</span>
<span id="cb1-287"><a href="#cb1-287" aria-hidden="true" tabindex="-1"></a>    hash_table[to_uint8(alphabet)] <span class="op">=</span> np.eye(<span class="bu">len</span>(alphabet), dtype<span class="op">=</span>dtype)</span>
<span id="cb1-288"><a href="#cb1-288" aria-hidden="true" tabindex="-1"></a>    hash_table[to_uint8(neutral_alphabet)] <span class="op">=</span> neutral_value</span>
<span id="cb1-289"><a href="#cb1-289" aria-hidden="true" tabindex="-1"></a>    hash_table <span class="op">=</span> hash_table.astype(dtype)</span>
<span id="cb1-290"><a href="#cb1-290" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> hash_table[to_uint8(sequence)]</span>
<span id="cb1-291"><a href="#cb1-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-292"><a href="#cb1-292" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> exponential_linspace_int(start, end, num, divisible_by<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb1-293"><a href="#cb1-293" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Generates exponentially spaced integers."""</span></span>
<span id="cb1-294"><a href="#cb1-294" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _round(x):</span>
<span id="cb1-295"><a href="#cb1-295" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">int</span>(np.<span class="bu">round</span>(x <span class="op">/</span> divisible_by) <span class="op">*</span> divisible_by)</span>
<span id="cb1-296"><a href="#cb1-296" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-297"><a href="#cb1-297" aria-hidden="true" tabindex="-1"></a>    base <span class="op">=</span> np.exp(np.log(end <span class="op">/</span> start) <span class="op">/</span> (num <span class="op">-</span> <span class="dv">1</span>))</span>
<span id="cb1-298"><a href="#cb1-298" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [_round(start <span class="op">*</span> base<span class="op">**</span>i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num)]</span>
<span id="cb1-299"><a href="#cb1-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-300"><a href="#cb1-300" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> accepts_is_training(module):</span>
<span id="cb1-301"><a href="#cb1-301" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Checks if module accepts is_training parameter."""</span></span>
<span id="cb1-302"><a href="#cb1-302" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">'is_training'</span> <span class="kw">in</span> <span class="bu">list</span>(inspect.signature(module.<span class="fu">__call__</span>).parameters)</span>
<span id="cb1-303"><a href="#cb1-303" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>


<!-- -->

</section>
</section>

<p>© <a href="https://hakyimlab.org">HakyImLab and Listed Authors</a> - <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0 License</a></p></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb2" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> Enformer Architecture</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> Haky Im</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 2025-05-01</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">  - gene46100</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="an">eval:</span><span class="co"> false</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="fu"># Enformer Architecture Overview</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="fu">## What is Enformer?</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>Enformer is a deep learning model designed for predicting gene expression and other functional genomics signals directly from DNA sequence. It was introduced by DeepMind and Calico, and is notable for its ability to model long-range interactions in DNA using transformer architectures.</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="fu">## Key Components in the Code</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a><span class="fu">### 1. Enformer Class</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>The main model class (<span class="in">`Enformer`</span>) inherits from <span class="in">`snt.Module`</span> (Sonnet, a neural network library).</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Inputs and Outputs</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Inputs**: DNA sequence (as a one-hot encoded tensor)</span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Outputs**: Predictions for multiple genomic tracks (e.g., gene expression, chromatin accessibility) for human and mouse</span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Constructor Arguments</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`channels`</span>: Model width (number of convolutional filters)</span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`num_transformer_layers`</span>: Number of transformer blocks</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`num_heads`</span>: Number of attention heads in each transformer block</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`pooling_type`</span>: Type of pooling (attention or max)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="in">`name`</span>: Module name</span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a><span class="fu">#### Model Structure</span></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>**Stem**: Initial convolution and pooling layers to process the input</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>**Convolutional Tower**: Series of convolutional blocks with increasing filter sizes, interleaved with pooling</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>**Transformer Blocks**: Stack of transformer layers to capture long-range dependencies</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>**Crop Layer**: Crops the output to a fixed target length</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>**Final Pointwise Layer**: Final convolution and activation</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a><span class="ss">6. </span>**Heads**: Separate output layers for human and mouse predictions</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a><span class="fu">### 2. Supporting Modules</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Sequential**: Custom sequential container that passes is_training flag to layers that accept it</span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Residual**: Residual connection wrapper (adds input to output of a submodule)</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**SoftmaxPooling1D**: Custom pooling layer that uses softmax-weighted pooling</span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**TargetLengthCrop1D**: Crops the sequence to a target length (centered)</span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**gelu**: Gaussian Error Linear Unit activation function</span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**one_hot_encode**: Utility to convert DNA sequence strings to one-hot encoded numpy arrays</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a><span class="fu">### 3. Forward Pass</span></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>The model processes the input sequence through:</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a><span class="ss">1. </span>The stem</span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a><span class="ss">2. </span>Convolutional tower</span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a><span class="ss">3. </span>Transformer stack</span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a><span class="ss">4. </span>Cropping</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a><span class="ss">5. </span>Final pointwise layer</span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a>The resulting embedding is passed to each output head (for human and mouse), producing the final predictions.</span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a><span class="fu">## Why is Enformer Special?</span></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Long-range modeling**: Uses transformers to capture dependencies across hundreds of thousands of base pairs</span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Multi-task**: Predicts thousands of genomic signals at once</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Flexible pooling**: Uses attention-based pooling to better aggregate information</span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a><span class="fu">## Example Usage</span></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Input**: One-hot encoded DNA sequence of length 196,608 (with 4 channels for A, C, G, T)</span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Output**: Dictionary with predictions for human and mouse tracks</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a><span class="fu">## Summary Table</span></span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>| Component | Purpose |</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>|-----------|---------|</span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>| Stem | Initial feature extraction |</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a>| Conv Tower | Hierarchical feature extraction |</span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>| Transformer | Long-range dependency modeling |</span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>| Crop | Fix output length |</span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>| Final Pointwise | Final feature transformation |</span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a>| Heads | Task-specific outputs (human/mouse) |</span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a><span class="fu">## Enformer Class Implementation</span></span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a>The following code shows the TensorFlow implementation of the Enformer model, downloaded from <span class="co">[</span><span class="ot">enformer.py</span><span class="co">](https://github.com/google-deepmind/deepmind-research/blob/master/enformer/enformer.py)</span> and annotated with Gemini.</span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a><span class="co"># Copyright 2021 DeepMind Technologies Limited</span></span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a><span class="co"># Licensed under the Apache License, Version 2.0</span></span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-92"><a href="#cb2-92" aria-hidden="true" tabindex="-1"></a><span class="co">"""Tensorflow implementation of Enformer model.</span></span>
<span id="cb2-93"><a href="#cb2-93" aria-hidden="true" tabindex="-1"></a><span class="co">"Effective gene expression prediction from sequence by integrating long-range interactions"</span></span>
<span id="cb2-94"><a href="#cb2-94" aria-hidden="true" tabindex="-1"></a><span class="co">Authors: Žiga Avsec1, Vikram Agarwal2,4, Daniel Visentin1,4, Joseph R. Ledsam1,3,</span></span>
<span id="cb2-95"><a href="#cb2-95" aria-hidden="true" tabindex="-1"></a><span class="co">Agnieszka Grabska-Barwinska1, Kyle R. Taylor1, Yannis Assael1, John Jumper1,</span></span>
<span id="cb2-96"><a href="#cb2-96" aria-hidden="true" tabindex="-1"></a><span class="co">Pushmeet Kohli1, David R. Kelley2*</span></span>
<span id="cb2-97"><a href="#cb2-97" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-98"><a href="#cb2-98" aria-hidden="true" tabindex="-1"></a><span class="co">1 DeepMind, London, UK</span></span>
<span id="cb2-99"><a href="#cb2-99" aria-hidden="true" tabindex="-1"></a><span class="co">2 Calico Life Sciences, South San Francisco, CA, USA</span></span>
<span id="cb2-100"><a href="#cb2-100" aria-hidden="true" tabindex="-1"></a><span class="co">3 Google, Tokyo, Japan</span></span>
<span id="cb2-101"><a href="#cb2-101" aria-hidden="true" tabindex="-1"></a><span class="co">4 These authors contributed equally.</span></span>
<span id="cb2-102"><a href="#cb2-102" aria-hidden="true" tabindex="-1"></a><span class="co">* correspondence: avsec@google.com, pushmeet@google.com, drk@calicolabs.com</span></span>
<span id="cb2-103"><a href="#cb2-103" aria-hidden="true" tabindex="-1"></a><span class="co">"""</span></span>
<span id="cb2-104"><a href="#cb2-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-105"><a href="#cb2-105" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> inspect</span>
<span id="cb2-106"><a href="#cb2-106" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Any, Callable, Dict, Optional, Text, Union, Iterable</span>
<span id="cb2-107"><a href="#cb2-107" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> attention_module</span>
<span id="cb2-108"><a href="#cb2-108" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb2-109"><a href="#cb2-109" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> sonnet <span class="im">as</span> snt</span>
<span id="cb2-110"><a href="#cb2-110" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb2-111"><a href="#cb2-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-112"><a href="#cb2-112" aria-hidden="true" tabindex="-1"></a><span class="co"># Model configuration constants</span></span>
<span id="cb2-113"><a href="#cb2-113" aria-hidden="true" tabindex="-1"></a>SEQUENCE_LENGTH <span class="op">=</span> <span class="dv">196_608</span>  <span class="co"># Input DNA sequence length</span></span>
<span id="cb2-114"><a href="#cb2-114" aria-hidden="true" tabindex="-1"></a>BIN_SIZE <span class="op">=</span> <span class="dv">128</span>            <span class="co"># Output bin size</span></span>
<span id="cb2-115"><a href="#cb2-115" aria-hidden="true" tabindex="-1"></a>TARGET_LENGTH <span class="op">=</span> <span class="dv">896</span>       <span class="co"># Target output length</span></span>
<span id="cb2-116"><a href="#cb2-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-117"><a href="#cb2-117" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Enformer(snt.Module):</span>
<span id="cb2-118"><a href="#cb2-118" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Main model for predicting genomic signals from DNA sequence."""</span></span>
<span id="cb2-119"><a href="#cb2-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-120"><a href="#cb2-120" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb2-121"><a href="#cb2-121" aria-hidden="true" tabindex="-1"></a>                 channels: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1536</span>,              <span class="co"># Model width/dimensionality</span></span>
<span id="cb2-122"><a href="#cb2-122" aria-hidden="true" tabindex="-1"></a>                 num_transformer_layers: <span class="bu">int</span> <span class="op">=</span> <span class="dv">11</span>,  <span class="co"># Number of transformer blocks</span></span>
<span id="cb2-123"><a href="#cb2-123" aria-hidden="true" tabindex="-1"></a>                 num_heads: <span class="bu">int</span> <span class="op">=</span> <span class="dv">8</span>,                <span class="co"># Number of attention heads</span></span>
<span id="cb2-124"><a href="#cb2-124" aria-hidden="true" tabindex="-1"></a>                 pooling_type: <span class="bu">str</span> <span class="op">=</span> <span class="st">'attention'</span>,   <span class="co"># Pooling type ('attention' or 'max')</span></span>
<span id="cb2-125"><a href="#cb2-125" aria-hidden="true" tabindex="-1"></a>                 name: <span class="bu">str</span> <span class="op">=</span> <span class="st">'enformer'</span>):           <span class="co"># Module name</span></span>
<span id="cb2-126"><a href="#cb2-126" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(name<span class="op">=</span>name)</span>
<span id="cb2-127"><a href="#cb2-127" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-128"><a href="#cb2-128" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output channels for different species</span></span>
<span id="cb2-129"><a href="#cb2-129" aria-hidden="true" tabindex="-1"></a>        heads_channels <span class="op">=</span> {<span class="st">'human'</span>: <span class="dv">5313</span>, <span class="st">'mouse'</span>: <span class="dv">1643</span>}</span>
<span id="cb2-130"><a href="#cb2-130" aria-hidden="true" tabindex="-1"></a>        dropout_rate <span class="op">=</span> <span class="fl">0.4</span></span>
<span id="cb2-131"><a href="#cb2-131" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb2-132"><a href="#cb2-132" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Validate model configuration</span></span>
<span id="cb2-133"><a href="#cb2-133" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> channels <span class="op">%</span> num_heads <span class="op">==</span> <span class="dv">0</span>, (<span class="st">'channels must be divisible by num_heads'</span>)</span>
<span id="cb2-134"><a href="#cb2-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-135"><a href="#cb2-135" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Multi-head attention configuration</span></span>
<span id="cb2-136"><a href="#cb2-136" aria-hidden="true" tabindex="-1"></a>        whole_attention_kwargs <span class="op">=</span> {</span>
<span id="cb2-137"><a href="#cb2-137" aria-hidden="true" tabindex="-1"></a>            <span class="st">'attention_dropout_rate'</span>: <span class="fl">0.05</span>,</span>
<span id="cb2-138"><a href="#cb2-138" aria-hidden="true" tabindex="-1"></a>            <span class="st">'initializer'</span>: <span class="va">None</span>,</span>
<span id="cb2-139"><a href="#cb2-139" aria-hidden="true" tabindex="-1"></a>            <span class="st">'key_size'</span>: <span class="dv">64</span>,</span>
<span id="cb2-140"><a href="#cb2-140" aria-hidden="true" tabindex="-1"></a>            <span class="st">'num_heads'</span>: num_heads,</span>
<span id="cb2-141"><a href="#cb2-141" aria-hidden="true" tabindex="-1"></a>            <span class="st">'num_relative_position_features'</span>: channels <span class="op">//</span> num_heads,</span>
<span id="cb2-142"><a href="#cb2-142" aria-hidden="true" tabindex="-1"></a>            <span class="st">'positional_dropout_rate'</span>: <span class="fl">0.01</span>,</span>
<span id="cb2-143"><a href="#cb2-143" aria-hidden="true" tabindex="-1"></a>            <span class="st">'relative_position_functions'</span>: [</span>
<span id="cb2-144"><a href="#cb2-144" aria-hidden="true" tabindex="-1"></a>                <span class="st">'positional_features_exponential'</span>,</span>
<span id="cb2-145"><a href="#cb2-145" aria-hidden="true" tabindex="-1"></a>                <span class="st">'positional_features_central_mask'</span>,</span>
<span id="cb2-146"><a href="#cb2-146" aria-hidden="true" tabindex="-1"></a>                <span class="st">'positional_features_gamma'</span></span>
<span id="cb2-147"><a href="#cb2-147" aria-hidden="true" tabindex="-1"></a>            ],</span>
<span id="cb2-148"><a href="#cb2-148" aria-hidden="true" tabindex="-1"></a>            <span class="st">'relative_positions'</span>: <span class="va">True</span>,</span>
<span id="cb2-149"><a href="#cb2-149" aria-hidden="true" tabindex="-1"></a>            <span class="st">'scaling'</span>: <span class="va">True</span>,</span>
<span id="cb2-150"><a href="#cb2-150" aria-hidden="true" tabindex="-1"></a>            <span class="st">'value_size'</span>: channels <span class="op">//</span> num_heads,</span>
<span id="cb2-151"><a href="#cb2-151" aria-hidden="true" tabindex="-1"></a>            <span class="st">'zero_initialize'</span>: <span class="va">True</span></span>
<span id="cb2-152"><a href="#cb2-152" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb2-153"><a href="#cb2-153" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-154"><a href="#cb2-154" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Build model trunk</span></span>
<span id="cb2-155"><a href="#cb2-155" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> tf.name_scope(<span class="st">'trunk'</span>):</span>
<span id="cb2-156"><a href="#cb2-156" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Convolutional block helper</span></span>
<span id="cb2-157"><a href="#cb2-157" aria-hidden="true" tabindex="-1"></a>            <span class="kw">def</span> conv_block(filters, width<span class="op">=</span><span class="dv">1</span>, w_init<span class="op">=</span><span class="va">None</span>, name<span class="op">=</span><span class="st">'conv_block'</span>, <span class="op">**</span>kwargs):</span>
<span id="cb2-158"><a href="#cb2-158" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> Sequential(<span class="kw">lambda</span>: [</span>
<span id="cb2-159"><a href="#cb2-159" aria-hidden="true" tabindex="-1"></a>                    snt.distribute.CrossReplicaBatchNorm(</span>
<span id="cb2-160"><a href="#cb2-160" aria-hidden="true" tabindex="-1"></a>                        create_scale<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-161"><a href="#cb2-161" aria-hidden="true" tabindex="-1"></a>                        create_offset<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-162"><a href="#cb2-162" aria-hidden="true" tabindex="-1"></a>                        scale_init<span class="op">=</span>snt.initializers.Ones(),</span>
<span id="cb2-163"><a href="#cb2-163" aria-hidden="true" tabindex="-1"></a>                        moving_mean<span class="op">=</span>snt.ExponentialMovingAverage(<span class="fl">0.9</span>),</span>
<span id="cb2-164"><a href="#cb2-164" aria-hidden="true" tabindex="-1"></a>                        moving_variance<span class="op">=</span>snt.ExponentialMovingAverage(<span class="fl">0.9</span>)),</span>
<span id="cb2-165"><a href="#cb2-165" aria-hidden="true" tabindex="-1"></a>                    gelu,</span>
<span id="cb2-166"><a href="#cb2-166" aria-hidden="true" tabindex="-1"></a>                    snt.Conv1D(filters, width, w_init<span class="op">=</span>w_init, <span class="op">**</span>kwargs)</span>
<span id="cb2-167"><a href="#cb2-167" aria-hidden="true" tabindex="-1"></a>                ], name<span class="op">=</span>name)</span>
<span id="cb2-168"><a href="#cb2-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-169"><a href="#cb2-169" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Initial stem</span></span>
<span id="cb2-170"><a href="#cb2-170" aria-hidden="true" tabindex="-1"></a>            stem <span class="op">=</span> Sequential(<span class="kw">lambda</span>: [</span>
<span id="cb2-171"><a href="#cb2-171" aria-hidden="true" tabindex="-1"></a>                snt.Conv1D(channels <span class="op">//</span> <span class="dv">2</span>, <span class="dv">15</span>),</span>
<span id="cb2-172"><a href="#cb2-172" aria-hidden="true" tabindex="-1"></a>                Residual(conv_block(channels <span class="op">//</span> <span class="dv">2</span>, <span class="dv">1</span>, name<span class="op">=</span><span class="st">'pointwise_conv_block'</span>)),</span>
<span id="cb2-173"><a href="#cb2-173" aria-hidden="true" tabindex="-1"></a>                pooling_module(pooling_type, pool_size<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb2-174"><a href="#cb2-174" aria-hidden="true" tabindex="-1"></a>            ], name<span class="op">=</span><span class="st">'stem'</span>)</span>
<span id="cb2-175"><a href="#cb2-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-176"><a href="#cb2-176" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Convolutional tower</span></span>
<span id="cb2-177"><a href="#cb2-177" aria-hidden="true" tabindex="-1"></a>            filter_list <span class="op">=</span> exponential_linspace_int(</span>
<span id="cb2-178"><a href="#cb2-178" aria-hidden="true" tabindex="-1"></a>                start<span class="op">=</span>channels <span class="op">//</span> <span class="dv">2</span>, </span>
<span id="cb2-179"><a href="#cb2-179" aria-hidden="true" tabindex="-1"></a>                end<span class="op">=</span>channels,</span>
<span id="cb2-180"><a href="#cb2-180" aria-hidden="true" tabindex="-1"></a>                num<span class="op">=</span><span class="dv">6</span>, </span>
<span id="cb2-181"><a href="#cb2-181" aria-hidden="true" tabindex="-1"></a>                divisible_by<span class="op">=</span><span class="dv">128</span></span>
<span id="cb2-182"><a href="#cb2-182" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb2-183"><a href="#cb2-183" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb2-184"><a href="#cb2-184" aria-hidden="true" tabindex="-1"></a>            conv_tower <span class="op">=</span> Sequential(<span class="kw">lambda</span>: [</span>
<span id="cb2-185"><a href="#cb2-185" aria-hidden="true" tabindex="-1"></a>                Sequential(<span class="kw">lambda</span>: [</span>
<span id="cb2-186"><a href="#cb2-186" aria-hidden="true" tabindex="-1"></a>                    conv_block(num_filters, <span class="dv">5</span>),</span>
<span id="cb2-187"><a href="#cb2-187" aria-hidden="true" tabindex="-1"></a>                    Residual(conv_block(num_filters, <span class="dv">1</span>, name<span class="op">=</span><span class="st">'pointwise_conv_block'</span>)),</span>
<span id="cb2-188"><a href="#cb2-188" aria-hidden="true" tabindex="-1"></a>                    pooling_module(pooling_type, pool_size<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb2-189"><a href="#cb2-189" aria-hidden="true" tabindex="-1"></a>                ], name<span class="op">=</span><span class="ss">f'conv_tower_block_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb2-190"><a href="#cb2-190" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i, num_filters <span class="kw">in</span> <span class="bu">enumerate</span>(filter_list)</span>
<span id="cb2-191"><a href="#cb2-191" aria-hidden="true" tabindex="-1"></a>            ], name<span class="op">=</span><span class="st">'conv_tower'</span>)</span>
<span id="cb2-192"><a href="#cb2-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-193"><a href="#cb2-193" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Transformer blocks</span></span>
<span id="cb2-194"><a href="#cb2-194" aria-hidden="true" tabindex="-1"></a>            <span class="kw">def</span> transformer_mlp():</span>
<span id="cb2-195"><a href="#cb2-195" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> Sequential(<span class="kw">lambda</span>: [</span>
<span id="cb2-196"><a href="#cb2-196" aria-hidden="true" tabindex="-1"></a>                    snt.LayerNorm(axis<span class="op">=-</span><span class="dv">1</span>, create_scale<span class="op">=</span><span class="va">True</span>, create_offset<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb2-197"><a href="#cb2-197" aria-hidden="true" tabindex="-1"></a>                    snt.Linear(channels <span class="op">*</span> <span class="dv">2</span>),</span>
<span id="cb2-198"><a href="#cb2-198" aria-hidden="true" tabindex="-1"></a>                    snt.Dropout(dropout_rate),</span>
<span id="cb2-199"><a href="#cb2-199" aria-hidden="true" tabindex="-1"></a>                    tf.nn.relu,</span>
<span id="cb2-200"><a href="#cb2-200" aria-hidden="true" tabindex="-1"></a>                    snt.Linear(channels),</span>
<span id="cb2-201"><a href="#cb2-201" aria-hidden="true" tabindex="-1"></a>                    snt.Dropout(dropout_rate)</span>
<span id="cb2-202"><a href="#cb2-202" aria-hidden="true" tabindex="-1"></a>                ], name<span class="op">=</span><span class="st">'mlp'</span>)</span>
<span id="cb2-203"><a href="#cb2-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-204"><a href="#cb2-204" aria-hidden="true" tabindex="-1"></a>            transformer <span class="op">=</span> Sequential(<span class="kw">lambda</span>: [</span>
<span id="cb2-205"><a href="#cb2-205" aria-hidden="true" tabindex="-1"></a>                Sequential(<span class="kw">lambda</span>: [</span>
<span id="cb2-206"><a href="#cb2-206" aria-hidden="true" tabindex="-1"></a>                    Residual(Sequential(<span class="kw">lambda</span>: [</span>
<span id="cb2-207"><a href="#cb2-207" aria-hidden="true" tabindex="-1"></a>                        snt.LayerNorm(axis<span class="op">=-</span><span class="dv">1</span>, create_scale<span class="op">=</span><span class="va">True</span>, create_offset<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-208"><a href="#cb2-208" aria-hidden="true" tabindex="-1"></a>                                    scale_init<span class="op">=</span>snt.initializers.Ones()),</span>
<span id="cb2-209"><a href="#cb2-209" aria-hidden="true" tabindex="-1"></a>                        attention_module.MultiheadAttention(<span class="op">**</span>whole_attention_kwargs,</span>
<span id="cb2-210"><a href="#cb2-210" aria-hidden="true" tabindex="-1"></a>                                                          name<span class="op">=</span><span class="ss">f'attention_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>),</span>
<span id="cb2-211"><a href="#cb2-211" aria-hidden="true" tabindex="-1"></a>                        snt.Dropout(dropout_rate)</span>
<span id="cb2-212"><a href="#cb2-212" aria-hidden="true" tabindex="-1"></a>                    ], name<span class="op">=</span><span class="st">'mha'</span>)),</span>
<span id="cb2-213"><a href="#cb2-213" aria-hidden="true" tabindex="-1"></a>                    Residual(transformer_mlp())</span>
<span id="cb2-214"><a href="#cb2-214" aria-hidden="true" tabindex="-1"></a>                ], name<span class="op">=</span><span class="ss">f'transformer_block_</span><span class="sc">{</span>i<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb2-215"><a href="#cb2-215" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_transformer_layers)</span>
<span id="cb2-216"><a href="#cb2-216" aria-hidden="true" tabindex="-1"></a>            ], name<span class="op">=</span><span class="st">'transformer'</span>)</span>
<span id="cb2-217"><a href="#cb2-217" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-218"><a href="#cb2-218" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Final layers</span></span>
<span id="cb2-219"><a href="#cb2-219" aria-hidden="true" tabindex="-1"></a>            crop_final <span class="op">=</span> TargetLengthCrop1D(TARGET_LENGTH, name<span class="op">=</span><span class="st">'target_input'</span>)</span>
<span id="cb2-220"><a href="#cb2-220" aria-hidden="true" tabindex="-1"></a>            final_pointwise <span class="op">=</span> Sequential(<span class="kw">lambda</span>: [</span>
<span id="cb2-221"><a href="#cb2-221" aria-hidden="true" tabindex="-1"></a>                conv_block(channels <span class="op">*</span> <span class="dv">2</span>, <span class="dv">1</span>),</span>
<span id="cb2-222"><a href="#cb2-222" aria-hidden="true" tabindex="-1"></a>                snt.Dropout(dropout_rate <span class="op">/</span> <span class="dv">8</span>),</span>
<span id="cb2-223"><a href="#cb2-223" aria-hidden="true" tabindex="-1"></a>                gelu</span>
<span id="cb2-224"><a href="#cb2-224" aria-hidden="true" tabindex="-1"></a>            ], name<span class="op">=</span><span class="st">'final_pointwise'</span>)</span>
<span id="cb2-225"><a href="#cb2-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-226"><a href="#cb2-226" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Combine trunk modules</span></span>
<span id="cb2-227"><a href="#cb2-227" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._trunk <span class="op">=</span> Sequential([</span>
<span id="cb2-228"><a href="#cb2-228" aria-hidden="true" tabindex="-1"></a>                stem,</span>
<span id="cb2-229"><a href="#cb2-229" aria-hidden="true" tabindex="-1"></a>                conv_tower,</span>
<span id="cb2-230"><a href="#cb2-230" aria-hidden="true" tabindex="-1"></a>                transformer,</span>
<span id="cb2-231"><a href="#cb2-231" aria-hidden="true" tabindex="-1"></a>                crop_final,</span>
<span id="cb2-232"><a href="#cb2-232" aria-hidden="true" tabindex="-1"></a>                final_pointwise</span>
<span id="cb2-233"><a href="#cb2-233" aria-hidden="true" tabindex="-1"></a>            ], name<span class="op">=</span><span class="st">'trunk'</span>)</span>
<span id="cb2-234"><a href="#cb2-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-235"><a href="#cb2-235" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Output heads</span></span>
<span id="cb2-236"><a href="#cb2-236" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> tf.name_scope(<span class="st">'heads'</span>):</span>
<span id="cb2-237"><a href="#cb2-237" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._heads <span class="op">=</span> {</span>
<span id="cb2-238"><a href="#cb2-238" aria-hidden="true" tabindex="-1"></a>                head: Sequential(</span>
<span id="cb2-239"><a href="#cb2-239" aria-hidden="true" tabindex="-1"></a>                    <span class="kw">lambda</span>: [snt.Linear(num_channels), tf.nn.softplus],</span>
<span id="cb2-240"><a href="#cb2-240" aria-hidden="true" tabindex="-1"></a>                    name<span class="op">=</span><span class="ss">f'head_</span><span class="sc">{</span>head<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb2-241"><a href="#cb2-241" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> head, num_channels <span class="kw">in</span> heads_channels.items()</span>
<span id="cb2-242"><a href="#cb2-242" aria-hidden="true" tabindex="-1"></a>            }</span>
<span id="cb2-243"><a href="#cb2-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-244"><a href="#cb2-244" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb2-245"><a href="#cb2-245" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> trunk(<span class="va">self</span>):</span>
<span id="cb2-246"><a href="#cb2-246" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._trunk</span>
<span id="cb2-247"><a href="#cb2-247" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-248"><a href="#cb2-248" aria-hidden="true" tabindex="-1"></a>    <span class="at">@property</span></span>
<span id="cb2-249"><a href="#cb2-249" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> heads(<span class="va">self</span>):</span>
<span id="cb2-250"><a href="#cb2-250" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>._heads</span>
<span id="cb2-251"><a href="#cb2-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-252"><a href="#cb2-252" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, inputs: tf.Tensor, is_training: <span class="bu">bool</span>) <span class="op">-&gt;</span> Dict[<span class="bu">str</span>, tf.Tensor]:</span>
<span id="cb2-253"><a href="#cb2-253" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Forward pass through the model."""</span></span>
<span id="cb2-254"><a href="#cb2-254" aria-hidden="true" tabindex="-1"></a>        trunk_embedding <span class="op">=</span> <span class="va">self</span>.trunk(inputs, is_training<span class="op">=</span>is_training)</span>
<span id="cb2-255"><a href="#cb2-255" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> {</span>
<span id="cb2-256"><a href="#cb2-256" aria-hidden="true" tabindex="-1"></a>            head: head_module(trunk_embedding, is_training<span class="op">=</span>is_training)</span>
<span id="cb2-257"><a href="#cb2-257" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> head, head_module <span class="kw">in</span> <span class="va">self</span>.heads.items()</span>
<span id="cb2-258"><a href="#cb2-258" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb2-259"><a href="#cb2-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-260"><a href="#cb2-260" aria-hidden="true" tabindex="-1"></a>    <span class="at">@tf.function</span>(input_signature<span class="op">=</span>[</span>
<span id="cb2-261"><a href="#cb2-261" aria-hidden="true" tabindex="-1"></a>        tf.TensorSpec([<span class="va">None</span>, SEQUENCE_LENGTH, <span class="dv">4</span>], tf.float32)])</span>
<span id="cb2-262"><a href="#cb2-262" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> predict_on_batch(<span class="va">self</span>, x):</span>
<span id="cb2-263"><a href="#cb2-263" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Prediction method for SavedModel."""</span></span>
<span id="cb2-264"><a href="#cb2-264" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>(x, is_training<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb2-265"><a href="#cb2-265" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-266"><a href="#cb2-266" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> TargetLengthCrop1D(snt.Module):</span>
<span id="cb2-267"><a href="#cb2-267" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Crops sequence to target length."""</span></span>
<span id="cb2-268"><a href="#cb2-268" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-269"><a href="#cb2-269" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, target_length: Optional[<span class="bu">int</span>], name: <span class="bu">str</span> <span class="op">=</span> <span class="st">'target_length_crop'</span>):</span>
<span id="cb2-270"><a href="#cb2-270" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(name<span class="op">=</span>name)</span>
<span id="cb2-271"><a href="#cb2-271" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._target_length <span class="op">=</span> target_length</span>
<span id="cb2-272"><a href="#cb2-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-273"><a href="#cb2-273" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, inputs):</span>
<span id="cb2-274"><a href="#cb2-274" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>._target_length <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb2-275"><a href="#cb2-275" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> inputs</span>
<span id="cb2-276"><a href="#cb2-276" aria-hidden="true" tabindex="-1"></a>        trim <span class="op">=</span> (inputs.shape[<span class="op">-</span><span class="dv">2</span>] <span class="op">-</span> <span class="va">self</span>._target_length) <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb2-277"><a href="#cb2-277" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> trim <span class="op">&lt;</span> <span class="dv">0</span>:</span>
<span id="cb2-278"><a href="#cb2-278" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">'inputs longer than target length'</span>)</span>
<span id="cb2-279"><a href="#cb2-279" aria-hidden="true" tabindex="-1"></a>        <span class="cf">elif</span> trim <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb2-280"><a href="#cb2-280" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> inputs</span>
<span id="cb2-281"><a href="#cb2-281" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb2-282"><a href="#cb2-282" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> inputs[..., trim:<span class="op">-</span>trim, :]</span>
<span id="cb2-283"><a href="#cb2-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-284"><a href="#cb2-284" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Sequential(snt.Module):</span>
<span id="cb2-285"><a href="#cb2-285" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Sequential container with is_training support."""</span></span>
<span id="cb2-286"><a href="#cb2-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-287"><a href="#cb2-287" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb2-288"><a href="#cb2-288" aria-hidden="true" tabindex="-1"></a>                 layers: Optional[Union[Callable[[], Iterable[snt.Module]],</span>
<span id="cb2-289"><a href="#cb2-289" aria-hidden="true" tabindex="-1"></a>                                      Iterable[Callable[..., Any]]]] <span class="op">=</span> <span class="va">None</span>,</span>
<span id="cb2-290"><a href="#cb2-290" aria-hidden="true" tabindex="-1"></a>                 name: Optional[Text] <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb2-291"><a href="#cb2-291" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(name<span class="op">=</span>name)</span>
<span id="cb2-292"><a href="#cb2-292" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> layers <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb2-293"><a href="#cb2-293" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._layers <span class="op">=</span> []</span>
<span id="cb2-294"><a href="#cb2-294" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb2-295"><a href="#cb2-295" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="bu">hasattr</span>(layers, <span class="st">'__call__'</span>):</span>
<span id="cb2-296"><a href="#cb2-296" aria-hidden="true" tabindex="-1"></a>                layers <span class="op">=</span> layers()</span>
<span id="cb2-297"><a href="#cb2-297" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>._layers <span class="op">=</span> [layer <span class="cf">for</span> layer <span class="kw">in</span> layers <span class="cf">if</span> layer <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>]</span>
<span id="cb2-298"><a href="#cb2-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-299"><a href="#cb2-299" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, inputs: tf.Tensor, is_training: <span class="bu">bool</span>, <span class="op">**</span>kwargs):</span>
<span id="cb2-300"><a href="#cb2-300" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> inputs</span>
<span id="cb2-301"><a href="#cb2-301" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> mod <span class="kw">in</span> <span class="va">self</span>._layers:</span>
<span id="cb2-302"><a href="#cb2-302" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> accepts_is_training(mod):</span>
<span id="cb2-303"><a href="#cb2-303" aria-hidden="true" tabindex="-1"></a>                outputs <span class="op">=</span> mod(outputs, is_training<span class="op">=</span>is_training, <span class="op">**</span>kwargs)</span>
<span id="cb2-304"><a href="#cb2-304" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span>:</span>
<span id="cb2-305"><a href="#cb2-305" aria-hidden="true" tabindex="-1"></a>                outputs <span class="op">=</span> mod(outputs, <span class="op">**</span>kwargs)</span>
<span id="cb2-306"><a href="#cb2-306" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> outputs</span>
<span id="cb2-307"><a href="#cb2-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-308"><a href="#cb2-308" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pooling_module(kind, pool_size):</span>
<span id="cb2-309"><a href="#cb2-309" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Returns appropriate pooling module."""</span></span>
<span id="cb2-310"><a href="#cb2-310" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> kind <span class="op">==</span> <span class="st">'attention'</span>:</span>
<span id="cb2-311"><a href="#cb2-311" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> SoftmaxPooling1D(pool_size<span class="op">=</span>pool_size, per_channel<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb2-312"><a href="#cb2-312" aria-hidden="true" tabindex="-1"></a>                              w_init_scale<span class="op">=</span><span class="fl">2.0</span>)</span>
<span id="cb2-313"><a href="#cb2-313" aria-hidden="true" tabindex="-1"></a>    <span class="cf">elif</span> kind <span class="op">==</span> <span class="st">'max'</span>:</span>
<span id="cb2-314"><a href="#cb2-314" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tf.keras.layers.MaxPool1D(pool_size<span class="op">=</span>pool_size, padding<span class="op">=</span><span class="st">'same'</span>)</span>
<span id="cb2-315"><a href="#cb2-315" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb2-316"><a href="#cb2-316" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="ss">f'Invalid pooling kind: </span><span class="sc">{</span>kind<span class="sc">}</span><span class="ss">.'</span>)</span>
<span id="cb2-317"><a href="#cb2-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-318"><a href="#cb2-318" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> SoftmaxPooling1D(snt.Module):</span>
<span id="cb2-319"><a href="#cb2-319" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Softmax-weighted pooling operation."""</span></span>
<span id="cb2-320"><a href="#cb2-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-321"><a href="#cb2-321" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,</span>
<span id="cb2-322"><a href="#cb2-322" aria-hidden="true" tabindex="-1"></a>                 pool_size: <span class="bu">int</span> <span class="op">=</span> <span class="dv">2</span>,</span>
<span id="cb2-323"><a href="#cb2-323" aria-hidden="true" tabindex="-1"></a>                 per_channel: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb2-324"><a href="#cb2-324" aria-hidden="true" tabindex="-1"></a>                 w_init_scale: <span class="bu">float</span> <span class="op">=</span> <span class="fl">0.0</span>,</span>
<span id="cb2-325"><a href="#cb2-325" aria-hidden="true" tabindex="-1"></a>                 name: <span class="bu">str</span> <span class="op">=</span> <span class="st">'softmax_pooling'</span>):</span>
<span id="cb2-326"><a href="#cb2-326" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(name<span class="op">=</span>name)</span>
<span id="cb2-327"><a href="#cb2-327" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._pool_size <span class="op">=</span> pool_size</span>
<span id="cb2-328"><a href="#cb2-328" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._per_channel <span class="op">=</span> per_channel</span>
<span id="cb2-329"><a href="#cb2-329" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._w_init_scale <span class="op">=</span> w_init_scale</span>
<span id="cb2-330"><a href="#cb2-330" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._logit_linear <span class="op">=</span> <span class="va">None</span></span>
<span id="cb2-331"><a href="#cb2-331" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-332"><a href="#cb2-332" aria-hidden="true" tabindex="-1"></a>    <span class="at">@snt.once</span></span>
<span id="cb2-333"><a href="#cb2-333" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _initialize(<span class="va">self</span>, num_features):</span>
<span id="cb2-334"><a href="#cb2-334" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._logit_linear <span class="op">=</span> snt.Linear(</span>
<span id="cb2-335"><a href="#cb2-335" aria-hidden="true" tabindex="-1"></a>            output_size<span class="op">=</span>num_features <span class="cf">if</span> <span class="va">self</span>._per_channel <span class="cf">else</span> <span class="dv">1</span>,</span>
<span id="cb2-336"><a href="#cb2-336" aria-hidden="true" tabindex="-1"></a>            with_bias<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb2-337"><a href="#cb2-337" aria-hidden="true" tabindex="-1"></a>            w_init<span class="op">=</span>snt.initializers.Identity(<span class="va">self</span>._w_init_scale))</span>
<span id="cb2-338"><a href="#cb2-338" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-339"><a href="#cb2-339" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, inputs):</span>
<span id="cb2-340"><a href="#cb2-340" aria-hidden="true" tabindex="-1"></a>        _, length, num_features <span class="op">=</span> inputs.shape</span>
<span id="cb2-341"><a href="#cb2-341" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._initialize(num_features)</span>
<span id="cb2-342"><a href="#cb2-342" aria-hidden="true" tabindex="-1"></a>        inputs <span class="op">=</span> tf.reshape(</span>
<span id="cb2-343"><a href="#cb2-343" aria-hidden="true" tabindex="-1"></a>            inputs,</span>
<span id="cb2-344"><a href="#cb2-344" aria-hidden="true" tabindex="-1"></a>            (<span class="op">-</span><span class="dv">1</span>, length <span class="op">//</span> <span class="va">self</span>._pool_size, <span class="va">self</span>._pool_size, num_features))</span>
<span id="cb2-345"><a href="#cb2-345" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> tf.reduce_sum(</span>
<span id="cb2-346"><a href="#cb2-346" aria-hidden="true" tabindex="-1"></a>            inputs <span class="op">*</span> tf.nn.softmax(<span class="va">self</span>._logit_linear(inputs), axis<span class="op">=-</span><span class="dv">2</span>),</span>
<span id="cb2-347"><a href="#cb2-347" aria-hidden="true" tabindex="-1"></a>            axis<span class="op">=-</span><span class="dv">2</span>)</span>
<span id="cb2-348"><a href="#cb2-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-349"><a href="#cb2-349" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Residual(snt.Module):</span>
<span id="cb2-350"><a href="#cb2-350" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Residual connection wrapper."""</span></span>
<span id="cb2-351"><a href="#cb2-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-352"><a href="#cb2-352" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, module: snt.Module, name<span class="op">=</span><span class="st">'residual'</span>):</span>
<span id="cb2-353"><a href="#cb2-353" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(name<span class="op">=</span>name)</span>
<span id="cb2-354"><a href="#cb2-354" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._module <span class="op">=</span> module</span>
<span id="cb2-355"><a href="#cb2-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-356"><a href="#cb2-356" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__call__</span>(<span class="va">self</span>, inputs: tf.Tensor, is_training: <span class="bu">bool</span>, <span class="op">*</span>args,</span>
<span id="cb2-357"><a href="#cb2-357" aria-hidden="true" tabindex="-1"></a>                 <span class="op">**</span>kwargs) <span class="op">-&gt;</span> tf.Tensor:</span>
<span id="cb2-358"><a href="#cb2-358" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> inputs <span class="op">+</span> <span class="va">self</span>._module(inputs, is_training, <span class="op">*</span>args, <span class="op">**</span>kwargs)</span>
<span id="cb2-359"><a href="#cb2-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-360"><a href="#cb2-360" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> gelu(x: tf.Tensor) <span class="op">-&gt;</span> tf.Tensor:</span>
<span id="cb2-361"><a href="#cb2-361" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Gaussian Error Linear Unit activation function."""</span></span>
<span id="cb2-362"><a href="#cb2-362" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> tf.nn.sigmoid(<span class="fl">1.702</span> <span class="op">*</span> x) <span class="op">*</span> x</span>
<span id="cb2-363"><a href="#cb2-363" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-364"><a href="#cb2-364" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> one_hot_encode(sequence: <span class="bu">str</span>,</span>
<span id="cb2-365"><a href="#cb2-365" aria-hidden="true" tabindex="-1"></a>                   alphabet: <span class="bu">str</span> <span class="op">=</span> <span class="st">'ACGT'</span>,</span>
<span id="cb2-366"><a href="#cb2-366" aria-hidden="true" tabindex="-1"></a>                   neutral_alphabet: <span class="bu">str</span> <span class="op">=</span> <span class="st">'N'</span>,</span>
<span id="cb2-367"><a href="#cb2-367" aria-hidden="true" tabindex="-1"></a>                   neutral_value: Any <span class="op">=</span> <span class="dv">0</span>,</span>
<span id="cb2-368"><a href="#cb2-368" aria-hidden="true" tabindex="-1"></a>                   dtype<span class="op">=</span>np.float32) <span class="op">-&gt;</span> np.ndarray:</span>
<span id="cb2-369"><a href="#cb2-369" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""One-hot encodes DNA sequence."""</span></span>
<span id="cb2-370"><a href="#cb2-370" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> to_uint8(string):</span>
<span id="cb2-371"><a href="#cb2-371" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.frombuffer(string.encode(<span class="st">'ascii'</span>), dtype<span class="op">=</span>np.uint8)</span>
<span id="cb2-372"><a href="#cb2-372" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb2-373"><a href="#cb2-373" aria-hidden="true" tabindex="-1"></a>    hash_table <span class="op">=</span> np.zeros((np.iinfo(np.uint8).<span class="bu">max</span>, <span class="bu">len</span>(alphabet)), dtype<span class="op">=</span>dtype)</span>
<span id="cb2-374"><a href="#cb2-374" aria-hidden="true" tabindex="-1"></a>    hash_table[to_uint8(alphabet)] <span class="op">=</span> np.eye(<span class="bu">len</span>(alphabet), dtype<span class="op">=</span>dtype)</span>
<span id="cb2-375"><a href="#cb2-375" aria-hidden="true" tabindex="-1"></a>    hash_table[to_uint8(neutral_alphabet)] <span class="op">=</span> neutral_value</span>
<span id="cb2-376"><a href="#cb2-376" aria-hidden="true" tabindex="-1"></a>    hash_table <span class="op">=</span> hash_table.astype(dtype)</span>
<span id="cb2-377"><a href="#cb2-377" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> hash_table[to_uint8(sequence)]</span>
<span id="cb2-378"><a href="#cb2-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-379"><a href="#cb2-379" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> exponential_linspace_int(start, end, num, divisible_by<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb2-380"><a href="#cb2-380" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Generates exponentially spaced integers."""</span></span>
<span id="cb2-381"><a href="#cb2-381" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> _round(x):</span>
<span id="cb2-382"><a href="#cb2-382" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">int</span>(np.<span class="bu">round</span>(x <span class="op">/</span> divisible_by) <span class="op">*</span> divisible_by)</span>
<span id="cb2-383"><a href="#cb2-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-384"><a href="#cb2-384" aria-hidden="true" tabindex="-1"></a>    base <span class="op">=</span> np.exp(np.log(end <span class="op">/</span> start) <span class="op">/</span> (num <span class="op">-</span> <span class="dv">1</span>))</span>
<span id="cb2-385"><a href="#cb2-385" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [_round(start <span class="op">*</span> base<span class="op">**</span>i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num)]</span>
<span id="cb2-386"><a href="#cb2-386" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-387"><a href="#cb2-387" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> accepts_is_training(module):</span>
<span id="cb2-388"><a href="#cb2-388" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Checks if module accepts is_training parameter."""</span></span>
<span id="cb2-389"><a href="#cb2-389" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="st">'is_training'</span> <span class="kw">in</span> <span class="bu">list</span>(inspect.signature(module.<span class="fu">__call__</span>).parameters)</span>
<span id="cb2-390"><a href="#cb2-390" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>