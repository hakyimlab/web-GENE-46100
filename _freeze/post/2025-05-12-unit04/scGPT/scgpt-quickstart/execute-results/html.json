{
  "hash": "e35878692526e8932638c7704ff934c7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: scgpt quickstart - qmd version\nfreeze: true\neval: false\nauthor: Haky Im based on cziscience page\ndate: 2025-05-13\ncategories: \n    - notebook\n    - gene46100\n---\n\n\nThis tutorial will guide you through using scGPT, a foundation model for single-cell data analysis. We'll learn how to process single-cell data and generate meaningful embeddings that can be used for various downstream analyses.\n\nDownloaded from \nhttps://virtualcellmodels.cziscience.com/quickstart/scgpt-quickstart\n\n## Quick Start: scGPT\n\nThis quick start will guide you through using the scGPT model, trained on 33 million cells (including data from the CZ CELLxGENE Census), to generate embeddings for single-cell transcriptomic data analysis.\n\n### Learning Goals\n\nBy the end of this tutorial, you will understand how to:\n\n* Access and prepare the scGPT model for use\n* Generate embeddings to analyze and compare your dataset against the CZ CELLxGENE Census\n* Visualize the results using a UMAP, colored by cell type\n\n### Pre-requisites and Requirements\n\n> **Important**: Before starting, make sure you have:\n> 1. Python 3.9 installed\n> 2. Basic understanding of single-cell data analysis\n> 3. At least 32GB of RAM (tested on M3 MacBook)\n\nBefore starting, ensure you are familiar with:\n\n* Python and AnnData\n* Single-cell data analysis (see this tutorial for a primer on the subject)  \n  You can run this tutorial locally (tested on an M3 MacBook with 32 GiB memory) or in Google Colab using a T4 instance. Environment setup will be covered in a later section.\n\n![AnnData schema](/post/images/anndata_schema.svg){width=50%}\n\n### AnnData Structure\n\n> **Key Concept**: AnnData is the fundamental data structure for single-cell analysis in Python. Understanding its structure is necessary for working with scGPT and other tools.\n\nThe AnnData object is a Python-based data structure used by scanpy, scGPT, and other single-cell analysis tools. Here are its key components:\n\n#### Main Attributes\n- `.X`: Main data matrix (cells Ã— genes)\n- `.obs`: Cell annotations (pandas dataframe with metadata for each cell)\n- `.var`: Gene annotations (pandas dataframe with metadata for each gene)\n\n#### Embedding Storage\n- `.obsm`: Cell embeddings (e.g., PCA, UMAP coordinates)\n- `.varm`: Gene embeddings (e.g., gene loadings)\n\n### Overview\n\nThis notebook provides a step-by-step guide to:\n\n* Setting up your environment\n* Downloading the necessary model checkpoints and h5ad dataset\n* Performing model inference to create embeddings\n* Visualizing the results with UMAP\n\n### Setup\n\n> **Environment Setup Note**: \n> 1. We'll create a new conda environment for this tutorial\n> 2. We need specific versions of PyTorch and other dependencies\n> 3. The setup process might take a few minutes\n\nLet's start by setting up dependencies. The released version of scGPT requires PyTorch 2.1.2, so we will remove the existing PyTorch installation and replace it with the required one. If you want to run this on another environment, this step might not be necessary.\n\n**Note**: I encountered an error with torch version. I installed torch 2.3.0 and torchvision 0.16.2, and then reinstalled torch 2.1.2.\n\n```{{bash}}\n# Create and activate a new conda environment\nconda create -n scgpt python=3.9\nconda activate scgpt\n```\n\n::: {#setup-dependencies .cell fold='true' execution_count=1}\n``` {.python .cell-code}\n# Environment setup and package installation\nfirst_time = False\nif first_time:\n    # First uninstall the conflicting packages\n    %pip uninstall -y -q torch torchvision\n    %pip uninstall -y numpy pandas scipy scikit-learn anndata cell-gears datasets dcor\n    #%pip install -q torchvision==0.16.2 torch==2.1.2\n    %pip install -q torch==2.3.0 torchvision==0.16.2\n    %pip install -q scgpt scanpy gdown\n\n    # Then install them in the correct order with specific versions\n    %pip install numpy==1.23.5\n    %pip install pandas==1.5.3  # This version is compatible with anndata 0.10.9\n    %pip install scipy==1.10.1  # This version is >1.8 as required by anndata\n    %pip install scikit-learn==1.2.2\n    %pip install anndata==0.10.9\n    %pip install cell-gears==0.0.2\n    %pip install dcor==0.6\n    %pip install datasets==2.3.0\n\n    # First uninstall both packages\n    %pip uninstall -y torch torchtext\n\n    # Then install compatible versions\n    %pip install torch==2.1.2 torchtext==0.16.2\n```\n:::\n\n\n> **Package Installation Note**: \n> - We install specific versions of packages to ensure compatibility\n> - The order of installation matters\n> - Some packages are uninstalled first to avoid conflicts\n\n::: {#import-libraries .cell fold='true' execution_count=2}\n``` {.python .cell-code}\n# Import libraries\nimport os\nimport multiprocessing\n\n# Set MPS fallback for unimplemented operations\nos.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n\n# Monkey-patch os.sched_getaffinity for macOS\nif not hasattr(os, 'sched_getaffinity'):\n    def sched_getaffinity(pid):\n        return set(range(multiprocessing.cpu_count()))\n    os.sched_getaffinity = sched_getaffinity\n\nimport warnings\nimport urllib.request\nfrom pathlib import Path\n\nimport scgpt as scg\nimport scanpy as sc\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Check for MPS availability\ndevice = (\n    torch.device(\"mps\")\n    if torch.backends.mps.is_available()\n    else torch.device(\"cpu\")\n)\nprint(f\"Using device: {device}\")\nprint(\"Note: Some operations may fall back to CPU due to MPS limitations\")\n\nwarnings.filterwarnings(\"ignore\")\n```\n:::\n\n\n> **Library Import Note**:\n> - We import all necessary libraries for data processing and model inference\n> - Special handling for macOS MPS (Metal Performance Shaders)\n> - Warning suppression for cleaner output\n\n::: {#setup-directories .cell execution_count=3}\n``` {.python .cell-code}\n# Define the base working directory\nWORKDIR = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100/scgpt\"\n# Convert to Path objects for better path handling\nWORKDIR = Path(WORKDIR)\nDATA_DIR = WORKDIR / \"data\"\nMODEL_DIR = WORKDIR / \"model\" \n```\n:::\n\n\n> **Directory Setup Note**:\n> - We create a structured directory for data and model files\n> - Using Path objects for cross-platform compatibility\n\n### Download and Prepare Data\n\n> **Data Download Note**:\n> - We'll download both the model and a sample dataset\n> - The dataset will be processed to reduce memory usage\n> - We'll use highly variable genes for better analysis\n\n::: {#download-model .cell execution_count=4}\n``` {.python .cell-code}\nwarnings.simplefilter(\"ignore\", ResourceWarning)\nwarnings.filterwarnings(\"ignore\", category=ImportWarning)\n\n# Use gdown with the recursive flag to download the folder\nfolder_id = '1oWh_-ZRdhtoGQ2Fw24HP41FgLoomVo-y'\n\n# Check if model files already exist\nif not (MODEL_DIR / \"args.json\").exists():\n    print(\"Downloading model checkpoint...\")\n    !gdown --folder {folder_id} -O {MODEL_DIR}\nelse:\n    print(\"Model files already exist in\", MODEL_DIR)\n```\n:::\n\n\n> **Model Download Note**:\n> - The model is downloaded from Google Drive\n> - We check if files exist to avoid re-downloading\n> - The model is about 200MB in size\n\n::: {#download-data .cell execution_count=5}\n``` {.python .cell-code}\nuri = \"https://datasets.cellxgene.cziscience.com/f50deffa-43ae-4f12-85ed-33e45040a1fa.h5ad\"\nsource_path = DATA_DIR / \"source.h5ad\"\n\n# Check if file exists before downloading\nif not source_path.exists():\n    print(f\"Downloading dataset to {source_path}...\")\n    urllib.request.urlretrieve(uri, filename=str(source_path))\nelse:\n    print(f\"Dataset already exists at {source_path}\")\n\n# Read the data\nadata = sc.read_h5ad(source_path)\n\nbatch_key = \"sample\"\nN_HVG = 3000  # Number of highly variable genes to select\n\n# Select highly variable genes to reduce memory usage\nsc.pp.highly_variable_genes(adata, n_top_genes=N_HVG, flavor='seurat_v3')\nadata_hvg = adata[:, adata.var['highly_variable']]\n```\n:::\n\n\n> **Data Processing Note**:\n> - We download a sample dataset from CZ CELLxGENE\n> - The dataset is reduced to 3000 highly variable genes\n> - This reduction helps with memory usage and computation speed\n\n### Model Inference and Embedding Generation\n\n> **Key Concept**: In this section, we'll:\n> 1. Set up the model for inference\n> 2. Generate cell embeddings using scGPT\n> 3. Process these embeddings for downstream analysis\n> \n> **Note**: The embedding generation process might take several minutes depending on your hardware.\n\n::: {#setup-embedding .cell fold='true' execution_count=6}\n``` {.python .cell-code}\n# Monkey patch get_batch_cell_embeddings to force single processor\nimport types\nfrom scgpt.tasks.cell_emb import get_batch_cell_embeddings as original_get_batch_cell_embeddings\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, SequentialSampler\nfrom scgpt.data_collator import DataCollator\nimport numpy as np\nfrom tqdm import tqdm\n\n# Define Dataset class at module level\nclass CellEmbeddingDataset(Dataset):\n    \"\"\"\n    Custom dataset class for cell embedding generation.\n    Processes single-cell data into a format suitable for the scGPT model.\n    \"\"\"\n    def __init__(self, count_matrix, gene_ids, batch_ids=None, vocab=None, model_configs=None):\n        self.count_matrix = count_matrix\n        self.gene_ids = gene_ids\n        self.batch_ids = batch_ids\n        self.vocab = vocab\n        self.model_configs = model_configs\n\n    def __len__(self):\n        return len(self.count_matrix)\n\n    def __getitem__(self, idx):\n        # Process a single cell's data\n        row = self.count_matrix[idx]\n        nonzero_idx = np.nonzero(row)[0]\n        values = row[nonzero_idx]\n        genes = self.gene_ids[nonzero_idx]\n        # append <cls> token at the beginning\n        genes = np.insert(genes, 0, self.vocab[\"<cls>\"])\n        values = np.insert(values, 0, self.model_configs[\"pad_value\"])\n        genes = torch.from_numpy(genes).long()\n        values = torch.from_numpy(values).float()\n        output = {\n            \"id\": idx,\n            \"genes\": genes,\n            \"expressions\": values,\n        }\n        if self.batch_ids is not None:\n            output[\"batch_labels\"] = self.batch_ids[idx]\n        return output\n\ndef patched_get_batch_cell_embeddings(\n    adata,\n    cell_embedding_mode: str = \"cls\",\n    model=None,\n    vocab=None,\n    max_length=1200,\n    batch_size=64,\n    model_configs=None,\n    gene_ids=None,\n    use_batch_labels=False,\n) -> np.ndarray:\n    \"\"\"\n    Patched version of get_batch_cell_embeddings that uses the module-level Dataset class\n    and forces num_workers=0 for better compatibility.\n    \"\"\"\n    # Convert data to appropriate format\n    count_matrix = adata.X\n    count_matrix = (\n        count_matrix if isinstance(count_matrix, np.ndarray) else count_matrix.toarray()\n    )\n\n    # Get gene vocabulary ids\n    if gene_ids is None:\n        gene_ids = np.array(adata.var[\"id_in_vocab\"])\n        assert np.all(gene_ids >= 0)\n\n    if use_batch_labels:\n        batch_ids = np.array(adata.obs[\"batch_id\"].tolist())\n\n    if cell_embedding_mode == \"cls\":\n        # Set up dataset and data loader\n        dataset = CellEmbeddingDataset(\n            count_matrix, \n            gene_ids, \n            batch_ids if use_batch_labels else None,\n            vocab=vocab,\n            model_configs=model_configs\n        )\n        collator = DataCollator(\n            do_padding=True,\n            pad_token_id=vocab[model_configs[\"pad_token\"]],\n            pad_value=model_configs[\"pad_value\"],\n            do_mlm=False,\n            do_binning=True,\n            max_length=max_length,\n            sampling=True,\n            keep_first_n_tokens=1,\n        )\n        data_loader = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            sampler=SequentialSampler(dataset),\n            collate_fn=collator,\n            drop_last=False,\n            num_workers=0,  # Force single worker for compatibility\n            pin_memory=True,\n        )\n\n        # Generate embeddings\n        cell_embeddings = np.zeros(\n            (len(dataset), model_configs[\"embsize\"]), dtype=np.float32\n        )\n        with torch.no_grad():\n            count = 0\n            for data_dict in tqdm(data_loader, desc=\"Embedding cells\"):\n                # Process each batch of cells\n                input_gene_ids = data_dict[\"gene\"].to(device)\n                src_key_padding_mask = input_gene_ids.eq(\n                    vocab[model_configs[\"pad_token\"]]\n                )\n                embeddings = model._encode(\n                    input_gene_ids,\n                    data_dict[\"expr\"].to(device),\n                    src_key_padding_mask=src_key_padding_mask,\n                    batch_labels=data_dict[\"batch_labels\"].to(device)\n                    if use_batch_labels\n                    else None,\n                )\n\n                # Extract CLS token embeddings and normalize\n                embeddings = embeddings[:, 0, :]  # get the <cls> position embedding\n                embeddings = embeddings.cpu().numpy()\n                cell_embeddings[count : count + len(embeddings)] = embeddings\n                count += len(embeddings)\n        cell_embeddings = cell_embeddings / np.linalg.norm(\n            cell_embeddings, axis=1, keepdims=True\n        )\n    else:\n        raise ValueError(f\"Unknown cell embedding mode: {cell_embedding_mode}\")\n    return cell_embeddings\n\n# Replace the original function with our patched version\nimport scgpt.tasks.cell_emb\nscgpt.tasks.cell_emb.get_batch_cell_embeddings = patched_get_batch_cell_embeddings\n\nos.environ['PYTHONWARNINGS'] = 'ignore'\n```\n:::\n\n\n> **Technical Note**: The code above:\n> - Creates a custom dataset class for processing single-cell data\n> - Implements a patched version of the embedding function for better compatibility\n> - Handles data batching and normalization\n> - Uses the CLS token for cell representation\n\n::: {#generate-embeddings .cell execution_count=7}\n``` {.python .cell-code}\nmodel_dir = MODEL_DIR\ngene_col = \"feature_name\"\ncell_type_key = \"cell_type\"\n\nembedding_file = DATA_DIR / \"ref_embed_adata.h5ad\"\n\nif embedding_file.exists():\n    print(f\"Loading existing embeddings from {embedding_file}\")\n    ref_embed_adata = sc.read_h5ad(str(embedding_file))\nelse:\n    print(\"Computing new embeddings...\")\n    ref_embed_adata = scg.tasks.embed_data(\n        adata_hvg,\n        model_dir,\n        gene_col=gene_col,\n        obs_to_save=cell_type_key,\n        batch_size=64,\n        return_new_adata=True,\n        device=device,\n    )\n    print(f\"Saving embeddings to {embedding_file}\")\n    ref_embed_adata.write(str(embedding_file))\n```\n:::\n\n\n> **Embedding Generation Note**:\n> - We first check if embeddings already exist to avoid recomputing\n> - The embedding process generates a 512-dimensional representation for each cell\n> - Results are saved for future use\n\n::: {#check-embeddings .cell execution_count=8}\n``` {.python .cell-code}\n# Check the shape of our embeddings\nref_embed_adata.X.shape\n```\n:::\n\n\n> **Shape Explanation**:\n> - The output shows (number of cells Ã— 512)\n> - 512 is the embedding dimension of scGPT\n> - Each cell is represented by a 512-dimensional vector\n\n### Dimensionality Reduction and Visualization\n\n> **Key Concept**: In this section, we'll:\n> 1. Compute cell-cell similarities using the embeddings\n> 2. Generate a UMAP visualization\n> 3. Compare different cell type annotations\n> \n> **Note**: UMAP helps us visualize high-dimensional data in 2D while preserving important relationships.\n\n::: {#compute-neighbors .cell execution_count=9}\n``` {.python .cell-code}\n# Compute cell-cell similarities\nsc.pp.neighbors(ref_embed_adata, use_rep=\"X\")\n# Generate UMAP coordinates\nsc.tl.umap(ref_embed_adata)\n```\n:::\n\n\n> **Neighborhood Graph Note**:\n> - We compute a neighborhood graph based on the embeddings\n> - This graph is used to generate the UMAP visualization\n\n::: {#transfer-embeddings .cell execution_count=10}\n``` {.python .cell-code}\n# Transfer embeddings and UMAP to original adata object\nadata.obsm[\"X_scgpt\"] = ref_embed_adata.X\nadata.obsm[\"X_umap\"] = ref_embed_adata.obsm[\"X_umap\"]\n```\n:::\n\n\n> **Data Transfer Note**:\n> - We transfer the embeddings and UMAP coordinates to our original dataset\n> - This allows us to use the original annotations with our new embeddings\n> - The embeddings are stored in `.obsm` for easy access\n\n::: {#prepare-gene-names .cell execution_count=11}\n``` {.python .cell-code}\n# Add the current index ('ensembl_id') as a new column\nadata.var['ensembl_id'] = adata.var.index\n\n# Set the new index to the 'feature_name' column\nadata.var.set_index('feature_name', inplace=True)\n\n# Add a copy of the gene symbols back to the var dataframe\nadata.var['gene_symbol'] = adata.var.index\n```\n:::\n\n\n> **Gene Name Processing Note**:\n> - We reorganize the gene annotations for easier access\n> - This allows us to use gene symbols in visualizations\n> - Both Ensembl IDs and gene symbols are preserved\n\n### Visualization\n\n> **Key Concept**: In this section, we'll:\n> 1. Visualize the UMAP colored by different cell type annotations\n> 2. Examine marker gene expression\n> 3. Compare different annotation schemes\n\n::: {#plot-umap-celltypes .cell execution_count=12}\n``` {.python .cell-code}\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\")\n    sc.pl.umap(adata, color=[\"cell_type\", \"annotation_res0.34_new2\"], wspace = 0.6)\n```\n:::\n\n\n> **UMAP Visualization Note**:\n> - The plot shows cells colored by two different annotation schemes\n> - This helps us compare different cell type classifications\n> - The spatial organization reflects cell type relationships\n\n::: {#plot-umap-markers .cell execution_count=13}\n``` {.python .cell-code}\n# Visualize marker genes for different cell types\nsc.pl.umap(adata, \n           color=['cell_type', 'MKI67', 'LYZ', 'RBP2', 'MUC2', 'CHGA', 'TAGLN', 'ELAVL3'], \n           frameon=False, \n           use_raw=False, \n           legend_fontsize=\"xx-small\", \n           legend_loc=\"none\")\n```\n:::\n\n\n> **Marker Gene Note**:\n> - We visualize expression of key marker genes\n> - These genes help identify different cell types:\n>   - MKI67: Proliferating cells\n>   - LYZ: Myeloid cells\n>   - RBP2: Enterocytes\n>   - MUC2: Goblet cells\n>   - CHGA: Enteroendocrine cells\n>   - TAGLN: Smooth muscle cells\n>   - ELAVL3: Neurons\n\n### References\n\n> **Key Papers**:\n> 1. scGPT paper: Foundation model for single-cell multi-omics\n> 2. Dataset paper: iPSC-derived small intestine-on-chip\n> 3. CZ CELLxGENE platform paper\n\nPlease refer to the following papers for more information:\n\n1. **scGPT: Toward building a foundation model for single-cell multi-omics using generative AI**\nCui, H., Wang, C., Maan, H. et al. Nature Methods 21, 1470â€“1480 (2024) https://doi.org/10.1038/s41592-024-02201-0\n\n2. **The dataset used in this tutorial**\nMoerkens, R., et al. Cell Reports, 43(7) (2024) https://doi.org/10.1016/j.celrep.2024.114247\n\n3. **CZ CELLxGENE Discover and Census**\nCZI Single-Cell Biology, et al. bioRxiv 2023.10.30 doi: https://doi.org/10.1101/2023.10.30.563174\n\n",
    "supporting": [
      "scgpt-quickstart_files"
    ],
    "filters": [],
    "includes": {}
  }
}