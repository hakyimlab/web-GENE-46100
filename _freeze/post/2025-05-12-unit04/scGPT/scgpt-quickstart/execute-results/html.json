{
  "hash": "c1dad5b24b27c60188b0813fef64f49e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: scgpt quickstart - qmd version\nfreeze: true\neval: false\nauthor: Haky Im based on cziscience page\ndate: 2025-05-13\ncategories: \n    - notebook\n    - gene46100\n---\n\n\nDownloaded from \nhttps://virtualcellmodels.cziscience.com/quickstart/scgpt-quickstart\n\n## Quick Start: scGPT\n\nThis quick start will guide you through using the scGPT model, trained on 33 million cells (including data from the CZ CELLxGENE Census), to generate embeddings for single-cell transcriptomic data analysis.\n\n### Learning Goals\n\nBy the end of this tutorial, you will understand how to:\n\n* Access and prepare the scGPT model for use\n* Generate embeddings to analyze and compare your dataset against the CZ CELLxGENE Census\n* Visualize the results using a UMAP, colored by cell type\n\n### Pre-requisites and Requirements\n\nBefore starting, ensure you are familiar with:\n\n* Python and AnnData\n* Single-cell data analysis (see this tutorial for a primer on the subject)  \n  You can run this tutorial locally (tested on an M3 MacBook with 32 GiB memory) or in Google Colab using a T4 instance. Environment setup will be covered in a later section.\n\n![AnnData schema](/post/images/anndata_schema.svg){width=50%}\n\n### AnnData Structure\n\nThe AnnData object is a Python-based data structure used by scanpy, scGPT, and other single-cell analysis tools. Here are its key components:\n\n#### Main Attributes\n- `.X`: Main data matrix (cells Ã— genes)\n- `.obs`: Cell annotations (pandas dataframe with metadata for each cell)\n- `.var`: Gene annotations (pandas dataframe with metadata for each gene)\n\n#### Embedding Storage\n- `.obsm`: Cell embeddings (e.g., PCA, UMAP coordinates)\n- `.varm`: Gene embeddings (e.g., gene loadings)\n\n### Overview\n\nThis notebook provides a step-by-step guide to:\n\n* Setting up your environment\n* Downloading the necessary model checkpoints and h5ad dataset\n* Performing model inference to create embeddings\n* Visualizing the results with UMAP\n\n\n### Setup\n\nLet's start by setting up dependencies. The released version of scGPT requires PyTorch 2.1.2, so we will remove the existing PyTorch installation and replace it with the required one. If you want to run this on another environment, this step might not be necessary.\n\n** but then I got error about torch version. So I installed torch 2.3.0 and torchvision 0.16.2. and then reinstalled torch 2.1.2.**\n\n```{{bash}}\nconda create -n scgpt python=3.9\nconda activate scgpt\n```\n\n::: {#fc55f234 .cell execution_count=1}\n``` {.python .cell-code}\nfirst_time = False\nif first_time:\n    # First uninstall the conflicting packages\n    %pip uninstall -y -q torch torchvision\n    %pip uninstall -y numpy pandas scipy scikit-learn anndata cell-gears datasets dcor\n    #%pip install -q torchvision==0.16.2 torch==2.1.2\n    %pip install -q torch==2.3.0 torchvision==0.16.2\n    %pip install -q scgpt scanpy gdown\n\n\n    # Then install them in the correct order with specific versions\n    %pip install numpy==1.23.5\n    %pip install pandas==1.5.3  # This version is compatible with anndata 0.10.9\n    %pip install scipy==1.10.1  # This version is >1.8 as required by anndata\n    %pip install scikit-learn==1.2.2\n    %pip install anndata==0.10.9\n    %pip install cell-gears==0.0.2\n    %pip install dcor==0.6\n\n    %pip install datasets==2.3.0\n\n\n    # First uninstall both packages\n    %pip uninstall -y torch torchtext\n\n    # Then install compatible versions\n    %pip install torch==2.1.2 torchtext==0.16.2\n```\n:::\n\n\nWe can install the rest of our dependencies and import the relevant libraries.\n\n::: {#f4282403 .cell execution_count=2}\n``` {.python .cell-code}\n# Import libraries\n\n# Import required packages\nimport os\nimport multiprocessing\n\n# Set MPS fallback for unimplemented operations\nos.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n\n# Monkey-patch os.sched_getaffinity for macOS\nif not hasattr(os, 'sched_getaffinity'):\n    def sched_getaffinity(pid):\n        return set(range(multiprocessing.cpu_count()))\n    os.sched_getaffinity = sched_getaffinity\n\nimport warnings\nimport urllib.request\nfrom pathlib import Path\n\nimport scgpt as scg\nimport scanpy as sc\nimport numpy as np\nimport pandas as pd\nimport torch\n\n# Check for MPS availability\ndevice = (\n    torch.device(\"mps\")\n    if torch.backends.mps.is_available()\n    else torch.device(\"cpu\")\n)\nprint(f\"Using device: {device}\")\nprint(\"Note: Some operations may fall back to CPU due to MPS limitations\")\n\nwarnings.filterwarnings(\"ignore\")\n```\n:::\n\n\n::: {#98ab16a4 .cell execution_count=3}\n``` {.python .cell-code}\n# Define the base working directory\nWORKDIR = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/data-Github/web-data/web-GENE-46100/scgpt\"\n# Convert to Path objects for better path handling\nWORKDIR = Path(WORKDIR)\nDATA_DIR = WORKDIR / \"data\"\nMODEL_DIR = WORKDIR / \"model\" \n```\n:::\n\n\nDownload Model Checkpoints and Data\n\nLet's download the checkpoints from the scGPT repository.\n\n::: {#31706b6a .cell execution_count=4}\n``` {.python .cell-code}\nwarnings.simplefilter(\"ignore\", ResourceWarning)\nwarnings.filterwarnings(\"ignore\", category=ImportWarning)\n\n# Use gdown with the recursive flag to download the folder\n# Replace the folder ID with the ID of your folder\nfolder_id = '1oWh_-ZRdhtoGQ2Fw24HP41FgLoomVo-y'\n\n# Check if model files already exist\nif not (MODEL_DIR / \"args.json\").exists():\n    print(\"Downloading model checkpoint...\")\n    !gdown --folder {folder_id} -O {MODEL_DIR}\nelse:\n    print(\"Model files already exist in\", MODEL_DIR)\n```\n:::\n\n\nWe will now download an H5AD dataset from CZ CELLxGENE. To reduce memory utilization, we will also perform a reduction to the top 3000 highly variable genes using scanpy's highly_variable_genes function.\n\n::: {#9ecbcf51 .cell execution_count=5}\n``` {.python .cell-code}\nuri = \"https://datasets.cellxgene.cziscience.com/f50deffa-43ae-4f12-85ed-33e45040a1fa.h5ad\"\nsource_path = DATA_DIR / \"source.h5ad\"\n\n# Check if file exists before downloading\nif not source_path.exists():\n    print(f\"Downloading dataset to {source_path}...\")\n    urllib.request.urlretrieve(uri, filename=str(source_path))\nelse:\n    print(f\"Dataset already exists at {source_path}\")\n\n# Read the data\nadata = sc.read_h5ad(source_path)\n\nbatch_key = \"sample\"\nN_HVG = 3000\n\nsc.pp.highly_variable_genes(adata, n_top_genes=N_HVG, flavor='seurat_v3')\nadata_hvg = adata[:, adata.var['highly_variable']]\n```\n:::\n\n\nWe can now use embed_data to generate the embeddings. Note that gene_col needs to point to the column where the gene names (not symbols!) are defined. For CZ CELLxGENE datasets, they are stored in the feature_name column.\n\n::: {#92dae1d5 .cell execution_count=6}\n``` {.python .cell-code}\n# Monkey patch get_batch_cell_embeddings to force single processor\nimport types\nfrom scgpt.tasks.cell_emb import get_batch_cell_embeddings as original_get_batch_cell_embeddings\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, SequentialSampler\nfrom scgpt.data_collator import DataCollator\nimport numpy as np\nfrom tqdm import tqdm\n\n# Define Dataset class at module level\nclass CellEmbeddingDataset(Dataset):\n    def __init__(self, count_matrix, gene_ids, batch_ids=None, vocab=None, model_configs=None):\n        self.count_matrix = count_matrix\n        self.gene_ids = gene_ids\n        self.batch_ids = batch_ids\n        self.vocab = vocab\n        self.model_configs = model_configs\n\n    def __len__(self):\n        return len(self.count_matrix)\n\n    def __getitem__(self, idx):\n        row = self.count_matrix[idx]\n        nonzero_idx = np.nonzero(row)[0]\n        values = row[nonzero_idx]\n        genes = self.gene_ids[nonzero_idx]\n        # append <cls> token at the beginning\n        genes = np.insert(genes, 0, self.vocab[\"<cls>\"])\n        values = np.insert(values, 0, self.model_configs[\"pad_value\"])\n        genes = torch.from_numpy(genes).long()\n        values = torch.from_numpy(values).float()\n        output = {\n            \"id\": idx,\n            \"genes\": genes,\n            \"expressions\": values,\n        }\n        if self.batch_ids is not None:\n            output[\"batch_labels\"] = self.batch_ids[idx]\n        return output\n\ndef patched_get_batch_cell_embeddings(\n    adata,\n    cell_embedding_mode: str = \"cls\",\n    model=None,\n    vocab=None,\n    max_length=1200,\n    batch_size=64,\n    model_configs=None,\n    gene_ids=None,\n    use_batch_labels=False,\n) -> np.ndarray:\n    \"\"\"\n    Patched version of get_batch_cell_embeddings that uses the module-level Dataset class\n    and forces num_workers=0.\n    \"\"\"\n    count_matrix = adata.X\n    count_matrix = (\n        count_matrix if isinstance(count_matrix, np.ndarray) else count_matrix.toarray()\n    )\n\n    # gene vocabulary ids\n    if gene_ids is None:\n        gene_ids = np.array(adata.var[\"id_in_vocab\"])\n        assert np.all(gene_ids >= 0)\n\n    if use_batch_labels:\n        batch_ids = np.array(adata.obs[\"batch_id\"].tolist())\n\n    if cell_embedding_mode == \"cls\":\n        dataset = CellEmbeddingDataset(\n            count_matrix, \n            gene_ids, \n            batch_ids if use_batch_labels else None,\n            vocab=vocab,\n            model_configs=model_configs\n        )\n        collator = DataCollator(\n            do_padding=True,\n            pad_token_id=vocab[model_configs[\"pad_token\"]],\n            pad_value=model_configs[\"pad_value\"],\n            do_mlm=False,\n            do_binning=True,\n            max_length=max_length,\n            sampling=True,\n            keep_first_n_tokens=1,\n        )\n        data_loader = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            sampler=SequentialSampler(dataset),\n            collate_fn=collator,\n            drop_last=False,\n            num_workers=0,  # Force single worker\n            pin_memory=True,\n        )\n\n        # Use the global device variable instead of getting it from model\n        cell_embeddings = np.zeros(\n            (len(dataset), model_configs[\"embsize\"]), dtype=np.float32\n        )\n        with torch.no_grad():\n            # Disable autocast for MPS as it's not supported\n            count = 0\n            for data_dict in tqdm(data_loader, desc=\"Embedding cells\"):\n                input_gene_ids = data_dict[\"gene\"].to(device)\n                src_key_padding_mask = input_gene_ids.eq(\n                    vocab[model_configs[\"pad_token\"]]\n                )\n                embeddings = model._encode(\n                    input_gene_ids,\n                    data_dict[\"expr\"].to(device),\n                    src_key_padding_mask=src_key_padding_mask,\n                    batch_labels=data_dict[\"batch_labels\"].to(device)\n                    if use_batch_labels\n                    else None,\n                )\n\n                embeddings = embeddings[:, 0, :]  # get the <cls> position embedding\n                embeddings = embeddings.cpu().numpy()\n                cell_embeddings[count : count + len(embeddings)] = embeddings\n                count += len(embeddings)\n        cell_embeddings = cell_embeddings / np.linalg.norm(\n            cell_embeddings, axis=1, keepdims=True\n        )\n    else:\n        raise ValueError(f\"Unknown cell embedding mode: {cell_embedding_mode}\")\n    return cell_embeddings\n\n# Replace the original function with our patched version\nimport scgpt.tasks.cell_emb\nscgpt.tasks.cell_emb.get_batch_cell_embeddings = patched_get_batch_cell_embeddings\n\nos.environ['PYTHONWARNINGS'] = 'ignore'\n```\n:::\n\n\n::: {#764a55cd .cell execution_count=7}\n``` {.python .cell-code}\nmodel_dir = MODEL_DIR #/ \"scGPT_human\"\ngene_col = \"feature_name\"\ncell_type_key = \"cell_type\"\n\nembedding_file = DATA_DIR / \"ref_embed_adata.h5ad\"\n\nif embedding_file.exists():\n    print(f\"Loading existing embeddings from {embedding_file}\")\n    ref_embed_adata = sc.read_h5ad(str(embedding_file))\nelse:\n    print(\"Computing new embeddings...\")\n    ref_embed_adata = scg.tasks.embed_data(\n        adata_hvg,\n        model_dir,\n        gene_col=gene_col,\n        obs_to_save=cell_type_key,\n        batch_size=64,\n        return_new_adata=True,\n        device=device,  # Pass the device to embed_data\n    )\n    print(f\"Saving embeddings to {embedding_file}\")\n    ref_embed_adata.write(str(embedding_file))\n\n```\n:::\n\n\nOur scGPT embeddings are stored in the .X attribute of the returned AnnData object and have a dimensionality of 512.\n\n::: {#d6033458 .cell execution_count=8}\n``` {.python .cell-code}\nref_embed_adata.X.shape\n```\n:::\n\n\nWe can now calculate neighbors based on scGPT embeddings.\n\n::: {#e5b8ae85 .cell execution_count=9}\n``` {.python .cell-code}\nsc.pp.neighbors(ref_embed_adata, use_rep=\"X\")\nsc.tl.umap(ref_embed_adata)\n```\n:::\n\n\nWe will put our calculated UMAP and embeddings in our original adata object with our original annotations.\n\n::: {#2becdf8e .cell execution_count=10}\n``` {.python .cell-code}\nadata.obsm[\"X_scgpt\"] = ref_embed_adata.X\nadata.obsm[\"X_umap\"] = ref_embed_adata.obsm[\"X_umap\"]\n```\n:::\n\n\nWe can also switch our .var index which is currently set to Ensembl ID's, to be gene symbols, allowing us to plot gene expression more easily.\n\n::: {#22d8422d .cell execution_count=11}\n``` {.python .cell-code}\n# Add the current index ('ensembl_id') as a new column\nadata.var['ensembl_id'] = adata.var.index\n\n# Set the new index to the 'feature_name' column\nadata.var.set_index('feature_name', inplace=True)\n```\n:::\n\n\n::: {#9aadb9b3 .cell execution_count=12}\n``` {.python .cell-code}\n# Add a copy of the gene symbols back to the var dataframe\nadata.var['gene_symbol'] = adata.var.index\n```\n:::\n\n\nWe can now plot a UMAP, coloring it by cell type to visualize our embeddings. Below, we color by both the standard cell type labels provided by CZ CELLxGENE and the original cell type annotations from the authors. The embeddings generated by scGPT effectively capture the structure of the data, closely aligning with the original author annotations.\n\n::: {#ec3f099c .cell execution_count=13}\n``` {.python .cell-code}\nwith warnings.catch_warnings():\n    warnings.filterwarnings(\"ignore\")\n    #sc.pp.neighbors(ref_embed_adata, use_rep=\"X\")\n    #sc.tl.umap(ref_embed_adata)\n    sc.pl.umap(adata, color=[\"cell_type\", \"annotation_res0.34_new2\"], wspace = 0.6)\n```\n:::\n\n\nWe can also take a look at some markers of the major cell types represented in the dataset.\n\n::: {#3f424229 .cell execution_count=14}\n``` {.python .cell-code}\nsc.pl.umap(adata, color=['cell_type', 'MKI67', 'LYZ', 'RBP2', 'MUC2', 'CHGA', 'TAGLN', 'ELAVL3'], frameon=False, use_raw=False, legend_fontsize =\"xx-small\", legend_loc=\"none\")\n```\n:::\n\n\nReferences\n\nPlease refer to the following papers for information about:\n\nscGPT: Toward building a foundation model for single-cell multi-omics using generative AI\n\nCui, H., Wang, C., Maan, H. et al. scGPT: toward building a foundation model for single-cell multi-omics using generative AI. Nat Methods 21, 1470â€“1480 (2024). https://doi.org/10.1038/s41592-024-02201-0\n\nThe dataset used in this tutorial\n\nMoerkens, R., Mooiweer, J., RamÃ­rez-SÃ¡nchez, A. D., Oelen, R., Franke, L., Wijmenga, C., Barrett, R. J., Jonkers, I. H., & Withoff, S. (2024). An iPSC-derived small intestine-on-chip with self-organizing epithelial, mesenchymal, and neural cells. Cell Reports, 43(7). https://doi.org/10.1016/j.celrep.2024.114247\n\nCZ CELLxGENE Discover and Census\n\nCZ CELLxGENE Discover: A single-cell data platform for scalable exploration, analysis and modeling of aggregated data CZI Single-Cell Biology, et al. bioRxiv 2023.10.30; doi: https://doi.org/10.1101/2023.10.30.563174\n\n",
    "supporting": [
      "scgpt-quickstart_files"
    ],
    "filters": [],
    "includes": {}
  }
}