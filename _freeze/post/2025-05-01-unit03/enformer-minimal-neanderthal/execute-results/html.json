{
  "hash": "494d0a49860d59fc5dc83d7a8254b857",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Enformer usage neanderthal\ndate: '2025-04-28'\ndescription: predict neanderthal epigenome with enformer\njupyter: \n  kernelspec:\n    name: \"conda-env-enformer46100-py\"\n    language: \"python\"\n    display_name: \"enformer46100\"\neval: false\ncategories:\n  - gene46100\n  - notebook\n---\n\n\n> modified from Enformer usage notebook in https://github.com/google-deepmind/deepmind-research/blob/master/enformer/enformer-usage.ipynb\n\n### Steps\n\nThis notebook demonstrates how to\n- Make predictions with Enformer using human reference\n- Make predictions using Neanderthal vcf\n\n## 1. set up and function definitions\nThis is Sabrina's EnformerVCF.py file with functions necessary to run vcf modified enformer, based on functions from Temi and Sai, in turn based on Avsec et al's code\n\n### Install Required Python Packages (First Run Only)\n\n::: {#78700264 .cell execution_count=1}\n``` {.python .cell-code}\n# Install Required Python Packages (First Run Only)\nfirst_time = False  # Set to False after first run\n\nif first_time:\n    %pip install tensorflow\n    ## this requires  numpy<2.2.0,>=1.26.0 (from tensorflow)\n    ## it will uninstall numpy not compatible with tensorflow\n\n    import platform\n    if platform.processor() == 'arm':\n        print(\"Apple Silicon detected, installing tensorflow-metal...\")\n        %pip install tensorflow-metal\n    else:\n        print(\"Not running on Apple Silicon, skipping tensorflow-metal installation\")\n\n    %pip install tensorflow_hub\n    # %pip install joblib # already installed\n    %pip install kipoiseq\n    # %pip install pyfaidx # already installed\n    # %pip install pandas # already installed\n    # %pip install numpy # already installed\n    # %pip install matplotlib # already installed\n    # %pip install seaborn # already installed\n    %pip install cyvcf2\n\n    %pip install Bio\n```\n:::\n\n\n### Import Libraries and Define Utility Functions\n\n::: {#7541f3d2 .cell execution_count=2}\n``` {.python .cell-code}\n# Import Libraries and Define Utility Functions\nimport tensorflow as tf\n\n# Make sure the GPU is enabled \nassert tf.config.list_physical_devices('GPU'), 'Start the colab kernel with GPU: Runtime -> Change runtime type -> GPU'\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n\nimport tensorflow_hub as hub # for interacting with saved models and tensorflow hub\nimport joblib\nimport gzip # for manipulating compressed files\nimport kipoiseq # for manipulating fasta files\nfrom kipoiseq import Interval # same as above, really\nimport pyfaidx # to index our reference genome file\nimport pandas as pd # for manipulating dataframes\nimport numpy as np # for numerical computations\nimport matplotlib.pyplot as plt # for plotting\nimport matplotlib as mpl # for plotting\nimport seaborn as sns # for plotting\nimport pickle # for saving large objects\nimport os, sys # functions for interacting with the operating system\nimport cyvcf2\n\nimport Bio\nfrom Bio.Seq import Seq\ndef create_rev_complement(dna_string):\n    return(str(Seq(dna_string).reverse_complement()))\n\nimport io\nimport os\nimport gzip\n```\n:::\n\n\n### Define Enformer Model Classes and Sequence Extraction\n\n::: {#29b4174b .cell execution_count=3}\n``` {.python .cell-code}\n# Define Enformer Model Classes and Sequence Extraction\n\nSEQUENCE_LENGTH = 393216\n\n\nclass Enformer:\n\n  def __init__(self, tfhub_url):\n    self._model = hub.load(tfhub_url).model\n\n  def predict_on_batch(self, inputs):\n    predictions = self._model.predict_on_batch(inputs)\n    return {k: v.numpy() for k, v in predictions.items()}\n\n  @tf.function\n  def contribution_input_grad(self, input_sequence,\n                              target_mask, output_head='human'):\n    input_sequence = input_sequence[tf.newaxis]\n\n    target_mask_mass = tf.reduce_sum(target_mask)\n    with tf.GradientTape() as tape:\n      tape.watch(input_sequence)\n      prediction = tf.reduce_sum(\n          target_mask[tf.newaxis] *\n          self._model.predict_on_batch(input_sequence)[output_head]) / target_mask_mass\n\n    input_grad = tape.gradient(prediction, input_sequence) * input_sequence\n    input_grad = tf.squeeze(input_grad, axis=0)\n    return tf.reduce_sum(input_grad, axis=-1)\n\nclass EnformerScoreVariantsRaw:\n\n  def __init__(self, tfhub_url, organism='human'):\n    self._model = Enformer(tfhub_url)\n    self._organism = organism\n\n  def predict_on_batch(self, inputs):\n    ref_prediction = self._model.predict_on_batch(inputs['ref'])[self._organism]\n    alt_prediction = self._model.predict_on_batch(inputs['alt'])[self._organism]\n\n    return alt_prediction.mean(axis=1) - ref_prediction.mean(axis=1)\n\nclass EnformerScoreVariantsNormalized:\n\n  def __init__(self, tfhub_url, transform_pkl_path,\n               organism='human'):\n    assert organism == 'human', 'Transforms only compatible with organism=human'\n    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n      transform_pipeline = joblib.load(f)\n    self._transform = transform_pipeline.steps[0][1]  # StandardScaler.\n\n  def predict_on_batch(self, inputs):\n    scores = self._model.predict_on_batch(inputs)\n    return self._transform.transform(scores)\n\nclass EnformerScoreVariantsPCANormalized:\n\n  def __init__(self, tfhub_url, transform_pkl_path,\n               organism='human', num_top_features=500):\n    self._model = EnformerScoreVariantsRaw(tfhub_url, organism)\n    with tf.io.gfile.GFile(transform_pkl_path, 'rb') as f:\n      self._transform = joblib.load(f)\n    self._num_top_features = num_top_features\n\n  def predict_on_batch(self, inputs):\n    scores = self._model.predict_on_batch(inputs)\n    return self._transform.transform(scores)[:, :self._num_top_features]\n\n# @title `variant_centered_sequences`\n\nclass FastaStringExtractor:\n\n    def __init__(self, fasta_file):\n        self.fasta = pyfaidx.Fasta(fasta_file)\n        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n    #import pd.Interval as Interval\n    def extract(self, interval: Interval, **kwargs) -> str:\n        # Truncate interval if it extends beyond the chromosome lengths.\n        chromosome_length = self._chromosome_sizes[interval.chrom]\n        trimmed_interval = Interval(interval.chrom,\n                                    max(interval.start, 0),\n                                    min(interval.end, chromosome_length),\n                                    )\n        # pyfaidx wants a 1-based interval\n        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n                                          trimmed_interval.start + 1,\n                                          trimmed_interval.stop).seq).upper()\n        # Fill truncated values with N's.\n        pad_upstream = 'N' * max(-interval.start, 0)\n        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n        return pad_upstream + sequence + pad_downstream\n\n    def close(self):\n        return self.fasta.close()\n```\n:::\n\n\n### Data Processing and Visualization Functions\n\n::: {#08e4ecee .cell execution_count=4}\n``` {.python .cell-code}\n# Data Processing and Visualization Functions\ndef one_hot_encode(sequence):\n  return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)\n\n# @title `plot_tracks`\n\ndef plot_tracks(tracks, interval, height=1.5):\n  fig, axes = plt.subplots(len(tracks), 1, figsize=(20, height * len(tracks)), sharex=True)\n  for ax, (title, y) in zip(axes, tracks.items()):\n    ax.fill_between(np.linspace(interval.start, interval.end, num=len(y)), y)\n    ax.set_title(title)\n    sns.despine(top=True, right=True, bottom=True)\n  ax.set_xlabel(str(interval))\n  plt.tight_layout()\n\ndef read_vcf(path):\n    with gzip.open(path, 'rt') as f:\n        lines = [l for l in f if not l.startswith('##')]\n    return pd.read_csv(\n        io.StringIO(''.join(lines)),\n        dtype={'#CHROM': str, 'POS': int, 'ID': str, 'REF': str, 'ALT': str,\n               'QUAL': str, 'FILTER': str, 'INFO': str},\n        sep='\\t'\n    ).rename(columns={'#CHROM': 'CHROM'})\n\ndef vcf_to_seq(target_interval, individual, vcf, fasta_extractor):\n  ## should be replaced with vcf_to_seq_faster\n  target_fa = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))\n  window_coords = target_interval.resize(SEQUENCE_LENGTH)\n      # two haplotypes per individual\n  haplo_1 = list(target_fa[:])\n  haplo_2 = list(target_fa[:])\n\n  ref_mismatch_count = 0\n  for i,row in vcf.iterrows():\n    geno = row[individual].split(\"|\")\n    if (row[\"POS\"]-window_coords.start-1) >= len(haplo_2):\n      continue\n    if (row[\"POS\"]-window_coords.start-1) < 0:\n      continue\n    if geno[0] == \"1\":\n      haplo_1[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n    if geno[1] == \"1\":\n      haplo_2[row[\"POS\"]-window_coords.start-1] = row[\"ALT\"]\n  return haplo_1, haplo_2\n\ndef vcf_to_seq_faster(target_interval, individual, vcf_file, fasta_extractor):\n  # target_inverval is a kipoiseq interval, e.g. kipoiseq.Interval(\"chr22\", 18118779, 18145669)\n  # individual is the id of the individual in the vcf file\n  \n  target_fa = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))\n  window_coords = target_interval.resize(SEQUENCE_LENGTH)\n      # two haplotypes per individual\n  haplo_1 = list(target_fa[:])\n  haplo_2 = list(target_fa[:])\n\n  # Open the VCF file\n  vcf_reader = cyvcf2.VCF(vcf_file)\n\n  # Specific genomic region\n  CHROM = window_coords.chrom\n  START = max(1,window_coords.start)\n  END = min(window_coords.end, fasta_extractor._chromosome_sizes[CHROM]-1)\n\n  count1 = 0\n  count2 = 0\n\n  # Iterate through variants in the specified region\n  for variant in vcf_reader(CHROM + ':' + str(START) + '-' + str(END)):\n      # Access the individual's genotype using index (0-based) of the sample\n      individual_index = vcf_reader.samples.index(individual)\n      genotype = variant.genotypes[individual_index]\n      ALT=variant.ALT[0]\n      REF=variant.REF\n      POS=variant.POS\n      if REF == target_fa[POS - window_coords.start - 1]:\n        if genotype[0] == 1:\n          haplo_1[POS - window_coords.start - 1] = ALT\n          count1 = count1 + 1\n        if genotype[1] == 1:\n          haplo_2[POS - window_coords.start - 1] = ALT  \n          count2 = count2 + 1\n      else:\n        print(\"ERROR: REF in vcf \"+ REF + \"!= REF from the build\" + target_fa[POS - window_coords.start - 1])\n  print(\"number of changes haplo1:\")\n  print(count1)\n  print(\"number of changes haplo2:\")\n  print(count2)\n  return haplo_1, haplo_2\n```\n:::\n\n\n### Define comparison functions\n\n::: {#70d9ca49 .cell execution_count=5}\n``` {.python .cell-code}\n# Comparison and Summary Statistics Functions\n\n\ndef get_diffmat(mat1, mat2):\n    \n    diffmat = mat1 - mat2\n    abs_diffmat = np.abs(diffmat)\n\n    colwise_maxes1 = np.max(mat1, axis=0)\n    colwise_maxes2 = np.max(mat2, axis=0)\n\n    colwise_maxes_maxes = np.maximum(colwise_maxes1, colwise_maxes2)\n\n    relmax3_diffmat = diffmat / colwise_maxes_maxes\n    relmax3_diffmat = np.abs(relmax3_diffmat)\n\n    return relmax3_diffmat\n\n\ndef get_summary(arr):\n    summary = {\n        \"mean\": np.mean(arr),\n        \"median\": np.median(arr),\n        \"minimum\": np.min(arr),\n        \"maximum\": np.max(arr),\n        \"q1\": np.percentile(arr, 25),\n        \"q3\": np.percentile(arr, 75),\n    }\n    return summary\n\n\ndef plot_hist(arr, bin_num, xlab='Value', ylab='Frequency', title='Histogram'):\n    plt.hist(arr, bins=bin_num)\n    plt.title(title)\n    plt.xlabel(xlab)\n    plt.ylabel(ylab)\n    plt.show()\n\ndef column_correlations(mat1, mat2):\n    if mat1.shape != mat2.shape:\n        raise ValueError(\"Input matrices must have the same shape\")\n\n    num_columns = mat1.shape[1]\n    correlations = np.empty(num_columns)\n\n    for col in range(num_columns):\n        correlation = np.corrcoef(mat1[:, col], mat2[:, col])[0, 1]\n        correlations[col] = correlation\n\n    return correlations\n```\n:::\n\n\n## 2. Set File Paths and Load Enformer Model\n\nIf first time, download the model from here \nhttps://uchicago.box.com/s/ppzn9lqqsnr3i9jqcgc52zf668sllkjx\n\nand the hg19 fasta file from here \nhttps://uchicago.box.com/s/0rh4q4syucn66ne1d8n2aw9g3yyst9a0\n\nIf needed, you can also download the hg38 fasta file from here (but I believe neanderthal vcf is based on hg19) \nhttps://uchicago.box.com/s/wl50ji7jms2c8alyqxyk4q6uru37nnt9\n\n::: {#8394b31e .cell execution_count=6}\n``` {.python .cell-code}\n# Set File Paths and Load Enformer Model\n\n## edit this path to the location of the files on your computer\nPRE = \"/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/Reference-Data/\"\n\nmodel_path = PRE + \"models/enformer/raw\"\nfasta_file = PRE + \"ref_sequences/hg19/raw/genome.fa\"\n## check whether specific reference fasta used for the calling of the neanderthal vcf should be used\n#fasta_file = PRE + \"ref_sequences/hg38/Homo_sapiens_assembly38.fasta\"\n\nmodel = Enformer(model_path) # here we load the model architecture.\nfasta_extractor = FastaStringExtractor(fasta_file)\n```\n:::\n\n\n### Load Target Annotation Data\n\n::: {#f4541f67 .cell execution_count=7}\n``` {.python .cell-code}\n## Load Target Annotation Data\ntargets_txt = 'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_human.txt'\n# df_targets = pd.read_csv(targets_txt, sep='\\t')\ntargets_slim_file = PRE + \"models/enformer/targets_slims.csv\"\ntargets_slim_df = pd.read_csv(targets_slim_file)\n```\n:::\n\n\n## 3. Run Enformer on Neanderthal genomes\n\n### Shell Script: Preprocess Neanderthal VCF Files\n\nDownload Altai ch5 filtered vcf\nbrew install htslib\nbgzip AltaiNea.hg19_1000g.5.vcf\ntabix -p vcf AltaiNea.hg19_1000g.5.vcf.gz\n\ncreate file filter-add-chr.sh with the following content\nchmod u+x filter-add-chr.sh to make it executable \n\n```{{bash}}\n#!/bin/bash\n\nfor NUM in {1..22}; do\n    # Filter missing genotypes and non-variant sites\n    bcftools view -e '(GT=\"./.\") || (GT=\"0/0\") || (ALT=\".\")' AltaiNea.hg19_1000g.${NUM}.vcf.gz > AltaiNea.hg19_1000g.${NUM}.nomiss.vcf\n\n    # Compress the resulting VCF\n    bgzip AltaiNea.hg19_1000g.${NUM}.nomiss.vcf\n    \n    # Add \"chr\" prefix to all non-header lines and compress\n    # zcat < ... is used on a mac terminal; in linux, it should be without <,i.e., zcat AltaiNea...\n    zcat < AltaiNea.hg19_1000g.${NUM}.nomiss.vcf.gz | awk 'BEGIN{OFS=FS=\"\\t\"} /^#/ {print; next} {print \"chr\"$0}' | bgzip > AltaiNea.hg19_1000g.chr${NUM}.nomiss.vcf.gz\n    \n    # Filter to retain only SNPs\n    bcftools view -i 'strlen(REF) = 1 && strlen(ALT) = 1' AltaiNea.hg19_1000g.chr${NUM}.nomiss.vcf.gz > AltaiNea.hg19_1000g.chr${NUM}.nomiss.snps_only.vcf\ndone\n```\n### Load Neanderthal vcf and predict epigenome\n\n::: {#e98ba37c .cell execution_count=8}\n``` {.python .cell-code}\n# Load Neanderthal vcf and predict epigenome\n\n# download the vcf file from here https://uchicago.box.com/s/f682q1c6tl3cdnqwbvga0z72u5e203zs\n# and put it in your data folder\n\n# read VCFs and encode haplotypes\nCHROM='chr5'\nvcf_file = PRE + \"neanderthal/AltaiNea.hg19_1000g.\" + CHROM + \".nomiss.snps_only.vcf.gz\"\n\ntarget_interval = kipoiseq.Interval(CHROM,96875939 , 96919716)\nhaplo1, haplo2 = vcf_to_seq_faster(target_interval, 'AltaiNea', vcf_file, fasta_extractor)\nhaplo0 = fasta_extractor.extract(target_interval.resize(SEQUENCE_LENGTH))\n\nhaplo1_enc = one_hot_encode(\"\".join(haplo1))[np.newaxis]\nhaplo2_enc = one_hot_encode(\"\".join(haplo2))[np.newaxis]\nhaplo0_enc = one_hot_encode(\"\".join(haplo0))[np.newaxis]\n\nprint(\"number of changes\");print(np.sum(haplo2_enc != haplo0_enc))\n\npred_human = model.predict_on_batch(haplo0_enc)['human'][0]\npred_altai = model.predict_on_batch((haplo1_enc + haplo2_enc)/2)['human'][0]\n```\n:::\n\n\n### Plot Human reference epigenome\n\n::: {#ae1aee90 .cell execution_count=9}\n``` {.python .cell-code}\n#  Plot Human and Neanderthal epigenomes\n\npredictions = pred_human\ntracks = {'DNASE:CD14-positive monocyte female': predictions[:, 41],\n          'DNASE:keratinocyte female': predictions[:, 42],\n          'CHIP:H3K27ac:keratinocyte female': predictions[:, 706],\n          'CAGE:LCL': np.log10(1 + predictions[:, 5110])}\nplot_tracks(tracks, target_interval)\n```\n:::\n\n\n### Plot Neanderthal epigenome\n\n::: {#f24afba2 .cell execution_count=10}\n``` {.python .cell-code}\npredictions = pred_altai\ntracks = {'DNASE:CD14-positive monocyte female': predictions[:, 41],\n          'DNASE:keratinocyte female': predictions[:, 42],\n          'CHIP:H3K27ac:keratinocyte female': predictions[:, 706],\n          'CAGE:LCL': np.log10(1 + predictions[:, 5110])}\nplot_tracks(tracks, target_interval)\n```\n:::\n\n\n::: {#7b65ce87 .cell execution_count=11}\n``` {.python .cell-code}\nget_summary(get_diffmat(pred_human,pred_altai))\nget_summary(column_correlations(pred_human,pred_altai))\n```\n:::\n\n\n",
    "supporting": [
      "enformer-minimal-neanderthal_files"
    ],
    "filters": [],
    "includes": {}
  }
}