{
  "hash": "26c15ea0f5a21f91d2004a1767f7e34c",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Building a GPT - companion notebook qmd\nauthor: Andrey Karpathy\ndate: '2025-04-11'\nfreeze: true\njupyter: \n  kernelspec:\n    name: \"conda-env-gene46100-py\"\n    language: \"python\"\n    display_name: \"gene46100\"\nformat:\n  html:\n    code-fold: true\n    code-line-numbers: true\n    code-tools: true\n    code-wrap: true\n---\n\n\n\n\n## Building a GPT\n\nCompanion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT. Downloaded from [here](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing)\n\n(https://github.com/karpathy/nanoGPT)\n\n### download the tiny shakespeare dataset\n\n::: {#d62311ec .cell execution_count=1}\n``` {.python .cell-code}\n# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n```\n:::\n\n\n::: {#48398be4 .cell execution_count=2}\n``` {.python .cell-code}\n# read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n```\n:::\n\n\n::: {#85c2d53a .cell execution_count=3}\n``` {.python .cell-code}\n# print the length of the dataset\nprint(\"length of dataset in characters: \", len(text))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlength of dataset in characters:  1115394\n```\n:::\n:::\n\n\n::: {#6d25d44b .cell execution_count=4}\n``` {.python .cell-code}\n# let's look at the first 1000 characters\nprint(text[:1000])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n```\n:::\n:::\n\n\n::: {#6484eb10 .cell execution_count=5}\n``` {.python .cell-code}\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n```\n:::\n:::\n\n\n::: {#bcccb3c2 .cell execution_count=6}\n``` {.python .cell-code}\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n```\n:::\n:::\n\n\n::: {#6af4d1a5 .cell execution_count=7}\n``` {.python .cell-code}\n# %pip install torch torchvision torchaudio scikit-learn\n\n# import os\n# import time\n# import math\n# import pickle\n# from contextlib import nullcontext\n\n# import numpy as np\n# import torch\n# from torch.nn.parallel import DistributedDataParallel as DDP\n# from torch.distributed import init_process_group, destroy_process_group\n\n# from model import GPTConfig, GPT\n```\n:::\n\n\n::: {#94f37b12 .cell execution_count=8}\n``` {.python .cell-code}\n# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch # we use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n```\n:::\n:::\n\n\n::: {#688c877a .cell execution_count=9}\n``` {.python .cell-code}\n# split up the data into train and validation sets\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n```\n:::\n\n\n::: {#c51b4a66 .cell execution_count=10}\n``` {.python .cell-code}\n# define the block size\nblock_size = 8\ntrain_data[:block_size+1]\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n```\n:::\n:::\n\n\n### define the context and target: 8 examples in one batch\n\n::: {#0a2c998e .cell execution_count=11}\n``` {.python .cell-code}\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nwhen input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n```\n:::\n:::\n\n\n### define the batch size and get the batch\n\n::: {#32ba9b02 .cell execution_count=12}\n``` {.python .cell-code}\ntorch.manual_seed(1337)\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ninputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\nwhen input is [24] the target: 43\nwhen input is [24, 43] the target: 58\nwhen input is [24, 43, 58] the target: 5\nwhen input is [24, 43, 58, 5] the target: 57\nwhen input is [24, 43, 58, 5, 57] the target: 1\nwhen input is [24, 43, 58, 5, 57, 1] the target: 46\nwhen input is [24, 43, 58, 5, 57, 1, 46] the target: 43\nwhen input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\nwhen input is [44] the target: 53\nwhen input is [44, 53] the target: 56\nwhen input is [44, 53, 56] the target: 1\nwhen input is [44, 53, 56, 1] the target: 58\nwhen input is [44, 53, 56, 1, 58] the target: 46\nwhen input is [44, 53, 56, 1, 58, 46] the target: 39\nwhen input is [44, 53, 56, 1, 58, 46, 39] the target: 58\nwhen input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52] the target: 58\nwhen input is [52, 58] the target: 1\nwhen input is [52, 58, 1] the target: 58\nwhen input is [52, 58, 1, 58] the target: 46\nwhen input is [52, 58, 1, 58, 46] the target: 39\nwhen input is [52, 58, 1, 58, 46, 39] the target: 58\nwhen input is [52, 58, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\nwhen input is [25] the target: 17\nwhen input is [25, 17] the target: 27\nwhen input is [25, 17, 27] the target: 10\nwhen input is [25, 17, 27, 10] the target: 0\nwhen input is [25, 17, 27, 10, 0] the target: 21\nwhen input is [25, 17, 27, 10, 0, 21] the target: 1\nwhen input is [25, 17, 27, 10, 0, 21, 1] the target: 54\nwhen input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n```\n:::\n:::\n\n\n### start with a simple model: the bigram language model\nThis model predicts the next character based on the current character only. \nIt uses the logits for the next character in a lookup table.\n\n::: {#9b87d4ce .cell execution_count=13}\n``` {.python .cell-code}\n# define the bigram language model\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    # Text Generation Method\n    # This method implements autoregressive text generation by:\n    # 1. Taking the current context (idx)\n    # 2. Predicting the next token probabilities\n    # 3. Sampling from these probabilities\n    # 4. Appending the sampled token to the context\n    # 5. Repeating for max_new_tokens iterations\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n```\n:::\n\n\n### cross entropy loss\nLoss = -Σ(y * log(p))\nwhere:\n- y = actual probability (0 or 1)\n- p = predicted probability\n- Σ = sum over all classes\n\nThis is the loss for a single token. The total loss is the average across all B*T tokens in the batch, where:\n- B = batch size (number of sequences processed in parallel)\n- T = sequence length (number of tokens in each sequence)\n\nBefore training, we would expect the model to predict the next character from a uniform distribution.\n\n-Σ(y * log(p)) = \n= - ( 1 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) ) = - log(1/65) \n= 4.1744 per token.\n\ni.e. at the beginning we expect loss of -log(1/65) = 4.1744 per token.\n\n\n### initialize the model and compute the loss\n\n::: {#b4b0ab2d .cell execution_count=14}\n``` {.python .cell-code}\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([32, 65])\ntensor(4.8786, grad_fn=<NllLossBackward0>)\n```\n:::\n:::\n\n\n### generate text\n\n::: {#c31763a1 .cell execution_count=15}\n``` {.python .cell-code}\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\nwnYWmnxKWWev-tDqXErVKLgJ\n```\n:::\n:::\n\n\n### choose AdamW as the optimizer\n\n::: {#49aa952c .cell execution_count=16}\n``` {.python .cell-code}\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n```\n:::\n\n\n### train the model\n\n::: {#d31903b4 .cell execution_count=17}\n``` {.python .cell-code}\nbatch_size = 32\nfor steps in range(100): # increase number of steps for good results...\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4.65630578994751\n```\n:::\n:::\n\n\n### generate text starting with \\n as initial context\n\n::: {#52fa67dd .cell execution_count=18}\n``` {.python .cell-code}\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\noTo.JUZ!!zqe!\nxBP qbs$Gy'AcOmrLwwt\np$x;Seh-onQbfM?OjKbn'NwUAW -Np3fkz$FVwAUEa-wzWC -wQo-R!v -Mj?,SPiTyZ;o-opr$mOiPJEYD-CfigkzD3p3?zvS;ADz;.y?o,ivCuC'zqHxcVT cHA\nrT'Fd,SBMZyOslg!NXeF$sBe,juUzLq?w-wzP-h\nERjjxlgJzPbHxf$ q,q,KCDCU fqBOQT\nSV&CW:xSVwZv'DG'NSPypDhKStKzC -$hslxIVzoivnp ,ethA:NCCGoi\ntN!ljjP3fwJMwNelgUzzPGJlgihJ!d?q.d\npSPYgCuCJrIFtb\njQXg\npA.P LP,SPJi\nDBcuBM:CixjJ$Jzkq,OLf3KLQLMGph$O 3DfiPHnXKuHMlyjxEiyZib3FaHV-oJa!zoc'XSP :CKGUhd?lgCOF$;;DTHZMlvvcmZAm;:iv'MMgO&Ywbc;BLCUd&vZINLIzkuTGZa\nD.?\n```\n:::\n:::\n\n\n## The mathematical trick in self-attention\n\n### toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n\n::: {#f83f7b52 .cell execution_count=19}\n``` {.python .cell-code}\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0,10,(3,2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('--')\nprint('b=')\nprint(b)\nprint('--')\nprint('c=')\nprint(c)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\na=\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n--\nb=\ntensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n--\nc=\ntensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n```\n:::\n:::\n\n\n::: {#024db3e4 .cell execution_count=20}\n``` {.python .cell-code}\n# consider the following toy example:\n\ntorch.manual_seed(1337)\nB,T,C = 4,8,2 # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=20}\n```\ntorch.Size([4, 8, 2])\n```\n:::\n:::\n\n\n### version 1: using a for loop to compute the weighted aggregation\n\n::: {#2dc88b5c .cell execution_count=21}\n``` {.python .cell-code}\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n```\n:::\n\n\n### version 2: using matrix multiply for a weighted aggregation\n\n::: {#9ef722a4 .cell execution_count=22}\n``` {.python .cell-code}\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\ntorch.allclose(xbow, xbow2)\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\nTrue\n```\n:::\n:::\n\n\n### version 3: use Softmax\n\n::: {#a85f5fd4 .cell execution_count=23}\n``` {.python .cell-code}\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\nTrue\n```\n:::\n:::\n\n\n**softmax function**\n\nsoftmax($z_i$) = $\\frac{e^{z_i}}{\\sum_j e^{z_j}}$\n\n# version 4: self-attention\n\n::: {#285a1676 .cell execution_count=24}\n``` {.python .cell-code}\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)   # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\n#wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n#out = wei @ x\n\nout.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\ntorch.Size([4, 8, 16])\n```\n:::\n:::\n\n\n::: {#14d8afb4 .cell execution_count=25}\n``` {.python .cell-code}\nwei[0]\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n       grad_fn=<SelectBackward0>)\n```\n:::\n:::\n\n\nNotes:\n- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n\n\n### why scaled attention?\n- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n\n::: {#d272ed2a .cell execution_count=26}\n``` {.python .cell-code}\nk = torch.randn(B,T,head_size)\nq = torch.randn(B,T,head_size)\nwei = q @ k.transpose(-2, -1) * head_size**-0.5\n```\n:::\n\n\n::: {#fbd850fd .cell execution_count=27}\n``` {.python .cell-code}\nk.var()\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\ntensor(1.0449)\n```\n:::\n:::\n\n\n::: {#e474f33f .cell execution_count=28}\n``` {.python .cell-code}\nq.var()\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\ntensor(1.0700)\n```\n:::\n:::\n\n\n::: {#49aa7d17 .cell execution_count=29}\n``` {.python .cell-code}\nwei.var()\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\ntensor(1.0918)\n```\n:::\n:::\n\n\n::: {#28c82f93 .cell execution_count=30}\n``` {.python .cell-code}\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\ntensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n```\n:::\n:::\n\n\n::: {#ab5d4322 .cell execution_count=31}\n``` {.python .cell-code}\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\ntensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n```\n:::\n:::\n\n\n### LayerNorm1d\n\n::: {#faee1526 .cell execution_count=32}\n``` {.python .cell-code}\nclass LayerNorm1d: # (used to be BatchNorm1d)\n\n  def __init__(self, dim, eps=1e-5, momentum=0.1):\n    self.eps = eps\n    self.gamma = torch.ones(dim)\n    self.beta = torch.zeros(dim)\n\n  def __call__(self, x):\n    # calculate the forward pass\n    xmean = x.mean(1, keepdim=True) # batch mean\n    xvar = x.var(1, keepdim=True) # batch variance\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n    return self.out\n\n  def parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = LayerNorm1d(100)\nx = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\nx = module(x)\nx.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\ntorch.Size([32, 100])\n```\n:::\n:::\n\n\n::: {#8ac37706 .cell execution_count=33}\n``` {.python .cell-code}\nx[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```\n(tensor(0.1469), tensor(0.8803))\n```\n:::\n:::\n\n\n::: {#733db5bd .cell execution_count=34}\n``` {.python .cell-code}\nx[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```\n(tensor(2.3842e-09), tensor(1.0000))\n```\n:::\n:::\n\n\n::: {#e4f32cc1 .cell execution_count=35}\n``` {.python .cell-code}\n# French to English translation example:\n\n# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n```\n:::\n\n\n### Full finished code, for reference\n\nYou may want to refer directly to the git repo instead though.\n\n::: {#7f62a059 .cell execution_count=36}\n``` {.python .cell-code}\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 16 # how many independent sequences will we process in parallel?\nblock_size = 32 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 100\nlearning_rate = 1e-3\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 64\nn_head = 4\nn_layer = 4\ndropout = 0.0\n# ------------\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinys Shakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,C)\n        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\n# super simple bigram model\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = BigramLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.209729 M parameters\nstep 0: train loss 4.4116, val loss 4.4022\nstep 100: train loss 2.6568, val loss 2.6670\nstep 200: train loss 2.5090, val loss 2.5059\nstep 300: train loss 2.4195, val loss 2.4336\nstep 400: train loss 2.3502, val loss 2.3566\nstep 500: train loss 2.2964, val loss 2.3128\nstep 600: train loss 2.2410, val loss 2.2499\nstep 700: train loss 2.2052, val loss 2.2190\nstep 800: train loss 2.1640, val loss 2.1872\nstep 900: train loss 2.1243, val loss 2.1506\nstep 1000: train loss 2.1025, val loss 2.1295\nstep 1100: train loss 2.0690, val loss 2.1180\nstep 1200: train loss 2.0370, val loss 2.0782\nstep 1300: train loss 2.0249, val loss 2.0641\nstep 1400: train loss 1.9917, val loss 2.0352\nstep 1500: train loss 1.9698, val loss 2.0305\nstep 1600: train loss 1.9644, val loss 2.0495\nstep 1700: train loss 1.9404, val loss 2.0141\nstep 1800: train loss 1.9093, val loss 1.9967\nstep 1900: train loss 1.9063, val loss 1.9853\nstep 2000: train loss 1.8859, val loss 1.9967\nstep 2100: train loss 1.8710, val loss 1.9753\nstep 2200: train loss 1.8600, val loss 1.9630\nstep 2300: train loss 1.8567, val loss 1.9561\nstep 2400: train loss 1.8424, val loss 1.9435\nstep 2500: train loss 1.8141, val loss 1.9422\nstep 2600: train loss 1.8240, val loss 1.9372\nstep 2700: train loss 1.8140, val loss 1.9382\nstep 2800: train loss 1.8061, val loss 1.9217\nstep 2900: train loss 1.8034, val loss 1.9297\nstep 3000: train loss 1.7949, val loss 1.9211\nstep 3100: train loss 1.7716, val loss 1.9195\nstep 3200: train loss 1.7515, val loss 1.9071\nstep 3300: train loss 1.7574, val loss 1.9046\nstep 3400: train loss 1.7588, val loss 1.8995\nstep 3500: train loss 1.7364, val loss 1.8957\nstep 3600: train loss 1.7274, val loss 1.8867\nstep 3700: train loss 1.7284, val loss 1.8823\nstep 3800: train loss 1.7184, val loss 1.8895\nstep 3900: train loss 1.7212, val loss 1.8728\nstep 4000: train loss 1.7147, val loss 1.8587\nstep 4100: train loss 1.7112, val loss 1.8750\nstep 4200: train loss 1.7051, val loss 1.8598\nstep 4300: train loss 1.6994, val loss 1.8420\nstep 4400: train loss 1.7088, val loss 1.8676\nstep 4500: train loss 1.6871, val loss 1.8471\nstep 4600: train loss 1.6870, val loss 1.8330\nstep 4700: train loss 1.6842, val loss 1.8427\nstep 4800: train loss 1.6650, val loss 1.8424\nstep 4900: train loss 1.6709, val loss 1.8404\nstep 4999: train loss 1.6634, val loss 1.8222\n\nFlie?\n\nWARWICK:\nOur his you bewiter are a toom hemen breabet the TI see it\nWhat satell in rowers clanme; artand, those it;\nGives time.\n\nARCHILIV:\nGive sear that, wille the is news it cy as explace and serves no cause's\ney let herefords in a blood sean up sommity.\n\nPOLIXENE:\nThe son:\nThel as use to, set these I I vourt,-afor this proak and the rien allate a noncead,\nAnger in may Crembreak in that's beant,\nRoman,; I 'till! Poyant of ouchber\nTo\nThis nog must he anour,\nAs coutinstage: but but the beamied,\nAnd for may lie them yet nob the boy not.\n\nDUKE IV:\nTo state the be, but and too her promenator him,\nBut much some not make thee, for you\nhis stage was babsom! and now your and\ntowing-sweet you,\nGo coursein to state of the stender of Tyberroke all gentled to plartes no adds in en shails.\n\nPAULINGS:\nWith to merselfry manners for the ploceiness and wiltwien,\nAnd adve the now my ageingrale mine!\nOr he him my spaut, I somut.\nYou as where'shefore, I githed stide to seinged forbying me begom as goody.\nGo and neer thy from me pronme-his here.\n\nROMEO:\nCain his you will! I but\nbut these it were is turnstry to all gok\nshouls' boties. joy to you would dost\nFrom Buckry to thy will;\nBut in an in himself I addstleb\nThat that gentrante well.\nO, hither, is the dalts it soon to thee,\nyou knows wheep his seeps and in togs,\nHere thou make of commets, neyes;\nWhereigh my sincrees; and, yet, huse party of us Voly.\n\nSetily;\nI wear his crumpust set, that would Edwift a those as old\non his good from wit-be in his in\nHarth wie the chanter: me the ortiss\nAnd yearges us coman. Your were you, where 'twon enmer, 'word.\n\nPOMBY:\nThou king azmouTher:\nWhat flear you must mine ry set gone,\nOr sappure, contessinous to have their they as I\nhave heil and scorfole wefpareter the by it.\n\nFirst I knounger-move,\nOur wout our as of formselfess.\n\nBUCHINTI:\nAs one thumsely, we it\nShare dayneds flase thout?\nAt you, and weing as move, it by hurself is on the for the not,\nAndmner, I Leturn, Warmine and you,\nThat wha\n```\n:::\n:::\n\n\n",
    "supporting": [
      "gpt_dev_files"
    ],
    "filters": [],
    "includes": {}
  }
}