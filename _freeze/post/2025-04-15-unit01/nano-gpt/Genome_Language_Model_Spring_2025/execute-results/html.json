{
  "hash": "e8fb8c0655f154808dee4376c495b3f7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Genomic language model training with nanoGPT - qmd version\nauthor: Henry Raeder\ndate: '2025-04-22'\ncategories:\n  - notebook\n  - gene46100\ndescription: Henry's genomic language model trained on human reference genome with  nanogpt\ndate-modified: last-modified\ndraft: false\neval: false\njupyter: \n  kernelspec:\n    name: \"conda-env-nanogpt46100-py\"\n    language: \"python\"\n    display_name: \"nanogpt46100\"\n---\n\n\n\n\n**Acknowledgement**:\n\n*   Andrej Karpathy (nanoGPT model framework)\n*   Dalla-torre et. al. (Nucleotide Transformer workbook)\n\n## Introduction\n\nIn this notebook, we explore how one can pretrain and fine-tune a basic genomic language model (gLM). As the basis of our model, we use Andrej Karpathy's publicly-available [nanoGPT framework](https://github.com/karpathy/nanoGPT) (with some slight edits). This is a flexible implementation of a basic GPT model (based on GPT-2) that can take generic text as input. Here, we showcase how relatively sophisticated models can be trained efficiently, and how these can be fine-tuned to specific prediction tasks using datasets pulled from the [Nucleotide Transformer training notebook](https://github.com/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling.ipynb) (direct sources will be cited when datasets are introduced). Generally, the goal of this notebook is to provide hands-on experience in working with language models, and give insight into the fundamentals of attention-based deep learning.\n\n## Steps\n\nThis notebook demonstrates how to:\n\n\n*   Tokenize genomic data to be read by a language model\n*   Train and sample from a generative language model\n*   Fine-tune a model with additional training to perform specific tasks\n\n\n## Setup\n\nFirst, it is necessary to connect to the GPU. If you are working in Colab, this can be done in the upper right corner by selecting \"T4 GPU\" as your runtime option.\n\nNext, we need to install a few things that help us build and run the model efficiently.\n\n::: {#66cb4954 .cell execution_count=1}\n``` {.python .cell-code}\n#%pip install torch numpy transformers datasets tiktoken wandb tqdm biopython huggingface_hub accelerate\nimport os\nimport pickle\nimport requests\nimport numpy as np\nimport math\nimport inspect\nfrom dataclasses import dataclass\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport gc\nimport time\nfrom contextlib import nullcontext\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.distributed import init_process_group, destroy_process_group\nfrom datasets import load_dataset, Dataset\nfrom torch.utils.data import DataLoader, TensorDataset\n#from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification\nfrom sklearn.metrics import matthews_corrcoef, f1_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n## Download and Read the Reference Genome\n\nFor our training, we will be using the GRCh38 human reference genome (credit to the Genome Reference Consortium: https://www.ncbi.nlm.nih.gov/grc). In the next block, we simply create a place to store our genome in the Colab filesystem and download the genome file remotely.\n\n::: {#49746c35 .cell execution_count=2}\n``` {.python .cell-code}\nrunning_in_colab = False\nif running_in_colab:\n  WORKDIR = '/root/data/'\n  !mkdir /root/data\n  fasta_file = WORKDIR + 'genome.fa'\n  !wget -O - https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz | gunzip -c > {fasta_file}\n  input_file_path = os.path.join(WORKDIR, 'genome.fa')\nelse:\n  WORKDIR = '~/Downloads/'\n  fasta_path = '/Users/haekyungim/Library/CloudStorage/Box-Box/LargeFiles/imlab-data/Reference-Data/ref_sequences/hg38/'\n  fasta_file = fasta_path + 'Homo_sapiens_assembly38.fasta'\n  input_file_path = fasta_file\n  # Properly expand the home directory if using ~\n  WORKDIR = os.path.expanduser('~/Downloads')\n  # Create directory if it doesn't exist\n  os.makedirs(WORKDIR, exist_ok=True)\n\n\n\"\"\"\nPrepare the hg38 genomic dataset for character-level language modeling.\nSo instead of encoding with BPE tokens, we just map characters to ints.\nWill save train.bin, val.bin containing the ids, and meta.pkl containing the\nencoder and decoder and some other related info.\n\"\"\"\n```\n:::\n\n\nNow that we have the genome file saved locally, we have to read it into our environment in order to make it usable. To do this, we just read in the entire file as one long string, which we can then subset and investigate as we wish. For example, we may want to know the total length of our dataset, and all of the unique characters we have:\n\n::: {#82824195 .cell execution_count=3}\n``` {.python .cell-code}\nwith open(input_file_path, 'r') as f:\n    data = f.read()\nprint(f\"length of dataset in characters: {len(data):,}\")\n\n# get all the unique characters that occur in this text\nchars = sorted(list(set(data)))\nvocab_size = len(chars)\nprint(\"all the unique characters:\", ''.join(chars))\nprint(f\"vocab size: {vocab_size:,}\")\n```\n:::\n\n\nGreat! The number of characters we have is approximately the same as the number of nucleotides in the human genome (about 3.2 billion), so it looks like our data imported correctly. However, you may notice that we have more characters than just \"A\", \"T\", \"C\", \"G\", and \"N\" as you may expect from genomic data. This is because our text file includes headers for each part of the genome assembly (i.e. chromosomes and fragments).\n\nUPDATE: Daniel tested that performance of downstream task is better when removing non DNA characters. ~~One nice thing about working with a language model is that we don't need to worry about this. Even if the input data is a bit messy, the model is essentially designed to predict the most likely characters based on a general input. Since the headers make up a tiny fraction of the total number of characters, our model will assign a very very low probability to each of the non-ATCGN characters, and they will not influence our outputs.~~\nTo keep the example simple, we will keep using the full genome file with non-DNA characters.\n\n## Mapping Characters to Integers (i.e. Tokenization)\n\nNow that we have our text, we need to put it into a format that is more easily readable for our model. Generally, this process is called Tokenization. There are many different methods for this, and each will impact the performance of the model differently (for example byte pair encoding or non-overlapping kmer tokenization used by other gLMs), but for the purpose of this notebook we will use a simple character-level tokenization. This means that we will simply assign a different integer to each unique character, and then transform our input into a long list of integers to be read by our model:\n\n::: {#b4ab15b0 .cell execution_count=4}\n``` {.python .cell-code}\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\n```\n:::\n\n\n::: {#d7423e28 .cell execution_count=5}\n``` {.python .cell-code}\ndef encode(s):\n    return [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndef decode(l):\n    return ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n```\n:::\n\n\nNow that we have our functions defined, it is time to actually do the tokenization. This will take a few minutes, so go ahead and run the block, and see the description of what it is doing below:\n\n::: {#57b8ebb8 .cell execution_count=6}\n``` {.python .cell-code}\n# create the train and test splits\nn = len(data)\ntrain_data = data[:int(n*0.9)]\nval_data = data[int(n*0.9):]\n\n#save memory\ndel data\n\n#helper function to store/append data to binary files and save RAM\ndef append_to_binary_file(filename, obj):\n    with open(filename, 'ab') as file:  # Open in append-binary mode\n        obj.tofile(file)\n\n# encode both to integers\nntrain=len(train_data)\nnval = len(val_data)\nfor i in np.arange(0,1,0.05):\n  print(f\"Tokenizing: {int(i*100):,}% complete\")\n  train_ids = encode(train_data[int(ntrain*i):int(ntrain*(i+0.05))])\n  val_ids = encode(val_data[int(nval*i):int(nval*(i+0.05))])\n  train_ids = np.array(train_ids, dtype=np.uint16)\n  val_ids = np.array(val_ids, dtype=np.uint16)\n  append_to_binary_file(os.path.join(WORKDIR, 'train.bin'), train_ids)\n  append_to_binary_file(os.path.join(WORKDIR, 'val.bin'), val_ids)\nprint(f\"train has {ntrain:,} tokens\")\nprint(f\"val has {nval:,} tokens\")\n```\n:::\n\n\n1.   First, we are dividing the genome into a training and testing dataset using a simple 90/10 split.\n2.   Next, we create a helper function that will help us save the integer lists locally. These end up taking up a lot of space in our Python environment, so it is better to save them to the disk and only access them when needed rather than having them sit and take up RAM.\n3.   Finally, we actually go through and encode/tokenize our data. Since the dataset is so large, we don't have enough memory to do this all at once in Colab, so we encode our data in batches and then push each new batch to our locally-saved files.\n\n\nWe also want to save some of our metadata (i.e. vocabulary size, and our mappings from characters to integers and back) to pull back in later.\n\n::: {#a8ea1ebb .cell execution_count=7}\n``` {.python .cell-code}\n# save the meta information as well, to help us encode/decode later\nmeta = {\n    'vocab_size': vocab_size,\n    'itos': itos,\n    'stoi': stoi,\n}\nwith open(os.path.join(WORKDIR, 'meta.pkl'), 'wb') as f:\n    pickle.dump(meta, f)\n```\n:::\n\n\n::: {#50ab9022 .cell execution_count=8}\n``` {.python .cell-code}\nif running_in_colab:\n  os.kill(os.getpid(), 9) #If running in a low-memory environment, this will clear our RAM. After this, go run the very first block again!\n```\n:::\n\n\n#### Questions:\n\n\n1.   What does each \"token\" in our model represent? I.e. a single base, a stretch of a few bases, some functional region, etc.?\n2.   What is the purpose of tokenization? Why is it necessary to train our model?\n\n\n## Creating the Model\n\nNow that we have our training data ready to go, it's time to actually create the gLM. First, we define the function we are going to use to do the heavy lifting in the attention layers. We will go into more detail on what we mean by attention during training, but if you are interested in what the implementation looks like, see the block below (and be sure to run it either way!)\n\n::: {#9f08fe47 .cell execution_count=9}\n``` {.python .cell-code}\n#Defining attention function (copied from Pytorch documentation, just need to save weights correctly)\ndef scaled_dot_product_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, scale=None):\n    L, S = query.size(-2), key.size(-2)\n    scale_factor = 1 / math.sqrt(query.size(-1)) if scale is None else scale\n    attn_bias = torch.zeros(L, S, dtype=query.dtype, device=query.device)\n    if is_causal:\n        assert attn_mask is None\n        temp_mask = torch.ones(L, S, dtype=torch.bool, device=query.device).tril(diagonal=0)\n        attn_bias.masked_fill_(temp_mask.logical_not(), float(\"-inf\"))\n        attn_bias.to(query.dtype)\n\n    if attn_mask is not None:\n        if attn_mask.dtype == torch.bool:\n            attn_bias.masked_fill_(attn_mask.logical_not(), float(\"-inf\"))\n        else:\n            attn_bias += attn_mask\n    attn_weight = query @ key.transpose(-2, -1) * scale_factor\n    attn_weight += attn_bias\n    attn_weight = torch.softmax(attn_weight, dim=-1)\n    attn_weight = torch.dropout(attn_weight, dropout_p, train=True)\n    return attn_weight, attn_weight @ value\n```\n:::\n\n\nNow that we have our attention function, it's time to define what our model will look like. This is a lot of code, so we will provide a short summary of what each layer is doing and the order in which they occur. If you are interested in the specifics, feel free to read through the code in detail.\n\nThe main model is created as a class simply called **GPT**. Within this model, we have a few different types of layers/elements:\n\n\n*   **Embedding**: In our model, we project our relatively simple vocabulary (40 characters) into a higher-dimensional space in order to increase the model's capacity to understand more complex relationships between tokens. This is a bit difficult to visualize, but in essence Embedding layers just perform this projection and return the resulting dictionary to use later in our model. For language models, we usually embed both the tokens and the positions of the tokens, for reasons we will discuss later.\n*   **LayerNorm**: As the name suggests, this layer simply applies normalization across the input features. This stabilizes the training, as it ensures each feature has a similar scale.\n*   **Dropout**: Dropout layers randomly zero-out some of the elements in the input tensor. This serves as a form of regularization, and helps avoid overfitting our training data.\n*   **Causal Self-Attention**: The mechanism of this layer will be described in further detail below, but this is the layer where our model learns the relationship between each of the tokens in the input, which is what gives it predictive power to generate what token will come next.\n*   **MLP**: One can think of the MLP class as a mini neural network within our model. It is made up of a linear layer, a GELU activation layer, another linear layer, and finally a dropout layer. In short, the MLP element introduces non-linearity into our model, and helps us transform the output of our attention layer back into something we can pass through attention again, and decode into DNA sequence.\n*   **Block**: This is not really a layer, it is more a sequence of layers. Specifically, we define a block as LayerNorm → Causal Self-Attention → LayerNorm → MLP\n\nNow that we have these modules defined, the general outline of our GPT model is as below (assume the outputs of each step are passed to the next):\n\n\n1.   Token and Position Embedding Layers\n2.   Dropout layer\n3.   Blocks (repeated $n$ times, i.e. if we want 6 attention layers we will have 6 Blocks)\n4.   LayerNorm\n5.   Linear layer to transform embedded outputs back into our vocab size\n6.   Loss calculation (if training)\n\n::: {#e2fa2583 .cell execution_count=10}\n``` {.python .cell-code}\n\"\"\"\nFull definition of a GPT Language Model, all of it in this single file.\nReferences:\n1) the official GPT-2 TensorFlow implementation released by OpenAI:\nhttps://github.com/openai/gpt-2/blob/master/src/model.py\n2) huggingface/transformers PyTorch implementation:\nhttps://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py\n\"\"\"\ngc.collect()\n\nclass LayerNorm(nn.Module):\n    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n\nclass CausalSelfAttention(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0\n        # key, query, value projections for all heads, but in a batch\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n        # output projection\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n        # regularization\n        self.attn_dropout = nn.Dropout(config.dropout)\n        self.resid_dropout = nn.Dropout(config.dropout)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.dropout = config.dropout\n        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n\n    def forward(self, x):\n        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n\n        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n\n        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n        if self.flash:\n            # efficient attention using Flash Attention CUDA kernels\n            attn_weight, y = scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n\n        # output projection\n        y = self.resid_dropout(self.c_proj(y))\n        return y, torch.mean(attn_weight, dim=0)\n\nclass MLP(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.gelu    = nn.GELU()\n        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.dropout)\n\n    def forward(self, x):\n        x = self.c_fc(x)\n        x = self.gelu(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x\n\nclass Block(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.mlp = MLP(config)\n\n    def forward(self, x):\n        val, attn_weight = self.attn(self.ln_1(x))\n        x = x + val\n        x = x + self.mlp(self.ln_2(x))\n        return x, attn_weight\n\n@dataclass\nclass GPTConfig: #Default values for configuration, originally designed to match GPT-2, will override for training\n    block_size: int = 1024\n    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    dropout: float = 0.0\n    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n\nclass GPT(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None\n        assert config.block_size is not None\n        self.config = config\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            drop = nn.Dropout(config.dropout),\n            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n        ))\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n\n        # init all weights\n        self.apply(self._init_weights)\n        # apply special scaled init to the residual projections, per GPT-2 paper\n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):\n                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\n        # report number of parameters\n        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n\n    def get_num_params(self, non_embedding=True):\n        \"\"\"\n        Return the number of parameters in the model.\n        For non-embedding count (default), the position embeddings get subtracted.\n        The token embeddings would too, except due to the parameter sharing these\n        params are actually used as weights in the final layer, so we include them.\n        \"\"\"\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= self.transformer.wpe.weight.numel()\n        return n_params\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        #device = idx.device\n        b, t = idx.size()\n        weight_lst = []\n        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n\n        # forward the GPT model itself\n        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x, attn_weight = block(x)\n            weight_lst.append(attn_weight)\n        x = self.transformer.ln_f(x)\n\n        if targets is not None:\n            # if we are given some desired targets also calculate the loss\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n        else:\n            # inference-time mini-optimization: only forward the lm_head on the very last position\n            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n            loss = None\n\n        return logits, loss, weight_lst\n\n    def crop_block_size(self, block_size):\n        # model surgery to decrease the block size if necessary\n        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n        # but want to use a smaller block size for some smaller, simpler model\n        assert block_size <= self.config.block_size\n        self.config.block_size = block_size\n        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n        for block in self.transformer.h:\n            if hasattr(block.attn, 'bias'):\n                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n\n    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n        # start with all of the candidate parameters\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        # filter out those that do not require grad\n        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n        optim_groups = [\n            {'params': decay_params, 'weight_decay': weight_decay},\n            {'params': nodecay_params, 'weight_decay': 0.0}\n        ]\n        num_decay_params = sum(p.numel() for p in decay_params)\n        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n        # Create AdamW optimizer and use the fused version if it is available\n        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n        use_fused = fused_available and device_type == 'cuda'\n        extra_args = dict(fused=True) if use_fused else dict()\n        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n        print(f\"using fused AdamW: {use_fused}\")\n\n        return optimizer\n\n    def estimate_mfu(self, fwdbwd_per_iter, dt):\n        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n        # first estimate the number of flops we do per iteration.\n        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n        N = self.get_num_params()\n        cfg = self.config\n        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n        flops_per_token = 6*N + 12*L*H*Q*T\n        flops_per_fwdbwd = flops_per_token * T\n        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n        # express our flops throughput as ratio of A100 bfloat16 peak flops\n        flops_achieved = flops_per_iter * (1.0/dt) # per second\n        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n        mfu = flops_achieved / flops_promised\n        return mfu\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        \"\"\"\n        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n        \"\"\"\n        for _ in range(max_new_tokens):\n            # if the sequence context is growing too long we must crop it at block_size\n            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n            # forward the model to get the logits for the index in the sequence\n            logits, _ = self(idx_cond)[:2]\n            # pluck the logits at the final step and scale by desired temperature\n            logits = logits[:, -1, :] / temperature\n            # optionally crop the logits to only the top k options\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            # apply softmax to convert logits to (normalized) probabilities\n            probs = F.softmax(logits, dim=-1)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1)\n            # append sampled index to the running sequence and continue\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx\n```\n:::\n\n\n## Training\n\nNow that our model constructors are ready, it's time to start training! First, see below for the configuration settings we are using for the model. Most of these are not important for the purposes of this notebook, but just note that we are creating a model with 4 attention layers with 4 heads each (more on that below), our embedding size is 384 (so our vocabulary gets projected into 384-dimensional space), and our block size/context size is 300, so our model will look at 300 bp of sequence as each input. Training will take a while (about 5-10 minutes), so run the code block below and go to the end for a description of the Attention mechanism and a rundown of what our training loop does.\n\n::: {#fc4c0ca0 .cell execution_count=11}\n``` {.python .cell-code}\n#@title Training Time!\n\ngc.collect()\n# -----------------------------------------------------------------------------\n# default config values designed to train a gpt on hg38 reference genome\n# I/O\nout_dir = WORKDIR + 'out'\neval_interval = 1000\nlog_interval = 100\neval_iters = 100\neval_only = False # if True, script exits right after the first eval\nalways_save_checkpoint = False # if True, always save a checkpoint after each eval\ninit_from = 'scratch' # 'scratch' or 'resume' or 'gpt2*'\n# wandb logging\nwandb_log = False # disabled by default\nwandb_project = 'genome_char'\nwandb_run_name = 'mini-gpt' # 'run' + str(time.time())\n# data\ndataset = 'genome_char'\ngradient_accumulation_steps = 1 # used to simulate larger batch sizes\nbatch_size = 64 # if gradient_accumulation_steps > 1, this is the micro-batch size\nblock_size = 300\n# model\nn_layer = 4\nn_head = 4\nn_embd = 384\ndropout = 0.2 # for pretraining 0 is good, for finetuning try 0.1+\nbias = False # do we use bias inside LayerNorm and Linear layers?\n# adamw optimizer\nlearning_rate = 1e-3 # max learning rate\nmax_iters = 4000 # total number of training iterations\nweight_decay = 1e-1\nbeta1 = 0.9\nbeta2 = 0.99\ngrad_clip = 1.0 # clip gradients at this value, or disable if == 0.0\n# learning rate decay settings\ndecay_lr = True # whether to decay the learning rate\nwarmup_iters = 100 # how many steps to warm up for\nlr_decay_iters = 4000 # should be ~= max_iters per Chinchilla\nmin_lr = 1e-4 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla\n# DDP settings\n#backend = 'nccl' # 'nccl', 'gloo', etc.\n# system\ndevice = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\ndtype = 'float16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\nif torch.cuda.is_available() and device == 'cuda':\n    print('cuda')\nelse:\n    print('CPU')\ncompile = True # use PyTorch 2.0 to compile the model to be faster\n# -----------------------------------------------------------------------------\nconfig_keys = [k for k,v in globals().items() if not k.startswith('_') and isinstance(v, (int, float, bool, str))]\n#exec(open('configurator.py').read()) # overrides from command line or config file\nconfig = {k: globals()[k] for k in config_keys} # will be useful for logging\n# -----------------------------------------------------------------------------\n\n# various inits, derived attributes, I/O setup\nmaster_process = True\nseed_offset = 0\nddp_world_size = 1\ntokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\nprint(f\"tokens per iteration will be: {tokens_per_iter:,}\")\n\nif master_process:\n    os.makedirs(out_dir, exist_ok=True)\ntorch.manual_seed(1337 + seed_offset)\ntorch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\ntorch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\ndevice_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n# note: float16 data type will automatically use a GradScaler\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\nctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n\n# poor man's data loader\ndata_dir = '/root/data'\ndef get_batch(split):\n    # We recreate np.memmap every batch to avoid a memory leak, as per\n    # https://stackoverflow.com/questions/45132940/numpy-memmap-memory-usage-want-to-iterate-once/61472122#61472122\n    if split == 'train':\n        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n    else:\n        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n    if device_type == 'cuda':\n        # pin arrays x,y, which allows us to move them to GPU asynchronously (non_blocking=True)\n        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n    else:\n        x, y = x.to(device), y.to(device)\n    return x, y\n\n# init these up here, can override if init_from='resume' (i.e. from a checkpoint)\niter_num = 0\nbest_val_loss = 1e9\n\n# attempt to derive vocab_size from the dataset\nmeta_path = os.path.join(data_dir, 'meta.pkl')\nmeta_vocab_size = None\nif os.path.exists(meta_path):\n    with open(meta_path, 'rb') as f:\n        meta = pickle.load(f)\n    meta_vocab_size = meta['vocab_size']\n    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")\n\n# model init\nmodel_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n                  bias=bias, vocab_size=None, dropout=dropout) # start with model_args from command line\nif init_from == 'scratch':\n    # init a new model from scratch\n    print(\"Initializing a new model from scratch\")\n    # determine the vocab size we'll use for from-scratch training\n    if meta_vocab_size is None:\n        print(\"defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\")\n    model_args['vocab_size'] = meta_vocab_size if meta_vocab_size is not None else 50304\n    gptconf = GPTConfig(**model_args)\n    model = GPT(gptconf)\n# crop down the model block size if desired, using model surgery\nif block_size < model.config.block_size:\n    model.crop_block_size(block_size)\n    model_args['block_size'] = block_size # so that the checkpoint will have the right value\nmodel.to(device)\n\n# initialize a GradScaler. If enabled=False scaler is a no-op\nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n\n# optimizer\noptimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)\n#if init_from == 'resume':\n#    optimizer.load_state_dict(checkpoint['optimizer'])\ncheckpoint = None # free up memory\n\n# compile the model\nif compile:\n    print(\"compiling the model... (takes a ~minute)\")\n    unoptimized_model = model\n    model = torch.compile(model) # requires PyTorch 2.0\n\n# helps estimate an arbitrarily accurate loss over either split using many batches\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            with ctx:\n                logits, loss = model(X, Y)[:2]\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n# learning rate decay scheduler (cosine with warmup)\ndef get_lr(it):\n    # 1) linear warmup for warmup_iters steps\n    if it < warmup_iters:\n        return learning_rate * it / warmup_iters\n    # 2) if it > lr_decay_iters, return min learning rate\n    if it > lr_decay_iters:\n        return min_lr\n    # 3) in between, use cosine decay down to min learning rate\n    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n    return min_lr + coeff * (learning_rate - min_lr)\n\n# logging\nif wandb_log and master_process:\n    import wandb\n    wandb.init(project=wandb_project, name=wandb_run_name, config=config)\n\n# training loop\nX, Y = get_batch('train') # fetch the very first batch\nt0 = time.time()\nlocal_iter_num = 0 # number of iterations in the lifetime of this process\nraw_model = model\nrunning_mfu = -1.0\nwhile True:\n\n    # determine and set the learning rate for this iteration\n    lr = get_lr(iter_num) if decay_lr else learning_rate\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n    # evaluate the loss on train/val sets and write checkpoints\n    if iter_num % eval_interval == 0 and master_process:\n        losses = estimate_loss()\n        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n        if wandb_log:\n            wandb.log({\n                \"iter\": iter_num,\n                \"train/loss\": losses['train'],\n                \"val/loss\": losses['val'],\n                \"lr\": lr,\n                \"mfu\": running_mfu*100, # convert to percentage\n            })\n        if losses['val'] < best_val_loss or always_save_checkpoint:\n            best_val_loss = losses['val']\n            if iter_num > 0:\n                checkpoint = {\n                    'model': raw_model.state_dict(),\n                    'optimizer': optimizer.state_dict(),\n                    'model_args': model_args,\n                    'iter_num': iter_num,\n                    'best_val_loss': best_val_loss,\n                    'config': config,\n                }\n                print(f\"saving checkpoint to {out_dir}\")\n                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n    if iter_num == 0 and eval_only:\n        break\n\n    # forward backward update, with optional gradient accumulation to simulate larger batch size\n    # and using the GradScaler if data type is float16\n    for micro_step in range(gradient_accumulation_steps):\n        with ctx:\n            logits, loss = model(X, Y)[:2]\n            loss = loss / gradient_accumulation_steps # scale the loss to account for gradient accumulation\n        # immediately async prefetch next batch while model is doing the forward pass on the GPU\n        X, Y = get_batch('train')\n        # backward pass, with gradient scaling if training in fp16\n        scaler.scale(loss).backward()\n    # clip the gradient\n    if grad_clip != 0.0:\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n    # step the optimizer and scaler if training in fp16\n    scaler.step(optimizer)\n    scaler.update()\n    # flush the gradients as soon as we can, no need for this memory anymore\n    optimizer.zero_grad(set_to_none=True)\n\n    # timing and logging\n    t1 = time.time()\n    dt = t1 - t0\n    t0 = t1\n    if iter_num % log_interval == 0 and master_process:\n        # get loss as float. note: this is a CPU-GPU sync point\n        # scale up to undo the division above, approximating the true total loss (exact would have been a sum)\n        lossf = loss.item() * gradient_accumulation_steps\n        if local_iter_num >= 5: # let the training loop settle a bit\n            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n    iter_num += 1\n    local_iter_num += 1\n\n    # termination conditions\n    if iter_num > max_iters:\n        break\n```\n:::\n\n\nNow that we've started training, let's talk about Attention. The first paper which outlines the Transformer neural network architecture (which relies heavily on the Attention mechanism and is the basis for most large language models today) is [Attention is All you Need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf). This paper describes the application of Attention in a machine translation model, but the underlying ideas can be applied across any model that processes sequences of text.\n\nAt its core, the Attention mechanism allows us to analyze the pairwise relationship between every token in a given sequence. So, in a very simple example, if you're looking at the sentence \"The cow jumped over the moon\", and you are tokenizing by word, you may expect there to be a strong relationship between \"jumped\" and \"over\" (as one tends to jump over things), but not between the two iterations of \"the\". Below, I describe the actual mechanism behind this in plain language. It will be an oversimplification, but hopefully it can provide some context into what our model is doing.\n\nThe way we model this using Attention is via Query (Q), Key (K), and Value (V) matrices. In short, the input into the Attention layer goes through three separate matrix multiplication processes with different weights to create Q, K, and V. The idea behind these matrices is that Q represents each position/token in the sequence, and what information each of those positions is \"looking for\". K represents the information that each position contains, and therefore the actual Attention Score is obtained by doing matrix multiplication on Q and K. If two positions are a good match, they will have a higher attention score. The authors of the [Enformer](https://www.nature.com/articles/s41592-021-01252-x) paper provide a good, simplified visualization of this, where darker colors represent a stronger relationship in the Key vector to the given Q position:\n\n![query-key-value](/post/images/key-query.png)\n\nOnce we have our Attention score matrix, we then multiply this by the V matrix, which represents the actual information of each token. This puts the Attention scores back into the context of the original token embeddings, which can then be passed to future layers.\n\nOne thing to note is that this calculation is often done in parallel, across multiple \"heads\" within a single layer. The idea behind this is that each head can learn different weights, and therefore multiple types of relationships between tokens can be represented within the same model. The [Attention is All you Need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) paper provides a good diagram of the Attention process, and what multi-head attention looks like (note that Scaled Dot-Produce Attention is essentially the process we just described):\n\n![Attention_Diagram](/post/images/scale-dot-attention-multihead.png)\n\nNow that we've defined the Attention mechanism, our training loop is actually pretty straightforward. See below for the steps:\n\n\n1.   Generate a batch from our training data: This consists of sampling a 300 bp sequence out of the genome as our \"data\", then the same 300 bp but shifted over by 1 as our \"labels\". Our batch size is 64, so a single batch will be 64 of these pairs of sequences\n2.   Plug the batch into our model to calculate loss. Our model is \"causal\", so it tries to predict the next token based on all previous tokens (i.e. data[0] is used to predict label[0], data[0:1] is used to predict label[1], data[0:2] is used to predict label[2], etc.)\n3.   Once we have calculated the loss (using Cross Entropy Loss), we begin the backward pass and take a step based on the gradients of all of our parameters.\n4.   Grab another batch and repeat\n5.   Once we hit the evaluation interval (1000 batches), we pause our model training, and evaluate the average loss over 100 additional batches of the training and validation sets, to get a checkpoint of our model performance\n6.   If the performance on the validation data is the best we've seen yet, we save this version of the model as a checkpoint to use later.\n\nObviously there is quite a bit more code than just these steps above, but this summary covers all of the functionally important parts. Much of the extra code above is related to making the model run efficiently, keeping track of how long it has been running, ensuring we are allocating GPU/CPU memory correctly, etc.\n\n## Sampling from the model\n\nGreat! Now that we have a trained model, let's see what it can generate it. The code block below simply pulls out the highest-performing version of the model we just trained, and feeds it a newline character to see what it will generate:\n\nSampling from the model\n\n::: {#9d78c838 .cell execution_count=12}\n``` {.python .cell-code}\n\"\"\"\nSample from a trained model\n\"\"\"\n\n# -----------------------------------------------------------------------------\ninit_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\nout_dir = '/root/data/out' # ignored if init_from is not 'resume'\nstart = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\nnum_samples = 5 # number of samples to draw\nmax_new_tokens = 250 # number of tokens generated in each sample\ntemperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\ntop_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\nseed = 1337\ndevice = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\ndtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\ncompile = True # use PyTorch 2.0 to compile the model to be faster\n# -----------------------------------------------------------------------------\n\ntorch.cuda.init()\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\ntorch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\ndevice_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}['float16']\nctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n\n# model\nif init_from == 'resume':\n    # init from a model saved in a specific directory\n    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n    checkpoint = torch.load(ckpt_path, map_location=device)\n    gptconf = GPTConfig(**checkpoint['model_args'])\n    model = GPT(gptconf)\n    state_dict = checkpoint['model']\n    unwanted_prefix = '_orig_mod.'\n    for k,v in list(state_dict.items()):\n        if k.startswith(unwanted_prefix):\n            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n    model.load_state_dict(state_dict)\n\nmodel.eval()\nmodel.to(device)\nif compile:\n    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n\n# look for the meta pickle in case it is available in the dataset folder\nload_meta = False\nif init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n    meta_path = os.path.join('/root/data', 'meta.pkl')\n    load_meta = os.path.exists(meta_path)\nif load_meta:\n    print(f\"Loading meta from {meta_path}...\")\n    with open(meta_path, 'rb') as f:\n        meta = pickle.load(f)\n    # TODO want to make this more general to arbitrary encoder/decoder schemes\n    stoi, itos = meta['stoi'], meta['itos']\n    encode = lambda s: [stoi[c] for c in s]\n    decode = lambda l: ''.join([itos[i] for i in l])\n\n# encode the beginning of the prompt\nif start.startswith('FILE:'):\n    with open(start[5:], 'r', encoding='utf-8') as f:\n        start = f.read()\nstart_ids = encode(start)\nx = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n\n# run generation\nwith torch.no_grad():\n    with ctx:\n        for k in range(num_samples):\n            y = model.generate(x, max_new_tokens, temperature=temperature, top_k=top_k)\n            print(decode(y[0].tolist()))\n            print('---------------')\n```\n:::\n\n\nAnd voila, it looks like our model is generating DNA sequence! But, that begs the question, how do we evaluate whether our model is actually performing well? With natural language, it's pretty easy to tell whether an output makes sense or not, but it is not so simple with DNA. Therefore, let's talk about the process of fine-tuning our model for more specific tasks.\n\n**Note for those interested**: You may also notice some uppercase and lowercase sequences. This is expected, as in our original file any soft-masked bases (i.e. lower-confidence segments) are lowercase rather than uppercase. The fact that our model is not mixing lower and uppercase bases is a good sign, as these should not be mixed based on sequencing assembly techniques.\n\n#### Questions\n\n\n1.   As mentioned above, our model is using \"causal\" self-attention, meaning that it is only paying attention to the ~300 tokens/base pairs that come before the current one. Does this align with how DNA interacts in a biological sense? Why or why not?\n2.   What effect do you think adding more attention layers/heads to our model would have? What about training for more epochs? Are there any downsides to expanding our model in this way?\n3.   What is the difference between the Query, Key, and Value matrices?\n\n\n## Fine-Tuning (Promoter Classification)\n\nNow that we have our gLM up and running, let's try applying it to two different problems. First, let's look at a promoter classification problem pulled from the [Nucleotide Transformer training notebook](https://github.com/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling.ipynb). In short, this dataset is made up of 300 bp sequences, each labelled as 1 for promoter or 0 for non-promoter. Let's try to train our language model to separate these classes!\n\nNote: This data was pulled from the [DeePromoter](https://www.frontiersin.org/journals/genetics/articles/10.3389/fgene.2019.00286/full) paper, as cited below:\n\nOubounyt, M., Louadi, Z., Tayara, H., & Chong, K. T. (2019). DeePromoter: Robust Promoter Predictor Using Deep Learning. Frontiers in Genetics, 10. https://doi.org/10.3389/fgene.2019.00286\n\nFirst, let's actually load in the data:\n\n::: {#179a29db .cell execution_count=13}\n``` {.python .cell-code}\n# Load the promoter dataset from the InstaDeep Hugging Face resources\ndataset_name = \"promoter_all\"\ntrain_dataset_promoter = load_dataset(\n        \"InstaDeepAI/nucleotide_transformer_downstream_tasks\",\n        dataset_name,\n        split=\"train\",\n        streaming= False,\n        trust_remote_code=True\n    )\ntest_dataset_promoter = load_dataset(\n        \"InstaDeepAI/nucleotide_transformer_downstream_tasks\",\n        dataset_name,\n        split=\"test\",\n        streaming= False,\n        trust_remote_code=True\n    )\n\nnum_labels_promoter = 2\n```\n:::\n\n\nNext, let's separate out the labels from the actual DNA sequences, and separate 5% of our training data to use as a validation set (the test set is already separated).\n\n::: {#6d931140 .cell execution_count=14}\n``` {.python .cell-code}\n# Get training data\ntrain_sequences_promoter = train_dataset_promoter['sequence']\ntrain_labels_promoter = train_dataset_promoter['label']\n\n# Split the dataset into a training and a validation dataset\ntrain_sequences_promoter, validation_sequences_promoter, train_labels_promoter, validation_labels_promoter = train_test_split(train_sequences_promoter,\n                                                                              train_labels_promoter, test_size=0.05, random_state=42)\n\n# Get test data\ntest_sequences_promoter = test_dataset_promoter['sequence']\ntest_labels_promoter = test_dataset_promoter['label']\n```\n:::\n\n\nNow, let's take a quick look at what one of these examples looks like:\n\n::: {#36fb447f .cell execution_count=15}\n``` {.python .cell-code}\nidx_sequence = -1\nsequence, label = train_sequences_promoter[idx_sequence], train_labels_promoter[idx_sequence]\nprint(f\"The DNA sequence is {sequence}.\")\nprint(f\"Its associated label is label {label}.\")\n\nidx_TATA = sequence.find(\"TATA\")\n\nprint(f\"This promoter is a TATA promoter, as the TATA motif is present at the {idx_TATA}st nucleotide.\")\n```\n:::\n\n\nNow that we have all of our datasets, it is time to tokenize them using the same mapping that we created for our language model. first, we load them into a Dataset object so we can work with them more easily:\n\n::: {#0b550077 .cell execution_count=16}\n``` {.python .cell-code}\n# Promoter dataset\nds_train_promoter = Dataset.from_dict({\"data\": train_sequences_promoter,'labels':train_labels_promoter})\nds_validation_promoter = Dataset.from_dict({\"data\": validation_sequences_promoter,'labels':validation_labels_promoter})\nds_test_promoter = Dataset.from_dict({\"data\": test_sequences_promoter,'labels':test_labels_promoter})\n```\n:::\n\n\nNext, we do the actual tokenization. However, remember that our previous encoding function could only handle a single example at a time (since our initial data file was one long string). We define a new tokenizing function below to use the same process, but apply it to multiple separate sequences:\n\n::: {#5c0e5e5a .cell execution_count=17}\n``` {.python .cell-code}\ndef tokenize_function(examples):\n  outputs = np.empty((0, 300), dtype='int16')\n  for example in examples[\"data\"]:\n    outputs = np.vstack([outputs, encode(example)])\n  return {\n      'input_ids': outputs,\n      'attention_mask': [1] * len(outputs),\n  }\n\ntokenized_datasets_train_promoter = ds_train_promoter.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=['data'],\n)\ntokenized_datasets_validation_promoter = ds_validation_promoter.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=['data'],\n)\ntokenized_datasets_test_promoter = ds_test_promoter.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=['data'],\n)\n```\n:::\n\n\nNow that we've tokenized the data, let's load it into Tensor format (which Pytorch uses in its models). Then, we can create a DataLoader, which will automatically create batches of our data and put them into a format which our model can read easily:\n\n::: {#5b6a0d6e .cell execution_count=18}\n``` {.python .cell-code}\ntrain_dataset = TensorDataset(torch.tensor(tokenized_datasets_train_promoter['input_ids'], device=device),\n                              torch.tensor(tokenized_datasets_train_promoter['labels'], device=device))\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=64)\n\nvalidation_dataset = TensorDataset(torch.tensor(tokenized_datasets_validation_promoter['input_ids'], device=device),\n                              torch.tensor(tokenized_datasets_validation_promoter['labels'], device=device))\n\nvalidation_dataloader = DataLoader(validation_dataset, shuffle=True, batch_size=64)\n\ntest_dataset = TensorDataset(torch.tensor(tokenized_datasets_test_promoter['input_ids'], device=device),\n                              torch.tensor(tokenized_datasets_test_promoter['labels'], device=device))\n\ntest_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=1)\n```\n:::\n\n\nIn the last step before training, we have to actually define a slightly new type of model. Essentially all we need to do here is add one more linear layer to our old model, which takes the original output and projects it into a two-dimensional output. This means that the \"new\" model will output the probability of the example belonging to each class. To do this, we just create a new model which pulls in our language model and adds the new layer on top:\n\n::: {#51cb570b .cell execution_count=19}\n``` {.python .cell-code}\nclass ClassificationModel(nn.Module):\n  def __init__(self, base_model, num_labels):\n    super(ClassificationModel, self).__init__()\n    self.orig_model = base_model\n    self.classifier = nn.Linear(base_model.config.vocab_size, num_labels)\n    self.num_labels = num_labels\n\n  def forward(self, input_ids, labels=None):\n    outputs = self.orig_model(input_ids)\n    logits = self.classifier(outputs[0][:, 0, :])\n    attn_weights = outputs[2]\n\n    loss = None\n    if labels is not None:\n      loss_fct = nn.CrossEntropyLoss()\n      loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n    return (logits, loss, attn_weights) if loss is not None else (logits, attn_weights)\n```\n:::\n\n\nNow that we have our new model defined, let's pull in the model we trained previously, and use it to create this new 2-label classification model:\n\n::: {#ba1d6970 .cell execution_count=20}\n``` {.python .cell-code}\n# -----------------------------------------------------------------------------\ninit_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\nout_dir = '/root/data/out' # ignored if init_from is not 'resume'\nseed = 1337\ndevice = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\ndtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\ncompile = True # use PyTorch 2.0 to compile the model to be faster\n# -----------------------------------------------------------------------------\n\ntorch.cuda.init()\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\ntorch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\ndevice_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}['float16']\nctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n\n# model\nif init_from == 'resume':\n    # init from a model saved in a specific directory\n    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n    checkpoint = torch.load(ckpt_path, map_location=device)\n    gptconf = GPTConfig(**checkpoint['model_args'])\n    model = GPT(gptconf)\n    state_dict = checkpoint['model']\n    unwanted_prefix = '_orig_mod.'\n    for k,v in list(state_dict.items()):\n        if k.startswith(unwanted_prefix):\n            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n    model.load_state_dict(state_dict)\n\nnew_model = ClassificationModel(base_model=model, num_labels=2)\nnew_model.eval()\nnew_model.to(device)\nif compile:\n    new_model = torch.compile(new_model) # requires PyTorch 2.0 (optional)\n```\n:::\n\n\nNow, go ahead and run the next block of code to train our new model. Again, this will take about 10 minutes, so go to the next paragraph for a description of what our training is actually doing.\n\n::: {#7ed7e66c .cell execution_count=21}\n``` {.python .cell-code}\noptimizer = torch.optim.AdamW(new_model.parameters(), lr=2e-5)\nnum_epochs=2\ntotal_steps = len(train_dataloader) * num_epochs\n\ndef calc_f1(predictions, true_labels):\n    true_pos = 0\n    false_pos = 0\n    false_neg = 0\n    true_neg = 0\n    for i in np.arange(len(predictions)) :\n        if predictions[i] == 0 and true_labels[i] == 0:\n            true_neg += 1\n        if predictions[i] == 1 and true_labels[i] == 0:\n            false_pos += 1\n        if predictions[i] == 0 and true_labels[i] == 1:\n            false_neg += 1\n        if predictions[i] == 1 and true_labels[i] == 1:\n            true_pos += 1\n    precision = true_pos / (true_pos + false_pos) if true_pos + false_pos > 0 else 0\n    recall = true_pos / (true_pos + false_neg)\n    F1 = (2*precision*recall / (precision + recall))\n    return F1\n\nval_predictions = []\nval_true_labels = []\n\n#Training Loop\nloss_lst = []\nval_loss_lst = []\nf1_scores = []\nnew_model.train()\nfor epoch in range(num_epochs):\n    i = 0\n    for batch in train_dataloader:\n        input_ids, labels = batch\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = new_model(input_ids, labels=labels)\n        loss = outputs[1]\n        loss_lst.append(loss.item())\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        i += 1\n        if i % 100 == 99:\n            new_model.eval()\n            with torch.no_grad():\n                for val_batch in validation_dataloader:\n                    val_input_ids, val_labels = val_batch\n                    optimizer.zero_grad()\n                    val_outputs = new_model(val_input_ids, labels=val_labels)\n                    val_loss = val_outputs[1]\n                    val_loss_lst.append(val_loss.item())\n                    val_logits = val_outputs[0]  # Get the logits\n                    val_predictions.append(val_logits)\n                    val_true_labels.append(val_labels)\n                val_predictions = torch.cat(val_predictions, dim=0)\n                val_true_labels = torch.cat(val_true_labels, dim=0)\n                val_predictions = torch.argmax(val_predictions, dim=1).tolist()\n                val_true_labels = val_true_labels.tolist()\n                f1_scores.append(calc_f1(val_predictions, val_true_labels))\n            print(f\"Epoch {epoch}, Batch {i}, Train Loss: {np.mean(loss_lst)}, Val Loss: {np.mean(val_loss_lst)}, Val F1: {calc_f1(val_predictions, val_true_labels)}\")\n            loss_lst = []\n            val_loss_lst = []\n            new_model.train()\n            val_predictions = []\n            val_true_labels = []\n\nplt.plot(f1_scores, label='Validation F1 score')\nplt.title(\"Validation F1 score for promoter prediction\")\nplt.xlabel(\"Batch\")\nplt.ylabel(\"F1 score\")\nplt.legend()\nplt.show()\n```\n:::\n\n\nWith this new model, we want to be able to classify promoter sequences. But, how do we do this when our base model has already been trained on a different task? The answer lies in a process called **fine-tuning**. This is where you take a pre-trained model, and train it again to perform better on a specific task. In this case, we are relying on the fact that our model already understands the intrinsic relationships in genomic sequences well, and are just shifting its goal. Rather than predicting future genomic sequence, we want it to apply its \"knowledge\" of sequences to predict whether they are promoters or not.\n\nFor language models, oftentimes fine-tuning tasks are much simpler than the initial learning process (and we assume that the layers of our model other than the new classification layer are already trained), so we usually use a much smaller learning rate (in this case $2e^{-5}$ and fewer training iterations. This is not always true, but it is a good rule of thumb. There are many options you have to optimize fine-tuning, including freezing various layers so that their weights are not updated, using different learning rates for different layers, and much more. But, for the purposes of this notebook, we are simply going to fine-tune the entire model.\n\nOur training loop is very similar here, but even simpler than our initial training:\n\n\n1.   Take a batch from the training dataloader and pass it through the model to get loss\n2.   Do the backward pass, and have our optimizer adjust weights according to each parameter's gradient and the learning rate\n3.   Repeat for all batches\n4.   Every 100 batches, run all validation samples through the model and calculate an average loss and F1 score (a metric for classification performance) for tracking purposes\n\n\nNow that we've trained our model, let's run the test data through it and see how it performs. While we do so, let's also look at the attention weights for a specific test sample, and try to get some idea of what our model is doing:\n\n::: {#4587f7b7 .cell execution_count=22}\n``` {.python .cell-code}\nnew_model.eval()  # Set the model to evaluation mode\n\npredictions = []\ntrue_labels = []\ni = 0\n\nwith torch.no_grad():  # Disable gradient calculation for inference\n    for batch in test_dataloader:\n        input_ids, labels = batch\n        outputs = new_model(input_ids)\n        logits = outputs[0]  # Get the logits\n        predictions.append(torch.argmax(logits).item())\n        true_labels.append(labels.item())\n        if i == 3000:\n            l = 0\n            fig, axs = plt.subplots(2, 2)\n            x = np.arange(2)\n            y = np.arange(2)\n            for j in x:\n                for k in y:\n                    axs[j,k].imshow(outputs[1][0][l].cpu().detach().numpy(), cmap='gist_stern', interpolation='nearest')\n                    l += 1\n            fig.set_figwidth(10)\n            fig.set_figheight(6)\n            fig.suptitle(\"Attention weights of Each Head in Layer 0\")\n            plt.show()\n            l = 0\n            fig, axs = plt.subplots(2, 2)\n            x = np.arange(2)\n            y = np.arange(2)\n            for j in x:\n                for k in y:\n                    axs[j,k].imshow(torch.mean(outputs[1][l], dim=0).cpu().detach().numpy(), cmap='gist_stern', interpolation='nearest')\n                    l += 1\n            fig.set_figwidth(10)\n            fig.set_figheight(6)\n            fig.suptitle(\"Attention Weights of Each Layer Averaged Across Heads\")\n            plt.show()\n        i += 1\n```\n:::\n\n\n::: {#259a290d .cell execution_count=23}\n``` {.python .cell-code}\ncorrect = 0\n#print(len(predictions))\nfor i in np.arange(len(predictions)) :\n    if predictions[i] == true_labels[i]:\n        correct += 1\nprint(f\"Accuracy:{int(correct / len(predictions) * 100):,}%\")\n\ntrue_pos = 0\nfalse_pos = 0\nfalse_neg = 0\ntrue_neg = 0\nfor i in np.arange(len(predictions)) :\n    if predictions[i] == 0 and true_labels[i] == 0:\n        true_neg += 1\n    if predictions[i] == 1 and true_labels[i] == 0:\n        false_pos += 1\n    if predictions[i] == 0 and true_labels[i] == 1:\n        false_neg += 1\n    if predictions[i] == 1 and true_labels[i] == 1:\n        true_pos += 1\nprecision = true_pos / (true_pos + false_pos)\nrecall = true_pos / (true_pos + false_neg)\nprint(f\"Precision: {round(precision, 2):,}\")\nprint(f\"Recall: {round(recall, 2):,}\")\n\nF1 = calc_f1(predictions, true_labels)\nprint(f\"F1 Score: {round(F1, 2):,}\")\n```\n:::\n\n\nWow, it looks like our model actually performs pretty well! Especially for a model that's gone through a total of 20 minutes of training on a single GPU. If we expanded our model or trained it over more data it would improve even further, but these are still good results.\n\nTaking a look at the Attention graphs above, we can get some insight into what our model thinks is important for the given sample. The way to read these heatmaps is that the Queries are on the y-axis, and the Keys are on the x-axis. This is all mapped back to the original input, so we essentially have a set of base-by-base relationship heatmaps! For example, a vertical red line indicates that many Query positions saw that Key position as important in the classification task.\n\nThe first set is a breakdown of all of the heads of our first attention layer. As I mentioned above, we would expect each head to learn something slightly different about our input, and it appears that this is the case! For example, the second head was very focused on the relationships between the bases directly surrounding the query (which is why we have a bright diagonal line), while the 3rd and 4th heads had much more dispersed attention weights.\n\nThis pattern becomes even more apparent as we move to the second set of heatmaps, where we average the attention weights for all heads in each layer (to give an approximate idea of what each full layer learns about the input). At this level, we see some interesting patterns emerge. For example, multiple layers have strong attention weights at the position around 160-170 bp into the sequence, which means that it likely played an important role in deciding this sequence's classification. Each head and layer learned some unique things about the sequence, so take some time to try and understand what these heatmaps are showing!\n\n#### Questions\n\n\n1.   Promoters are known for having very characteristic structures of DNA sequence (i.e. TATA boxes as I mentioned above). How could this influence our classification accuracy (i.e. make the classification problem easier or harder)? Why?\n2.   You can see that our validation F1 score metric does not uniformly increase across the training process. How can you explain this?\n3.   Provide your interpretation of the graphs of the Attention weights above. Are there any locations in the input sequence that appear to be highly important (i.e. have a lot of \"attention\" on them)?\n\n\n## Fine-Tuning (Enhancer Classification)\n\nNow that we've introduced the idea of fine-tuning and have a simple example, let's move on to a more complicated task. We have another set of data from the [Nucleotide Transformer training notebook](https://github.com/huggingface/notebooks/blob/main/examples/nucleotide_transformer_dna_sequence_modelling.ipynb) which is made up of 200-bp sequences that are either labeled as strong enhancers, weak enhancers, or non-enhancers. This is a more complicated task not only because it is multi-class classification, enhancers also generally have less \"characteristic\" genomic structure compared to promoters. This means that (in theory) our model will have to understand more subtle relationships within each sequence to classify them correctly. This data comes from the below paper:\n\nGeng, Q., Yang, R., & Zhang, L. (2022). A deep learning framework for enhancer prediction using word embedding and sequence generation. Biophysical Chemistry, 286, 106822. https://doi.org/10.1016/j.bpc.2022.106822\n\nTo start, we are going to load the data and tokenize it the exact same way we did previously:\n\n::: {#45e5c13d .cell execution_count=24}\n``` {.python .cell-code}\n# Load the enhancer dataset from the InstaDeep Hugging Face ressources\ndataset_name = \"enhancers_types\"\ntrain_dataset_enhancers = load_dataset(\n        \"InstaDeepAI/nucleotide_transformer_downstream_tasks\",\n        dataset_name,\n        split=\"train\",\n        streaming= False,\n        trust_remote_code=True\n    )\ntest_dataset_enhancers = load_dataset(\n        \"InstaDeepAI/nucleotide_transformer_downstream_tasks\",\n        dataset_name,\n        split=\"test\",\n        streaming= False,\n        trust_remote_code=True\n    )\n\nnum_labels_enhancer = 3\n```\n:::\n\n\n::: {#dbd1ebf8 .cell execution_count=25}\n``` {.python .cell-code}\n# Get training data\ntrain_sequences_enhancers = train_dataset_enhancers['sequence']\ntrain_labels_enhancers = train_dataset_enhancers['label']\n\n# Split the dataset into a training and a validation dataset\ntrain_sequences_enhancers, validation_sequences_enhancers, train_labels_enhancers, validation_labels_enhancers = train_test_split(train_sequences_enhancers,\n                                                                              train_labels_enhancers, test_size=0.10, random_state=42)\n\n# Get test data\ntest_sequences_enhancers = test_dataset_enhancers['sequence']\ntest_labels_enhancers = test_dataset_enhancers['label']\n```\n:::\n\n\n::: {#c6ebecb9 .cell execution_count=26}\n``` {.python .cell-code}\nds_train_enhancers = Dataset.from_dict({\"data\": train_sequences_enhancers,'labels':train_labels_enhancers})\nds_validation_enhancers = Dataset.from_dict({\"data\": validation_sequences_enhancers,'labels':validation_labels_enhancers})\nds_test_enhancers = Dataset.from_dict({\"data\": test_sequences_enhancers,'labels':test_labels_enhancers})\n```\n:::\n\n\n::: {#a6e1ad71 .cell execution_count=27}\n``` {.python .cell-code}\ndef tokenize_function(examples):\n  min_length = min(len(i) for i in examples['data'])\n  outputs = np.empty((0, min_length), dtype='int16')\n  for example in examples[\"data\"]:\n    outputs = np.vstack([outputs, encode(example[:min_length])])\n  return {\n      'input_ids': outputs,\n      'attention_mask': [1] * len(outputs),\n  }\n\ntokenized_datasets_train_enhancer = ds_train_enhancers.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=['data'],\n)\ntokenized_datasets_validation_enhancer = ds_validation_enhancers.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=['data'],\n)\ntokenized_datasets_test_enhancer = ds_test_enhancers.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=['data'],\n)\n\ntrain_dataset = TensorDataset(torch.tensor(tokenized_datasets_train_enhancer['input_ids'], device=device),\n                              torch.tensor(tokenized_datasets_train_enhancer['labels'], device=device))\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=64)\n\nvalidation_dataset = TensorDataset(torch.tensor(tokenized_datasets_validation_enhancer['input_ids'], device=device),\n                              torch.tensor(tokenized_datasets_validation_enhancer['labels'], device=device))\n\nvalidation_dataloader = DataLoader(validation_dataset, shuffle=True, batch_size=1)\n\ntest_dataset = TensorDataset(torch.tensor(tokenized_datasets_test_enhancer['input_ids'], device=device),\n                              torch.tensor(tokenized_datasets_test_enhancer['labels'], device=device))\n\ntest_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=1)\n```\n:::\n\n\nNow, let's load back in our **original** gLM, i.e. not the one we fine-tuned to classify promoters. This time, we need it to classify on 3 labels rather than 2, so it is simpler to just recreate a new version and fine-tune it from scratch.\n\n::: {#673293d0 .cell execution_count=28}\n``` {.python .cell-code}\n# -----------------------------------------------------------------------------\ninit_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\nout_dir = '/root/data/out' # ignored if init_from is not 'resume'\nseed = 1337\ndevice = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\ndtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\ncompile = True # use PyTorch 2.0 to compile the model to be faster\n# -----------------------------------------------------------------------------\n\ntorch.cuda.init()\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\ntorch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\ndevice_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}['float16']\nctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n\n# model\nif init_from == 'resume':\n    # init from a model saved in a specific directory\n    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n    checkpoint = torch.load(ckpt_path, map_location=device)\n    gptconf = GPTConfig(**checkpoint['model_args'])\n    model = GPT(gptconf)\n    state_dict = checkpoint['model']\n    unwanted_prefix = '_orig_mod.'\n    for k,v in list(state_dict.items()):\n        if k.startswith(unwanted_prefix):\n            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n    model.load_state_dict(state_dict)\n\nnew_model = ClassificationModel(base_model=model, num_labels=3)\nnew_model.eval()\nnew_model.to(device)\nif compile:\n    new_model = torch.compile(new_model) # requires PyTorch 2.0 (optional)\n```\n:::\n\n\nNow, let's run through the same training loop. You may notice that we have greatly increased the number of epochs we are training for. This is partially due to the fact that the enhancer dataset is much smaller than the promoter dataset (so we need more epochs to get the equivalent number of training batches), but the task is also more complex, so it benefits our model to have more training iterations in general.\n\n::: {#66ddfd24 .cell execution_count=29}\n``` {.python .cell-code}\noptimizer = torch.optim.AdamW(new_model.parameters(), lr=2e-5)\nnum_epochs=10\ntotal_steps = len(train_dataloader) * num_epochs\n\nval_predictions = []\nval_true_labels = []\n\n#Training Loop\nloss_lst = []\nval_loss_lst = []\nmcc_scores = []\nnew_model.train()\nfor epoch in range(num_epochs):\n    i = 0\n    for batch in train_dataloader:\n        input_ids, labels = batch\n        optimizer.zero_grad()\n\n        # Forward pass\n        outputs = new_model(input_ids, labels=labels)\n        loss = outputs[1]\n        loss_lst.append(loss.item())\n\n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        i += 1\n        if i % 100 == 99:\n            new_model.eval()\n            with torch.no_grad():\n                for val_batch in validation_dataloader:\n                    val_input_ids, val_labels = val_batch\n                    optimizer.zero_grad()\n                    val_outputs = new_model(val_input_ids, labels=val_labels)\n                    val_loss = val_outputs[1]\n                    val_loss_lst.append(val_loss.item())\n                    val_logits = val_outputs[0]  # Get the logits\n                    val_predictions.append(torch.argmax(val_logits).item())\n                    val_true_labels.append(val_labels.item())\n            print(f\"Epoch {epoch}, Batch {i}, Train Loss: {np.mean(loss_lst)}, Val Loss: {np.mean(val_loss_lst)}, Val MCC: {matthews_corrcoef(val_true_labels, val_predictions)}\")\n            mcc_scores.append(matthews_corrcoef(val_true_labels, val_predictions))\n            loss_lst = []\n            val_loss_lst = []\n            new_model.train()\n            val_predictions = []\n            val_true_labels = []\n```\n:::\n\n\nNow that we've finished training, we can see that our training loss remains significantly higher for this task than our promoter prediction task, as expected. Note that we use a slightly different performance metric here as well (MCC), which gives us a single scalar value for classification performance (whereas using F1 again would not be so simple). In any case, let's see how this model performs on the test data.\n\n::: {#3528b5f6 .cell execution_count=30}\n``` {.python .cell-code}\nnew_model.eval()  # Set the model to evaluation mode\n\npredictions = []\ntrue_labels = []\ni = 0\n\nwith torch.no_grad():  # Disable gradient calculation for inference\n    for batch in test_dataloader:\n        input_ids, labels = batch\n        outputs = new_model(input_ids)\n        logits = outputs[0]  # Get the logits\n        predictions.append(torch.argmax(logits).item())\n        true_labels.append(labels.item())\n        if i == 200:\n            l = 0\n            fig, axs = plt.subplots(2, 2)\n            x = np.arange(2)\n            y = np.arange(2)\n            for j in x:\n                for k in y:\n                    axs[j,k].imshow(outputs[1][0][l].cpu().detach().numpy(), cmap='gist_stern', interpolation='nearest')\n                    l += 1\n            fig.set_figwidth(10)\n            fig.set_figheight(6)\n            fig.suptitle(\"Attention weights of Each Head in Layer 0\")\n            plt.show()\n            l = 0\n            fig, axs = plt.subplots(2, 2)\n            x = np.arange(2)\n            y = np.arange(2)\n            for j in x:\n                for k in y:\n                    axs[j,k].imshow(torch.mean(outputs[1][l], dim=0).cpu().detach().numpy(), cmap='gist_stern', interpolation='nearest')\n                    l += 1\n            fig.set_figwidth(10)\n            fig.set_figheight(6)\n            fig.suptitle(\"Attention Weights of Each Layer Averaged Across Heads\")\n            plt.show()\n            print(labels.item())\n        i += 1\n```\n:::\n\n\n::: {#c5462837 .cell execution_count=31}\n``` {.python .cell-code}\ncorrect = 0\nprint(len(predictions))\nfor i in np.arange(len(predictions)) :\n    if predictions[i] == true_labels[i]:\n        correct += 1\nprint(f\"Accuracy: {int(correct / len(predictions) * 100):,}%\")\n\nprint(f\"MCC: {round(matthews_corrcoef(true_labels, predictions), 2):,}\")\n```\n:::\n\n\nOof, those numbers are a little rough, especially compared to our previous model's performance on promoter classification. If you look at the attention weight heatmaps, you can also see that the weights are significantly more spread out compared to the promoter model. This could mean many things, and it would take more digging to understand exactly what is happening, but one hypothesis is that our model couldn't quite understand what exactly to focus on for classification, so we get more \"cloudy\" attention signal.\n\nIn any case, this goes to show that sometimes there are limitations to the complexity that a single model can accurately represent. If we increased the size of our initial gLM and let it run for more training iterations, our performance on this task would likely improve dramatically. Unfortunately, we don't have the time or computing resources for that in this notebook, but I would encourage all of you to continue to investigate if you are interested!\n\n#### Questions\n\n\n1.   Why is this enhancer classification problem harder than the promoter classification problem we looked at previously?\n2.   Similarly to the question above, provide your interpretation of the graphs of Attention weights.\n3.   One characteristic of enhancers is that they tend to have some distance from the gene they influence (much greater than 300 bp). How could we potentially change our original gLM in order to potentially improve our performance in this problem?\n\n\n## Conclusion\n\nTo sum up everything we have gone through in this notebook, here are a few major points:\n\n\n1.   Large language models (LLMs) have become well-known for their popular use (i.e. ChatGPT), but they have strong potential for use as research tools in various fields as well.\n2.   Genomic langauge models (gLMs) are a type of LLM that attempt to learn relationships between different of regions of DNA directly from DNA sequence.\n3.   In general, gLMs can be created similarly to any other type of language model (usually using the Attention mechanism), however strategies for tokenization can differ due to the simplified vocabulary of DNA sequence.\n4.   Once tokenization and embeddings are defined, a gLM can be trained just like any other language model.\n5.   gLMs can be fine-tuned to perform specific tasks.\n\nThe applications of language modeling to functional genomic research is a very new field, and publications of high-performing models are just beginning to emerge. If you are interested in further investigation, we would highly recommend the [DNABERT-2](https://arxiv.org/abs/2306.15006) and [Nucleotide Transformer](https://www.biorxiv.org/content/10.1101/2023.01.11.523679v3.full) papers. These represent two distinctly different approaches to DNA language modeling, each with their own benefits and downsides.\n\n",
    "supporting": [
      "Genome_Language_Model_Spring_2025_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}