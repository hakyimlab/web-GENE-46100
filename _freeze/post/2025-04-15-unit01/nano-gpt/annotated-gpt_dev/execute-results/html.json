{
  "hash": "05009403265ea0af2579792c5a897dee",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Building a GPT - companion notebook annotated\nauthor: Andrey Karpathy\ndate: '2025-04-15'\nfreeze: true\njupyter: \n  kernelspec:\n    name: \"conda-env-gene46100-py\"\n    language: \"python\"\n    display_name: \"gene46100\"\nformat:\n  html:\n    code-fold: true\n    code-line-numbers: true\n    code-tools: true\n    code-wrap: true\ndescription: Companion notebook from Karpathys video on building a minimal GPT, annotated by cursors LLM with summary from gemini.\n---\n\n\n\n\n## Building a GPT\n\nCompanion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT. Downloaded from [here](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing)\n\n(https://github.com/karpathy/nanoGPT)\n\n### download the tiny shakespeare dataset\n\n::: {#59fc39e7 .cell execution_count=1}\n``` {.python .cell-code}\n# Download the tiny shakespeare dataset\n#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n```\n:::\n\n\n::: {#29ba3e6c .cell execution_count=2}\n``` {.python .cell-code}\n# read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n```\n:::\n\n\n::: {#cbaabcfa .cell execution_count=3}\n``` {.python .cell-code}\n# print the length of the dataset\nprint(\"length of dataset in characters: \", len(text))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlength of dataset in characters:  1115394\n```\n:::\n:::\n\n\n::: {#fa026a17 .cell execution_count=4}\n``` {.python .cell-code}\n# let's look at the first 1000 characters\nprint(text[:1000])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n```\n:::\n:::\n\n\n::: {#1aac16f0 .cell execution_count=5}\n``` {.python .cell-code}\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n```\n:::\n:::\n\n\n### mapping characters to integers and vice versa\n\n::: {#2453f382 .cell execution_count=6}\n``` {.python .cell-code}\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n```\n:::\n:::\n\n\n### encode the data into torch tensor\n\n::: {#742e63db .cell execution_count=7}\n``` {.python .cell-code}\n# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch # we use PyTorch: [https://pytorch.org](https://pytorch.org)\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n```\n:::\n:::\n\n\n### split up the data into train and validation sets\n\n::: {#14c10d42 .cell execution_count=8}\n``` {.python .cell-code}\n# split up the data into train and validation sets\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n```\n:::\n\n\n### define the block size\n\n::: {#48af431c .cell execution_count=9}\n``` {.python .cell-code}\nblock_size = 8\ntrain_data[:block_size+1]\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n```\n:::\n:::\n\n\n### define the context and target: 8 examples in one batch\n\n::: {#01664152 .cell execution_count=10}\n``` {.python .cell-code}\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nwhen input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n```\n:::\n:::\n\n\n### define the batch size and get the batch\n\n::: {#d1a80647 .cell execution_count=11}\n``` {.python .cell-code}\ntorch.manual_seed(1337)\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ninputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\nwhen input is [24] the target: 43\nwhen input is [24, 43] the target: 58\nwhen input is [24, 43, 58] the target: 5\nwhen input is [24, 43, 58, 5] the target: 57\nwhen input is [24, 43, 58, 5, 57] the target: 1\nwhen input is [24, 43, 58, 5, 57, 1] the target: 46\nwhen input is [24, 43, 58, 5, 57, 1, 46] the target: 43\nwhen input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\nwhen input is [44] the target: 53\nwhen input is [44, 53] the target: 56\nwhen input is [44, 53, 56] the target: 1\nwhen input is [44, 53, 56, 1] the target: 58\nwhen input is [44, 53, 56, 1, 58] the target: 46\nwhen input is [44, 53, 56, 1, 58, 46] the target: 39\nwhen input is [44, 53, 56, 1, 58, 46, 39] the target: 58\nwhen input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52] the target: 58\nwhen input is [52, 58] the target: 1\nwhen input is [52, 58, 1] the target: 58\nwhen input is [52, 58, 1, 58] the target: 46\nwhen input is [52, 58, 1, 58, 46] the target: 39\nwhen input is [52, 58, 1, 58, 46, 39] the target: 58\nwhen input is [52, 58, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\nwhen input is [25] the target: 17\nwhen input is [25, 17] the target: 27\nwhen input is [25, 17, 27] the target: 10\nwhen input is [25, 17, 27, 10] the target: 0\nwhen input is [25, 17, 27, 10, 0] the target: 21\nwhen input is [25, 17, 27, 10, 0, 21] the target: 1\nwhen input is [25, 17, 27, 10, 0, 21, 1] the target: 54\nwhen input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n```\n:::\n:::\n\n\n### start with a simple model: the bigram language model\n\n::: {#a8d1ed22 .cell execution_count=12}\n``` {.python .cell-code}\n# define the bigram language model\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n```\n:::\n\n\n### cross entropy loss\n\nLoss = $-\\sum_{i}(y_i * \\log(p_i))$x\n\nwhere:\n\n$y_i$ = actual probability (0 or 1 for the $i$-th class)\n$p_i$ = predicted probability for the $i$-th class\n$\\sum$ = sum over all classes (characters)\n\nThis is the loss for a single token prediction. The total loss reported by F.cross_entropy is the average loss across all B*T tokens in the batch, where:\n\nB = batch_size\nT = block_size (sequence length)\n\nBefore training, we would expect the model to predict the next character from a uniform distribution (random guessing). The probability for the correct character would be $1 / \\text{vocab_size}$.\n\nExpected initial loss $\\approx - \\log(1 / \\text{vocab_size}) = \\log(\\text{vocab_size})$\n$\\log(65) \\approx 4.1744$\n\n### initialize the model and compute the loss\n\n::: {#173be945 .cell execution_count=13}\n``` {.python .cell-code}\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb) # xb/yb are from the previous cell (B=4, T=8)\nprint(logits.shape) # Expected: (B, T, C) = (4, 8, 65)\nprint(loss) # Expected: Around 4.17\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([32, 65])\ntensor(4.8786, grad_fn=<NllLossBackward0>)\n```\n:::\n:::\n\n\n### generate text\n\n::: {#75f78621 .cell execution_count=14}\n``` {.python .cell-code}\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\nwnYWmnxKWWev-tDqXErVKLgJ\n```\n:::\n:::\n\n\n### choose AdamW as the optimizer\n\n::: {#38dc5e3d .cell execution_count=15}\n``` {.python .cell-code}\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n```\n:::\n\n\n### train the model\n\n::: {#f071e081 .cell execution_count=16}\n``` {.python .cell-code}\nbatch_size = 32 # Redefine batch size for training\nfor steps in range(100): # # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4.65630578994751\n```\n:::\n:::\n\n\n### generate text starting with 0=`\\n` as initial context\n\n::: {#cffc0479 .cell execution_count=17}\n``` {.python .cell-code}\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\noTo.JUZ!!zqe!\nxBP qbs$Gy'AcOmrLwwt\np$x;Seh-onQbfM?OjKbn'NwUAW -Np3fkz$FVwAUEa-wzWC -wQo-R!v -Mj?,SPiTyZ;o-opr$mOiPJEYD-CfigkzD3p3?zvS;ADz;.y?o,ivCuC'zqHxcVT cHA\nrT'Fd,SBMZyOslg!NXeF$sBe,juUzLq?w-wzP-h\nERjjxlgJzPbHxf$ q,q,KCDCU fqBOQT\nSV&CW:xSVwZv'DG'NSPypDhKStKzC -$hslxIVzoivnp ,ethA:NCCGoi\ntN!ljjP3fwJMwNelgUzzPGJlgihJ!d?q.d\npSPYgCuCJrIFtb\njQXg\npA.P LP,SPJi\nDBcuBM:CixjJ$Jzkq,OLf3KLQLMGph$O 3DfiPHnXKuHMlyjxEiyZib3FaHV-oJa!zoc'XSP :CKGUhd?lgCOF$;;DTHZMlvvcmZAm;:iv'MMgO&Ywbc;BLCUd&vZINLIzkuTGZa\nD.?\n```\n:::\n:::\n\n\n### The mathematical trick in self-attention\ntoy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n\n::: {#8b585ce4 .cell execution_count=18}\n``` {.python .cell-code}\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3)) # Lower triangular matrix of 1s\na = a / torch.sum(a, 1, keepdim=True) # Normalize rows to sum to 1\nb = torch.randint(0,10,(3,2)).float() # Some data\nc = a @ b # Matrix multiply\nprint('a=')\nprint(a)\nprint('--')\nprint('b=')\nprint(b)\nprint('--')\nprint('c=')\nprint(c)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\na=\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n--\nb=\ntensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n--\nc=\ntensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n```\n:::\n:::\n\n\n::: {#e5dcfd8f .cell execution_count=19}\n``` {.python .cell-code}\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2 # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\ntorch.Size([4, 8, 2])\n```\n:::\n:::\n\n\n### version 1: using a for loop to compute the weighted aggregation\n\n::: {#25bd55f8 .cell execution_count=20}\n``` {.python .cell-code}\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C)) # x bag-of-words (running average)\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # Select vectors from start up to time t: shape (t+1, C)\n        xbow[b,t] = torch.mean(xprev, 0) # Compute mean along the time dimension (dim 0)\n```\n:::\n\n\n### version 2: using matrix multiply for a weighted aggregation\n\n::: {#b640bb4b .cell execution_count=21}\n``` {.python .cell-code}\n# Create the averaging weight matrix\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True) # Normalize rows to sum to 1\n# Perform batched matrix multiplication\nxbow2 = wei @ x # (T, T) @ (B, T, C) -> (B, T, C) via broadcasting\ntorch.allclose(xbow, xbow2) # Check if results are identical\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\nTrue\n```\n:::\n:::\n\n\n### version 3: use Softmax\n\n::: {#cd0536c4 .cell execution_count=22}\n``` {.python .cell-code}\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\n# Mask out future positions by setting them to -infinity before softmax\nwei = wei.masked_fill(tril == 0, float('-inf'))\n# Apply softmax to get row-wise probability distributions (weights)\nwei = F.softmax(wei, dim=-1)\n# Perform weighted aggregation\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3) # Check if results are identical\n```\n\n::: {.cell-output .cell-output-display execution_count=22}\n```\nTrue\n```\n:::\n:::\n\n\n### softmax function\nsoftmax($z_i$) = $\\frac{e^{z_i}}{\\sum_j e^{z_j}}$\n\n### version 4: self-attention\n\n::: {#bc387af0 .cell execution_count=23}\n``` {.python .cell-code}\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels (embedding dimension)\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)   # (B, T, head_size)\nq = query(x) # (B, T, head_size)\n# Compute attention scores (\"affinities\")\nwei = q @ k.transpose(-2, -1) # (B, T, hs) @ (B, hs, T) ---> (B, T, T)\n\n# Scale the scores\n# Note: Karpathy uses C**-0.5 here (sqrt(embedding_dim)). Standard Transformer uses sqrt(head_size).\nwei = wei * (C**-0.5)\n\n# Apply causal mask\ntril = torch.tril(torch.ones(T, T))\n#wei = torch.zeros((T,T)) # This line is commented out in original, was from softmax demo\nwei = wei.masked_fill(tril == 0, float('-inf')) # Mask future tokens\n\n# Apply softmax to get attention weights\nwei = F.softmax(wei, dim=-1) # (B, T, T)\n\n# Perform weighted aggregation of Values\nv = value(x) # (B, T, head_size)\nout = wei @ v # (B, T, T) @ (B, T, hs) ---> (B, T, hs)\n#out = wei @ x # This would aggregate original x, not the projected values 'v'\n\nout.shape # Expected: (B, T, head_size) = (4, 8, 16)\n```\n\n::: {.cell-output .cell-output-display execution_count=23}\n```\ntorch.Size([4, 8, 16])\n```\n:::\n:::\n\n\n::: {#64febc53 .cell execution_count=24}\n``` {.python .cell-code}\nwei[0] # Show attention weights for the first sequence in the batch\n```\n\n::: {.cell-output .cell-output-display execution_count=24}\n```\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.4264, 0.5736, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3151, 0.3022, 0.3827, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3007, 0.2272, 0.2467, 0.2253, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.1635, 0.2048, 0.1776, 0.1616, 0.2926, 0.0000, 0.0000, 0.0000],\n        [0.1403, 0.2272, 0.1454, 0.1244, 0.2678, 0.0949, 0.0000, 0.0000],\n        [0.1554, 0.1815, 0.1224, 0.1213, 0.1428, 0.1603, 0.1164, 0.0000],\n        [0.0952, 0.1217, 0.1130, 0.1453, 0.1137, 0.1180, 0.1467, 0.1464]],\n       grad_fn=<SelectBackward0>)\n```\n:::\n:::\n\n\n### Check that X X'/C is is the correlation matrix if X is normalized\n\n\n\n\n```{r showing xx'/C is correlation matrix if X is normalized}\n\nnC = 64\nX = matrix(rnorm(4*64), nrow=4, ncol=nC)\n## make it so that the third token is similar to the last one\nX[2,] = X[4,]*0.5 + X[2,]*0.5\n## normalize X\nX = t(scale(t(X)))\n\nq = X\nk = X\nv = X\n\nqkt = q %*% t(k)/(nC-1)\nxcor = cor(t(q),t(k))\ndim(xcor)\ndim(qkt)\ncat(\"xcor\\n\")\nxcor\ncat(\"---\\n qkt\\n\")\nqkt\n\ncat(\"are xcor and qkt equal?\")\nall.equal(xcor, qkt)\n\npar(mar=c(5, 6, 4, 2) + 0.1)  # increase left margin to avoid cutting of the y label\npar(pty=\"s\")  # Set plot type to \"square\"\nplot(c(xcor), c(qkt),cex=3,cex.lab=3,cex.axis=2,cex.main=2,cex.sub=2); abline(0,1)\npar(pty=\"m\")  # Reset to default plot type\npar(mar=c(5, 4, 4, 2) + 0.1)  # Reset to default margins\n\n\n```\n\n\n\n\n\n### Notes:\n\nAttention is a communication mechanism. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights. \n\n- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens. \n    example: \"the cat sat on the mat\" should be different from \"the mat sat on the cat\"\n- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other. \n- In an \"encoder\" attention block just delete the single line that does masking with tril, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling. \n- \"self-attention\" just means that the keys and values are produced from the same source as queries (all come from x). In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n\n### why scaled attention?\n\n\"Scaled\" attention additionaly divides wei by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n\n::: {#751f4b08 .cell execution_count=25}\n``` {.python .cell-code}\n# Demonstrate variance without scaling\nk_unscaled = torch.randn(B,T,head_size)\nq_unscaled = torch.randn(B,T,head_size)\nwei_unscaled = q_unscaled @ k_unscaled.transpose(-2, -1)\nprint(f\"k var: {k_unscaled.var():.4f}, q var: {q_unscaled.var():.4f}, wei (unscaled) var: {wei_unscaled.var():.4f}\")\n\n# Demonstrate variance *with* scaling (using head_size for illustration)\nk = torch.randn(B,T,head_size)\nq = torch.randn(B,T,head_size)\nwei = q @ k.transpose(-2, -1) * head_size**-0.5 # Scale by sqrt(head_size)\nprint(f\"k var: {k.var():.4f}, q var: {q.var():.4f}, wei (scaled) var: {wei.var():.4f}\") # Variance should be closer to 1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nk var: 1.0449, q var: 1.0700, wei (unscaled) var: 17.4690\nk var: 0.9006, q var: 1.0037, wei (scaled) var: 0.9957\n```\n:::\n:::\n\n\n::: {#7ab86db5 .cell execution_count=26}\n``` {.python .cell-code}\nk.var() # Should be close to 1\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\ntensor(0.9006)\n```\n:::\n:::\n\n\n::: {#79937ef8 .cell execution_count=27}\n``` {.python .cell-code}\nq.var() # Should be close to 1\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\ntensor(1.0037)\n```\n:::\n:::\n\n\n::: {#e21a12c0 .cell execution_count=28}\n``` {.python .cell-code}\nwei.var() # With scaling, should be closer to 1 than head_size (16)\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\ntensor(0.9957)\n```\n:::\n:::\n\n\n::: {#c5184f63 .cell execution_count=29}\n``` {.python .cell-code}\n# Softmax with small inputs (diffuse distribution)\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\ntensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n```\n:::\n:::\n\n\n::: {#5de133c9 .cell execution_count=30}\n``` {.python .cell-code}\n# Softmax with large inputs (simulating unscaled attention scores) -> peaks\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\ntensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n```\n:::\n:::\n\n\n### LayerNorm1d\n\n::: {#5b02faec .cell execution_count=31}\n``` {.python .cell-code}\nclass LayerNorm1d: # (used to be BatchNorm1d)\n    def __init__(self, dim, eps=1e-5, momentum=0.1): # Momentum is not used in typical LayerNorm\n        self.eps = eps\n        # Learnable scale and shift parameters, initialized to 1 and 0\n        self.gamma = torch.ones(dim)\n        self.beta = torch.zeros(dim)\n\n    def __call__(self, x):\n        # calculate the forward pass\n        # Calculate mean over the *last* dimension (features/embedding)\n        xmean = x.mean(1, keepdim=True) # batch mean (shape B, 1, C if input B, T, C) --> Needs adjustment for (B,C) input shape here. Assumes input is (B, dim)\n        # Correction: x is (32, 100). dim=1 is correct for features. Shape (32, 1)\n        xvar = x.var(1, keepdim=True) # batch variance (shape 32, 1)\n        # Normalize each feature vector independently\n        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n        # Apply scale and shift\n        self.out = self.gamma * xhat + self.beta\n        return self.out\n\n    def parameters(self):\n        # Expose gamma and beta as learnable parameters\n        return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = LayerNorm1d(100) # Create LayerNorm for 100 features\nx = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\nx = module(x)\nx.shape # Should be (32, 100)\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\ntorch.Size([32, 100])\n```\n:::\n:::\n\n\nExplanation of layernorm\n\nInput shape: (B, T, C) where:\nB = batch size\nT = sequence length (number of tokens)\nC = embedding dimension (features of each token)\nFor each token in the sequence (each position T), LayerNorm:\nTakes its embedding vector of size C\nCalculates the mean and standard deviation of just that vector\nNormalizes that vector by subtracting its mean and dividing by its standard deviation\nApplies the learnable scale (gamma) and shift (beta) parameters\nSo if you have a sequence like \"The cat sat\", and each word is represented by a 64-dimensional embedding vector, LayerNorm would:\nTake \"The\"'s 64-dimensional vector and normalize it\nTake \"cat\"'s 64-dimensional vector and normalize it\nTake \"sat\"'s 64-dimensional vector and normalize it\nEach token's vector is normalized independently of the others. This is different from BatchNorm, which would normalize across the batch dimension (i.e., looking at the same position across different examples in the batch).\nThis per-token normalization helps maintain stable gradients during training and is particularly important in Transformers where the attention mechanism needs to work with normalized vectors to compute meaningful attention scores.\n\n::: {#3d5d4112 .cell execution_count=32}\n``` {.python .cell-code}\n# Mean and std of the first feature *across the batch*. Not expected to be 0 and 1.\nx[:,0].mean(), x[:,0].std()\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\n(tensor(0.1469), tensor(0.8803))\n```\n:::\n:::\n\n\n::: {#31a3c1fe .cell execution_count=33}\n``` {.python .cell-code}\n# Mean and std *across features* for the first item in the batch. Expected to be ~0 and ~1.\nx[0,:].mean(), x[0,:].std()\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```\n(tensor(2.3842e-09), tensor(1.0000))\n```\n:::\n:::\n\n\n### French to English translation example:\n\n::: {#1aea58e6 .cell execution_count=34}\n``` {.python .cell-code}\n# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n```\n:::\n\n\n### Full finished code, for reference\n\n::: {#83e132d7 .cell execution_count=35}\n``` {.python .cell-code}\n# Import necessary PyTorch modules\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# ===== HYPERPARAMETERS =====\nbatch_size = 16       # Number of sequences per batch (Smaller than Bigram training)\nblock_size = 32       # Context length (Larger than Bigram demo)\nmax_iters = 5000      # Total training iterations (More substantial training) TODO change to 5000 later\neval_interval = 100   # How often to check validation loss\nlearning_rate = 1e-3  # Optimizer learning rate\neval_iters = 200      # Number of batches to average for validation loss estimate\nn_embd = 64           # Embedding dimension (Size of token vectors)\nn_head = 4            # Number of attention heads\nn_layer = 4           # Number of Transformer blocks (layers)\ndropout = 0.0         # Dropout probability (0.0 means no dropout here)\n# ==========================\n\n# Device selection: MPS (Apple Silicon) > CUDA > CPU\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")   # Apple Silicon GPU\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")  # NVIDIA GPU\nelse:\n    device = torch.device(\"cpu\")   # CPU fallback\nprint(f\"Using device: {device}\")\n\n# Set random seed for reproducibility\ntorch.manual_seed(1337)\nif device.type == 'cuda':\n    torch.cuda.manual_seed(1337)\nelif device.type == 'mps':\n    torch.mps.manual_seed(1337)\n\n# Load and read the training text (assuming input.txt is available)\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# ===== DATA PREPROCESSING =====\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nstoi = { ch:i for i,ch in enumerate(chars) }   # string to index\nitos = { i:ch for i,ch in enumerate(chars) }   # index to string\nencode = lambda s: [stoi[c] for c in s]   # convert string to list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])   # convert list of integers to string\n\n# Split data into training and validation sets\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data))   # first 90% for training\ntrain_data = data[:n]\nval_data = data[n:]\n# =============================\n\n# ===== DATA LOADING FUNCTION =====\ndef get_batch(split):\n    \"\"\"Generate a batch of data for training or validation.\"\"\"\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device) # Move data to the target device\n    return x, y\n# ================================\n\n# ===== LOSS ESTIMATION FUNCTION =====\n@torch.no_grad()   # Disable gradient calculation for efficiency\ndef estimate_loss():\n    \"\"\"Estimate the loss on training and validation sets.\"\"\"\n    out = {}\n    model.eval()   # Set model to evaluation mode\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()  # Set model back to training mode\n    return out\n# ===================================\n\n# ===== ATTENTION HEAD IMPLEMENTATION =====\nclass Head(nn.Module):\n    \"\"\"Single head of self-attention.\"\"\"\n    \n    def __init__(self, head_size):\n        super().__init__()\n        # Linear projections for Key, Query, Value\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        # Causal mask (tril). 'register_buffer' makes it part of the model state but not a parameter to be trained.\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        # Dropout layer (applied after softmax)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape # C here is n_embd\n        # Project input to K, Q, V\n        k = self.key(x)   # (B,T,head_size)\n        q = self.query(x) # (B,T,head_size)\n        # Compute attention scores, scale, mask, softmax\n        # Note the scaling by C**-0.5 (sqrt(n_embd)) as discussed before\n        wei = q @ k.transpose(-2,-1) * C**-0.5   # (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))   # Use dynamic slicing [:T, :T] for flexibility if T < block_size\n        wei = F.softmax(wei, dim=-1)   # (B, T, T)\n        wei = self.dropout(wei) # Apply dropout to attention weights\n        # Weighted aggregation of values\n        v = self.value(x) # (B,T,head_size)\n        out = wei @ v # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n        return out\n# ========================================\n\n# ===== MULTI-HEAD ATTENTION =====\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        # Linear layer after concatenating heads\n        self.proj = nn.Linear(n_embd, n_embd) # Projects back to n_embd dimension\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # Compute attention for each head and concatenate results\n        out = torch.cat([h(x) for h in self.heads], dim=-1) # Shape (B, T, num_heads * head_size) = (B, T, n_embd)\n        # Apply final projection and dropout\n        out = self.dropout(self.proj(out))\n        return out\n# ===============================\n\n# ===== FEED-FORWARD NETWORK =====\nclass FeedFoward(nn.Module):\n    \"\"\"Simple position-wise feed-forward network with one hidden layer.\"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),   # Expand dimension (common practice)\n            nn.ReLU(),                      # Non-linearity\n            nn.Linear(4 * n_embd, n_embd),   # Project back to original dimension\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n# ==============================\n\n# ===== TRANSFORMER BLOCK =====\nclass Block(nn.Module):\n    \"\"\"Transformer block: communication (attention) followed by computation (FFN).\"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head   # Calculate size for each head\n        self.sa = MultiHeadAttention(n_head, head_size) # Self-Attention layer\n        self.ffwd = FeedFoward(n_embd) # Feed-Forward layer\n        self.ln1 = nn.LayerNorm(n_embd) # LayerNorm for Attention input\n        self.ln2 = nn.LayerNorm(n_embd) # LayerNorm for FFN input\n\n    def forward(self, x):\n        # Pre-Normalization variant: Norm -> Sublayer -> Residual\n        x = x + self.sa(self.ln1(x))  # Attention block\n        x = x + self.ffwd(self.ln2(x)) # Feed-forward block\n        return x\n# ============================\n\n# ===== LANGUAGE MODEL =====\nclass BigramLanguageModel(nn.Module):\n    \"\"\"GPT-like language model using Transformer blocks.\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        # Token Embedding Table: Maps character index to embedding vector. (vocab_size, n_embd)\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        # Position Embedding Table: Maps position index (0 to block_size-1) to embedding vector. (block_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        # Sequence of Transformer Blocks\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        # Final Layer Normalization (applied after blocks)\n        self.ln_f = nn.LayerNorm(n_embd)   # Final layer norm\n        # Linear Head: Maps final embedding back to vocabulary size to get logits. (n_embd, vocab_size)\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        \n        # Get token embeddings from indices: (B, T) -> (B, T, n_embd)\n        tok_emb = self.token_embedding_table(idx)\n        # Get position embeddings: Create indices 0..T-1, look up embeddings -> (T, n_embd)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n        # Combine token and position embeddings by addition: (B, T, n_embd). Broadcasting handles the addition.\n        x = tok_emb + pos_emb   # (B,T,C)\n        # Pass through Transformer blocks: (B, T, n_embd) -> (B, T, n_embd)\n        x = self.blocks(x)\n        # Apply final LayerNorm\n        x = self.ln_f(x)\n        # Map to vocabulary logits: (B, T, n_embd) -> (B, T, vocab_size)\n        logits = self.lm_head(x)\n\n        # Calculate loss if targets are provided (same as before)\n        if targets is None:\n            loss = None\n        else:\n            # Reshape for cross_entropy: (B*T, vocab_size) and (B*T)\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        \"\"\"Generate new text given a starting sequence.\"\"\"\n        for _ in range(max_new_tokens):\n            # Crop context `idx` to the last `block_size` tokens. Important as position embeddings only go up to block_size.\n            idx_cond = idx[:, -block_size:]\n            # Get predictions (logits) from the model\n            logits, loss = self(idx_cond)\n            # Focus on the logits for the *last* time step: (B, C)\n            logits = logits[:, -1, :]\n            # Convert logits to probabilities via softmax\n            probs = F.softmax(logits, dim=-1)   # (B, C)\n            # Sample next token index from the probability distribution\n            idx_next = torch.multinomial(probs, num_samples=1)   # (B, 1)\n            # Append the sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1)   # (B, T+1)\n        return idx\n# =========================\n\n# ===== MODEL INITIALIZATION AND TRAINING =====\n# Create model instance and move it to the selected device\nmodel = BigramLanguageModel()\nm = model.to(device)\n# Print number of parameters (useful for understanding model size)\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters') # Calculate and print M parameters\n\n# Create optimizer (AdamW again)\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor iter in range(max_iters):\n    # Evaluate loss periodically\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss() # Get train/val loss using the helper function\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\") # Print losses\n\n    # Sample a batch of data\n    xb, yb = get_batch('train')\n\n    # Forward pass: Evaluate loss\n    logits, loss = model(xb, yb)\n    # Backward pass: Calculate gradients\n    optimizer.zero_grad(set_to_none=True) # Zero gradients\n    loss.backward() # Backpropagation\n    # Update parameters\n    optimizer.step() # Optimizer step\n\n# Generate text from the trained model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device) # Starting context: [[0]]\nprint(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n# ============================================\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing device: mps\n0.209729 M parameters\nstep 0: train loss 4.4116, val loss 4.4022\nstep 100: train loss 2.6568, val loss 2.6670\nstep 200: train loss 2.5091, val loss 2.5059\nstep 300: train loss 2.4194, val loss 2.4336\nstep 400: train loss 2.3499, val loss 2.3563\nstep 500: train loss 2.2963, val loss 2.3126\nstep 600: train loss 2.2411, val loss 2.2501\nstep 700: train loss 2.2053, val loss 2.2188\nstep 800: train loss 2.1645, val loss 2.1882\nstep 900: train loss 2.1238, val loss 2.1498\nstep 1000: train loss 2.1027, val loss 2.1297\nstep 1100: train loss 2.0699, val loss 2.1186\nstep 1200: train loss 2.0394, val loss 2.0806\nstep 1300: train loss 2.0255, val loss 2.0644\nstep 1400: train loss 1.9924, val loss 2.0376\nstep 1500: train loss 1.9697, val loss 2.0303\nstep 1600: train loss 1.9644, val loss 2.0482\nstep 1700: train loss 1.9413, val loss 2.0122\nstep 1800: train loss 1.9087, val loss 1.9949\nstep 1900: train loss 1.9106, val loss 1.9898\nstep 2000: train loss 1.8858, val loss 1.9993\nstep 2100: train loss 1.8722, val loss 1.9762\nstep 2200: train loss 1.8602, val loss 1.9636\nstep 2300: train loss 1.8577, val loss 1.9551\nstep 2400: train loss 1.8442, val loss 1.9467\nstep 2500: train loss 1.8153, val loss 1.9439\nstep 2600: train loss 1.8224, val loss 1.9363\nstep 2700: train loss 1.8125, val loss 1.9370\nstep 2800: train loss 1.8054, val loss 1.9250\nstep 2900: train loss 1.8045, val loss 1.9336\nstep 3000: train loss 1.7950, val loss 1.9202\nstep 3100: train loss 1.7707, val loss 1.9197\nstep 3200: train loss 1.7545, val loss 1.9107\nstep 3300: train loss 1.7569, val loss 1.9075\nstep 3400: train loss 1.7533, val loss 1.8942\nstep 3500: train loss 1.7374, val loss 1.8960\nstep 3600: train loss 1.7268, val loss 1.8909\nstep 3700: train loss 1.7277, val loss 1.8814\nstep 3800: train loss 1.7188, val loss 1.8889\nstep 3900: train loss 1.7194, val loss 1.8714\nstep 4000: train loss 1.7127, val loss 1.8636\nstep 4100: train loss 1.7073, val loss 1.8710\nstep 4200: train loss 1.7022, val loss 1.8597\nstep 4300: train loss 1.6994, val loss 1.8488\nstep 4400: train loss 1.7048, val loss 1.8664\nstep 4500: train loss 1.6860, val loss 1.8461\nstep 4600: train loss 1.6854, val loss 1.8304\nstep 4700: train loss 1.6841, val loss 1.8469\nstep 4800: train loss 1.6655, val loss 1.8454\nstep 4900: train loss 1.6713, val loss 1.8387\nstep 4999: train loss 1.6656, val loss 1.8277\n\nFoast.\n\nMENENIUS:\nPraviely your niews? I cank, CORiced aggele;\nOr heave worth sunt bone Ammiod, Lord,\nWho is make thy batted oub! servilings\nToke as lihtch you basw to see swife,\nIs letsts lown'd us; to lace and though mistrair took\nAnd the proply enstriaghte for a shien.\nWhy, they foul tlead,\nup is later and\nbehoy cried men as thou beatt his you.\n\nHERRY VI:\nThere, you weaks mirre and all was imper, Then death, doth those I will read;\nWeas sul't is King me, I what lady so not this dire.\n\nROMEO:\nO, upon to death! him not this bornorow-prove.\n\nMUCIOND:\nWhy leave ye no you?\n\nDUCUCHESTEH:\nBut one thyies, if will the save your blages wore I mong father you hast;\nAlaitle not arm thither crown tow doth.\n\nFROM WTARDit't me reven.\n\nWARWICK:\nOr, as extress womb voishmas!\nGood me you; and incaes up done! make,\nOr I serigh to emmequerel, to speak, herse to supomet?\n\nLUCIO:\nThe like, But twast on was theirs\npoor of thou do\nAs hath lay but so bredaint, forweet of For which his lictless me,\nThat while fumseriands thy unclity,\nWheree I wam my broth? am the too to virsant, whould enterfuly,\nAll there, ontreman one his him;\nWhen whom to Luvinge one the rews,\nWarwixt kill himfined me the bights the with and\nThost will in him,\nMor Sonme man, make to men, Must took.\n\nServer:\nIs aid the underer you: if\nThe I holseld at most lost! Comioli his but a bedrip thy lord,\nAnd then you pringent, and what you kingle is a gestreface is ears.\nBut take me. Tis basdeh,--\nCendom to nie,\nYou lordone turn to mine hath dels in woo forth.\nPoy devisecity, Ineed and encont\nOnking, pleasiness, here's me?\nWhat the have of the doet.\n\nClaytAM:\nNow tweett, cour is plose,\nOstate, and you raint this made untu\nWith ould to Warwith that me bone;\nWill him drown the have wesest: doth,\nAre goody gent yours the pot opings, time same, that BI thirself have gative. I' cown love this mind,\nNether if thou her fortune they have fight my ftlair aggainst for him burry.\n\nBRUTUS:\nWhoth lost for for leth\nAnd, being eyes\nAnd if for\n```\n:::\n:::\n\n\n::: {.callout-note}\nWith 5000 iterations, the model is able to generate text that is similar to the training text.\n:::\n\n",
    "supporting": [
      "annotated-gpt_dev_files"
    ],
    "filters": [],
    "includes": {}
  }
}