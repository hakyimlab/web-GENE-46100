{
  "hash": "169fc66c7611871b241f8cb3f597c567",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Building a GPT - companion notebook qmd\nauthor: Andrey Karpathy\ndate: '2025-04-11'\nfreeze: true\njupyter: \n  kernelspec:\n    name: \"conda-env-gene46100-py\"\n    language: \"python\"\n    display_name: \"gene46100\"\nformat:\n  html:\n    code-fold: true\n    code-line-numbers: true\n    code-tools: true\n    code-wrap: true\ndescription: Companion notebook from Karpathys video on building a minimal GPT, annotated by cursors LLM with summary from gemini.\n---\n\n\n\n\n## Building a GPT\n\nCompanion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT. Downloaded from [here](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing)\n\n(https://github.com/karpathy/nanoGPT)\n\n## Data Preparation\n\n### Step 1: Loading & Inspection\n\n::: {#11899972 .cell execution_count=1}\n``` {.python .cell-code}\n# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n#!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n```\n:::\n\n\n::: {#b199cc78 .cell execution_count=2}\n``` {.python .cell-code}\n# read it in to inspect it\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n```\n:::\n\n\n::: {#eeb32253 .cell execution_count=3}\n``` {.python .cell-code}\n# print the length of the dataset\nprint(\"length of dataset in characters: \", len(text))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlength of dataset in characters:  1115394\n```\n:::\n:::\n\n\n::: {#476d833e .cell execution_count=4}\n``` {.python .cell-code}\n# let's look at the first 1000 characters\nprint(text[:1000])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it be done: away, away!\n\nSecond Citizen:\nOne word, good citizens.\n\nFirst Citizen:\nWe are accounted poor citizens, the patricians good.\nWhat authority surfeits on would relieve us: if they\nwould yield us but the superfluity, while it were\nwholesome, we might guess they relieved us humanely;\nbut they think we are too dear: the leanness that\nafflicts us, the object of our misery, is as an\ninventory to particularise their abundance; our\nsufferance is a gain to them Let us revenge this with\nour pikes, ere we become rakes: for the gods know I\nspeak this in hunger for bread, not in thirst for revenge.\n\n\n```\n:::\n:::\n\n\n### Step 2: Tokenization\n\n::: {#d398aa3b .cell execution_count=5}\n``` {.python .cell-code}\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n```\n:::\n:::\n\n\n::: {#6f77187d .cell execution_count=6}\n``` {.python .cell-code}\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hii there\"))\nprint(decode(encode(\"hii there\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[46, 47, 47, 1, 58, 46, 43, 56, 43]\nhii there\n```\n:::\n:::\n\n\n### Step 3: Creating Batches for Training\n\n::: {#bb6b384f .cell execution_count=7}\n``` {.python .cell-code}\n# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch # we use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n```\n:::\n:::\n\n\n::: {#5d2e938c .cell execution_count=8}\n``` {.python .cell-code}\n# split up the data into train and validation sets\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n```\n:::\n\n\n::: {#eab1c6c1 .cell execution_count=9}\n``` {.python .cell-code}\n# define the block size (context length)\nblock_size = 8\ntrain_data[:block_size+1]\n```\n\n::: {.cell-output .cell-output-display execution_count=62}\n```\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n```\n:::\n:::\n\n\n### Understanding the Context and Target\n\n::: {#a097222b .cell execution_count=10}\n``` {.python .cell-code}\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nwhen input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n```\n:::\n:::\n\n\n### Creating Batches\n\n::: {#aab67146 .cell execution_count=11}\n``` {.python .cell-code}\ntorch.manual_seed(1337)\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ninputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\nwhen input is [24] the target: 43\nwhen input is [24, 43] the target: 58\nwhen input is [24, 43, 58] the target: 5\nwhen input is [24, 43, 58, 5] the target: 57\nwhen input is [24, 43, 58, 5, 57] the target: 1\nwhen input is [24, 43, 58, 5, 57, 1] the target: 46\nwhen input is [24, 43, 58, 5, 57, 1, 46] the target: 43\nwhen input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\nwhen input is [44] the target: 53\nwhen input is [44, 53] the target: 56\nwhen input is [44, 53, 56] the target: 1\nwhen input is [44, 53, 56, 1] the target: 58\nwhen input is [44, 53, 56, 1, 58] the target: 46\nwhen input is [44, 53, 56, 1, 58, 46] the target: 39\nwhen input is [44, 53, 56, 1, 58, 46, 39] the target: 58\nwhen input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52] the target: 58\nwhen input is [52, 58] the target: 1\nwhen input is [52, 58, 1] the target: 58\nwhen input is [52, 58, 1, 58] the target: 46\nwhen input is [52, 58, 1, 58, 46] the target: 39\nwhen input is [52, 58, 1, 58, 46, 39] the target: 58\nwhen input is [52, 58, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\nwhen input is [25] the target: 17\nwhen input is [25, 17] the target: 27\nwhen input is [25, 17, 27] the target: 10\nwhen input is [25, 17, 27, 10] the target: 0\nwhen input is [25, 17, 27, 10, 0] the target: 21\nwhen input is [25, 17, 27, 10, 0, 21] the target: 1\nwhen input is [25, 17, 27, 10, 0, 21, 1] the target: 54\nwhen input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n```\n:::\n:::\n\n\n## Model Architecture\n\n### Starting Simple: Bigram Language Model\nThis model predicts the next character based on the current character only. \nIt uses the logits for the next character in a lookup table.\n\nKey points:\n- Simplest possible model\n- Uses an Embedding Table of size vocab_size x vocab_size\n- Each row contains predicted scores for next character\n- Ignores context beyond last character\n\n::: {#001b713a .cell execution_count=12}\n``` {.python .cell-code}\n# define the bigram language model\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    # Text Generation Method\n    # This method implements autoregressive text generation by:\n    # 1. Taking the current context (idx)\n    # 2. Predicting the next token probabilities\n    # 3. Sampling from these probabilities\n    # 4. Appending the sampled token to the context\n    # 5. Repeating for max_new_tokens iterations\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n```\n:::\n\n\n### Understanding Cross Entropy Loss\nLoss = -Σ(y * log(p))\nwhere:\n- y = actual probability (0 or 1)\n- p = predicted probability\n- Σ = sum over all classes\n\nThis is the loss for a single token. The total loss is the average across all B*T tokens in the batch, where:\n- B = batch size (number of sequences processed in parallel)\n- T = sequence length (number of tokens in each sequence)\n\nBefore training, we would expect the model to predict the next character from a uniform distribution.\n\n-Σ(y * log(p)) = \n= - ( 1 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) ) = - log(1/65) \n= 4.1744 per token.\n\ni.e. at the beginning we expect loss of -log(1/65) = 4.1744 per token.\n\n### Model Initialization and Training\n\n::: {#b4e8345e .cell execution_count=13}\n``` {.python .cell-code}\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([32, 65])\ntensor(4.8786, grad_fn=<NllLossBackward0>)\n```\n:::\n:::\n\n\n### Text Generation\n\n::: {#ec4ea471 .cell execution_count=14}\n``` {.python .cell-code}\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\nwnYWmnxKWWev-tDqXErVKLgJ\n```\n:::\n:::\n\n\n### Training Setup\n\n::: {#a11776f2 .cell execution_count=15}\n``` {.python .cell-code}\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n```\n:::\n\n\n### Training Loop\n\n::: {#abc8f410 .cell execution_count=16}\n``` {.python .cell-code}\nbatch_size = 32\nfor steps in range(100): # increase number of steps for good results...\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n```\n:::\n\n\n::: {#00019643 .cell execution_count=17}\n``` {.python .cell-code}\n# %pip install torch torchvision torchaudio scikit-learn\n\n# import os\n# import time\n# import math\n# import pickle\n# from contextlib import nullcontext\n\n# import numpy as np\n# import torch\n# from torch.nn.parallel import DistributedDataParallel as DDP\n# from torch.distributed import init_process_group, destroy_process_group\n\n# from model import GPTConfig, GPT\n```\n:::\n\n\n::: {#d019f5c2 .cell execution_count=18}\n``` {.python .cell-code}\n# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch # we use PyTorch: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\nprint(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([1115394]) torch.int64\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n```\n:::\n:::\n\n\n::: {#0b7c7096 .cell execution_count=19}\n``` {.python .cell-code}\n# split up the data into train and validation sets\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n```\n:::\n\n\n::: {#abca0dcb .cell execution_count=20}\n``` {.python .cell-code}\n# define the block size\nblock_size = 8\ntrain_data[:block_size+1]\n```\n\n::: {.cell-output .cell-output-display execution_count=73}\n```\ntensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n```\n:::\n:::\n\n\n### define the context and target: 8 examples in one batch\n\n::: {#5deb23cc .cell execution_count=21}\n``` {.python .cell-code}\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nwhen input is tensor([18]) the target: 47\nwhen input is tensor([18, 47]) the target: 56\nwhen input is tensor([18, 47, 56]) the target: 57\nwhen input is tensor([18, 47, 56, 57]) the target: 58\nwhen input is tensor([18, 47, 56, 57, 58]) the target: 1\nwhen input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\nwhen input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n```\n:::\n:::\n\n\n### define the batch size and get the batch\n\n::: {#ff3e8036 .cell execution_count=22}\n``` {.python .cell-code}\ntorch.manual_seed(1337)\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(batch_size): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ninputs:\ntorch.Size([4, 8])\ntensor([[24, 43, 58,  5, 57,  1, 46, 43],\n        [44, 53, 56,  1, 58, 46, 39, 58],\n        [52, 58,  1, 58, 46, 39, 58,  1],\n        [25, 17, 27, 10,  0, 21,  1, 54]])\ntargets:\ntorch.Size([4, 8])\ntensor([[43, 58,  5, 57,  1, 46, 43, 39],\n        [53, 56,  1, 58, 46, 39, 58,  1],\n        [58,  1, 58, 46, 39, 58,  1, 46],\n        [17, 27, 10,  0, 21,  1, 54, 39]])\n----\nwhen input is [24] the target: 43\nwhen input is [24, 43] the target: 58\nwhen input is [24, 43, 58] the target: 5\nwhen input is [24, 43, 58, 5] the target: 57\nwhen input is [24, 43, 58, 5, 57] the target: 1\nwhen input is [24, 43, 58, 5, 57, 1] the target: 46\nwhen input is [24, 43, 58, 5, 57, 1, 46] the target: 43\nwhen input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\nwhen input is [44] the target: 53\nwhen input is [44, 53] the target: 56\nwhen input is [44, 53, 56] the target: 1\nwhen input is [44, 53, 56, 1] the target: 58\nwhen input is [44, 53, 56, 1, 58] the target: 46\nwhen input is [44, 53, 56, 1, 58, 46] the target: 39\nwhen input is [44, 53, 56, 1, 58, 46, 39] the target: 58\nwhen input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52] the target: 58\nwhen input is [52, 58] the target: 1\nwhen input is [52, 58, 1] the target: 58\nwhen input is [52, 58, 1, 58] the target: 46\nwhen input is [52, 58, 1, 58, 46] the target: 39\nwhen input is [52, 58, 1, 58, 46, 39] the target: 58\nwhen input is [52, 58, 1, 58, 46, 39, 58] the target: 1\nwhen input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\nwhen input is [25] the target: 17\nwhen input is [25, 17] the target: 27\nwhen input is [25, 17, 27] the target: 10\nwhen input is [25, 17, 27, 10] the target: 0\nwhen input is [25, 17, 27, 10, 0] the target: 21\nwhen input is [25, 17, 27, 10, 0, 21] the target: 1\nwhen input is [25, 17, 27, 10, 0, 21, 1] the target: 54\nwhen input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n```\n:::\n:::\n\n\n### start with a simple model: the bigram language model\nThis model predicts the next character based on the current character only. \nIt uses the logits for the next character in a lookup table.\n\n::: {#2088892b .cell execution_count=23}\n``` {.python .cell-code}\n# define the bigram language model\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n\n        # idx and targets are both (B,T) tensor of integers\n        logits = self.token_embedding_table(idx) # (B,T,C)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    # Text Generation Method\n    # This method implements autoregressive text generation by:\n    # 1. Taking the current context (idx)\n    # 2. Predicting the next token probabilities\n    # 3. Sampling from these probabilities\n    # 4. Appending the sampled token to the context\n    # 5. Repeating for max_new_tokens iterations\n    def generate(self, idx, max_new_tokens):\n        for _ in range(max_new_tokens):\n            # get the predictions\n            logits, loss = self(idx)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n```\n:::\n\n\n### cross entropy loss\nLoss = -Σ(y * log(p))\nwhere:\n- y = actual probability (0 or 1)\n- p = predicted probability\n- Σ = sum over all classes\n\nThis is the loss for a single token. The total loss is the average across all B*T tokens in the batch, where:\n- B = batch size (number of sequences processed in parallel)\n- T = sequence length (number of tokens in each sequence)\n\nBefore training, we would expect the model to predict the next character from a uniform distribution.\n\n-Σ(y * log(p)) = \n= - ( 1 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) + 0 * log(1/65) ) = - log(1/65) \n= 4.1744 per token.\n\ni.e. at the beginning we expect loss of -log(1/65) = 4.1744 per token.\n\n\n### initialize the model and compute the loss\n\n::: {#2c111834 .cell execution_count=24}\n``` {.python .cell-code}\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)\nprint(loss)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntorch.Size([32, 65])\ntensor(4.8786, grad_fn=<NllLossBackward0>)\n```\n:::\n:::\n\n\n### generate text\n\n::: {#20bb748a .cell execution_count=25}\n``` {.python .cell-code}\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nSKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\nwnYWmnxKWWev-tDqXErVKLgJ\n```\n:::\n:::\n\n\n### choose AdamW as the optimizer\n\n::: {#e7a346a1 .cell execution_count=26}\n``` {.python .cell-code}\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n```\n:::\n\n\n### train the model\n\n::: {#c20cc718 .cell execution_count=27}\n``` {.python .cell-code}\nbatch_size = 32\nfor steps in range(100): # increase number of steps for good results...\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n4.65630578994751\n```\n:::\n:::\n\n\n### generate text starting with \\n as initial context\n\n::: {#7693ecf9 .cell execution_count=28}\n``` {.python .cell-code}\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\noTo.JUZ!!zqe!\nxBP qbs$Gy'AcOmrLwwt\np$x;Seh-onQbfM?OjKbn'NwUAW -Np3fkz$FVwAUEa-wzWC -wQo-R!v -Mj?,SPiTyZ;o-opr$mOiPJEYD-CfigkzD3p3?zvS;ADz;.y?o,ivCuC'zqHxcVT cHA\nrT'Fd,SBMZyOslg!NXeF$sBe,juUzLq?w-wzP-h\nERjjxlgJzPbHxf$ q,q,KCDCU fqBOQT\nSV&CW:xSVwZv'DG'NSPypDhKStKzC -$hslxIVzoivnp ,ethA:NCCGoi\ntN!ljjP3fwJMwNelgUzzPGJlgihJ!d?q.d\npSPYgCuCJrIFtb\njQXg\npA.P LP,SPJi\nDBcuBM:CixjJ$Jzkq,OLf3KLQLMGph$O 3DfiPHnXKuHMlyjxEiyZib3FaHV-oJa!zoc'XSP :CKGUhd?lgCOF$;;DTHZMlvvcmZAm;:iv'MMgO&Ywbc;BLCUd&vZINLIzkuTGZa\nD.?\n```\n:::\n:::\n\n\n## Self-Attention: The Key Innovation\n\n### Understanding Self-Attention\nSelf-attention allows tokens to communicate across the sequence in a data-dependent way. Each token can \"look\" at previous tokens and decide which ones are most relevant for predicting the next token.\n\n### The Mathematical Trick: Weighted Aggregation\n\n::: {#a3528399 .cell execution_count=29}\n``` {.python .cell-code}\ntorch.manual_seed(42)\na = torch.tril(torch.ones(3, 3))\na = a / torch.sum(a, 1, keepdim=True)\nb = torch.randint(0,10,(3,2)).float()\nc = a @ b\nprint('a=')\nprint(a)\nprint('--')\nprint('b=')\nprint(b)\nprint('--')\nprint('c=')\nprint(c)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\na=\ntensor([[1.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000],\n        [0.3333, 0.3333, 0.3333]])\n--\nb=\ntensor([[2., 7.],\n        [6., 4.],\n        [6., 5.]])\n--\nc=\ntensor([[2.0000, 7.0000],\n        [4.0000, 5.5000],\n        [4.6667, 5.3333]])\n```\n:::\n:::\n\n\n### Version 1: Using a For Loop\n\n::: {#d58cf1b9 .cell execution_count=30}\n``` {.python .cell-code}\n# consider the following toy example:\ntorch.manual_seed(1337)\nB,T,C = 4,8,2 # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=83}\n```\ntorch.Size([4, 8, 2])\n```\n:::\n:::\n\n\n::: {#6c11e0d1 .cell execution_count=31}\n``` {.python .cell-code}\n# We want x[b,t] = mean_{i<=t} x[b,i]\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t,C)\n        xbow[b,t] = torch.mean(xprev, 0)\n```\n:::\n\n\n### Version 2: Matrix Multiplication\n\n::: {#48601758 .cell execution_count=32}\n``` {.python .cell-code}\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\ntorch.allclose(xbow, xbow2)\n```\n\n::: {.cell-output .cell-output-display execution_count=85}\n```\nTrue\n```\n:::\n:::\n\n\n### Version 3: Using Softmax\n\n::: {#6122b94d .cell execution_count=33}\n``` {.python .cell-code}\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxbow3 = wei @ x\ntorch.allclose(xbow, xbow3)\n```\n\n::: {.cell-output .cell-output-display execution_count=86}\n```\nTrue\n```\n:::\n:::\n\n\n### Version 4: Self-Attention\n\n::: {#6ed189e2 .cell execution_count=34}\n``` {.python .cell-code}\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\nk = key(x)   # (B, T, 16)\nq = query(x) # (B, T, 16)\nwei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n\ntril = torch.tril(torch.ones(T, T))\n#wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n#out = wei @ x\n\nout.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=87}\n```\ntorch.Size([4, 8, 16])\n```\n:::\n:::\n\n\n### Why Scaled Attention?\n- Dividing by sqrt(head_size) prevents scores from becoming too large\n- Keeps softmax from producing overly sharp distributions\n- Aids training stability\n\n::: {#bb755d2f .cell execution_count=35}\n``` {.python .cell-code}\nk = torch.randn(B,T,head_size)\nq = torch.randn(B,T,head_size)\nwei = q @ k.transpose(-2, -1) * head_size**-0.5\n```\n:::\n\n\n::: {#91831d41 .cell execution_count=36}\n``` {.python .cell-code}\nk.var()\n```\n\n::: {.cell-output .cell-output-display execution_count=89}\n```\ntensor(1.0449)\n```\n:::\n:::\n\n\n::: {#75d33c2a .cell execution_count=37}\n``` {.python .cell-code}\nq.var()\n```\n\n::: {.cell-output .cell-output-display execution_count=90}\n```\ntensor(1.0700)\n```\n:::\n:::\n\n\n::: {#c39e9dc9 .cell execution_count=38}\n``` {.python .cell-code}\nwei.var()\n```\n\n::: {.cell-output .cell-output-display execution_count=91}\n```\ntensor(1.0918)\n```\n:::\n:::\n\n\n::: {#78135859 .cell execution_count=39}\n``` {.python .cell-code}\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)\n```\n\n::: {.cell-output .cell-output-display execution_count=92}\n```\ntensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])\n```\n:::\n:::\n\n\n::: {#32194e6a .cell execution_count=40}\n``` {.python .cell-code}\ntorch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot\n```\n\n::: {.cell-output .cell-output-display execution_count=93}\n```\ntensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])\n```\n:::\n:::\n\n\n## Layer Normalization\nLayer normalization normalizes features across the embedding dimension for each token independently, which helps stabilize training.\n\n::: {#5a773abd .cell execution_count=41}\n``` {.python .cell-code}\nclass LayerNorm1d: # (used to be BatchNorm1d)\n\n  def __init__(self, dim, eps=1e-5, momentum=0.1):\n    self.eps = eps\n    self.gamma = torch.ones(dim)\n    self.beta = torch.zeros(dim)\n\n  def __call__(self, x):\n    # calculate the forward pass\n    xmean = x.mean(1, keepdim=True) # batch mean\n    xvar = x.var(1, keepdim=True) # batch variance\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n    return self.out\n\n  def parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = LayerNorm1d(100)\nx = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\nx = module(x)\nx.shape\n```\n\n::: {.cell-output .cell-output-display execution_count=94}\n```\ntorch.Size([32, 100])\n```\n:::\n:::\n\n\n::: {#315f7f17 .cell execution_count=42}\n``` {.python .cell-code}\nx[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs\n```\n\n::: {.cell-output .cell-output-display execution_count=95}\n```\n(tensor(0.1469), tensor(0.8803))\n```\n:::\n:::\n\n\n::: {#c61fb337 .cell execution_count=43}\n``` {.python .cell-code}\nx[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features\n```\n\n::: {.cell-output .cell-output-display execution_count=96}\n```\n(tensor(2.3842e-09), tensor(1.0000))\n```\n:::\n:::\n\n\n::: {#d9d3d8ab .cell execution_count=44}\n``` {.python .cell-code}\n# French to English translation example:\n\n# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n```\n:::\n\n\n### Full finished code, for reference\n\n::: {#105a2fd7 .cell execution_count=45}\n``` {.python .cell-code}\n# Import necessary PyTorch modules\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# ===== HYPERPARAMETERS =====\n# These are the key parameters that control the model's behavior and training\nbatch_size = 16        # Number of independent sequences processed in parallel\nblock_size = 32        # Maximum context length for predictions (like a window size)\nmax_iters = 5000       # Total number of training iterations\neval_interval = 100    # How often to evaluate the model during training\nlearning_rate = 1e-3   # Step size for gradient descent\neval_iters = 200       # Number of iterations to average over when evaluating\nn_embd = 64           # Size of the embedding dimension (d_model in original paper)\nn_head = 4            # Number of attention heads\nn_layer = 4           # Number of transformer layers\ndropout = 0.0         # Probability of dropping out neurons (0 = no dropout)\n# ==========================\n\n# Device selection: MPS (Apple Silicon) > CUDA > CPU\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")  # Apple Silicon GPU\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")  # NVIDIA GPU\nelse:\n    device = torch.device(\"cpu\")   # CPU fallback\nprint(f\"Using device: {device}\")\n\n# Set random seed for reproducibility\ntorch.manual_seed(1337)\nif device.type == 'cuda':\n    torch.cuda.manual_seed(1337)\nelif device.type == 'mps':\n    torch.mps.manual_seed(1337)\n\n# Load and read the training text\n# Note: This assumes 'input.txt' exists in the current directory\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# ===== DATA PREPROCESSING =====\n# Create vocabulary from unique characters in the text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# Create mapping between characters and integers\nstoi = { ch:i for i,ch in enumerate(chars) }  # string to index\nitos = { i:ch for i,ch in enumerate(chars) }  # index to string\n# Define encoder and decoder functions\nencode = lambda s: [stoi[c] for c in s]  # convert string to list of integers\ndecode = lambda l: ''.join([itos[i] for i in l])  # convert list of integers to string\n\n# Split data into training and validation sets\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data))  # first 90% for training\ntrain_data = data[:n]\nval_data = data[n:]\n# =============================\n\n# ===== DATA LOADING FUNCTION =====\ndef get_batch(split):\n    \"\"\"Generate a batch of data for training or validation.\n    \n    Args:\n        split: 'train' or 'val' to specify which dataset to use\n    \n    Returns:\n        x: input sequence of shape (batch_size, block_size)\n        y: target sequence of shape (batch_size, block_size)\n    \"\"\"\n    data = train_data if split == 'train' else val_data\n    # Randomly select starting indices for sequences\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    # Create input and target sequences\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n# ================================\n\n# ===== LOSS ESTIMATION FUNCTION =====\n@torch.no_grad()  # Disable gradient calculation for efficiency\ndef estimate_loss():\n    \"\"\"Estimate the loss on training and validation sets.\n    \n    Returns:\n        Dictionary containing average loss for train and validation sets\n    \"\"\"\n    out = {}\n    model.eval()  # Set model to evaluation mode\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()  # Set model back to training mode\n    return out\n# ===================================\n\n# ===== ATTENTION HEAD IMPLEMENTATION =====\nclass Head(nn.Module):\n    \"\"\"Single head of self-attention.\"\"\"\n    \n    def __init__(self, head_size):\n        super().__init__()\n        # Key, Query, Value projections\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        # Create lower triangular mask for causal attention\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        # Compute key, query, and value matrices\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # Compute attention scores\n        wei = q @ k.transpose(-2,-1) * C**-0.5  # Scale by sqrt(d_k)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # Apply mask\n        wei = F.softmax(wei, dim=-1)  # Convert to probabilities\n        wei = self.dropout(wei)\n        # Weighted aggregation of values\n        v = self.value(x)\n        out = wei @ v\n        return out\n# ========================================\n\n# ===== MULTI-HEAD ATTENTION =====\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)  # Projection layer\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # Process through each head and concatenate results\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n# ===============================\n\n# ===== FEED-FORWARD NETWORK =====\nclass FeedFoward(nn.Module):\n    \"\"\"Simple feed-forward network with one hidden layer.\"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),  # Expand dimension\n            nn.ReLU(),                      # Non-linearity\n            nn.Linear(4 * n_embd, n_embd),  # Project back\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n# ==============================\n\n# ===== TRANSFORMER BLOCK =====\nclass Block(nn.Module):\n    \"\"\"Transformer block: communication (attention) followed by computation (FFN).\"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head  # floor(n_embd / n_head)\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)  # Layer normalization\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        # Apply attention with residual connection\n        x = x + self.sa(self.ln1(x))\n        # Apply feed-forward with residual connection\n        x = x + self.ffwd(self.ln2(x))\n        return x\n# ============================\n\n# ===== LANGUAGE MODEL =====\nclass BigramLanguageModel(nn.Module):\n    \"\"\"Simple bigram language model with transformer architecture.\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        # Token embeddings\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        # Position embeddings\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        # Stack of transformer blocks\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)  # Final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)  # Language model head\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        \n        # Get token embeddings\n        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n        # Get position embeddings\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n        # Combine token and position embeddings\n        x = tok_emb + pos_emb  # (B,T,C)\n        # Process through transformer blocks\n        x = self.blocks(x)  # (B,T,C)\n        x = self.ln_f(x)  # (B,T,C)\n        logits = self.lm_head(x)  # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            # Reshape for loss calculation\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        \"\"\"Generate new text given a starting sequence.\"\"\"\n        for _ in range(max_new_tokens):\n            # Crop context to block_size\n            idx_cond = idx[:, -block_size:]\n            # Get predictions\n            logits, loss = self(idx_cond)\n            # Focus on last time step\n            logits = logits[:, -1, :]  # (B, C)\n            # Convert to probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, C)\n            # Sample from distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n            # Append to sequence\n            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n        return idx\n# =========================\n\n# ===== MODEL INITIALIZATION AND TRAINING =====\n# Create model instance\nmodel = BigramLanguageModel()\nm = model.to(device)\n# Print number of parameters\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# Create optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor iter in range(max_iters):\n    # Evaluate loss periodically\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # Get batch of data\n    xb, yb = get_batch('train')\n\n    # Forward pass\n    logits, loss = model(xb, yb)\n    # Backward pass\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# Generate text from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n# ============================================\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing device: mps\n0.209729 M parameters\nstep 0: train loss 4.4116, val loss 4.4022\nstep 100: train loss 2.6568, val loss 2.6670\nstep 200: train loss 2.5091, val loss 2.5059\nstep 300: train loss 2.4195, val loss 2.4334\nstep 400: train loss 2.3499, val loss 2.3558\nstep 500: train loss 2.2965, val loss 2.3129\nstep 600: train loss 2.2419, val loss 2.2509\nstep 700: train loss 2.2046, val loss 2.2184\nstep 800: train loss 2.1625, val loss 2.1854\nstep 900: train loss 2.1240, val loss 2.1508\nstep 1000: train loss 2.1023, val loss 2.1297\nstep 1100: train loss 2.0693, val loss 2.1176\nstep 1200: train loss 2.0384, val loss 2.0796\nstep 1300: train loss 2.0245, val loss 2.0634\nstep 1400: train loss 1.9931, val loss 2.0369\nstep 1500: train loss 1.9681, val loss 2.0279\nstep 1600: train loss 1.9611, val loss 2.0457\nstep 1700: train loss 1.9423, val loss 2.0148\nstep 1800: train loss 1.9077, val loss 1.9942\nstep 1900: train loss 1.9070, val loss 1.9866\nstep 2000: train loss 1.8861, val loss 1.9969\nstep 2100: train loss 1.8694, val loss 1.9738\nstep 2200: train loss 1.8579, val loss 1.9621\nstep 2300: train loss 1.8505, val loss 1.9502\nstep 2400: train loss 1.8400, val loss 1.9444\nstep 2500: train loss 1.8126, val loss 1.9374\nstep 2600: train loss 1.8254, val loss 1.9381\nstep 2700: train loss 1.8136, val loss 1.9331\nstep 2800: train loss 1.8043, val loss 1.9234\nstep 2900: train loss 1.8030, val loss 1.9288\nstep 3000: train loss 1.7948, val loss 1.9185\nstep 3100: train loss 1.7679, val loss 1.9163\nstep 3200: train loss 1.7517, val loss 1.9090\nstep 3300: train loss 1.7556, val loss 1.9036\nstep 3400: train loss 1.7523, val loss 1.8927\nstep 3500: train loss 1.7376, val loss 1.8931\nstep 3600: train loss 1.7237, val loss 1.8854\nstep 3700: train loss 1.7286, val loss 1.8795\nstep 3800: train loss 1.7198, val loss 1.8912\nstep 3900: train loss 1.7207, val loss 1.8696\nstep 4000: train loss 1.7113, val loss 1.8567\nstep 4100: train loss 1.7122, val loss 1.8738\nstep 4200: train loss 1.7039, val loss 1.8664\nstep 4300: train loss 1.6980, val loss 1.8409\nstep 4400: train loss 1.7070, val loss 1.8637\nstep 4500: train loss 1.6888, val loss 1.8478\nstep 4600: train loss 1.6884, val loss 1.8369\nstep 4700: train loss 1.6833, val loss 1.8405\nstep 4800: train loss 1.6666, val loss 1.8441\nstep 4900: train loss 1.6667, val loss 1.8334\nstep 4999: train loss 1.6638, val loss 1.8222\n\nFoast.\n\nMENENIUS:\nPrave is your nie.\n\nMINur:\nIndeed extage, I horse, beary the deveroned,\nBut layst of you simpers way art where\nGoest with herk, as lihtched\nBing?\n\nMOMIONA:\nWill could streling'd us; to lace and though misful untoon\nhere swear 'tis prognierst for a seasn.\nWhy, they foul tlead,\nupeing notruny our than\nTown, what thou bey that prongmanoed, whose then your\nHe quingthing a fatwast, and, Then death, depity mine, that rever\nto smingeth so my dream, what thou so not this dire.\n\nROMEO:\nO, upe:\nI his it! him not this bornors\nWith charse tonder's in heurse no you?\n\nDUCUCHESSAH:\nBy not to the maide?\n\nRUKESBERLAND:\nO stay,\nThat us monger himsay off secame itles guary wisith his\nThat with there's off, Bothing eStents.\n\nMENENIUS:\nO, as ext town, Come; is biliengly.\n\nCOMuLINEY:\nThe dumposes! make, O, I serving dies.\n\nBUCKINGHARD IVIZING RICHARD III:\nBett. my, let, is by this as I how?\nIf these fortune!\nThat I sence Endy. Woust is daughter.\n\nKING RICHARD II:\nNo, thou as queen I know host?\nOr that our huscle?--worgises, they will\nfor mole some.\nAnd this that our fear ustxy, will not,\nNontreman one honth your burncele's wilthord--\nLarder whis amontaguel: howfunny make fin or cenper; of an was ston\nGood foll'd may,-amses what cheep'd\nnaccuse to knot and maker aid the underer you: if\nThe I hold thee faints may!\nCome, it. It 'twas do'time:\nYet not this they tope not\nwith trither flast for is a gring,\nAnd is earns. You ame marry that dexknossed,\nThe deleang-er.\n\nJOHN FDecet, mit!\nThe sewere now our crunnity deviless nummned it.\n\nHASTIS:\nLet make\nHerreford: I bust ma! part you blood,\nthou met dince to the hather thlee.\n\nFarr'st frost these, if\nyou would kingt not bu\ndid of loves Warwith that me bone I do requed of himsends.\n\nHENRY:\nMather's conspy gently vright: then toogs, time say matcande world good\ncomasted to first be the seconds Lorde,\nWhat, his arm he our ammove have fight untrtlaies go togen\nour whus.\n\nISABELLA:\nYou our hosse beaut all honour.\n\nWARWICK:\nAnd if for\n```\n:::\n:::\n\n\n## Full GPT Model Architecture\n\n### Key Components\n1. **Token Embeddings**: Convert input tokens to vectors\n2. **Position Embeddings**: Add positional information\n3. **Transformer Blocks**: Process information through attention and feed-forward layers\n4. **Layer Normalization**: Stabilize training\n5. **Language Model Head**: Predict next token probabilities\n\n### Hyperparameters\n\n::: {#a636f661 .cell execution_count=46}\n``` {.python .cell-code}\n# ===== HYPERPARAMETERS =====\nbatch_size = 16        # Number of independent sequences processed in parallel\nblock_size = 32        # Maximum context length for predictions\nmax_iters = 5000       # Total number of training iterations\neval_interval = 100    # How often to evaluate the model\nlearning_rate = 1e-3   # Step size for gradient descent\neval_iters = 200       # Number of iterations to average over when evaluating\nn_embd = 64           # Size of the embedding dimension\nn_head = 4            # Number of attention heads\nn_layer = 4           # Number of transformer layers\ndropout = 0.0         # Probability of dropping out neurons\n# ==========================\n```\n:::\n\n\n### Device Selection\n\n::: {#c816aba3 .cell execution_count=47}\n``` {.python .cell-code}\n# Device selection: MPS (Apple Silicon) > CUDA > CPU\nif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")  # Apple Silicon GPU\nelif torch.cuda.is_available():\n    device = torch.device(\"cuda\")  # NVIDIA GPU\nelse:\n    device = torch.device(\"cpu\")   # CPU fallback\nprint(f\"Using device: {device}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nUsing device: mps\n```\n:::\n:::\n\n\n### Attention Head Implementation\n\n::: {#789e1275 .cell execution_count=48}\n``` {.python .cell-code}\nclass Head(nn.Module):\n    \"\"\"Single head of self-attention.\"\"\"\n    \n    def __init__(self, head_size):\n        super().__init__()\n        # Key, Query, Value projections\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        # Create lower triangular mask for causal attention\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        B,T,C = x.shape\n        # Compute key, query, and value matrices\n        k = self.key(x)   # (B,T,C)\n        q = self.query(x) # (B,T,C)\n        # Compute attention scores\n        wei = q @ k.transpose(-2,-1) * C**-0.5  # Scale by sqrt(d_k)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # Apply mask\n        wei = F.softmax(wei, dim=-1)  # Convert to probabilities\n        wei = self.dropout(wei)\n        # Weighted aggregation of values\n        v = self.value(x)\n        out = wei @ v\n        return out\n```\n:::\n\n\n### Multi-Head Attention\n\n::: {#7da7a42d .cell execution_count=49}\n``` {.python .cell-code}\nclass MultiHeadAttention(nn.Module):\n    \"\"\"Multiple heads of self-attention in parallel.\"\"\"\n    \n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(n_embd, n_embd)  # Projection layer\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # Process through each head and concatenate results\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n```\n:::\n\n\n### Feed-Forward Network\n\n::: {#6b59c7e9 .cell execution_count=50}\n``` {.python .cell-code}\nclass FeedFoward(nn.Module):\n    \"\"\"Simple feed-forward network with one hidden layer.\"\"\"\n    \n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),  # Expand dimension\n            nn.ReLU(),                      # Non-linearity\n            nn.Linear(4 * n_embd, n_embd),  # Project back\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n```\n:::\n\n\n### Transformer Block\n\n::: {#10c7f98f .cell execution_count=51}\n``` {.python .cell-code}\nclass Block(nn.Module):\n    \"\"\"Transformer block: communication (attention) followed by computation (FFN).\"\"\"\n    \n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head  # floor(n_embd / n_head)\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)  # Layer normalization\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        # Apply attention with residual connection\n        x = x + self.sa(self.ln1(x))\n        # Apply feed-forward with residual connection\n        x = x + self.ffwd(self.ln2(x))\n        return x\n```\n:::\n\n\n### Full Language Model\n\n::: {#851f3feb .cell execution_count=52}\n``` {.python .cell-code}\nclass BigramLanguageModel(nn.Module):\n    \"\"\"Simple bigram language model with transformer architecture.\"\"\"\n    \n    def __init__(self):\n        super().__init__()\n        # Token embeddings\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        # Position embeddings\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        # Stack of transformer blocks\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd)  # Final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)  # Language model head\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n        \n        # Get token embeddings\n        tok_emb = self.token_embedding_table(idx)  # (B,T,C)\n        # Get position embeddings\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T,C)\n        # Combine token and position embeddings\n        x = tok_emb + pos_emb  # (B,T,C)\n        # Process through transformer blocks\n        x = self.blocks(x)  # (B,T,C)\n        x = self.ln_f(x)  # (B,T,C)\n        logits = self.lm_head(x)  # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            # Reshape for loss calculation\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        \"\"\"Generate new text given a starting sequence.\"\"\"\n        for _ in range(max_new_tokens):\n            # Crop context to block_size\n            idx_cond = idx[:, -block_size:]\n            # Get predictions\n            logits, loss = self(idx_cond)\n            # Focus on last time step\n            logits = logits[:, -1, :]  # (B, C)\n            # Convert to probabilities\n            probs = F.softmax(logits, dim=-1)  # (B, C)\n            # Sample from distribution\n            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n            # Append to sequence\n            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n        return idx\n```\n:::\n\n\n### Training Loop\n\n::: {#835f30d0 .cell execution_count=53}\n``` {.python .cell-code}\n# Create model instance\nmodel = BigramLanguageModel()\nm = model.to(device)\n# Print number of parameters\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# Create optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\n# Training loop\nfor iter in range(max_iters):\n    # Evaluate loss periodically\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # Get batch of data\n    xb, yb = get_batch('train')\n\n    # Forward pass\n    logits, loss = model(xb, yb)\n    # Backward pass\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# Generate text from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.209729 M parameters\nstep 0: train loss 4.3394, val loss 4.3431\nstep 100: train loss 2.6633, val loss 2.6792\nstep 200: train loss 2.5216, val loss 2.5417\nstep 300: train loss 2.4545, val loss 2.4652\nstep 400: train loss 2.3814, val loss 2.3960\nstep 500: train loss 2.3088, val loss 2.3466\nstep 600: train loss 2.2621, val loss 2.2755\nstep 700: train loss 2.2157, val loss 2.2505\nstep 800: train loss 2.1688, val loss 2.1934\nstep 900: train loss 2.1201, val loss 2.1472\nstep 1000: train loss 2.0916, val loss 2.1337\nstep 1100: train loss 2.0595, val loss 2.1217\nstep 1200: train loss 2.0396, val loss 2.0915\nstep 1300: train loss 2.0177, val loss 2.0805\nstep 1400: train loss 1.9807, val loss 2.0511\nstep 1500: train loss 1.9674, val loss 2.0359\nstep 1600: train loss 1.9317, val loss 2.0043\nstep 1700: train loss 1.9214, val loss 2.0005\nstep 1800: train loss 1.9140, val loss 1.9922\nstep 1900: train loss 1.8993, val loss 1.9905\nstep 2000: train loss 1.8729, val loss 1.9780\nstep 2100: train loss 1.8620, val loss 1.9702\nstep 2200: train loss 1.8475, val loss 1.9599\nstep 2300: train loss 1.8386, val loss 1.9541\nstep 2400: train loss 1.8293, val loss 1.9409\nstep 2500: train loss 1.8079, val loss 1.9338\nstep 2600: train loss 1.7920, val loss 1.9160\nstep 2700: train loss 1.7967, val loss 1.9073\nstep 2800: train loss 1.7881, val loss 1.9107\nstep 2900: train loss 1.7720, val loss 1.9055\nstep 3000: train loss 1.7751, val loss 1.8961\nstep 3100: train loss 1.7493, val loss 1.8954\nstep 3200: train loss 1.7417, val loss 1.8889\nstep 3300: train loss 1.7333, val loss 1.8844\nstep 3400: train loss 1.7386, val loss 1.8874\nstep 3500: train loss 1.7304, val loss 1.8735\nstep 3600: train loss 1.7257, val loss 1.8794\nstep 3700: train loss 1.7271, val loss 1.8637\nstep 3800: train loss 1.7114, val loss 1.8480\nstep 3900: train loss 1.7081, val loss 1.8436\nstep 4000: train loss 1.6998, val loss 1.8501\nstep 4100: train loss 1.6920, val loss 1.8522\nstep 4200: train loss 1.7040, val loss 1.8349\nstep 4300: train loss 1.6838, val loss 1.8337\nstep 4400: train loss 1.6839, val loss 1.8313\nstep 4500: train loss 1.6775, val loss 1.8462\nstep 4600: train loss 1.6730, val loss 1.8442\nstep 4700: train loss 1.6773, val loss 1.8395\nstep 4800: train loss 1.6751, val loss 1.8299\nstep 4900: train loss 1.6735, val loss 1.8137\nstep 4999: train loss 1.6590, val loss 1.7937\n\n\nSICINIUS:\nWas in a alracle, that is I to duke these,\nStay thou that gropence it him.\n\nAUTOLYCUMELLO:\nNo! and to Rifled dismeat erper; I made, but in me, no tregett be\nthengracian: that juld e'll\nSo yields her him\nNietder in the win morch to this fired\nFor butwents to you. \nSlook hom'd a grown I hund! are his could are the grow,\nA father shall could is chall baggen ty viloneswiant,\nI could the buttmor him all thou do me, as 'scidans ill in  should barthy tenswer thy ssight mave\nWhich onsergel as this world one pits bome thou that\nyou one take son'd the heaven.\nTrace But tartry Marwick: Joveit; me bning well like hung\nfidest for be and miciuseman. I may you whatness the moint\nTo cairing that parce-park, sign: no I cread,\nSay be sisfy that I, my crive; ay, and smoulde\nI-have your Tarce to trietty haste,\nLook me mas on, cheer his still immerrisged that to--\nAir mineass your time the all foreld\nAnd we your bid onur to our seet till ine.\n\nLUCIO:\nThe glood pardy body, in the strove; and your gappy.\n\nKING RICHARD IO:\nWhicker is sleign full be a mades,\nAnd bain it the part of me airts,\nAs in greed mort to at my toredien live poy the\nlike churse upread truth in holds to the circe to stakence.\n\nDEXETER:\nI sadvey's seech batt true be out\nthinks our guad but with or but my drod; hath she servity trespeasess.\n\nLADTI3\n\nNORSOR:\nWelaved thine, I have the childne;\nI die lar migaint Majoid: and 'twere to do know and thus follor to friends oposes. then\nA pread toods see a\nHis orried lettle men.\n\nSecond LicborUy:\nThat, in hearth one him says thou have for ascoud\nTo thee, for not your his strots do graces.\n'SAy:\nThe speccer in our vice? Whomed is the more we murdes. Whom be thou han'st rest downsse offore gumpey,\nAs is that ink thou wight, my lorck his grace:\nAnd their not ang not, apreed, well good me it regrent, to up to not---poor brid of this where you sust me,\nAs rest erpet thing too? sorus be fartmedly them that overe compake the gave a detersleming?\n\nFirst Kets Come tower.\n\nHERSH\n```\n:::\n:::\n\n\n## Key Takeaways\n\n1. **Data Preparation**\n   - Tokenization: Convert text to numerical representations\n   - Batching: Create efficient training batches\n   - Context and Targets: Understand the prediction task\n\n2. **Model Architecture**\n   - Token and Position Embeddings\n   - Self-Attention Mechanism\n   - Transformer Blocks\n   - Layer Normalization\n   - Language Model Head\n\n3. **Training Process**\n   - Loss Function: Cross-Entropy\n   - Optimization: AdamW\n   - Evaluation: Periodic validation checks\n   - Generation: Autoregressive text generation\n\n4. **Key Concepts**\n   - Self-Attention: Communication between tokens\n   - Positional Encoding: Preserving sequence order\n   - Residual Connections: Helping gradient flow\n   - Layer Normalization: Stabilizing training\n   - Multi-Head Attention: Capturing different relationships\n```\n\n",
    "supporting": [
      "annotated-gpt_dev_files"
    ],
    "filters": [],
    "includes": {}
  }
}